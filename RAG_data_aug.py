import os
import transformers
from typing import Union, Sequence
import argparse
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    pipeline,
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document

import torch 
from tqdm import tqdm 

from dataset import FactualQuestionAnsweringDataset
from utils import jload, jdump
import random

def parse_args():
    parser = argparse.ArgumentParser(description='data generator')

    # Data Configs
    parser.add_argument('--template_dataset_path',type=str, default="data/questions.json")
    parser.add_argument('--source_dataset_path',type=str, default="data/cmu/")
    #parser.add_argument('--streaming', type=bool, default=False)
    
    # Model Configs
    parser.add_argument('--model_name_or_path', type=str, default="mistralai/Mistral-7B-Instruct-v0.2")
    parser.add_argument('--trust_remote_code', type=bool, default=False)

    #Generation Configs
    #parser.add_argument('--num_samples', type=int, default=40000, help="number of samples subsampled from base dataset")
    parser.add_argument('--batch_size', type=int, default=2, help='inference batch size')
    parser.add_argument('--output_dir', type=str, default="data/augmented_data/", help="location for generated dataset")
    parser.add_argument('--num_samples', type=int, default=1000, help="number of samples to generate")

    args = parser.parse_args()

    return args

def load_documents(text_path, chunk_size=1000, chunk_overlap=150, num_samples=1000):
    '''
    text_path: str, a directory of .txt files
    return: 
    langchain document format (a list of trucated document)
    '''
    if 'cmu' in text_path:
        docs = []
        #files that need to be truncated
        truncate_files = ['buggy_history.json', 'cmu_history.json', 'handbook_text.json', 'kiltie_band_fact.json',
                          'scotty_fact.json', 'tartan_fact.json']
        files = os.listdir(text_path)
        num_per_file = num_samples // len(files)
        docs = []
        for file in files:
            doc = jload(text_path + '/' + file)
            #random draw without replacement
            doc = random.sample(doc, min(num_per_file, len(doc)))
            doc = [Document(page_content = s) for s in doc]
            if file not in truncate_files:
                docs.extend(doc)
            else:
                text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
                doc = text_splitter.split_documents(doc)
                docs.extend(doc)
        return docs
    else:
        raise NotImplementedError
    

def batch_to_prompt(batch, context, num_questions=3):
    prompt = "You are here to help constructing a factual question-answering dataset. \n"
    prompt += "The question/answer pairs generated by you will be given to a RAG model for retrieval augmented generation on facts. \n"
    prompt += "You will be given a context passage, and you will select a fact from the context, then ask a question about it, and at last generate the answer to the asked question based on the selected fact. \n"
    prompt += "It is very important to keep the generated answer to include only the facts that have appeared in the context. \n"
    prompt += f"We will show you some examples, which generates a question and answer pair based on the given context: \n"
    prompt += f"Context: {context}\n"
    for i, item in enumerate(batch):
        prompt += f"Example {i}: \nquestion: {item['question']}\nanswer: {item['answer']}\ncontext: {context}\n"
    prompt += f"\nPlease generate {num_questions} question/answer pairs using the following format: \n" +\
    "question: \n" + \
    "answer: \n" + \
    "Last but not least, please do not generate new context."
    return prompt

def mistral_add_special_tokens_to_prompt(text: Union[str, Sequence[str]]):
    if isinstance(text, str):
        return "<s>[INST]"+text+"[/INST]"
    elif isinstance(text, list):
        return [mistral_add_special_tokens_to_prompt(string) for string in text]

def parse_generated_text(text: str, context:str):
    answers= text.split('[/INST]')[1]
    result = [] 
    tmp = []
    for answer in answers.split('\n'):
        if answer.startswith('question') and len(tmp) == 0:
            s = answer.split('question: ')[1]
        elif answer.startswith('answer') and len(tmp) == 1:
            s = answer.split('answer: ')[1]
        else:
            continue
        tmp.append(s)
        if len(tmp) == 2:
            result.append({'question':tmp[0], 'answer':tmp[1], 'context':context})
            tmp = []
    return result
        
            
def main():
    args = parse_args()
    os.makedirs(args.output_dir, exist_ok=True)
    template_dataset_list = jload(args.template_dataset_path)
    source_dataset_list = load_documents(args.source_dataset_path, chunk_size=250, 
                                         chunk_overlap=50, num_samples=args.num_samples)
    source_dataset_list = [s.page_content for s in source_dataset_list]

    pipe = pipeline("text-generation", model=args.model_name_or_path, max_new_tokens=512, device="cuda")
    #---
    results, parsed_results = [], [] 
    for i, context in tqdm(enumerate(source_dataset_list), total = len(source_dataset_list)):
        batch = random.sample(template_dataset_list, args.batch_size)
        prompt = mistral_add_special_tokens_to_prompt(batch_to_prompt(batch, context))
        result = pipe(prompt)
        results += [s["generated_text"] for s in result]
        for string in results:
            parsed_results += parse_generated_text(string, context)
        # if i > 2: break
    #jdump(parsed_results, os.join(args.output_dir, "generated_dataset.json"))
    #---
    
    # results = []
    # parsed_results = []
    # for i in range(0, len(template_dataset_list), args.batch_size):
    #     batch = template_dataset_list[i:i+args.batch_size]
    #     context = random.choice(source_dataset_list)
    #     #print(context)
    #     prompt = mistral_add_special_tokens_to_prompt(batch_to_prompt(batch, context))
    #     result = pipe(prompt)
    #     results += [s["generated_text"] for s in result]
    #     parsed_results += [parse_generated_text(string["generated_text"], context) for string in result]
    #     if i > 2: break

    jdump(parsed_results, os.path.join(args.output_dir, f"generated_dataset_{args.num_samples}.json"))
    #jdump()
    
if __name__ == "__main__":
    main()