import os
import random
from typing import Union, Sequence
import argparse
from transformers import (
    pipeline,
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.docstore.document import Document
import pdb
import numpy as np
import torch 
from tqdm import tqdm 

# From local
from dataset import FactualQuestionAnsweringDataset
from utils import jload, jdump
from evaluate import compute_metrics


def parse_args():
    parser = argparse.ArgumentParser(description='data generator')

    # Data Configs
    parser.add_argument('--template_dataset_path',type=str, default="data/questions.json")
    parser.add_argument('--source_dataset_path',type=str, default="data/cmu/")
    parser.add_argument('--extract_fact_from_context', action='store_true', help='determine if the generated qa pair will come with facts')
    parser.add_argument('--eval_f1_recall_of_facts_answers', action='store_true')
    #parser.add_argument('--streaming', type=bool, default=False)
    
    # Model Configs
    parser.add_argument('--model_name_or_path', type=str, default="mistralai/Mistral-7B-Instruct-v0.2")

    #Generation Configs
    #parser.add_argument('--num_samples', type=int, default=40000, help="number of samples subsampled from base dataset")
    parser.add_argument('--batch_size', type=int, default=2, help='inference batch size')
    parser.add_argument('--output_dir', type=str, default="data/augmented_data/", help="location for generated dataset")
    parser.add_argument('--num_samples', type=int, default=1000, help="number of samples to generate")

    args = parser.parse_args()

    return args

def load_documents(text_path, chunk_size=1000, chunk_overlap=150, num_samples=1000):
    '''
    text_path: str, a directory of .txt files
    return: 
    langchain document format (a list of trucated document)
    '''
    if 'cmu' in text_path:
        docs = []
        #files that need to be truncated
        truncate_files = ['buggy_history.json', 'cmu_history.json', 'handbook_text.json', 'kiltie_band_fact.json',
                          'scotty_fact.json', 'tartan_fact.json']
        files = os.listdir(text_path)
        num_per_file = num_samples // len(files)
        docs = []
        for file in files:
            doc = jload(text_path + '/' + file)
            #random draw without replacement
            doc = random.sample(doc, min(num_per_file, len(doc)))
            doc = [Document(page_content = s) for s in doc]
            if file not in truncate_files:
                docs.extend(doc)
            else:
                text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
                doc = text_splitter.split_documents(doc)
                docs.extend(doc)
        return docs
    else:
        raise NotImplementedError
    

def batch_to_prompt(batch, context, num_questions=3):
    prompt = "You are here to help constructing a factual question-answering dataset. \n"
    prompt += "The question/answer pairs generated by you will be given to a RAG model for retrieval augmented generation on facts. \n"
    prompt += "You will be given a context passage, and you will select a fact from the context, then ask a question about it, and at last generate the answer to the asked question based on the selected fact. \n"
    prompt += "It is very important to keep the generated answer to include only the facts that have appeared in the context. \n"
    prompt += f"We will show you some examples, which generates a question and answer pair based on the given context: \n"
    prompt += f"Context: {context}\n"
    for i, item in enumerate(batch):
        prompt += f"Example {i}: \nquestion: {item['question']}\nanswer: {item['answer']}\ncontext: {item['context']}\n"
    prompt += f"\nPlease generate {num_questions} question/answer pairs using the following format: \n" +\
    "question: \n" + \
    "answer: \n" + \
    "Last but not least, please do not generate new context."
    return prompt

def batch_to_prompt_with_fact(batch, context, num_questions=3):
    prompt = "You are here to help constructing a factual question-answering dataset. \n"
    prompt += "The question/answer pairs generated by you will be given to a RAG model for retrieval augmented generation on facts. \n"
    prompt += "You will be given a context passage, and you will select a fact from the context, then ask a question about it, and at last generate the answer to the asked question based on the selected fact. \n"
    prompt += "It is very important to keep the generated answer to include only the facts that have appeared in the context. \n"
    prompt += f"We will show you some examples, which generates a question and answer pair based on some fact extracted from the given context: \n"
    for i, item in enumerate(batch):
        prompt += f"context: {item['context']} \nquestion: {item['question']}\nanswer: {item['answer']}\n\n"
    prompt += f"Now we show you the actual context for your generation: {context}\n"
    prompt += f"\nPlease extracted {num_questions} facts from the given context and generate {num_questions} question/answer pairs based on the extracted facts using the following format: \n"
    prompt += "fact: \n"
    prompt += "question: \n"
    prompt += "answer: \n"
    prompt += "Last but not least, please do not generate any new context."
    return prompt

def mistral_add_special_tokens_to_prompt(text: Union[str, Sequence[str]]):
    if isinstance(text, str):
        return "<s>[INST]"+text+"[/INST]"
    elif isinstance(text, list):
        return [mistral_add_special_tokens_to_prompt(string) for string in text]

def parse_generated_text(text: str, context:str):
    '''parse a generated text string into questions and answers, also integrating the context'''
    answers= text.split('[/INST]')[1]
    result = [] 
    answers = answers.replace('question', '<[Example]>question')
    for answer in answers.split('<[Example]>'):
        answer_split = answer.split('answer: ')
        if len(answer_split) != 2:
            continue
        question_split = answer_split[0].split('question: ')
        if len(question_split) != 2:
            continue
        question = question_split[1]
        answer = answer_split[1]
        if len(question) <= 5 or len(answer) <=2:
            continue
        
        result.append({'question':question, 'answer':answer, 'context':context})
    return result

def parse_generated_text_with_fact(text: str, context:str):
    '''parse a generated text string into questions and answers, also integrating the context'''
    answers= text.split('[/INST]')[1]
    result = [] 
    answers = answers.replace('fact', '<[Example]>fact')
    for answer in answers.split('<[Example]>'):
        answer_split = answer.split('answer: ')
        if len(answer_split) != 2:
            continue
        question_split = answer_split[0].split('question: ')
        if len(question_split) != 2:
            continue
        fact_split = question_split[0].split('fact: ')
        if len(fact_split) != 2:
            continue
        fact = fact_split[1]
        question = question_split[1]
        answer = answer_split[1]
        if len(fact) <=3 or len(question) <= 5 or len(answer) <=3:
            continue
        
        result.append({'fact': fact, 'question':question, 'answer':answer, 'context':context})
    return result
        
            
def main():
    args = parse_args()
    os.makedirs(args.output_dir, exist_ok=True)
    template_dataset_list = jload(args.template_dataset_path)
    source_dataset_list = load_documents(args.source_dataset_path, chunk_size=250, 
                                         chunk_overlap=50, num_samples=args.num_samples)
    source_dataset_list = [s.page_content for s in source_dataset_list]
    #print(source_dataset_list[:10])
    #pdb.set_trace()
    pipe = pipeline("text-generation", model=args.model_name_or_path, max_new_tokens=512, torch_dtype=torch.bfloat16, device_map="auto")
    #---
    parsed_results = [] 
    running_f1 = {"avg" : 0.0, "sum": 0.0, "num_samples": 0}
    running_recall = {"avg" : 0.0, "sum": 0.0, "num_samples": 0}
    for context in tqdm(source_dataset_list, total = len(source_dataset_list)):
        batch = random.sample(template_dataset_list, args.batch_size)
        if args.extract_fact_from_context:
            prompt = mistral_add_special_tokens_to_prompt(batch_to_prompt_with_fact(batch, context))
        else:
            prompt = mistral_add_special_tokens_to_prompt(batch_to_prompt(batch, context))
        result = pipe(prompt)
        results = [s["generated_text"] for s in result]
        print(results[0])
        if args.extract_fact_from_context and args.eval_f1_recall_of_facts_answers:
            print(f'Running average of f1: {running_f1["avg"]}, \nRunning average of recall: {running_recall["avg"]}.')
        for string in results:
            if args.extract_fact_from_context:
                parsed_result = parse_generated_text_with_fact(string, context)
            else:
                parsed_result = parse_generated_text(string, context)
            parsed_results += parsed_result
            if args.extract_fact_from_context and args.eval_f1_recall_of_facts_answers:
                f1_batch, recall_batch = [], []
                for sample in parsed_result:
                    f1, rec = compute_metrics(sample['answer'], sample['fact'])
                    f1_batch.append(f1)
                    recall_batch.append(rec)
                #recall = np.array([compute_metrics(sample['answer'], sample['fact'])[1] for sample in parsed_result])
                running_recall['sum'] += sum(recall_batch)
                running_recall['num_samples'] += len(recall_batch)
                running_recall['avg'] = running_recall['sum'] / running_recall['num_samples']
                running_f1['sum'] += sum(f1_batch)
                running_f1['num_samples'] += len(f1_batch)
                running_f1['avg'] = running_f1['sum'] / running_f1['num_samples']

            
        
        
    with_facts = 'with_facts' if args.extract_fact_from_context else ''
    jdump(parsed_results, os.path.join(args.output_dir, f"generated_dataset_{with_facts}_{args.num_samples}.json"))
    
if __name__ == "__main__":
    main()