[{"question": "Is Jessica Maguire ssica Maguire Assistant to the Institute Director of LTI?, Please answer yes or no: ", "answer": "Yes", "context": "Jessica Maguire Assistant to the Institute Director Email: jlm4@andrew.cmu.edu Office: 5721 Gates & Hillman Centers Phone: 412-268-5480"}, {"question": "How can I contact LTI staff Vicente Malave?", "answer": "Email: vlm@andrew.cmu.edu", "context": "Vicente Malave Data Science Course Developer Email: vlm@andrew.cmu.edu"}, {"question": "Please provide the contact information of LTI staff Susan E.", "answer": "Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591", "context": "Susan E. Holm Mitamura Lab/Sr. Knowledge Engineer Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591"}, {"question": "How can I contact LTI staff Daniel Vosler?", "answer": "Email: dvosler@andrew.cmu.edu", "context": "Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu"}, {"question": "Please tell me the Email of LTI staff Daniel Vosler based on the background information: ", "answer": "Daniel Vosler's Email is dvosler@andrew.cmu.edu", "context": "Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu"}, {"question": "What is the Email of LTI staff Casey Walker?", "answer": "Casey Walker's Email is clwalker@andrew.cmu.edu", "context": "Casey Walker Academic Program Coordinator Email: clwalker@andrew.cmu.edu Phone: +1 412 268 9315"}, {"question": "What is the Phone of LTI staff Emma Thomas?", "answer": "412-268-7812", "context": "Emma Thomas Sponsored Research Administrator Email: emmat@andrew.cmu.edu Office: 5715 Gates & Hillman Centers Phone: 412-268-7812"}, {"question": "Is Kate Schaich kari Yamakawa Mitamura Lab/Associate Data Analyst of LTI?, Please answer yes or no: ", "answer": "No", "context": "Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788"}, {"question": "Please tell me the Office of LTI staff Joan Axelson based on the background information: ", "answer": "Joan Axelson's Office is Manager", "context": "Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517"}, {"question": "How can I contact LTI staff Yukari Yamakawa?", "answer": "Email: yukariy@andrew.cmu.edu Office: 6717 Gates & Hillman Centers Phone: 412-268-6596", "context": "Yukari Yamakawa Mitamura Lab/Associate Data Analyst Email: yukariy@andrew.cmu.edu Office: 6717 Gates & Hillman Centers Phone: 412-268-6596"}, {"question": "Is Nicole Perrotta cole Perrotta Administrative Coordinator of LTI?, Please answer yes or no: ", "answer": "Yes", "context": "Nicole Perrotta Administrative Coordinator Email: nperrott@andrew.cmu.edu"}, {"question": "How can I contact LTI staff Emma Thomas?", "answer": "Email: emmat@andrew.cmu.edu Office: 5715 Gates & Hillman Centers Phone: 412-268-7812", "context": "Emma Thomas Sponsored Research Administrator Email: emmat@andrew.cmu.edu Office: 5715 Gates & Hillman Centers Phone: 412-268-7812"}, {"question": "Please tell me the Email of LTI staff Annette Han based on the background information: ", "answer": "Annette Han's Email is ahan2@andrew.cmu.edu", "context": "Annette Han Rose Lab/Data Science Course Developer Email: ahan2@andrew.cmu.edu"}, {"question": "What is the Email of LTI staff Nicole Perrotta?", "answer": "nperrott@andrew.cmu.edu", "context": "Nicole Perrotta Administrative Coordinator Email: nperrott@andrew.cmu.edu"}, {"question": "Please tell me the Email of LTI staff Daniel Vosler based on the background information: ", "answer": "dvosler@andrew.cmu.edu", "context": "Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu"}, {"question": "How can I contact LTI staff Allison Blair?", "answer": "Email: ablair2@cs.cmu.edu Office: 5715 TCS Hall Phone: 412-268-5489", "context": "Allison Blair Associate Business Manager Email: ablair2@cs.cmu.edu Office: 5715 TCS Hall Phone: 412-268-5489"}, {"question": "Is Caitlin Korpus itlin Korpus Senior Academic Program Coordinator of LTI?, Please answer yes or no: ", "answer": "Yes", "context": "Caitlin Korpus Senior Academic Program Coordinator Email: ckorpus@andrew.cmu.edu Office: 6719 Gates & Hillman Centers Phone: +1 412 268 7096"}, {"question": "Please provide the contact information of LTI staff Susan E.", "answer": "Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591", "context": "Susan E. Holm Mitamura Lab/Sr. Knowledge Engineer Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591"}, {"question": "How can I contact LTI staff Daniel Vosler?", "answer": "Email: dvosler@andrew.cmu.edu", "context": "Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu"}, {"question": "What is the Phone of LTI staff Brianna Eriksen?", "answer": "Brianna Eriksen's Phone is 412-268-4277", "context": "Brianna Eriksen Academic Program Manager Email: bfreema2@andrew.cmu.edu Phone: 412-268-4277"}, {"question": "Please tell me the Phone of LTI staff John Friday based on the background information: ", "answer": "412-268-1810", "context": "John Friday Sr. Administrative Coordinator Email: jfriday@andrew.cmu.edu Office: 5715 Gates & Hillman Centers Phone: 412-268-1810"}, {"question": "What is the Phone of LTI staff Susan E.?", "answer": "412-268-6591", "context": "Susan E. Holm Mitamura Lab/Sr. Knowledge Engineer Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591"}, {"question": "Is Stacey Young umi Maiti Watanabe Lab/Postdoctoral Research Associate of LTI?, Please answer yes or no: ", "answer": "No", "context": "Stacey Young Academic Program Manager \u2014 Ph.D. Email: staceyy@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-2623"}, {"question": "Please provide the contact information of LTI staff Amber Vivis", "answer": "Email: avivis@cs.cmu.edu Office: 357 TCS Hall Phone: 412-268-9998", "context": "Amber Vivis Sr. Academic Program Manager - MSAII Email: avivis@cs.cmu.edu Office: 357 TCS Hall Phone: 412-268-9998"}, {"question": "Please provide the contact information of LTI staff Joan Axelson", "answer": "Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517", "context": "Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517"}, {"question": "Is Wenhe Liu hn Friday Sr. Administrative Coordinator of LTI?, Please answer yes or no: ", "answer": "No", "context": "Wenhe Liu Research Scholar Email: wenhel@andrew.cmu.edu Office: 5403 Gates & Hillman Centers"}, {"question": "How can I contact LTI staff Brianna Eriksen?", "answer": "Email: bfreema2@andrew.cmu.edu Phone: 412-268-4277", "context": "Brianna Eriksen Academic Program Manager Email: bfreema2@andrew.cmu.edu Phone: 412-268-4277"}, {"question": "What is the Phone of LTI staff Kate Schaich?", "answer": "Kate Schaich's Phone is 412-268-4788", "context": "Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788"}, {"question": "Is Zaid Sheikh i-Qi Cheng Hauptmann Lab/Postdoctoral Research Associate of LTI?, Please answer yes or no: ", "answer": "No", "context": "Zaid Sheikh Neubig Lab/Senior Systems/Software Engineer Email: zsheikh@andrew.cmu.edu Office: 6416 Gates & Hillman Centers Phone:"}, {"question": "What is the Email of LTI staff Joan Axelson?", "answer": "jaxelson@andrew.cmu.edu", "context": "Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517"}, {"question": "Please provide the contact information of LTI staff Annette Han", "answer": "Email: ahan2@andrew.cmu.edu", "context": "Annette Han Rose Lab/Data Science Course Developer Email: ahan2@andrew.cmu.edu"}, {"question": "Please provide the contact information of LTI staff Kira Sullivan", "answer": "Email: kiras@andrew.cmu.edu Phone: 412-268-8737", "context": "Kira Sullivan Administrative Coordinator Email: kiras@andrew.cmu.edu Phone: 412-268-8737"}, {"question": "Please tell me the Phone of LTI staff Yukari Yamakawa based on the background information: ", "answer": "Yukari Yamakawa's Phone is 412-268-6596", "context": "Yukari Yamakawa Mitamura Lab/Associate Data Analyst Email: yukariy@andrew.cmu.edu Office: 6717 Gates & Hillman Centers Phone: 412-268-6596"}, {"question": "Please tell me the Email of LTI staff Robin Hammer based on the background information: ", "answer": "robinham@andrew.cmu.edu", "context": "Robin Hammer Levin Lab/Project Coordinator Email: robinham@andrew.cmu.edu"}, {"question": "How can I contact LTI staff Susan E.?", "answer": "Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591", "context": "Susan E. Holm Mitamura Lab/Sr. Knowledge Engineer Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591"}, {"question": "What is the Email of LTI staff Zhi-Qi Cheng?", "answer": "Zhi-Qi Cheng's Email is zhiqic@andrew.cmu.edu", "context": "Zhi-Qi Cheng Hauptmann Lab/Postdoctoral Research Associate Email: zhiqic@andrew.cmu.edu"}, {"question": "Is Jessica Maguire san E. Holm Mitamura Lab/Sr. Knowledge Engineer of LTI?, Please answer yes or no: ", "answer": "No", "context": "Jessica Maguire Assistant to the Institute Director Email: jlm4@andrew.cmu.edu Office: 5721 Gates & Hillman Centers Phone: 412-268-5480"}, {"question": "Is Susan E. san E. Holm Mitamura Lab/Sr. Knowledge Engineer of LTI?, Please answer yes or no: ", "answer": "Yes", "context": "Susan E. Holm Mitamura Lab/Sr. Knowledge Engineer Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591"}, {"question": "Is Vicente Malave cente Malave Data Science Course Developer of LTI?, Please answer yes or no: ", "answer": "Yes", "context": "Vicente Malave Data Science Course Developer Email: vlm@andrew.cmu.edu"}, {"question": "What is the Email of LTI staff Julie Nys?", "answer": "Julie Nys's Email is jnys@andrew.cmu.edu", "context": "Julie Nys Employment Processes Manager Email: jnys@andrew.cmu.edu Office: 5405 TCS Hall Phone: 412-268-3515"}, {"question": "Is Casey Walker nnifer Clark Administrative Coordinator of LTI?, Please answer yes or no: ", "answer": "No", "context": "Casey Walker Academic Program Coordinator Email: clwalker@andrew.cmu.edu Phone: +1 412 268 9315"}, {"question": "How can I contact LTI staff Daniel Gongaware?", "answer": "Email: dgongawa@cs.cmu.edu Office: 355 TCS Hall Phone: 412-268-6195", "context": "Daniel Gongaware Sponsored Research Administrator Email: dgongawa@cs.cmu.edu Office: 355 TCS Hall Phone: 412-268-6195"}, {"question": "Please tell me the Office of LTI staff Bryan Burtner based on the background information: ", "answer": "Bryan Burtner's Office is 6413 TCS Hall", "context": "Bryan Burtner Associate Business Manager Email: bburtner@cs.cmu.edu Office: 6413 TCS Hall Phone: 412-268-2805"}, {"question": "How can I contact LTI staff Kira Sullivan?", "answer": "Email: kiras@andrew.cmu.edu Phone: 412-268-8737", "context": "Kira Sullivan Administrative Coordinator Email: kiras@andrew.cmu.edu Phone: 412-268-8737"}, {"question": "What is the Email of LTI staff Zhong-Qiu Wang?", "answer": "zhongqiu@andrew.cmu.edu", "context": "Zhong-Qiu Wang Wantanabe Lab/Postdoctoral Research Associate Email: zhongqiu@andrew.cmu.edu Office: 6414 Gates & Hillman Centers Phone: 415-200-8662"}, {"question": "What is the Email of LTI staff Wenhe Liu?", "answer": "Wenhe Liu's Email is wenhel@andrew.cmu.edu", "context": "Wenhe Liu Research Scholar Email: wenhel@andrew.cmu.edu Office: 5403 Gates & Hillman Centers"}, {"question": "What is the Phone of LTI staff Stacey Young?", "answer": "Stacey Young's Phone is 412-268-2623", "context": "Stacey Young Academic Program Manager \u2014 Ph.D. Email: staceyy@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-2623"}, {"question": "Please tell me the Email of LTI staff Joan Axelson based on the background information: ", "answer": "jaxelson@andrew.cmu.edu", "context": "Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517"}, {"question": "Please provide the contact information of LTI staff Jennifer Lucas", "answer": "Email: jmlucas@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-9870", "context": "Jennifer Lucas Academic Program Manager \u2014 MCDS Email: jmlucas@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-9870"}, {"question": "What is the Phone of LTI staff Allison Blair?", "answer": "412-268-5489", "context": "Allison Blair Associate Business Manager Email: ablair2@cs.cmu.edu Office: 5715 TCS Hall Phone: 412-268-5489"}, {"question": "Please provide the contact information of LTI faculty Kemal Oflazer", "answer": "Email: ko@qatar.cmu.edu Office: 1009 Carnegie Mellon - Qatar Campus Phone:", "context": "Kemal Oflazer Teaching Professor of Computer Science Email: ko@qatar.cmu.edu Office: 1009 Carnegie Mellon - Qatar Campus Phone:"}, {"question": "Is Teruko Mitamura ruko Mitamura Research Professor of LTI?, Please answer yes or no: ", "answer": "Yes", "context": "Teruko Mitamura Research Professor Email: teruko@cs.cmu.edu Office: 6711 Gates & Hillman Centers Phone: 412-268-6596 Research Areas: Information Extraction, Summarization and Question Answering, Information Retrieval, Text Mining and Analytics, Language Technologies for Education, Natural Language Processing and Computational Linguistics"}, {"question": "Is Chenyan Xiong i Li Assistant Professor of LTI?, Please answer yes or no: ", "answer": "No", "context": "Chenyan Xiong Associate Professor Email: cx@andrew.cmu.edu Phone: 412-268-7641"}, {"question": "How can I contact LTI faculty Eric Nyberg?", "answer": "Email: ehn@cs.cmu.edu Office: 6715 Gates & Hillman Centers Phone: 412-268-7281", "context": "Eric Nyberg Professor Email: ehn@cs.cmu.edu Office: 6715 Gates & Hillman Centers Phone: 412-268-7281 Research Areas: Information Extraction, Summarization and Question Answering, Information Retrieval, Text Mining and Analytics, Language Technologies for Education"}, {"question": "Please provide the contact information of LTI faculty Bhiksha Ramakrishnan", "answer": "Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826", "context": "Bhiksha Ramakrishnan Professor Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826 Research Areas: Machine Learning, Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing, Privacy"}, {"question": "Is Bhiksha Ramakrishnan iksha Ramakrishnan Professor of LTI?, Please answer yes or no: ", "answer": "Yes", "context": "Bhiksha Ramakrishnan Professor Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826 Research Areas: Machine Learning, Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing, Privacy"}, {"question": "What is the Office of LTI faculty Lori Levin?", "answer": "Lori Levin's Office is 5717 Gates & Hillman Centers", "context": "Lori Levin Research Professor Email: lsl@cs.cmu.edu Office: 5717 Gates & Hillman Centers Phone: 412-268-6193 Research Areas: Machine Translation, Natural Language Processing and Computational Linguistics, Corpus Annotation and Resources"}, {"question": "How can I contact LTI faculty Emma Strubell?", "answer": "Email: estrubel@andrew.cmu.edu Office: Gates & Hillman Centers", "context": "Emma Strubell Assistant Professor Email: estrubel@andrew.cmu.edu Office: Gates & Hillman Centers"}, {"question": "What is the Research Areas of LTI faculty Daniel Fried?", "answer": "Daniel Fried's Research Areas is Natural Language Processing: Language and Code, Conversational AI, Intelligent Agents, and Dialogue, Discourse and Pragmatics, Multimodal AI", "context": "Daniel Fried Assistant Professor Email: dfried@andrew.cmu.edu Research Areas: Natural Language Processing: Language and Code, Conversational AI, Intelligent Agents, and Dialogue, Discourse and Pragmatics, Multimodal AI"}, {"question": "How can I contact LTI faculty Robert Frederking?", "answer": "Email: ref@cs.cmu.edu Office: 6515 Gates & Hillman Centers Phone: 412-268-6656", "context": "Robert Frederking Principal Systems Scientist/Associate Dean of Doctoral Programs/MLT Program Director Email: ref@cs.cmu.edu Office: 6515 Gates & Hillman Centers Phone: 412-268-6656"}, {"question": "Please tell me the Phone of LTI faculty Justine Cassell based on the background information: ", "answer": "Justine Cassell's Phone is 412-204-6268", "context": "Justine Cassell Professor (On Leave) Email: jcassell@andrew.cmu.edu Office: 5107 Gates & Hillman Centers Phone: 412-204-6268"}, {"question": "What is the Research Areas of LTI faculty Shinji Watanabe?", "answer": "Shinji Watanabe's Research Areas is Natural Language Processing: Conversational AI, Intelligent Agents, and Dialogue, Speech Processing (ASR, Speech Synthesis): Speech Recognition, Speech Synthesis, Multilingual/Low-Resource Speech Processing, Speech-to-Speech Translation, Speech Enhancement / Robust Speech Processing", "context": "Shinji Watanabe Associate Professor Email: swatanab@andrew.cmu.edu Phone: 412-268-3687 Research Areas: Natural Language Processing: Conversational AI, Intelligent Agents, and Dialogue, Speech Processing (ASR, Speech Synthesis): Speech Recognition, Speech Synthesis, Multilingual/Low-Resource Speech Processing, Speech-to-Speech Translation, Speech Enhancement / Robust Speech Processing"}, {"question": "Please tell me the Email of LTI faculty Alexander Hauptmann based on the background information: ", "answer": "alex@cs.cmu.edu", "context": "Alexander Hauptmann Research Professor Email: alex@cs.cmu.edu Office: 5519 Gates & Hillman Centers Phone: 412-268-1448 Research Areas: Information Extraction, Summarization and Question Answering, Information Retrieval, Text Mining and Analytics, Machine Learning, Multimodal Computing and Interaction"}, {"question": "Please provide the contact information of LTI faculty Eric Nyberg", "answer": "Email: ehn@cs.cmu.edu Office: 6715 Gates & Hillman Centers Phone: 412-268-7281", "context": "Eric Nyberg Professor Email: ehn@cs.cmu.edu Office: 6715 Gates & Hillman Centers Phone: 412-268-7281 Research Areas: Information Extraction, Summarization and Question Answering, Information Retrieval, Text Mining and Analytics, Language Technologies for Education"}, {"question": "Please tell me the Phone of LTI faculty Robert Frederking based on the background information: ", "answer": "412-268-6656", "context": "Robert Frederking Principal Systems Scientist/Associate Dean of Doctoral Programs/MLT Program Director Email: ref@cs.cmu.edu Office: 6515 Gates & Hillman Centers Phone: 412-268-6656"}, {"question": "Please provide the contact information of LTI faculty Rita Singh", "answer": "Email: rsingh@cs.cmu.edu Office: 6703 Gates & Hillman Centers Phone: 412-268-9859", "context": "Rita Singh Associate Research Professor Email: rsingh@cs.cmu.edu Office: 6703 Gates & Hillman Centers Phone: 412-268-9859"}, {"question": "Please tell me the Phone of LTI faculty Robert Frederking based on the background information: ", "answer": "Robert Frederking's Phone is 412-268-6656", "context": "Robert Frederking Principal Systems Scientist/Associate Dean of Doctoral Programs/MLT Program Director Email: ref@cs.cmu.edu Office: 6515 Gates & Hillman Centers Phone: 412-268-6656"}, {"question": "How can I contact LTI faculty Eric P.?", "answer": "Email: epxing@andrew.cmu.edu Office: 8101 Gates & Hillman Centers Phone: 412-268-2559", "context": "Eric P. Xing Professor (On Leave) Email: epxing@andrew.cmu.edu Office: 8101 Gates & Hillman Centers Phone: 412-268-2559"}, {"question": "What is the Email of LTI faculty Eric Nyberg?", "answer": "Eric Nyberg's Email is ehn@cs.cmu.edu", "context": "Eric Nyberg Professor Email: ehn@cs.cmu.edu Office: 6715 Gates & Hillman Centers Phone: 412-268-7281 Research Areas: Information Extraction, Summarization and Question Answering, Information Retrieval, Text Mining and Analytics, Language Technologies for Education"}, {"question": "Is Anatole Gershman natan Bisk Assistant Professor of LTI?, Please answer yes or no: ", "answer": "No", "context": "Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering"}, {"question": "How can I contact LTI faculty Rita Singh?", "answer": "Email: rsingh@cs.cmu.edu Office: 6703 Gates & Hillman Centers Phone: 412-268-9859", "context": "Rita Singh Associate Research Professor Email: rsingh@cs.cmu.edu Office: 6703 Gates & Hillman Centers Phone: 412-268-9859"}, {"question": "Please tell me the Research Areas of LTI faculty Maarten Sap based on the background information: ", "answer": "Fairness and Ethics in Language Technology, Computational Social Science, Discourse and Pragmatics, Conversational AI, Intelligent Agents, and Dialogue", "context": "Maarten Sap Assistant Professor Email: msap2@andrew.cmu.edu Research Areas: Fairness and Ethics in Language Technology, Computational Social Science, Discourse and Pragmatics, Conversational AI, Intelligent Agents, and Dialogue"}, {"question": "What is the Phone of LTI faculty Anatole Gershman?", "answer": "Anatole Gershman's Phone is 412-268-8259", "context": "Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering"}, {"question": "How can I contact LTI faculty Yonatan Bisk?", "answer": "Email: ybisk@cs.cmu.edu Office: Gates & Hillman Centers", "context": "Yonatan Bisk Assistant Professor Email: ybisk@cs.cmu.edu Office: Gates & Hillman Centers Research Areas: Grounding, RoboNLP, Vision and Language, Embodiment, Unsupervised Learning"}, {"question": "What is the Phone of LTI faculty Shinji Watanabe?", "answer": "Shinji Watanabe's Phone is 412-268-3687", "context": "Shinji Watanabe Associate Professor Email: swatanab@andrew.cmu.edu Phone: 412-268-3687 Research Areas: Natural Language Processing: Conversational AI, Intelligent Agents, and Dialogue, Speech Processing (ASR, Speech Synthesis): Speech Recognition, Speech Synthesis, Multilingual/Low-Resource Speech Processing, Speech-to-Speech Translation, Speech Enhancement / Robust Speech Processing"}, {"question": "How can I contact LTI faculty Louis-Philippe Morency?", "answer": "Email: morency@cs.cmu.edu Office: 5411 Gates & Hillman Centers Phone: 412-268-5508", "context": "Louis-Philippe Morency Leonardo Associate Professor of Computer Science Email: morency@cs.cmu.edu Office: 5411 Gates & Hillman Centers Phone: 412-268-5508 Research Areas: Machine Learning, Multimodal Computing and Interaction, Spoken Interfaces and Dialogue Processing"}, {"question": "Please tell me the Phone of LTI faculty Robert Frederking based on the background information: ", "answer": "412-268-6656", "context": "Robert Frederking Principal Systems Scientist/Associate Dean of Doctoral Programs/MLT Program Director Email: ref@cs.cmu.edu Office: 6515 Gates & Hillman Centers Phone: 412-268-6656"}, {"question": "How can I contact LTI faculty Rita Singh?", "answer": "Email: rsingh@cs.cmu.edu Office: 6703 Gates & Hillman Centers Phone: 412-268-9859", "context": "Rita Singh Associate Research Professor Email: rsingh@cs.cmu.edu Office: 6703 Gates & Hillman Centers Phone: 412-268-9859"}, {"question": "How can I contact LTI faculty Maarten Sap?", "answer": "Email: msap2@andrew.cmu.edu", "context": "Maarten Sap Assistant Professor Email: msap2@andrew.cmu.edu Research Areas: Fairness and Ethics in Language Technology, Computational Social Science, Discourse and Pragmatics, Conversational AI, Intelligent Agents, and Dialogue"}, {"question": "What is the Research Areas of LTI faculty Shinji Watanabe?", "answer": "Shinji Watanabe's Research Areas is Natural Language Processing: Conversational AI, Intelligent Agents, and Dialogue, Speech Processing (ASR, Speech Synthesis): Speech Recognition, Speech Synthesis, Multilingual/Low-Resource Speech Processing, Speech-to-Speech Translation, Speech Enhancement / Robust Speech Processing", "context": "Shinji Watanabe Associate Professor Email: swatanab@andrew.cmu.edu Phone: 412-268-3687 Research Areas: Natural Language Processing: Conversational AI, Intelligent Agents, and Dialogue, Speech Processing (ASR, Speech Synthesis): Speech Recognition, Speech Synthesis, Multilingual/Low-Resource Speech Processing, Speech-to-Speech Translation, Speech Enhancement / Robust Speech Processing"}, {"question": "How can I contact LTI faculty Eric P.?", "answer": "Email: epxing@andrew.cmu.edu Office: 8101 Gates & Hillman Centers Phone: 412-268-2559", "context": "Eric P. Xing Professor (On Leave) Email: epxing@andrew.cmu.edu Office: 8101 Gates & Hillman Centers Phone: 412-268-2559"}, {"question": "Please tell me the Research Areas of LTI faculty Shinji Watanabe based on the background information: ", "answer": "Natural Language Processing: Conversational AI, Intelligent Agents, and Dialogue, Speech Processing (ASR, Speech Synthesis): Speech Recognition, Speech Synthesis, Multilingual/Low-Resource Speech Processing, Speech-to-Speech Translation, Speech Enhancement / Robust Speech Processing", "context": "Shinji Watanabe Associate Professor Email: swatanab@andrew.cmu.edu Phone: 412-268-3687 Research Areas: Natural Language Processing: Conversational AI, Intelligent Agents, and Dialogue, Speech Processing (ASR, Speech Synthesis): Speech Recognition, Speech Synthesis, Multilingual/Low-Resource Speech Processing, Speech-to-Speech Translation, Speech Enhancement / Robust Speech Processing"}, {"question": "Is Scott Fahlman i Li Assistant Professor of LTI?, Please answer yes or no: ", "answer": "No", "context": "Scott Fahlman Research Professor Emeritus Email: sef@cs.cmu.edu Office: 6417 Gates & Hillman Centers Phone: 412-268-2575 Research Areas: AI, Knowledge Representation and Reasoning, Natural Language Understanding"}, {"question": "How can I contact LTI faculty Bhiksha Ramakrishnan?", "answer": "Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826", "context": "Bhiksha Ramakrishnan Professor Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826 Research Areas: Machine Learning, Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing, Privacy"}, {"question": "How can I contact LTI faculty Chenyan Xiong?", "answer": "Email: cx@andrew.cmu.edu Phone: 412-268-7641", "context": "Chenyan Xiong Associate Professor Email: cx@andrew.cmu.edu Phone: 412-268-7641"}, {"question": "How can I contact LTI faculty Bhiksha Ramakrishnan?", "answer": "Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826", "context": "Bhiksha Ramakrishnan Professor Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826 Research Areas: Machine Learning, Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing, Privacy"}, {"question": "How can I contact LTI faculty Maarten Sap?", "answer": "Email: msap2@andrew.cmu.edu", "context": "Maarten Sap Assistant Professor Email: msap2@andrew.cmu.edu Research Areas: Fairness and Ethics in Language Technology, Computational Social Science, Discourse and Pragmatics, Conversational AI, Intelligent Agents, and Dialogue"}, {"question": "What is the Office of LTI faculty Justine Cassell?", "answer": "Justine Cassell's Office is 5107 Gates & Hillman Centers", "context": "Justine Cassell Professor (On Leave) Email: jcassell@andrew.cmu.edu Office: 5107 Gates & Hillman Centers Phone: 412-204-6268"}, {"question": "What is the Research Areas of LTI faculty Louis-Philippe Morency?", "answer": "Louis-Philippe Morency's Research Areas is Machine Learning, Multimodal Computing and Interaction, Spoken Interfaces and Dialogue Processing", "context": "Louis-Philippe Morency Leonardo Associate Professor of Computer Science Email: morency@cs.cmu.edu Office: 5411 Gates & Hillman Centers Phone: 412-268-5508 Research Areas: Machine Learning, Multimodal Computing and Interaction, Spoken Interfaces and Dialogue Processing"}, {"question": "What is the Office of LTI faculty Ralf Brown?", "answer": "5711 Gates & Hillman Centers", "context": "Ralf Brown Principal Systems Scientist Email: ralf@andrew.cmu.edu Office: 5711 Gates & Hillman Centers Phone: 412-268-8298 Research Areas: Information Extraction, Summarization and Question Answering, Information Retrieval, Text Mining and Analytics, Machine Translation, Natural Language Processing and Computational Linguistics"}, {"question": "Please tell me the Email of LTI faculty Robert Frederking based on the background information: ", "answer": "Robert Frederking's Email is ref@cs.cmu.edu", "context": "Robert Frederking Principal Systems Scientist/Associate Dean of Doctoral Programs/MLT Program Director Email: ref@cs.cmu.edu Office: 6515 Gates & Hillman Centers Phone: 412-268-6656"}, {"question": "Please provide the contact information of LTI faculty Jamie Callan", "answer": "Email: callan@cs.cmu.edu Office: 5419 Gates & Hillman Centers Phone: 412-268-4525", "context": "Jamie Callan Professor and PhD Program Director Email: callan@cs.cmu.edu Office: 5419 Gates & Hillman Centers Phone: 412-268-4525 Research Areas: Information Retrieval, Text Mining and Analytics"}, {"question": "What is the Email of LTI faculty Bhiksha Ramakrishnan?", "answer": "bhiksha@cs.cmu.edu", "context": "Bhiksha Ramakrishnan Professor Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826 Research Areas: Machine Learning, Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing, Privacy"}, {"question": "What is the Phone of LTI faculty Justine Cassell?", "answer": "412-204-6268", "context": "Justine Cassell Professor (On Leave) Email: jcassell@andrew.cmu.edu Office: 5107 Gates & Hillman Centers Phone: 412-204-6268"}, {"question": "Is Rita Singh lf Brown Principal Systems Scientist of LTI?, Please answer yes or no: ", "answer": "No", "context": "Rita Singh Associate Research Professor Email: rsingh@cs.cmu.edu Office: 6703 Gates & Hillman Centers Phone: 412-268-9859"}, {"question": "Is Maarten Sap iksha Ramakrishnan Professor of LTI?, Please answer yes or no: ", "answer": "No", "context": "Maarten Sap Assistant Professor Email: msap2@andrew.cmu.edu Research Areas: Fairness and Ethics in Language Technology, Computational Social Science, Discourse and Pragmatics, Conversational AI, Intelligent Agents, and Dialogue"}, {"question": "Please tell me the Email of LTI faculty Sean Welleck based on the background information: ", "answer": "swelleck@andrew.cmu.edu", "context": "Sean Welleck Assistant Professor (Starting January 2024) Email: swelleck@andrew.cmu.edu"}, {"question": "Is Louis-Philippe Morency niel Fried Assistant Professor of LTI?, Please answer yes or no: ", "answer": "No", "context": "Louis-Philippe Morency Leonardo Associate Professor of Computer Science Email: morency@cs.cmu.edu Office: 5411 Gates & Hillman Centers Phone: 412-268-5508 Research Areas: Machine Learning, Multimodal Computing and Interaction, Spoken Interfaces and Dialogue Processing"}, {"question": "What is the Office of LTI faculty Anatole Gershman?", "answer": "Anatole Gershman's Office is 6415 Gates & Hillman Centers", "context": "Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering"}, {"question": "How can I contact LTI faculty Robert Frederking?", "answer": "Email: ref@cs.cmu.edu Office: 6515 Gates & Hillman Centers Phone: 412-268-6656", "context": "Robert Frederking Principal Systems Scientist/Associate Dean of Doctoral Programs/MLT Program Director Email: ref@cs.cmu.edu Office: 6515 Gates & Hillman Centers Phone: 412-268-6656"}, {"question": "According to background information, What happens in 2024-06-21 on university calendar?", "answer": "Mini-5 Final Exams", "context": "Summer One_All 2024: Date: 2024-06-21 Day: F Event: Mini-5 Final Exams"}, {"question": "Based on background information, What is the specific event listed for 2024-05-17, in Summer One_All 2024", "answer": "Mini-5 add, audit, & tuition adjustment drop deadline (1)", "context": "Summer One_All 2024: Date: 2024-05-17 Day: F Event: Mini-5 add, audit, & tuition adjustment drop deadline (1)"}, {"question": "According to background information, When is the [Mini-5 Last Day of Classes] in Summer One_All 2024?", "answer": "The event Mini-5 Last Day of Classes happens on 2024-06-20", "context": "Summer One_All 2024: Date: 2024-06-20 Day: Th Event: Mini-5 Last Day of Classes"}, {"question": "Based on background information, What is the specific event listed for 2023-11-27, in Fall 2023", "answer": "The event happening on 2023-11-27 is Mini-2 pass/no pass & withdrawal deadline (2)", "context": "Fall 2023: Date: 2023-11-27 Day: M Event: Mini-2 pass/no pass & withdrawal deadline (2)"}, {"question": "Based on background information, What is the specific event listed for 2023-10-23, in Fall 2023", "answer": "The event happening on 2023-10-23 is Mini-2 Classes Begin", "context": "Fall 2023: Date: 2023-10-23 Day: M Event: Mini-2 Classes Begin "}, {"question": "Based on background information, What is the specific event listed for 2024-01-15, in Spring 2024", "answer": "Martin Luther King Day; No Classes", "context": "Spring 2024: Date: 2024-01-15 Day: M Event: Martin Luther King Day; No Classes"}, {"question": "According to background information, When is the [Make-Up Final Examinations] in Spring 2024?", "answer": "The event Make-Up Final Examinations happens on 2024-05-07", "context": "Spring 2024: Date: 2024-05-07 Day: Tu Event: Make-Up Final Examinations"}, {"question": "According to background information, What happens in 2023-10-13 on university calendar?", "answer": "The event happening on 2023-10-13 is Mini-1 Last Day of Classes", "context": "Fall 2023: Date: 2023-10-13 Day: F Event: Mini-1 Last Day of Classes"}, {"question": "Based on background information, What is the specific event listed for 2024-07-08, in Summer One_All 2024", "answer": "The event happening on 2024-07-08 is Mini-6 drop deadline; withdrawal grade assigned after this date (2)", "context": "Summer One_All 2024: Date: 2024-07-08 Day: M Event: Mini-6 drop deadline; withdrawal grade assigned after this date (2)"}, {"question": "According to background information, When is the [Reading Day] in Spring 2024?", "answer": "2024-05-01", "context": "Spring 2024: Date: 2024-05-01 Day: W Event: Reading Day"}, {"question": "Based on the given information, Does Spring 2024 Mini-3 Faculty Course Evaluations open happen at 2024-01-15", "answer": "No", "context": "Spring 2024: Date: 2024-02-19 Day: M Event: Mini-3 Faculty Course Evaluations open"}, {"question": "According to background information, When is the [Mini-1 Exams] in Fall 2023?", "answer": "2023-10-14", "context": "Fall 2023: Date: 2023-10-14 Day: Sa Event: Mini-1 Exams"}, {"question": "Based on background information, What is the specific event listed for 2024-07-29, in Summer One_All 2024", "answer": "Semester & Mini-6 Faculty Course Evalutations open", "context": "Summer One_All 2024: Date: 2024-07-29 Day: M Event: Semester & Mini-6 Faculty Course Evalutations open "}, {"question": "According to background information, When is the [Mini-1 add, audit & tuition adjustment drop deadline  (1)] in Fall 2023?", "answer": "2023-09-01", "context": "Fall 2023: Date: 2023-09-01 Day: F Event: Mini-1 add, audit & tuition adjustment drop deadline  (1)"}, {"question": "Based on the given information, Does Summer One_All 2024 Independence Day; University Closed & No Classes happen at 2024-05-06", "answer": "No", "context": "Summer One_All 2024: Date: 2024-07-04 Day: Th Event: Independence Day; University Closed & No Classes"}, {"question": "Based on background information, What is the specific event listed for 2023-09-01, in Fall 2023", "answer": "Mini-1 add, audit & tuition adjustment drop deadline  (1)", "context": "Fall 2023: Date: 2023-09-01 Day: F Event: Mini-1 add, audit & tuition adjustment drop deadline  (1)"}, {"question": "Based on background information, What is the specific event listed for 2024-05-27, in Summer One_All 2024", "answer": "Memorial Day; University Closed & No Classes", "context": "Summer One_All 2024: Date: 2024-05-27 Day: M Event: Memorial Day; University Closed & No Classes"}, {"question": "Based on the given information, Does Fall 2023 Mini-2 drop deadline; withdrawal grade assigned after this date (2) happen at 2023-11-15", "answer": "Yes", "context": "Fall 2023: Date: 2023-11-15 Day: W Event: Mini-2 drop deadline; withdrawal grade assigned after this date (2)"}, {"question": "Based on the given information, Does Summer One_All 2024 Mini-5 Faculty Course Evaluations open happen at 2024-06-14", "answer": "Yes", "context": "Summer One_All 2024: Date: 2024-06-14 Day: F Event: Mini-5 Faculty Course Evaluations open"}, {"question": "Based on background information, What is the specific event listed for 2024-05-08, in Spring 2024", "answer": "Graduating Final Grades Due by 4 pm", "context": "Spring 2024: Date: 2024-05-08 Day: W Event: Graduating Final Grades Due by 4 pm"}, {"question": "Based on the given information, Does Summer One_All 2024 Mini-6 First Day of Classes happen at 2024-06-24", "answer": "Yes", "context": "Summer One_All 2024: Date: 2024-06-24 Day: M Event: Mini-6 First Day of Classes"}, {"question": "According to background information, What happens in 2024-07-29 on university calendar?", "answer": "The event happening on 2024-07-29 is Semester & Mini-6 Faculty Course Evalutations open", "context": "Summer One_All 2024: Date: 2024-07-29 Day: M Event: Semester & Mini-6 Faculty Course Evalutations open "}, {"question": "Based on the given information, Does Fall 2023 Mini-1 voucher election deadline (4) happen at 2023-10-13", "answer": "Yes", "context": "Fall 2023: Date: 2023-10-13 Day: F Event: Mini-1 voucher election deadline (4)"}, {"question": "According to background information, When is the [Final Examinations ] in Spring 2024?", "answer": "2024-04-29-2024-04-30", "context": "Spring 2024: Date: 2024-04-29-2024-04-30 Day: M-Tu Event: Final Examinations "}, {"question": "Based on the given information, Does Summer Two 2024 Summer Semester  Two drop deadline; withdrawal grade assigned after this date (2) happen at 2024-07-29", "answer": "No", "context": "Summer Two 2024: Date: 2024-07-08 Day: M Event: Summer Semester  Two drop deadline; withdrawal grade assigned after this date (2)"}, {"question": "According to background information, When is the [Semester pass/no pass & withdrawal deadline (3)] in Summer One_All 2024?", "answer": "The event Semester pass/no pass & withdrawal deadline (3) happens on 2024-07-05", "context": "Summer One_All 2024: Date: 2024-07-05 Day: F Event: Semester pass/no pass & withdrawal deadline (3)"}, {"question": "According to background information, What happens in 2024-05-06 on university calendar?", "answer": "The event happening on 2024-05-06 is Final Examinations", "context": "Spring 2024: Date: 2024-05-06 Day: M Event: Final Examinations"}, {"question": "Based on the given information, Does Summer One_All 2024 Mini-5 pass/no pass & withdrawal deadline (3) happen at 2024-06-24", "answer": "No", "context": "Summer One_All 2024: Date: 2024-06-07 Day: F Event: Mini-5 pass/no pass & withdrawal deadline (3)"}, {"question": "According to background information, When is the [Summer Semester  Two pass/no pass & withdrawal deadline (3)] in Summer Two 2024?", "answer": "The event Summer Semester  Two pass/no pass & withdrawal deadline (3) happens on 2024-07-22", "context": "Summer Two 2024: Date: 2024-07-22 Day: F Event: Summer Semester  Two pass/no pass & withdrawal deadline (3)"}, {"question": "Based on background information, What is the specific event listed for 2024-02-19, in Spring 2024", "answer": "The event happening on 2024-02-19 is Mini-3 Faculty Course Evaluations open", "context": "Spring 2024: Date: 2024-02-19 Day: M Event: Mini-3 Faculty Course Evaluations open"}, {"question": "Based on the given information, Does Spring 2024 Mini-3 voucher deadline (4) happen at 2023-10-13", "answer": "No", "context": "Spring 2024: Date: 2024-03-01 Day: F Event: Mini-3 voucher deadline (4)"}, {"question": "Based on background information, What is the specific event listed for 2024-04-01, in Spring 2024", "answer": "The event happening on 2024-04-01 is Semester pass/no pass & withdrawal deadline (3)", "context": "Spring 2024: Date: 2024-04-01 Day: M Event: Semester pass/no pass & withdrawal deadline (3)"}, {"question": "According to background information, When is the [Mini-5 pass/no pass & withdrawal deadline (3)] in Summer One_All 2024?", "answer": "The event Mini-5 pass/no pass & withdrawal deadline (3) happens on 2024-06-07", "context": "Summer One_All 2024: Date: 2024-06-07 Day: F Event: Mini-5 pass/no pass & withdrawal deadline (3)"}, {"question": "Based on the given information, Does Summer One_All 2024 Mini-5 Faculty Course Evaluations open happen at 2024-06-14", "answer": "Yes", "context": "Summer One_All 2024: Date: 2024-06-14 Day: F Event: Mini-5 Faculty Course Evaluations open"}, {"question": "Based on the given information, Does Spring 2024 Final Grades Due by 4 pm happen at 2023-12-17", "answer": "No", "context": "Spring 2024: Date: 2024-05-14 Day: Tu Event: Final Grades Due by 4 pm"}, {"question": "Based on background information, What is the specific event listed for 2024-06-21, in Summer One_All 2024", "answer": "Mini-5 Final Exams", "context": "Summer One_All 2024: Date: 2024-06-21 Day: F Event: Mini-5 Final Exams"}, {"question": "According to background information, What happens in 2023-12-18 on university calendar?", "answer": "The event happening on 2023-12-18 is Semester & Mini-2 Faculty Course Evaluations close", "context": "Fall 2023: Date: 2023-12-18 Day: M Event: Semester & Mini-2 Faculty Course Evaluations close"}, {"question": "Based on the given information, Does Summer Two 2024 Summer Semester  Two Last Day of Classes happen at 2024-08-01", "answer": "Yes", "context": "Summer Two 2024: Date: 2024-08-01 Day: Th Event: Summer Semester  Two Last Day of Classes"}, {"question": "Based on background information, What is the specific event listed for 2024-05-14, in Spring 2024", "answer": "The event happening on 2024-05-14 is Final Grades Due by 4 pm", "context": "Spring 2024: Date: 2024-05-14 Day: Tu Event: Final Grades Due by 4 pm"}, {"question": "According to background information, When is the [Final Exams ] in Fall 2023?", "answer": "The event Final Exams  happens on 2023-12-11-2023-12-12", "context": "Fall 2023: Date: 2023-12-11-2023-12-12 Day: M-Tu Event: Final Exams "}, {"question": "Based on background information, What is the specific event listed for 2024-07-04, in Summer Two 2024", "answer": "The event happening on 2024-07-04 is Independence Day; University Closed & No Classes", "context": "Summer Two 2024: Date: 2024-07-04 Day: Th Event: Independence Day; University Closed & No Classes"}, {"question": "Based on background information, What is the specific event listed for 2024-02-26, in Spring 2024", "answer": "The event happening on 2024-02-26 is Semester course drop deadline; withdrawal grade assigned after this date (2)", "context": "Spring 2024: Date: 2024-02-26 Day: M Event: Semester course drop deadline; withdrawal grade assigned after this date (2)"}, {"question": "According to background information, When is the [Reading Day] in Spring 2024?", "answer": "2024-05-01", "context": "Spring 2024: Date: 2024-05-01 Day: W Event: Reading Day"}, {"question": "According to background information, When is the [Spring Deans' Lists Posted] in Spring 2024?", "answer": "The event Spring Deans' Lists Posted happens on 2023-05-21", "context": "Spring 2024: Date: 2023-05-21 Day: Tu Event: Spring Deans' Lists Posted"}, {"question": "Based on the given information, Does Fall 2023 Spring 2024 Registration Week happen at 2023-11-13-2023-11-17", "answer": "Yes", "context": "Fall 2023: Date: 2023-11-13-2023-11-17 Day: M-F Event: Spring 2024 Registration Week"}, {"question": "Based on the given information, Does Summer One_All 2024 Semester & Mini-6 Faculty Course Evaluations close happen at 2024-08-02", "answer": "Yes", "context": "Summer One_All 2024: Date: 2024-08-02 Day: F Event: Semester & Mini-6 Faculty Course Evaluations close"}, {"question": "Based on the given information, Does Spring 2024 Mid-Semester & Mini-3 grades due by 4 pm happen at 2024-04-11-2024-04-13", "answer": "No", "context": "Spring 2024: Date: 2024-03-11 Day: M Event: Mid-Semester & Mini-3 grades due by 4 pm"}, {"question": "According to background information, What happens in 2024-05-13 on university calendar?", "answer": "Semester & Mini-5 Classes Begin", "context": "Summer One_All 2024: Date: 2024-05-13 Day: M Event: Semester & Mini-5 Classes Begin"}, {"question": "Based on background information, What is the specific event listed for 2024-06-07, in Summer One_All 2024", "answer": "The event happening on 2024-06-07 is Mini-5 pass/no pass & withdrawal deadline (3)", "context": "Summer One_All 2024: Date: 2024-06-07 Day: F Event: Mini-5 pass/no pass & withdrawal deadline (3)"}, {"question": "Based on the given information, Does Fall 2023 Final Exams  happen at 2023-12-11-2023-12-12", "answer": "Yes", "context": "Fall 2023: Date: 2023-12-11-2023-12-12 Day: M-Tu Event: Final Exams "}, {"question": "When does the class International Finance Recitation Section A4  offered in Sp end?", "answer": "11:45AM", "context": "Spring offering: Course: 45825 Title: International Finance Units: 6.0 Lec/Sec: Section A4 Days: Tuesday, Thursday Begin: 10:00AM End: 11:45AM Bldg/Room: TEP 2118 Location: Pittsburgh, Pennsylvania Instructor(s): Khokher "}, {"question": "At what time does the class Financial Statements and Analysis of Companies Recitation Section C1  offered in Fa begin?", "answer": "11:00AM", "context": "Fall offering: Course: 90723 Title: Financial Statements and Analysis of Companies Units: 6.0 Lec/Sec: Section C1 Days: Monday, Wednesday Begin: 11:00AM End: 12:20PM Bldg/Room: HBH 2008 Location: Pittsburgh, Pennsylvania Instructor(s): Gogolak "}, {"question": "What is the units of course Principles of Imperative Computation offered in Fa?", "answer": "12.0", "context": "Fall offering: Course: 15122 Title: Principles of Imperative Computation Units: 12.0 Lec/Sec: Section M Days: Monday Begin: 12:00PM End: 12:50PM Bldg/Room: GHC CLSTR Location: Pittsburgh, Pennsylvania Instructor(s): Cervesato, Kaynar "}, {"question": "On which days does the class Big Data Science Recitation Section A4  offered in Sp meet?", "answer": "Tuesday, Thursday", "context": "Spring offering: Course: 18788 Title: Big Data Science Units: 6.0 Lec/Sec: Section A4 Days: Tuesday, Thursday Begin: 08:00AM End: 09:20AM Bldg/Room: HH 1107 Location: Pittsburgh, Pennsylvania Instructor(s): McSharry "}, {"question": "Based on the background information, What is the units of course Statistics for IT Managers offered in Fa?", "answer": "The units of course Statistics for IT Managers is 6.0", "context": "Fall offering: Course: 95796 Title: Statistics for IT Managers Units: 6.0 Lec/Sec: Section D1 Days: Tuesday, Thursday Begin: 03:30PM End: 04:50PM Bldg/Room: HBH 1206 Location: Pittsburgh, Pennsylvania Instructor(s): Garin "}, {"question": "When does the class Tap II - Rhythmic Technique/Foundational to Complex Recitation Section B  offered in Fa end?", "answer": "01:20PM", "context": "Fall offering: Course: 54223 Title: Tap II - Rhythmic Technique/Foundational to Complex Units: 2.0 Lec/Sec: Section B Days: Friday Begin: 12:30PM End: 01:20PM Bldg/Room: PCA 306 Location: Pittsburgh, Pennsylvania Instructor(s): Conte "}, {"question": "On which days does the class Orchestration I Recitation Section A  offered in Fa meet?", "answer": "Tuesday, Thursday", "context": "Fall offering: Course: 57257 Title: Orchestration I Units: 6.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 09:00AM End: 09:50AM Bldg/Room: CFA M160 Location: Pittsburgh, Pennsylvania Instructor(s): Marthaler "}, {"question": "On which days does the class Weight Training: Recitation Section B3  offered in Sp meet?", "answer": "Tuesday, Thursday", "context": "Spring offering: Course: 69102 Title: Weight Training: Units: 3.0 Lec/Sec: Section B3 Days: Tuesday, Thursday Begin: 12:30PM End: 01:50PM Bldg/Room: CUC COURTS Location: Pittsburgh, Pennsylvania Instructor(s): Connelly "}, {"question": "Which campus is class Data Analytics for Organizational Impacts Recitation Section A2  offered in Pittsburgh in Fa?", "answer": "Pittsburgh, Pennsylvania", "context": "Fall offering: Course: 94865 Title: Data Analytics for Organizational Impacts Units: 6.0 Lec/Sec: Section A2 Days: Tuesday, Thursday Begin: 03:30PM End: 04:50PM Bldg/Room: HBH 1004 Location: Pittsburgh, Pennsylvania Instructor(s): Escallon Barrios "}, {"question": "Where does the class Accounting and Finance Analytics Recitation Section E4  offered in Sp meet?", "answer": "HBH 1206", "context": "Spring offering: Course: 95719 Title: Accounting and Finance Analytics Units: 6.0 Lec/Sec: Section E4 Days: Tuesday, Thursday Begin: 05:00PM End: 06:20PM Bldg/Room: HBH 1206 Location: Pittsburgh, Pennsylvania Instructor(s): Gogolak "}, {"question": "On which days does the class IDeATe: Soft Fabrication Skills Recitation Section A1  offered in Fa meet?", "answer": "Saturday", "context": "Fall offering: Course: 99352 Title: IDeATe: Soft Fabrication Skills Units: 1.0 Lec/Sec: Section A1 Days: Saturday Begin: 10:00AM End: 03:00PM Bldg/Room: HL 106B Location: Pittsburgh, Pennsylvania Instructor(s): Pinchuk "}, {"question": "Which campus is class Principles of Macroeconomics Recitation Section A  offered in Pittsburgh in Sp?", "answer": "Pittsburgh, Pennsylvania", "context": "Spring offering: Course: 73103 Title: Principles of Macroeconomics Units: 9.0 Lec/Sec: Section A Days: Friday Begin: 10:00AM End: 10:50AM Bldg/Room: TEP 3801 Location: Pittsburgh, Pennsylvania Instructor(s): Zetlin-Jones "}, {"question": "When does the class Funding Early Stage Ventures: Recitation Section A4  offered in Sp end?", "answer": "09:45AM", "context": "Spring offering: Course: 45905 Title: Funding Early Stage Ventures: Units: 6.0 Lec/Sec: Section A4 Days: Tuesday, Thursday Begin: 08:00AM End: 09:45AM Bldg/Room: TEP 2111 Location: Pittsburgh, Pennsylvania Instructor(s): Risch "}, {"question": "When does the class Design Center: Design for Social Innovation Recitation Section A  offered in Sp end?", "answer": "01:20PM", "context": "Spring offering: Course: 51782 Title: Design Center: Design for Social Innovation Units: 12.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 12:00PM End: 01:20PM Bldg/Room: MM 121 Location: Pittsburgh, Pennsylvania Instructor(s): Krishnaswami "}, {"question": "On which days does the class Fairness, Accountability, Transparency, and Ethics in Sociotechnical Recitation Section A  offered in Fa meet?", "answer": "Monday, Wednesday", "context": "Fall offering: Course: 05499 Title: Fairness, Accountability, Transparency, and Ethics in Sociotechnical Units: 12.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 11:00AM End: 12:20PM Bldg/Room: HH B103 Location: Pittsburgh, Pennsylvania Instructor(s): Shen "}, {"question": "When does the class Organizational Behavior Recitation Section W  offered in Fa end?", "answer": "12:45PM", "context": "Fall offering: Course: 70311 Title: Organizational Behavior Units: 9.0 Lec/Sec: Section W Days: Monday, Wednesday Begin: 11:30AM End: 12:45PM Bldg/Room: CMB 1190 Location: Doha, Qatar Instructor(s): Haan "}, {"question": "Who is the instructor(s) of course Software Engineering Management: Recitation Section A2  offered in Fa?", "answer": "Fang", "context": "Fall offering: Course: 49772 Title: Software Engineering Management: Units: 6.0 Lec/Sec: Section A2 Days: Monday, Wednesday Begin: 11:00AM End: 12:20PM Bldg/Room: B23 227 Location: San Jose, California Instructor(s): Fang "}, {"question": "On which days does the class Business Leadership Endeavor II Recitation Section C1  offered in Fa meet?", "answer": "Tuesday", "context": "Fall offering: Course: 70204 Title: Business Leadership Endeavor II Units: 3.0 Lec/Sec: Section C1 Days: Tuesday Begin: 02:00PM End: 03:20PM Bldg/Room: TEP 3808 Location: Pittsburgh, Pennsylvania Instructor(s): Jafry O'Connor "}, {"question": "On which days does the class Acting for Leadership and Communication Recitation Section B4  offered in Sp meet?", "answer": "Monday, Wednesday", "context": "Spring offering: Course: 94801 Title: Acting for Leadership and Communication Units: 6.0 Lec/Sec: Section B4 Days: Monday, Wednesday Begin: 11:00AM End: 12:20PM Bldg/Room: HBH 2009 Location: Pittsburgh, Pennsylvania Instructor(s): Murphy "}, {"question": "On which days does the class Mathematical Models for Consulting Recitation Section W  offered in Fa meet?", "answer": "Sunday, Tuesday", "context": "Fall offering: Course: 70460 Title: Mathematical Models for Consulting Units: 9.0 Lec/Sec: Section W Days: Sunday, Tuesday Begin: 04:00PM End: 05:15PM Bldg/Room: CMB 1031 Location: Doha, Qatar Instructor(s): Safak "}, {"question": "Based on the background information, What is the units of course Special Topics in ICT: offered in Fa?", "answer": "The units of course Special Topics in ICT: is 12.0", "context": "Fall offering: Course: 04800 Title: Special Topics in ICT: Units: 12.0 Lec/Sec: Section G Days: Tuesday, Thursday Begin: 10:00AM End: 11:50AM Bldg/Room: CMR F309 Location: Kigali, Rwanda Instructor(s): Tucker "}, {"question": "Which campus is class Techniques in Quantitative Analysis Recitation Section B  offered in Pittsburgh in Fa?", "answer": "Pittsburgh, Pennsylvania", "context": "Fall offering: Course: 09207 Title: Techniques in Quantitative Analysis Units: 9.0 Lec/Sec: Section B Days: Tuesday Begin: 06:30PM End: 08:50PM Bldg/Room: DH 1302 Location: Pittsburgh, Pennsylvania Instructor(s): Botcha "}, {"question": "Based on the background information, What is the units of course Singers offered in Fa?", "answer": "The units of course Singers is 3.0", "context": "Fall offering: Course: 57822 Title: Singers Units: 3.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 05:30PM End: 06:20PM Bldg/Room: MM 119 Location: Pittsburgh, Pennsylvania Instructor(s): Douglas "}, {"question": "What is the title of the paper published by LTI faculty Yiming Yang in 2023?", "answer": "Neutral Face Learning and Progressive Fusion Synthesis Network for NIR-VIS Face Recognition", "context": "Author: Yiming Yang Title: Neutral Face Learning and Progressive Fusion Synthesis Network for NIR-VIS Face Recognition Publication year: 2023 Coauthors: Yiming Yang, Weipeng Hu, Haifeng Hu Abstract: To meet the strong demand for deploying face recognition systems in low-light scenarios, the Near-InfraRed and VISible (NIR-VIS) face recognition task is receiving increasing attention. However, heterogeneous faces have the characteristics of heterogeneity and non-neutrality. Heterogeneity refers to the fact that the matching images are in different modalities, and non-neutrality means that the matching images are significantly different in pose, expression, lighting, etc. Both situations pose challenges for NIR-VIS face matching. To address this problem, we propose a novel Neutral face Learning and Progressive Fusion synthesis (NLPF) network to disentangle the latent attributes of heterogeneous faces and learn neutral face representations. Our approach naturally integrates Identity-related Neutral face Learning (INL) and Attribute Progressive Fusion (APF) into a joint framework. Firstly, INL eliminates modal variations and residual variations by guiding the network to learn homogeneous neutral face feature representations, which tackles the challenge of heterogeneity and non-neutrality by mapping cross-modal images to a common neutral representation subspace. Besides, APF is presented to perform the disentanglement and reintegration of identity-related features, modality-related features and residual features in a progressive fusion manner, which helps to further purify identity-related features. Comprehensive evaluations are carried out on three mainstream NIR-VIS datasets to verify the robustness and effectiveness of the NLPF model. In particular, NLPF has competitive recognition performance on LAMP-HQ, the most challenging NIR-VIS dataset so far."}, {"question": "What is the abstract of the paper Policy Representation via Diffusion Probability Model for Reinforcement Learning published by LTI faculty Yiming Yang in 2023?", "answer": "Popular reinforcement learning (RL) algorithms tend to produce a unimodal policy distribution, which weakens the expressiveness of complicated policy and decays the ability of exploration. The diffusion probability model is powerful to learn complicated multimodal distributions, which has shown promising and potential applications to RL. In this paper, we formally build a theoretical foundation of policy representation via the diffusion probability model and provide practical implementations of diffusion policy for online model-free RL. Concretely, we character diffusion policy as a stochastic process, which is a new approach to representing a policy. Then we present a convergence guarantee for diffusion policy, which provides a theory to understand the multimodality of diffusion policy. Furthermore, we propose the DIPO which is an implementation for model-free online RL with DIffusion POlicy. To the best of our knowledge, DIPO is the first algorithm to solve model-free online RL problems with the diffusion model. Finally, extensive empirical results show the effectiveness and superiority of DIPO on the standard continuous control Mujoco benchmark.", "context": "Author: Yiming Yang Title: Policy Representation via Diffusion Probability Model for Reinforcement Learning Publication year: 2023 Coauthors: Long Yang, Zhixiong Huang, Fenghao Lei, Yucun Zhong, Yiming Yang, Cong Fang, Shiting Wen, Binbin Zhou, Zhouchen Lin Abstract: Popular reinforcement learning (RL) algorithms tend to produce a unimodal policy distribution, which weakens the expressiveness of complicated policy and decays the ability of exploration. The diffusion probability model is powerful to learn complicated multimodal distributions, which has shown promising and potential applications to RL. In this paper, we formally build a theoretical foundation of policy representation via the diffusion probability model and provide practical implementations of diffusion policy for online model-free RL. Concretely, we character diffusion policy as a stochastic process, which is a new approach to representing a policy. Then we present a convergence guarantee for diffusion policy, which provides a theory to understand the multimodality of diffusion policy. Furthermore, we propose the DIPO which is an implementation for model-free online RL with DIffusion POlicy. To the best of our knowledge, DIPO is the first algorithm to solve model-free online RL problems with the diffusion model. Finally, extensive empirical results show the effectiveness and superiority of DIPO on the standard continuous control Mujoco benchmark."}, {"question": "Who is the author of the LTI paper The Multimodal Information Based Speech Processing (MISP) 2023 Challenge: Audio-Visual Target Speaker Extraction published in 2023?", "answer": "Shinji Watanabe", "context": "Author: Shinji Watanabe Title: The Multimodal Information Based Speech Processing (MISP) 2023 Challenge: Audio-Visual Target Speaker Extraction Publication year: 2023 Coauthors: Shilong Wu, Chenxi Wang, Hang Chen, Yusheng Dai, Chenyue Zhang, Ruoyu Wang, Hongbo Lan, Jun Du, Chin-Hui Lee, Jingdong Chen, Shinji Watanabe, Sabato Marco Siniscalchi, O. Scharenborg, Zhong-Qiu Wang, Jia Pan, Jianqing Gao Abstract: Previous Multimodal Information based Speech Processing (MISP) challenges mainly focused on audio-visual speech recognition (AVSR) with commendable success. However, the most advanced back-end recognition systems often hit performance limits due to the complex acoustic environments. This has prompted a shift in focus towards the Audio-Visual Target Speaker Extraction (AVTSE) task for the MISP 2023 challenge in ICASSP 2024 Signal Processing Grand Challenges. Unlike existing audio-visual speech enhance-ment challenges primarily focused on simulation data, the MISP 2023 challenge uniquely explores how front-end speech processing, combined with visual clues, impacts back-end tasks in real-world scenarios. This pioneering effort aims to set the first benchmark for the AVTSE task, offering fresh insights into enhancing the ac-curacy of back-end speech recognition systems through AVTSE in challenging and real acoustic environments. This paper delivers a thorough overview of the task setting, dataset, and baseline system of the MISP 2023 challenge. It also includes an in-depth analysis of the challenges participants may encounter. The experimental results highlight the demanding nature of this task, and we look forward to the innovative solutions participants will bring forward."}, {"question": "Who are the coauthors of the paper FindAdaptNet: Find and Insert Adapters by Learned Layer Importance published by LTI faculty Shinji Watanabe in 2023?", "answer": "Junwei Huang, Karthik Ganesan, Soumi Maiti, Young Min Kim, Xuankai Chang, Paul Liang, Shinji Watanabe", "context": "Author: Shinji Watanabe Title: FindAdaptNet: Find and Insert Adapters by Learned Layer Importance Publication year: 2023 Coauthors: Junwei Huang, Karthik Ganesan, Soumi Maiti, Young Min Kim, Xuankai Chang, Paul Liang, Shinji Watanabe Abstract: Adapters are lightweight bottleneck modules introduced to assist pre-trained self-supervised learning (SSL) models to be customized to new tasks. However, searching the appropriate layers to insert adapters on large models has become difficult due to the large number of possible layers and thus a vast search space (2N possibilities for N layers). In this paper, we propose a technique that achieves automatic insertion of adapters for downstream automatic speech recognition (ASR) and spoken language understanding (SLU) tasks. Our approach is based on two-stage training. First, we train our model for a specific downstream task with additional shallow learnable layers and weight parameters to obtain the weighted summation over the output of each layer in SSL. This training method is established by the SUPERB baseline [1]. This first-stage training determines the most important layers given their respective weights. In the second stage, we proceed to insert adapters to the most important layers, retaining both performance and neural architecture search efficiency. On the CommonVoice dataset[2] we obtain 20.6% absolute improvement in Word Error Rate (WER) on the Welsh language against the conventional method, which inserts the adapter modules into the highest layers without search. In the SLURP SLU task, our method yields 4.0% intent accuracy improvement against the same conventional baseline."}, {"question": "Who is the author of the LTI paper Speaker-Independent Acoustic-to-Articulatory Speech Inversion published in 2023?", "answer": "Shinji Watanabe", "context": "Author: Shinji Watanabe Title: Speaker-Independent Acoustic-to-Articulatory Speech Inversion Publication year: 2023 Coauthors: Peter Wu, Li-Wei Chen, Cheol Jun Cho, Shinji Watanabe, L. Goldstein, A. Black, G. Anumanchipalli Abstract: To build speech processing methods that can handle speech as naturally as humans, researchers have explored multiple ways of building an invertible mapping from speech to an interpretable space. The articulatory space is a promising inversion target, since this space captures the mechanics of speech production. To this end, we build an acoustic-to-articulatory inversion (AAI) model that leverages autoregression, adversarial training, and self supervision to generalize to unseen speakers. Our approach obtains 0.784 correlation on an electromagnetic articulography (EMA) dataset, improving the state-of-the-art by 12.5%. Additionally, we show the interpretability of these representations through directly com-paring the behavior of estimated representations with speech production behavior. Finally, we propose a resynthesis-based AAI evaluation metric that does not rely on articulatory labels, demonstrating its efficacy with an 18-speaker dataset."}, {"question": "Who is the author of the LTI paper Understanding the Effect of Model Compression on Social Bias in Large Language Models published in 2023?", "answer": "Emma Strubell", "context": "Author: Emma Strubell Title: Understanding the Effect of Model Compression on Social Bias in Large Language Models Publication year: 2023 Coauthors: Gustavo Gon\u00e7alves, Emma Strubell Abstract: Large Language Models (LLMs) trained with self-supervision on vast corpora of web text fit to the social biases of that text. Without intervention, these social biases persist in the model's predictions in downstream tasks, leading to representational harm. Many strategies have been proposed to mitigate the effects of inappropriate social biases learned during pretraining. Simultaneously, methods for model compression have become increasingly popular to reduce the computational burden of LLMs. Despite the popularity and need for both approaches, little work has been done to explore the interplay between these two. We perform a carefully controlled study of the impact of model compression via quantization and knowledge distillation on measures of social bias in LLMs. Longer pretraining and larger models led to higher social bias, and quantization showed a regularizer effect with its best trade-off around 20% of the original pretraining time."}, {"question": "What is the abstract of the paper One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning published by LTI faculty Eric P. Xing in 2023?", "answer": "We present Generalized LoRA (GLoRA), an advanced approach for universal parameter-efficient fine-tuning tasks. Enhancing Low-Rank Adaptation (LoRA), GLoRA employs a generalized prompt module to optimize pre-trained model weights and adjust intermediate activations, providing more flexibility and capability across diverse tasks and datasets. Moreover, GLoRA facilitates efficient parameter adaptation by employing a scalable, modular, layer-wise structure search that learns individual adapter of each layer. Originating from a unified mathematical formulation, GLoRA exhibits strong transfer learning, few-shot learning and domain generalization abilities, as it adapts to new tasks through not only weights but also additional dimensions like activations. Comprehensive experiments demonstrate that GLoRA outperforms all previous methods in natural, specialized, and structured vision benchmarks, achieving superior accuracy with fewer parameters and computations. The proposed method on LLaMA-1 and LLaMA-2 also show considerable enhancements compared to the original LoRA in the language domain. Furthermore, our structural re-parameterization design ensures that GLoRA incurs no extra inference cost, rendering it a practical solution for resource-limited applications. Code and models are available at: https://github.com/Arnav0400/ViT-Slim/tree/master/GLoRA.", "context": "Author: Eric P. Xing Title: One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning Publication year: 2023 Coauthors: Arnav Chavan, Zhuang Liu, D. Gupta, Eric P. Xing, Zhiqiang Shen Abstract: We present Generalized LoRA (GLoRA), an advanced approach for universal parameter-efficient fine-tuning tasks. Enhancing Low-Rank Adaptation (LoRA), GLoRA employs a generalized prompt module to optimize pre-trained model weights and adjust intermediate activations, providing more flexibility and capability across diverse tasks and datasets. Moreover, GLoRA facilitates efficient parameter adaptation by employing a scalable, modular, layer-wise structure search that learns individual adapter of each layer. Originating from a unified mathematical formulation, GLoRA exhibits strong transfer learning, few-shot learning and domain generalization abilities, as it adapts to new tasks through not only weights but also additional dimensions like activations. Comprehensive experiments demonstrate that GLoRA outperforms all previous methods in natural, specialized, and structured vision benchmarks, achieving superior accuracy with fewer parameters and computations. The proposed method on LLaMA-1 and LLaMA-2 also show considerable enhancements compared to the original LoRA in the language domain. Furthermore, our structural re-parameterization design ensures that GLoRA incurs no extra inference cost, rendering it a practical solution for resource-limited applications. Code and models are available at: https://github.com/Arnav0400/ViT-Slim/tree/master/GLoRA."}, {"question": "Who are the coauthors of the paper Structured Pruning of Self-Supervised Pre-Trained Models for Speech Recognition and Understanding published by LTI faculty Shinji Watanabe in 2023?", "answer": "Yifan Peng, Kwangyoun Kim, Felix Wu, Prashant Sridhar, Shinji Watanabe", "context": "Author: Shinji Watanabe Title: Structured Pruning of Self-Supervised Pre-Trained Models for Speech Recognition and Understanding Publication year: 2023 Coauthors: Yifan Peng, Kwangyoun Kim, Felix Wu, Prashant Sridhar, Shinji Watanabe Abstract: Self-supervised speech representation learning (SSL) has shown to be effective in various downstream tasks, but SSL models are usually large and slow. Model compression techniques such as pruning aim to reduce the model size and computation without degradation in accuracy. Prior studies focus on the pruning of Transformers; however, speech models not only utilize a stack of Transformer blocks, but also combine a frontend network based on multiple convolutional layers for low-level feature representation learning. This frontend has a small size but a heavy computational cost. In this work, we propose three task-specific structured pruning methods to deal with such heterogeneous networks. Experiments on LibriSpeech and SLURP show that the proposed method is more accurate than the original wav2vec2-base with 10% to 30% less computation, and is able to reduce the computation by 40% to 50% without any degradation."}, {"question": "What is the abstract of the paper SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents published by LTI faculty Yonatan Bisk in 2023?", "answer": "Humans are social beings; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and interact under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills. These findings demonstrate SOTOPIA's promise as a general platform for research on evaluating and improving social intelligence in artificial agents.", "context": "Author: Yonatan Bisk Title: SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents Publication year: 2023 Coauthors: Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, Maarten Sap Abstract: Humans are social beings; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and interact under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills. These findings demonstrate SOTOPIA's promise as a general platform for research on evaluating and improving social intelligence in artificial agents."}, {"question": "What is the title of the paper published by LTI faculty Daphne Ippolito in 2023?", "answer": "Extracting Training Data from Diffusion Models", "context": "Author: Daphne Ippolito Title: Extracting Training Data from Diffusion Models Publication year: 2023 Coauthors: Nicholas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tram\u00e8r, B. Balle, Daphne Ippolito, Eric Wallace Abstract: Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training."}, {"question": "Who is the author of the LTI paper AutoReply: Detecting Nonsense in Dialogue with Discriminative Replies published in 2023?", "answer": "Daniel Fried", "context": "Author: Daniel Fried Title: AutoReply: Detecting Nonsense in Dialogue with Discriminative Replies Publication year: 2023 Coauthors: Weiyan Shi, Emily Dinan, Adi Renduchintala, Daniel Fried, Athul Paul Jacob, Zhou Yu, Mike Lewis "}, {"question": "Who is the author of the LTI paper A Gold Standard Dataset for the Reviewer Assignment Problem published in 2023?", "answer": "Graham Neubig", "context": "Author: Graham Neubig Title: A Gold Standard Dataset for the Reviewer Assignment Problem Publication year: 2023 Coauthors: Ivan Stelmakh, J. Wieting, Graham Neubig, Nihar B. Shah Abstract: Many peer-review venues are either using or looking to use algorithms to assign submissions to reviewers. The crux of such automated approaches is the notion of the\"similarity score\"--a numerical estimate of the expertise of a reviewer in reviewing a paper--and many algorithms have been proposed to compute these scores. However, these algorithms have not been subjected to a principled comparison, making it difficult for stakeholders to choose the algorithm in an evidence-based manner. The key challenge in comparing existing algorithms and developing better algorithms is the lack of the publicly available gold-standard data that would be needed to perform reproducible research. We address this challenge by collecting a novel dataset of similarity scores that we release to the research community. Our dataset consists of 477 self-reported expertise scores provided by 58 researchers who evaluated their expertise in reviewing papers they have read previously. We use this data to compare several popular algorithms employed in computer science conferences and come up with recommendations for stakeholders. Our main findings are as follows. First, all algorithms make a non-trivial amount of error. For the task of ordering two papers in terms of their relevance for a reviewer, the error rates range from 12%-30% in easy cases to 36%-43% in hard cases, highlighting the vital need for more research on the similarity-computation problem. Second, most existing algorithms are designed to work with titles and abstracts of papers, and in this regime the Specter+MFR algorithm performs best. Third, to improve performance, it may be important to develop modern deep-learning based algorithms that can make use of the full texts of papers: the classical TD-IDF algorithm enhanced with full texts of papers is on par with the deep-learning based Specter+MFR that cannot make use of this information."}, {"question": "Who are the coauthors of the paper Evaluating Multilingual Speech Translation under Realistic Conditions with Resegmentation and Terminology published by LTI faculty Mona T. Diab in 2023?", "answer": "Elizabeth Salesky, Kareem Darwish, Mohamed Al-Badrashiny, Mona T. Diab, J. Niehues", "context": "Author: Mona T. Diab Title: Evaluating Multilingual Speech Translation under Realistic Conditions with Resegmentation and Terminology Publication year: 2023 Coauthors: Elizabeth Salesky, Kareem Darwish, Mohamed Al-Badrashiny, Mona T. Diab, J. Niehues Abstract: We present the ACL 60/60 evaluation sets for multilingual translation of ACL 2022 technical presentations into 10 target languages. This dataset enables further research into multilingual speech translation under realistic recording conditions with unsegmented audio and domain-specific terminology, applying NLP tools to text and speech in the technical domain, and evaluating and improving model robustness to diverse speaker demographics."}, {"question": "Who are the coauthors of the paper Text-Transport: Toward Learning Causal Effects of Natural Language published by LTI faculty Louis-Philippe Morency in 2023?", "answer": "Victoria Lin, Louis-Philippe Morency, Eli Ben-Michael", "context": "Author: Louis-Philippe Morency Title: Text-Transport: Toward Learning Causal Effects of Natural Language Publication year: 2023 Coauthors: Victoria Lin, Louis-Philippe Morency, Eli Ben-Michael Abstract: As language technologies gain prominence in real-world settings, it is important to understand how changes to language affect reader perceptions. This can be formalized as the causal effect of varying a linguistic attribute (e.g., sentiment) on a reader's response to the text. In this paper, we introduce Text-Transport, a method for estimation of causal effects from natural language under any text distribution. Current approaches for valid causal effect estimation require strong assumptions about the data, meaning the data from which one can estimate valid causal effects often is not representative of the actual target domain of interest. To address this issue, we leverage the notion of distribution shift to describe an estimator that transports causal effects between domains, bypassing the need for strong assumptions in the target domain. We derive statistical guarantees on the uncertainty of this estimator, and we report empirical results and analyses that support the validity of Text-Transport across data settings. Finally, we use Text-Transport to study a realistic setting--hate speech on social media--in which causal effects do shift significantly between text domains, demonstrating the necessity of transport when conducting causal inference on natural language."}, {"question": "What is the title of the paper published by LTI faculty Graham Neubig in 2023?", "answer": "DeMuX: Data-efficient Multilingual Learning", "context": "Author: Graham Neubig Title: DeMuX: Data-efficient Multilingual Learning Publication year: 2023 Coauthors: Simran Khanuja, Srinivas Gowriraj, L. Dery, Graham Neubig Abstract: We consider the task of optimally fine-tuning pre-trained multilingual models, given small amounts of unlabelled target data and an annotation budget. In this paper, we introduce DEMUX, a framework that prescribes the exact data-points to label from vast amounts of unlabelled multilingual data, having unknown degrees of overlap with the target set. Unlike most prior works, our end-to-end framework is language-agnostic, accounts for model representations, and supports multilingual target configurations. Our active learning strategies rely upon distance and uncertainty measures to select task-specific neighbors that are most informative to label, given a model. DeMuX outperforms strong baselines in 84% of the test cases, in the zero-shot setting of disjoint source and target language sets (including multilingual target pools), across three models and four tasks. Notably, in low-budget settings (5-100 examples), we observe gains of up to 8-11 F1 points for token-level tasks, and 2-5 F1 for complex tasks. Our code is released here: https://github.com/simran-khanuja/demux."}, {"question": "Who is the author of the LTI paper Distributionally Robust Unsupervised Dense Retrieval Training on Web Graphs published in 2023?", "answer": "Chenyan Xiong", "context": "Author: Chenyan Xiong Title: Distributionally Robust Unsupervised Dense Retrieval Training on Web Graphs Publication year: 2023 Coauthors: Peixuan Han, Zhenghao Liu, Zhiyuan Liu, Chenyan Xiong Abstract: This paper introduces Web-DRO, an unsupervised dense retrieval model, which clusters documents based on web structures and reweights the groups during contrastive training. Specifically, we first leverage web graph links and contrastively train an embedding model for clustering anchor-document pairs. Then we use Group Distributional Robust Optimization to reweight different clusters of anchor-document pairs, which guides the model to assign more weights to the group with higher contrastive loss and pay more attention to the worst case during training. Our experiments on MS MARCO and BEIR show that our model, Web-DRO, significantly improves the retrieval effectiveness in unsupervised scenarios. A comparison of clustering techniques shows that training on the web graph combining URL information reaches optimal performance on clustering. Further analysis confirms that group weights are stable and valid, indicating consistent model preferences as well as effective up-weighting of valuable groups and down-weighting of uninformative ones. The code of this paper can be obtained from https://github.com/OpenMatch/Web-DRO."}, {"question": "Which LTI faculty published the paper Are aligned neural networks adversarially aligned? in 2023?", "answer": "Daphne Ippolito", "context": "Author: Daphne Ippolito Title: Are aligned neural networks adversarially aligned? Publication year: 2023 Coauthors: Nicholas Carlini, Milad Nasr, Christopher A. Choquette-Choo, Matthew Jagielski, Irena Gao, Anas Awadalla, Pang Wei Koh, Daphne Ippolito, Katherine Lee, Florian Tram\u00e8r, Ludwig Schmidt Abstract: Large language models are now tuned to align with the goals of their creators, namely to be\"helpful and harmless.\"These models should respond helpfully to user questions, but refuse to answer requests that could cause harm. However, adversarial users can construct inputs which circumvent attempts at alignment. In this work, we study to what extent these models remain aligned, even when interacting with an adversarial user who constructs worst-case inputs (adversarial examples). These inputs are designed to cause the model to emit harmful content that would otherwise be prohibited. We show that existing NLP-based optimization attacks are insufficiently powerful to reliably attack aligned text models: even when current NLP-based attacks fail, we can find adversarial inputs with brute force. As a result, the failure of current attacks should not be seen as proof that aligned text models remain aligned under adversarial inputs. However the recent trend in large-scale ML models is multimodal models that allow users to provide images that influence the text that is generated. We show these models can be easily attacked, i.e., induced to perform arbitrary un-aligned behavior through adversarial perturbation of the input image. We conjecture that improved NLP attacks may demonstrate this same level of adversarial control over text-only models."}, {"question": "Who are the coauthors of the paper Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large Language Models published by LTI faculty Eric Xing in 2024?", "answer": "Loka Li, Guan-Hong Chen, Yusheng Su, Zhenhao Chen, Yixuan Zhang, Eric Xing, Kun Zhang", "context": "Author: Eric Xing Title: Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large Language Models Publication year: 2024 Coauthors: Loka Li, Guan-Hong Chen, Yusheng Su, Zhenhao Chen, Yixuan Zhang, Eric Xing, Kun Zhang Abstract: The recent success of Large Language Models (LLMs) has catalyzed an increasing interest in their self-correction capabilities. This paper presents a comprehensive investigation into the intrinsic self-correction of LLMs, attempting to address the ongoing debate about its feasibility. Our research has identified an important latent factor - the ``confidence'' of LLMs - during the self-correction process. Overlooking this factor may cause the models to over-criticize themselves, resulting in unreliable conclusions regarding the efficacy of self-correction. We have experimentally observed that LLMs possess the capability to understand the ``confidence'' in their own responses. It motivates us to develop an ``If-or-Else'' (IoE) prompting framework, designed to guide LLMs in assessing their own ``confidence'', facilitating intrinsic self-corrections. We conduct extensive experiments and demonstrate that our IoE-based Prompt can achieve a consistent improvement regarding the accuracy of self-corrected responses over the initial answers. Our study not only sheds light on the underlying factors affecting self-correction in LLMs, but also introduces a practical framework that utilizes the IoE prompting principle to efficiently improve self-correction capabilities with ``confidence''. The code is available at \\url{https://github.com/MBZUAI-CLeaR/IoE-Prompting.git}."}, {"question": "Which LTI faculty published the paper Convoifilter: A case study of doing cocktail party speech recognition in 2023?", "answer": "A. Waibel", "context": "Author: A. Waibel Title: Convoifilter: A case study of doing cocktail party speech recognition Publication year: 2023 Coauthors: T. Nguyen, A. Waibel Abstract: This paper presents an end-to-end model designed to improve automatic speech recognition (ASR) for a particular speaker in a crowded, noisy environment. The model utilizes a single-channel speech enhancement module that isolates the speaker's voice from background noise (ConVoiFilter) and an ASR module. The model can decrease ASR's word error rate (WER) from 80% to 26.4% through this approach. Typically, these two components are adjusted independently due to variations in data requirements. However, speech enhancement can create anomalies that decrease ASR efficiency. By implementing a joint fine-tuning strategy, the model can reduce the WER from 26.4% in separate tuning to 14.5% in joint tuning. We openly share our pre-trained model to foster further research hf.co/nguyenvulebinh/voice-filter."}, {"question": "What is the title of the paper published by LTI faculty Yiming Yang in 2023?", "answer": "Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers", "context": "Author: Yiming Yang Title: Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers Publication year: 2023 Coauthors: K. Choromanski, Shanda Li, Valerii Likhosherstov, Kumar Avinava Dubey, Shengjie Luo, Di He, Yiming Yang, Tam\u00e1s Sarl\u00f3s, Thomas Weingarten, Adrian Weller Abstract: We propose a new class of linear Transformers called FourierLearner-Transformers (FLTs), which incorporate a wide range of relative positional encoding mechanisms (RPEs). These include regular RPE techniques applied for nongeometric data, as well as novel RPEs operating on the sequences of tokens embedded in higher-dimensional Euclidean spaces (e.g. point clouds). FLTs construct the optimal RPE mechanism implicitly by learning its spectral representation. As opposed to other architectures combining efficient low-rank linear attention with RPEs, FLTs remain practical in terms of their memory usage and do not require additional assumptions about the structure of the RPE-mask. FLTs allow also for applying certain structural inductive bias techniques to specify masking strategies, e.g. they provide a way to learn the so-called local RPEs introduced in this paper and providing accuracy gains as compared with several other linear Transformers for language modeling. We also thoroughly tested FLTs on other data modalities and tasks, such as: image classification and 3D molecular modeling. For 3D-data FLTs are, to the best of our knowledge, the first Transformers architectures providing RPE-enhanced linear attention."}, {"question": "Who is the author of the LTI paper Unsupervised Dense Retrieval Training with Web Anchors published in 2023?", "answer": "Chenyan Xiong", "context": "Author: Chenyan Xiong Title: Unsupervised Dense Retrieval Training with Web Anchors Publication year: 2023 Coauthors: Yiqing Xie, X. Liu, Chenyan Xiong Abstract: In this work, we present an unsupervised retrieval method with contrastive learning on web anchors. The anchor text describes the content that is referenced from the linked page. This shows similarities to search queries that aim to retrieve pertinent information from relevant documents. Based on their commonalities, we train an unsupervised dense retriever, Anchor-DR, with a contrastive learning task that matches the anchor text and the linked document. To filter out uninformative anchors (such as \"homepage\" or other functional anchors), we present a novel filtering technique to only select anchors that contain similar types of information as search queries. Experiments show that Anchor-DR outperforms state-of-the-art methods on unsupervised dense retrieval by a large margin (e.g., by 5.3% NDCG@10 on MSMARCO). The gain of our method is especially significant for search and question answering tasks. Our analysis further reveals that the pattern of anchor-document pairs is similar to that of search query-document pairs. Code available at https://github.com/Veronicium/AnchorDR."}, {"question": "Who is the author of the LTI paper Faith and Fate: Limits of Transformers on Compositionality published in 2023?", "answer": "S. Welleck", "context": "Author: S. Welleck Title: Faith and Fate: Limits of Transformers on Compositionality Publication year: 2023 Coauthors: Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jian, Bill Yuchen Lin, Peter West, Chandra Bhagavatula, Ronan Le Bras, Jena D. Hwang, Soumya Sanyal, S. Welleck, Xiang Ren, Allyson Ettinger, Za\u00efd Harchaoui, Yejin Choi Abstract: Transformer large language models (LLMs) have sparked admiration for their exceptional performance on tasks that demand intricate multi-step reasoning. Yet, these models simultaneously show failures on surprisingly trivial problems. This begs the question: Are these errors incidental, or do they signal more substantial limitations? In an attempt to demystify transformer LLMs, we investigate the limits of these models across three representative compositional tasks -- multi-digit multiplication, logic grid puzzles, and a classic dynamic programming problem. These tasks require breaking problems down into sub-steps and synthesizing these steps into a precise answer. We formulate compositional tasks as computation graphs to systematically quantify the level of complexity, and break down reasoning steps into intermediate sub-procedures. Our empirical findings suggest that transformer LLMs solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, without necessarily developing systematic problem-solving skills. To round off our empirical study, we provide theoretical arguments on abstract multi-step reasoning problems that highlight how autoregressive generations' performance can rapidly decay with\\,increased\\,task\\,complexity."}, {"question": "Which LTI faculty published the paper Active Retrieval Augmented Generation in 2023?", "answer": "Graham Neubig", "context": "Author: Graham Neubig Title: Active Retrieval Augmented Generation Publication year: 2023 Coauthors: Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig Abstract: Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE."}, {"question": "Who are the coauthors of the paper The DARPA Wikidata Overlay: Wikidata as an ontology for natural language processing published by LTI faculty A. Gershman in 2023?", "answer": "Elizabeth Spaulding, Kathryn Conger, A. Gershman, Rosario Uceda-Sosa, S. Brown, James Pustejovsky, Peter Anick, Martha Palmer", "context": "Author: A. Gershman Title: The DARPA Wikidata Overlay: Wikidata as an ontology for natural language processing Publication year: 2023 Coauthors: Elizabeth Spaulding, Kathryn Conger, A. Gershman, Rosario Uceda-Sosa, S. Brown, James Pustejovsky, Peter Anick, Martha Palmer Abstract: With 102,530,067 items currently in its crowd-sourced knowledge base, Wikidata provides NLP practitioners a unique and powerful resource for inference and reasoning over real-world entities. However, because Wikidata is very entity focused, events and actions are often labeled with eventive nouns (e.g., the process of diagnosing a person\u2019s illness is labeled \u201cdiagnosis\u201d), and the typical participants in an event are not described or linked to that event concept (e.g., the medical professional or patient). Motivated by a need for an adaptable, comprehensive, domain-flexible ontology for information extraction, including identifying the roles entities are playing in an event, we present a curated subset of Wikidata in which events have been enriched with PropBank roles. To enable richer narrative understanding between events from Wikidata concepts, we have also provided a comprehensive mapping from temporal Qnodes and Pnodes to the Allen Interval Temporal Logic relations."}, {"question": "Who are the coauthors of the paper Robust Cross-Domain Pseudo-Labeling and Contrastive Learning for Unsupervised Domain Adaptation NIR-VIS Face Recognition published by LTI faculty Yiming Yang in 2023?", "answer": "Yiming Yang, Weipeng Hu, Haiqi Lin, Haifeng Hu", "context": "Author: Yiming Yang Title: Robust Cross-Domain Pseudo-Labeling and Contrastive Learning for Unsupervised Domain Adaptation NIR-VIS Face Recognition Publication year: 2023 Coauthors: Yiming Yang, Weipeng Hu, Haiqi Lin, Haifeng Hu Abstract: Near-infrared and visible face recognition (NIR-VIS) is attracting increasing attention because of the need to achieve face recognition in low-light conditions to enable 24-hour secure retrieval. However, annotating identity labels for a large number of heterogeneous face images is time-consuming and expensive, which limits the application of the NIR-VIS face recognition system to larger scale real-world scenarios. In this paper, we attempt to achieve NIR-VIS face recognition in an unsupervised domain adaptation manner. To get rid of the reliance on manual annotations, we propose a novel Robust cross-domain Pseudo-labeling and Contrastive learning (RPC) network which consists of three key components, i.e., NIR cluster-based Pseudo labels Sharing (NPS), Domain-specific cluster Contrastive Learning (DCL) and Inter-domain cluster Contrastive Learning (ICL). Firstly, NPS is presented to generate pseudo labels by exploring robust NIR clusters and sharing reliable label knowledge with VIS domain. Secondly, DCL is designed to learn intra-domain compact yet discriminative representations. Finally, ICL dynamically combines and refines intrinsic identity relationships to guide the instance-level features to learn robust and domain-independent representations. Extensive experiments are conducted to verify an accuracy of over 99% in pseudo label assignment and the advanced performance of RPC network on four mainstream NIR-VIS datasets."}, {"question": "Who are the coauthors of the paper PAM: Prompting Audio-Language Models for Audio Quality Assessment published by LTI faculty Rita Singh in 2024?", "answer": "Soham Deshmukh, Dareen Alharthi, Benjamin Elizalde, Hannes Gamper, Mahmoud Al Ismail, Rita Singh, Bhiksha Raj, Huaming Wang", "context": "Author: Rita Singh Title: PAM: Prompting Audio-Language Models for Audio Quality Assessment Publication year: 2024 Coauthors: Soham Deshmukh, Dareen Alharthi, Benjamin Elizalde, Hannes Gamper, Mahmoud Al Ismail, Rita Singh, Bhiksha Raj, Huaming Wang Abstract: While audio quality is a key performance metric for various audio processing tasks, including generative modeling, its objective measurement remains a challenge. Audio-Language Models (ALMs) are pre-trained on audio-text pairs that may contain information about audio quality, the presence of artifacts, or noise. Given an audio input and a text prompt related to quality, an ALM can be used to calculate a similarity score between the two. Here, we exploit this capability and introduce PAM, a no-reference metric for assessing audio quality for different audio processing tasks. Contrary to other\"reference-free\"metrics, PAM does not require computing embeddings on a reference dataset nor training a task-specific model on a costly set of human listening scores. We extensively evaluate the reliability of PAM against established metrics and human listening scores on four tasks: text-to-audio (TTA), text-to-music generation (TTM), text-to-speech (TTS), and deep noise suppression (DNS). We perform multiple ablation studies with controlled distortions, in-the-wild setups, and prompt choices. Our evaluation shows that PAM correlates well with existing metrics and human listening scores. These results demonstrate the potential of ALMs for computing a general-purpose audio quality metric."}, {"question": "What is the title of the paper published by LTI faculty Eric P. Xing in 2023?", "answer": "Making Scalable Meta Learning Practical", "context": "Author: Eric P. Xing Title: Making Scalable Meta Learning Practical Publication year: 2023 Coauthors: Sang Keun Choe, Sanket Vaibhav Mehta, Hwijeen Ahn, W. Neiswanger, Pengtao Xie, Emma Strubell, Eric P. Xing Abstract: Despite its flexibility to learn diverse inductive biases in machine learning programs, meta learning (i.e., learning to learn) has long been recognized to suffer from poor scalability due to its tremendous compute/memory costs, training instability, and a lack of efficient distributed training support. In this work, we focus on making scalable meta learning practical by introducing SAMA, which combines advances in both implicit differentiation algorithms and systems. Specifically, SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients. Evaluated on multiple large-scale meta learning benchmarks, SAMA showcases up to 1.7/4.8x increase in throughput and 2.0/3.8x decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms. Furthermore, we show that SAMA-based data optimization leads to consistent improvements in text classification accuracy with BERT and RoBERTa large language models, and achieves state-of-the-art results in both small- and large-scale data pruning on image classification tasks, demonstrating the practical applicability of scalable meta learning across language and vision domains."}, {"question": "What is the title of the paper published by LTI faculty Emma Strubell in 2023?", "answer": "Data-efficient Active Learning for Structured Prediction with Partial Annotation and Self-Training", "context": "Author: Emma Strubell Title: Data-efficient Active Learning for Structured Prediction with Partial Annotation and Self-Training Publication year: 2023 Coauthors: Zhisong Zhang, Emma Strubell, E. Hovy Abstract: In this work we propose a pragmatic method that reduces the annotation cost for structured label spaces using active learning. Our approach leverages partial annotation, which reduces labeling costs for structured outputs by selecting only the most informative sub-structures for annotation. We also utilize self-training to incorporate the current model's automatic predictions as pseudo-labels for un-annotated sub-structures. A key challenge in effectively combining partial annotation with self-training to reduce annotation cost is determining which sub-structures to select to label. To address this challenge, we adopt an error estimator to adaptively decide the partial selection ratio according to the current model's capability. In evaluations spanning four structured prediction tasks, we show that our combination of partial annotation and self-training using an adaptive selection ratio reduces annotation cost over strong full annotation baselines under a fair comparison scheme that takes reading time into consideration."}, {"question": "Which LTI faculty published the paper Improving Massively Multilingual ASR with Auxiliary CTC Objectives in 2023?", "answer": "Shinji Watanabe", "context": "Author: Shinji Watanabe Title: Improving Massively Multilingual ASR with Auxiliary CTC Objectives Publication year: 2023 Coauthors: William Chen, Brian Yan, Jiatong Shi, Yifan Peng, Soumi Maiti, Shinji Watanabe Abstract: Multilingual Automatic Speech Recognition (ASR) models have extended the usability of speech technologies to a wide variety of languages. With how many languages these models have to handle, however, a key to understanding their imbalanced performance across different languages is to examine if the model actually knows which language it should transcribe. In this paper, we introduce our work on improving performance on FLEURS, a 102-language open ASR benchmark, by conditioning the entire model on language identity (LID). We investigate techniques inspired from recent Connectionist Temporal Classification (CTC) studies to help the model handle the large number of languages, conditioning on the LID predictions of auxiliary tasks. Our experimental results demonstrate the effectiveness of our technique over standard CTC/Attention-based hybrid models. Furthermore, our state-of-the-art systems using self-supervised models with the Conformer architecture improve over the results of prior work on FLEURS by a relative 28.4% CER. Trained models are reproducible recipes are available at https://github.com/espnet/espnet/tree/master/egs2/fleurs/asr1."}, {"question": "Who are the coauthors of the paper LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning published by LTI faculty Eric P. Xing in 2023?", "answer": "Han Guo, P. Greengard, Eric P. Xing, Yoon Kim", "context": "Author: Eric P. Xing Title: LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning Publication year: 2023 Coauthors: Han Guo, P. Greengard, Eric P. Xing, Yoon Kim Abstract: We propose a simple approach for memory-efficient adaptation of pretrained language models. Our approach uses an iterative algorithm to decompose each pretrained matrix into a high-precision low-rank component and a memory-efficient quantized component. During finetuning, the quantized component remains fixed and only the low-rank component is updated. We present an integer linear programming formulation of the quantization component which enables dynamic configuration of quantization parameters (e.g., bit-width, block size) for each matrix given an overall target memory budget. We further explore a data-aware version of the algorithm which uses an approximation of the Fisher information matrix to weight the reconstruction objective during matrix decomposition. Experiments on finetuning RoBERTa and LLaMA-2 (7B and 70B) demonstrate that our low-rank plus quantized matrix decomposition approach (LQ-LoRA) outperforms strong QLoRA and GPTQ-LoRA baselines and enables aggressive quantization to sub-3 bits with only minor performance degradations. When finetuned on a language modeling calibration dataset, LQ-LoRA can also be used for model compression; in this setting our 2.75-bit LLaMA-2-70B model (which has 2.85 bits on average when including the low-rank components and requires 27GB of GPU memory) performs respectably compared to the 16-bit baseline."}, {"question": "What is the abstract of the paper Pengi: An Audio Language Model for Audio Tasks published by LTI faculty Rita Singh in 2023?", "answer": "In the domain of audio processing, Transfer Learning has facilitated the rise of Self-Supervised Learning and Zero-Shot Learning techniques. These approaches have led to the development of versatile models capable of tackling a wide array of tasks, while delivering state-of-the-art performance. However, current models inherently lack the capacity to produce the requisite language for open-ended tasks, such as Audio Captioning or Audio Question&Answering. We introduce Pengi, a novel Audio Language Model that leverages Transfer Learning by framing all audio tasks as text-generation tasks. It takes as input, an audio recording, and text, and generates free-form text as output. The input audio is represented as a sequence of continuous embeddings by an audio encoder. A text encoder does the same for the corresponding text input. Both sequences are combined as a prefix to prompt a pre-trained frozen language model. The unified architecture of Pengi enables open-ended tasks and close-ended tasks without any additional fine-tuning or task-specific extensions. When evaluated on 22 downstream tasks, our approach yields state-of-the-art performance in several of them. Our results show that connecting language models with audio models is a major step towards general-purpose audio understanding", "context": "Author: Rita Singh Title: Pengi: An Audio Language Model for Audio Tasks Publication year: 2023 Coauthors: Soham Deshmukh, Benjamin Elizalde, Rita Singh, Huaming Wang Abstract: In the domain of audio processing, Transfer Learning has facilitated the rise of Self-Supervised Learning and Zero-Shot Learning techniques. These approaches have led to the development of versatile models capable of tackling a wide array of tasks, while delivering state-of-the-art performance. However, current models inherently lack the capacity to produce the requisite language for open-ended tasks, such as Audio Captioning or Audio Question&Answering. We introduce Pengi, a novel Audio Language Model that leverages Transfer Learning by framing all audio tasks as text-generation tasks. It takes as input, an audio recording, and text, and generates free-form text as output. The input audio is represented as a sequence of continuous embeddings by an audio encoder. A text encoder does the same for the corresponding text input. Both sequences are combined as a prefix to prompt a pre-trained frozen language model. The unified architecture of Pengi enables open-ended tasks and close-ended tasks without any additional fine-tuning or task-specific extensions. When evaluated on 22 downstream tasks, our approach yields state-of-the-art performance in several of them. Our results show that connecting language models with audio models is a major step towards general-purpose audio understanding"}, {"question": "Who are the coauthors of the paper Distributionally Robust Unsupervised Dense Retrieval Training on Web Graphs published by LTI faculty Chenyan Xiong in 2023?", "answer": "Peixuan Han, Zhenghao Liu, Zhiyuan Liu, Chenyan Xiong", "context": "Author: Chenyan Xiong Title: Distributionally Robust Unsupervised Dense Retrieval Training on Web Graphs Publication year: 2023 Coauthors: Peixuan Han, Zhenghao Liu, Zhiyuan Liu, Chenyan Xiong Abstract: This paper introduces Web-DRO, an unsupervised dense retrieval model, which clusters documents based on web structures and reweights the groups during contrastive training. Specifically, we first leverage web graph links and contrastively train an embedding model for clustering anchor-document pairs. Then we use Group Distributional Robust Optimization to reweight different clusters of anchor-document pairs, which guides the model to assign more weights to the group with higher contrastive loss and pay more attention to the worst case during training. Our experiments on MS MARCO and BEIR show that our model, Web-DRO, significantly improves the retrieval effectiveness in unsupervised scenarios. A comparison of clustering techniques shows that training on the web graph combining URL information reaches optimal performance on clustering. Further analysis confirms that group weights are stable and valid, indicating consistent model preferences as well as effective up-weighting of valuable groups and down-weighting of uninformative ones. The code of this paper can be obtained from https://github.com/OpenMatch/Web-DRO."}, {"question": "Who is the author of the LTI paper Asking More Informative Questions for Grounded Retrieval published in 2023?", "answer": "Daniel Fried", "context": "Author: Daniel Fried Title: Asking More Informative Questions for Grounded Retrieval Publication year: 2023 Coauthors: Sedrick Scott Keh, Justin T. Chiu, Daniel Fried Abstract: When a model is trying to gather information in an interactive setting, it benefits from asking informative questions. However, in the case of a grounded multi-turn image identification task, previous studies have been constrained to polar yes/no questions, limiting how much information the model can gain in a single turn. We present an approach that formulates more informative, open-ended questions. In doing so, we discover that off-the-shelf visual question answering (VQA) models often make presupposition errors, which standard information gain question selection methods fail to account for. To address this issue, we propose a method that can incorporate presupposition handling into both question selection and belief updates. Specifically, we use a two-stage process, where the model first filters out images which are irrelevant to a given question, then updates its beliefs about which image the user intends. Through self-play and human evaluations, we show that our method is successful in asking informative open-ended questions, increasing accuracy over the past state-of-the-art by 14%, while resulting in 48% more efficient games in human evaluations."}, {"question": "Who is the author of the LTI paper Tensor decomposition for minimization of E2E SLU model toward on-device processing published in 2023?", "answer": "Shinji Watanabe", "context": "Author: Shinji Watanabe Title: Tensor decomposition for minimization of E2E SLU model toward on-device processing Publication year: 2023 Coauthors: Yosuke Kashiwagi, Siddhant Arora, Hayato Futami, Jessica Huynh, Shih-Lun Wu, Yifan Peng, Brian Yan, E. Tsunoo, Shinji Watanabe Abstract: Spoken Language Understanding (SLU) is a critical speech recognition application and is often deployed on edge devices. Consequently, on-device processing plays a significant role in the practical implementation of SLU. This paper focuses on the end-to-end (E2E) SLU model due to its small latency property, unlike a cascade system, and aims to minimize the computational cost. We reduce the model size by applying tensor decomposition to the Conformer and E-Branchformer architectures used in our E2E SLU models. We propose to apply singular value decomposition to linear layers and the Tucker decomposition to convolution layers, respectively. We also compare COMP/PARFAC decomposition and Tensor-Train decomposition to the Tucker decomposition. Since the E2E model is represented by a single neural network, our tensor decomposition can flexibly control the number of parameters without changing feature dimensions. On the STOP dataset, we achieved 70.9% exact match accuracy under the tight constraint of only 15 million parameters."}, {"question": "Who are the coauthors of the paper Transformed Protoform Reconstruction published by LTI faculty David R. Mortensen in 2023?", "answer": "Young Min Kim, Kalvin Chang, Chenxuan Cui, David R. Mortensen", "context": "Author: David R. Mortensen Title: Transformed Protoform Reconstruction Publication year: 2023 Coauthors: Young Min Kim, Kalvin Chang, Chenxuan Cui, David R. Mortensen Abstract: Protoform reconstruction is the task of inferring what morphemes or words appeared like in the ancestral languages of a set of daughter languages. Meloni et al (2021) achieved the state-of-the-art on Latin protoform reconstruction with an RNN-based encoder-decoder with attention model. We update their model with the state-of-the-art seq2seq model: the Transformer. Our model outperforms their model on a suite of different metrics on two different datasets: their Romance data of 8,000 cognates spanning 5 languages and a Chinese dataset (Hou 2004) of 800+ cognates spanning 39 varieties. We also probe our model for potential phylogenetic signal contained in the model. Our code is publicly available at https://github.com/cmu-llab/acl-2023."}, {"question": "What is the title of the paper published by LTI faculty A. Waibel in 2023?", "answer": "Audio-driven Talking Face Generation by Overcoming Unintended Information Flow", "context": "Author: A. Waibel Title: Audio-driven Talking Face Generation by Overcoming Unintended Information Flow Publication year: 2023 Coauthors: Dogucan Yaman, Fevziye Irem Eyiokur, Leonard Barmann, H. K. Ekenel, A. Waibel Abstract: Audio-driven talking face generation is the task of creating a lip-synchronized, realistic face video from given audio and reference frames. This involves two major challenges: overall visual quality of generated images on the one hand, and audio-visual synchronization of the mouth part on the other hand. In this paper, we start by identifying several problematic aspects of synchronization methods in recent audio-driven talking face generation approaches. Specifically, this involves unintended flow of lip, pose and other information from the reference to the generated image, as well as instabilities during model training. Subsequently, we propose various techniques for obviating these issues: First, a silent-lip reference image generator prevents leaking of lips from the reference to the generated image. Second, an adaptive triplet loss handles the pose leaking problem. Finally, we propose a stabilized formulation of synchronization loss, circumventing aforementioned training instabilities while additionally further alleviating the lip leaking issue. Combining the individual improvements, we present state-of-the-art visual quality and synchronization performance on LRS2 in five out of seven and LRW in six out of seven metrics, and competitive results on the remaining ones. We further validate our design in various ablation experiments, confirming the individual contributions as well as their complementary effects."}, {"question": "Who is the author of the LTI paper Learning to Ask Questions for Zero-shot Dialogue State Tracking published in 2023?", "answer": "Alexander I. Rudnicky", "context": "Author: Alexander I. Rudnicky Title: Learning to Ask Questions for Zero-shot Dialogue State Tracking Publication year: 2023 Coauthors: Diogo Tavares, David Semedo, Alexander I. Rudnicky, Jo\u00e3o Magalh\u00e3es Abstract: We present a method for performing zero-shot Dialogue State Tracking (DST) by casting the task as a learning-to-ask-questions framework. The framework learns to pair the best question generation (QG) strategy with in-domain question answering (QA) methods to extract slot values from a dialogue without any human intervention. A novel self-supervised QA pretraining step using in-domain data is essential to learn the structure without requiring any slot-filling annotations. Moreover, we show that QG methods need to be aligned with the same grammatical person used in the dialogue. Empirical evaluation on the MultiWOZ 2.1 dataset demonstrates that our approach, when used alongside robust QA models, outperforms existing zero-shot methods in the challenging task of zero-shot cross domain adaptation-given a comparable amount of domain knowledge during data creation. Finally, we analyze the impact of the types of questions used, and demonstrate that the algorithmic approach outperforms template-based question generation."}, {"question": "What is the abstract of the paper The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation published by LTI faculty Graham Neubig in 2023?", "answer": "Automatic evaluation of machine translation (MT) is a critical tool driving the rapid iterative development of MT systems. While considerable progress has been made on estimating a single scalar quality score, current metrics lack the informativeness of more detailed schemes that annotate individual errors, such as Multidimensional Quality Metrics (MQM). In this paper, we help fill this gap by proposing AutoMQM, a prompting technique which leverages the reasoning and in-context learning capabilities of large language models (LLMs) and asks them to identify and categorize errors in translations. We start by evaluating recent LLMs, such as PaLM and PaLM-2, through simple score prediction prompting, and we study the impact of labeled data through in-context learning and finetuning. We then evaluate AutoMQM with PaLM-2 models, and we find that it improves performance compared to just prompting for scores (with particularly large gains for larger models) while providing interpretability through error spans that align with human annotations.", "context": "Author: Graham Neubig Title: The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation Publication year: 2023 Coauthors: Patrick Fernandes, Daniel Deutsch, M. Finkelstein, Parker Riley, Andr\u00e9 F. T. Martins, Graham Neubig, Ankush Garg, J. Clark, Markus Freitag, Orhan Firat Abstract: Automatic evaluation of machine translation (MT) is a critical tool driving the rapid iterative development of MT systems. While considerable progress has been made on estimating a single scalar quality score, current metrics lack the informativeness of more detailed schemes that annotate individual errors, such as Multidimensional Quality Metrics (MQM). In this paper, we help fill this gap by proposing AutoMQM, a prompting technique which leverages the reasoning and in-context learning capabilities of large language models (LLMs) and asks them to identify and categorize errors in translations. We start by evaluating recent LLMs, such as PaLM and PaLM-2, through simple score prediction prompting, and we study the impact of labeled data through in-context learning and finetuning. We then evaluate AutoMQM with PaLM-2 models, and we find that it improves performance compared to just prompting for scores (with particularly large gains for larger models) while providing interpretability through error spans that align with human annotations."}, {"question": "What is the title of the paper published by LTI faculty Shinji Watanabe in 2023?", "answer": "Multi-Channel Speaker Extraction with Adversarial Training: The Wavlab Submission to The Clarity ICASSP 2023 Grand Challenge", "context": "Author: Shinji Watanabe Title: Multi-Channel Speaker Extraction with Adversarial Training: The Wavlab Submission to The Clarity ICASSP 2023 Grand Challenge Publication year: 2023 Coauthors: Samuele Cornell, Zhongqiu Wang, Yoshiki Masuyama, Shinji Watanabe, Manuel Pariente, Nobutaka Ono, S. Squartini Abstract: In this work we detail our submission to the Clarity ICASSP 2023 grand challenge, in which participants have to develop a strong target speech enhancement system for hearing-aid (HA) devices in noisy-reverberant environments. Our system builds on our previous submission at the Second Clarity Enhancement Challenge (CEC2): iNeuBe-X, which consists in an iterative neural/conventional beamforming enhancement pipeline, guided by an enrollment utterance from the target speaker. This model, which won by a large margin the CEC2, is an extension of the state-of-the-art TF-GridNet model for multi-channel, streamable target-speaker speech enhancement. Here, this approach is extended and further improved by leveraging generative adversarial training, which we show proves especially useful when the training data is limited. Using only the official 6k training scenes data, our best model achieves 0.80 hearing-aid speech perception index (HASPI) and 0.41 hearing-aid speech quality index (HASQI) scores on the synthetic evaluation set. However, our model generalized poorly on the semi-real evaluation set. This highlights the fact that our community should focus more on real-world evaluation and less on fully synthetic datasets."}, {"question": "What is the title of the paper published by LTI faculty Shinji Watanabe in 2023?", "answer": "Multilingual TTS Accent Impressions for Accented ASR", "context": "Author: Shinji Watanabe Title: Multilingual TTS Accent Impressions for Accented ASR Publication year: 2023 Coauthors: G. Karakasidis, Nathaniel R. Robinson, Yaroslav Getman, Atieno Ogayo, Ragheb Al-Ghezi, Ananya Ayasi, Shinji Watanabe, David R. Mortensen, M. Kurimo "}, {"question": "Who is the author of the LTI paper Completing Visual Objects via Bridging Generation and Segmentation published in 2023?", "answer": "Rita Singh", "context": "Author: Rita Singh Title: Completing Visual Objects via Bridging Generation and Segmentation Publication year: 2023 Coauthors: Xiang Li, Yinpeng Chen, Chung-Ching Lin, Rita Singh, Bhiksha Raj, Zicheng Liu Abstract: This paper presents a novel approach to object completion, with the primary goal of reconstructing a complete object from its partially visible components. Our method, named MaskComp, delineates the completion process through iterative stages of generation and segmentation. In each iteration, the object mask is provided as an additional condition to boost image generation, and, in return, the generated images can lead to a more accurate mask by fusing the segmentation of images. We demonstrate that the combination of one generation and one segmentation stage effectively functions as a mask denoiser. Through alternation between the generation and segmentation stages, the partial object mask is progressively refined, providing precise shape guidance and yielding superior object completion results. Our experiments demonstrate the superiority of MaskComp over existing approaches, e.g., ControlNet and Stable Diffusion, establishing it as an effective solution for object completion."}, {"question": "Who are the coauthors of the paper An In-depth Look at Gemini's Language Abilities published by LTI faculty Graham Neubig in 2023?", "answer": "Syeda Nahida Akter, Zichun Yu, Aashiq Muhamed, Tianyue Ou, Alex Bauerle, \u00c1ngel Alexander Cabrera, Krish Dholakia, Chenyan Xiong, Graham Neubig", "context": "Author: Graham Neubig Title: An In-depth Look at Gemini's Language Abilities Publication year: 2023 Coauthors: Syeda Nahida Akter, Zichun Yu, Aashiq Muhamed, Tianyue Ou, Alex Bauerle, \u00c1ngel Alexander Cabrera, Krish Dholakia, Chenyan Xiong, Graham Neubig Abstract: The recently released Google Gemini class of models are the first to comprehensively report results that rival the OpenAI GPT series across a wide variety of tasks. In this paper, we do an in-depth exploration of Gemini's language abilities, making two contributions. First, we provide a third-party, objective comparison of the abilities of the OpenAI GPT and Google Gemini models with reproducible code and fully transparent results. Second, we take a closer look at the results, identifying areas where one of the two model classes excels. We perform this analysis over 10 datasets testing a variety of language abilities, including reasoning, answering knowledge-based questions, solving math problems, translating between languages, generating code, and acting as instruction-following agents. From this analysis, we find that Gemini Pro achieves accuracy that is close but slightly inferior to the corresponding GPT 3.5 Turbo on all tasks that we benchmarked. We further provide explanations for some of this under-performance, including failures in mathematical reasoning with many digits, sensitivity to multiple-choice answer ordering, aggressive content filtering, and others. We also identify areas where Gemini demonstrates comparably high performance, including generation into non-English languages, and handling longer and more complex reasoning chains. Code and data for reproduction can be found at https://github.com/neulab/gemini-benchmark"}, {"question": "Who is the author of the LTI paper WebArena: A Realistic Web Environment for Building Autonomous Agents published in 2023?", "answer": "Daniel Fried", "context": "Author: Daniel Fried Title: WebArena: A Realistic Web Environment for Building Autonomous Agents Publication year: 2023 Coauthors: Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig Abstract: With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress."}, {"question": "What is the title of the paper published by LTI faculty Eric P. Xing in 2023?", "answer": "3D Semantic Segmentation in the Wild: Learning Generalized Models for Adverse-Condition Point Clouds", "context": "Author: Eric P. Xing Title: 3D Semantic Segmentation in the Wild: Learning Generalized Models for Adverse-Condition Point Clouds Publication year: 2023 Coauthors: Aoran Xiao, Jiaxing Huang, Weihao Xuan, Ruijie Ren, Kangcheng Liu, Dayan Guan, A. E. Saddik, Shijian Lu, Eric P. Xing Abstract: Robust point cloud parsing under all-weather conditions is crucial to level-5 autonomy in autonomous driving. However, how to learn a universal 3D semantic segmentation (3DSS) model is largely neglected as most existing benchmarks are dominated by point clouds captured under normal weather. We introduce SemanticSTF, an adverse-weather point cloud dataset that provides dense point-level annotations and allows to study 3DSS under various adverse weather conditions. We study all-weather 3DSS modeling under two setups: 1) domain adaptive 3DSS that adapts from normal-weather data to adverse-weather data; 2) domain generalizable 3DSS that learns all-weather 3DSS models from normal-weather data. Our studies reveal the challenge while existing 3DSS methods encounter adverse-weather data, showing the great value of SemanticSTF in steering the future endeavor along this very meaningful research direction. In addition, we design a domain randomization technique that alternatively randomizes the geometry styles of point clouds and aggregates their embeddings, ultimately leading to a generalizable model that can improve 3DSS under various adverse weather effectively. The SemanticSTF and related codes are available at https://github.com/xiaoaoran/SemanticSTF."}, {"question": "Who are the coauthors of the paper Energy and Carbon Considerations of Fine-Tuning BERT published by LTI faculty Emma Strubell in 2023?", "answer": "Xiaorong Wang, Clara Na, Emma Strubell, Sorelle A. Friedler, Sasha Luccioni", "context": "Author: Emma Strubell Title: Energy and Carbon Considerations of Fine-Tuning BERT Publication year: 2023 Coauthors: Xiaorong Wang, Clara Na, Emma Strubell, Sorelle A. Friedler, Sasha Luccioni Abstract: Despite the popularity of the `pre-train then fine-tune' paradigm in the NLP community, existing work quantifying energy costs and associated carbon emissions has largely focused on language model pre-training. Although a single pre-training run draws substantially more energy than fine-tuning, fine-tuning is performed more frequently by many more individual actors, and thus must be accounted for when considering the energy and carbon footprint of NLP. In order to better characterize the role of fine-tuning in the landscape of energy and carbon emissions in NLP, we perform a careful empirical study of the computational costs of fine-tuning across tasks, datasets, hardware infrastructure and measurement modalities. Our experimental results allow us to place fine-tuning energy and carbon costs into perspective with respect to pre-training and inference, and outline recommendations to NLP researchers and practitioners who wish to improve their fine-tuning energy efficiency."}, {"question": "Who is the author of the LTI paper Phonotactic Complexity across Dialects published in 2024?", "answer": "David R. Mortensen", "context": "Author: David R. Mortensen Title: Phonotactic Complexity across Dialects Publication year: 2024 Coauthors: Ryan Soh-Eun Shim, Kalvin Chang, David R. Mortensen Abstract: Received wisdom in linguistic typology holds that if the structure of a language becomes more complex in one dimension, it will simplify in another, building on the assumption that all languages are equally complex (Joseph and Newmeyer, 2012). We study this claim on a micro-level, using a tightly-controlled sample of Dutch dialects (across 366 collection sites) and Min dialects (across 60 sites), which enables a more fair comparison across varieties. Even at the dialect level, we find empirical evidence for a tradeoff between word length and a computational measure of phonotactic complexity from a LSTM-based phone-level language model-a result previously documented only at the language level. A generalized additive model (GAM) shows that dialects with low phonotactic complexity concentrate around the capital regions, which we hypothesize to correspond to prior hypotheses that language varieties of greater or more diverse populations show reduced phonotactic complexity. We also experiment with incorporating the auxiliary task of predicting syllable constituency, but do not find an increase in the negative correlation observed."}, {"question": "Who are the coauthors of the paper ALISON: Fast and Effective Stylometric Authorship Obfuscation published by LTI faculty Eric Xing in 2024?", "answer": "Eric Xing, Saranya Venkatraman, Thai Le, Dongwon Lee", "context": "Author: Eric Xing Title: ALISON: Fast and Effective Stylometric Authorship Obfuscation Publication year: 2024 Coauthors: Eric Xing, Saranya Venkatraman, Thai Le, Dongwon Lee Abstract: Authorship Attribution (AA) and Authorship Obfuscation (AO) are two competing tasks of increasing importance in privacy research. Modern AA leverages an author's consistent writing style to match a text to its author using an AA classifier. AO is the corresponding adversarial task, aiming to modify a text in such a way that its semantics are preserved, yet an AA model cannot correctly infer its authorship. To address privacy concerns raised by state-of-the-art (SOTA) AA methods, new AO methods have been proposed but remain largely impractical to use due to their prohibitively slow training and obfuscation speed, often taking hours. To this challenge, we propose a practical AO method, ALISON, that (1) dramatically reduces training/obfuscation time, demonstrating more than 10x faster obfuscation than SOTA AO methods, (2) achieves better obfuscation success through attacking three transformer-based AA methods on two benchmark datasets, typically performing 15% better than competing methods, (3) does not require direct signals from a target AA classifier during obfuscation, and (4) utilizes unique stylometric features, allowing sound model interpretation for explainable obfuscation. We also demonstrate that ALISON can effectively prevent four SOTA AA methods from accurately determining the authorship of ChatGPT-generated texts, all while minimally changing the original text semantics. To ensure the reproducibility of our findings, our code and data are available at: https://github.com/EricX003/ALISON."}, {"question": "What is the abstract of the paper Asking More Informative Questions for Grounded Retrieval published by LTI faculty Daniel Fried in 2023?", "answer": "When a model is trying to gather information in an interactive setting, it benefits from asking informative questions. However, in the case of a grounded multi-turn image identification task, previous studies have been constrained to polar yes/no questions, limiting how much information the model can gain in a single turn. We present an approach that formulates more informative, open-ended questions. In doing so, we discover that off-the-shelf visual question answering (VQA) models often make presupposition errors, which standard information gain question selection methods fail to account for. To address this issue, we propose a method that can incorporate presupposition handling into both question selection and belief updates. Specifically, we use a two-stage process, where the model first filters out images which are irrelevant to a given question, then updates its beliefs about which image the user intends. Through self-play and human evaluations, we show that our method is successful in asking informative open-ended questions, increasing accuracy over the past state-of-the-art by 14%, while resulting in 48% more efficient games in human evaluations.", "context": "Author: Daniel Fried Title: Asking More Informative Questions for Grounded Retrieval Publication year: 2023 Coauthors: Sedrick Scott Keh, Justin T. Chiu, Daniel Fried Abstract: When a model is trying to gather information in an interactive setting, it benefits from asking informative questions. However, in the case of a grounded multi-turn image identification task, previous studies have been constrained to polar yes/no questions, limiting how much information the model can gain in a single turn. We present an approach that formulates more informative, open-ended questions. In doing so, we discover that off-the-shelf visual question answering (VQA) models often make presupposition errors, which standard information gain question selection methods fail to account for. To address this issue, we propose a method that can incorporate presupposition handling into both question selection and belief updates. Specifically, we use a two-stage process, where the model first filters out images which are irrelevant to a given question, then updates its beliefs about which image the user intends. Through self-play and human evaluations, we show that our method is successful in asking informative open-ended questions, increasing accuracy over the past state-of-the-art by 14%, while resulting in 48% more efficient games in human evaluations."}, {"question": "What is the abstract of the paper Reasoning about the Unseen for Efficient Outdoor Object Navigation published by LTI faculty Yonatan Bisk in 2023?", "answer": "Robots should exist anywhere humans do: indoors, outdoors, and even unmapped environments. In contrast, the focus of recent advancements in Object Goal Navigation(OGN) has targeted navigating in indoor environments by leveraging spatial and semantic cues that do not generalize outdoors. While these contributions provide valuable insights into indoor scenarios, the broader spectrum of real-world robotic applications often extends to outdoor settings. As we transition to the vast and complex terrains of outdoor environments, new challenges emerge. Unlike the structured layouts found indoors, outdoor environments lack clear spatial delineations and are riddled with inherent semantic ambiguities. Despite this, humans navigate with ease because we can reason about the unseen. We introduce a new task OUTDOOR, a new mechanism for Large Language Models (LLMs) to accurately hallucinate possible futures, and a new computationally aware success metric for pushing research forward in this more complex domain. Additionally, we show impressive results on both a simulated drone and physical quadruped in outdoor environments. Our agent has no premapping and our formalism outperforms naive LLM-based approaches", "context": "Author: Yonatan Bisk Title: Reasoning about the Unseen for Efficient Outdoor Object Navigation Publication year: 2023 Coauthors: Quanting Xie, Tianyi Zhang, Kedi Xu, M. Johnson-Roberson, Yonatan Bisk Abstract: Robots should exist anywhere humans do: indoors, outdoors, and even unmapped environments. In contrast, the focus of recent advancements in Object Goal Navigation(OGN) has targeted navigating in indoor environments by leveraging spatial and semantic cues that do not generalize outdoors. While these contributions provide valuable insights into indoor scenarios, the broader spectrum of real-world robotic applications often extends to outdoor settings. As we transition to the vast and complex terrains of outdoor environments, new challenges emerge. Unlike the structured layouts found indoors, outdoor environments lack clear spatial delineations and are riddled with inherent semantic ambiguities. Despite this, humans navigate with ease because we can reason about the unseen. We introduce a new task OUTDOOR, a new mechanism for Large Language Models (LLMs) to accurately hallucinate possible futures, and a new computationally aware success metric for pushing research forward in this more complex domain. Additionally, we show impressive results on both a simulated drone and physical quadruped in outdoor environments. Our agent has no premapping and our formalism outperforms naive LLM-based approaches"}, {"question": "What is the abstract of the paper Leveraging body pose estimation for gesture recognition in human-robot interaction using synthetic data published by LTI faculty Alexander Hauptmann in 2023?", "answer": "Effectively recognizing human gestures from variant viewpoints plays a fundamental role in the successful collaboration between humans and robots. Deep learning approaches have achieved promising performance in gesture recognition. However, they are usually data-hungry and require large-scale labeled data, which are not usually accessible in a practical setting. Synthetic data, on the other hand, can be easily obtained from simulators with fine-grained annotations and variant modalities. Existing state-of-the-art approaches have shown promising results using synthetic data, but there is still a large performance gap between the models trained on synthetic data and real data. To learn domain-invariant feature representations, we propose a novel approach which jointly takes RGB videos and 3D meshes as inputs to perform robust action recognition. We empirically validate our model on the RoCoG-v2 dataset, which consists of a variety of real and synthetic videos of gestures from the ground and air perspectives. We show that our model trained on synthetic data can outperform state-of-the-art models under the same training setting and models trained on real data.", "context": "Author: Alexander Hauptmann Title: Leveraging body pose estimation for gesture recognition in human-robot interaction using synthetic data Publication year: 2023 Coauthors: Xiaoyu Zhu, Celso de Melo, Alexander Hauptmann Abstract: Effectively recognizing human gestures from variant viewpoints plays a fundamental role in the successful collaboration between humans and robots. Deep learning approaches have achieved promising performance in gesture recognition. However, they are usually data-hungry and require large-scale labeled data, which are not usually accessible in a practical setting. Synthetic data, on the other hand, can be easily obtained from simulators with fine-grained annotations and variant modalities. Existing state-of-the-art approaches have shown promising results using synthetic data, but there is still a large performance gap between the models trained on synthetic data and real data. To learn domain-invariant feature representations, we propose a novel approach which jointly takes RGB videos and 3D meshes as inputs to perform robust action recognition. We empirically validate our model on the RoCoG-v2 dataset, which consists of a variety of real and synthetic videos of gestures from the ground and air perspectives. We show that our model trained on synthetic data can outperform state-of-the-art models under the same training setting and models trained on real data."}]