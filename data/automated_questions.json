[{"question": "Is Jessica Maguire ssica Maguire Assistant to the Institute Director of LTI?, Please answer yes or no: ", "answer": "Yes", "context": "Jessica Maguire Assistant to the Institute Director Email: jlm4@andrew.cmu.edu Office: 5721 Gates & Hillman Centers Phone: 412-268-5480"}, {"question": "How can I contact LTI staff Vicente Malave?", "answer": "Email: vlm@andrew.cmu.edu", "context": "Vicente Malave Data Science Course Developer Email: vlm@andrew.cmu.edu"}, {"question": "Please provide the contact information of LTI staff Susan E.", "answer": "Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591", "context": "Susan E. Holm Mitamura Lab/Sr. Knowledge Engineer Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591"}, {"question": "How can I contact LTI staff Daniel Vosler?", "answer": "Email: dvosler@andrew.cmu.edu", "context": "Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu"}, {"question": "Please tell me the Email of LTI staff Daniel Vosler based on the background information: ", "answer": "Daniel Vosler's Email is dvosler@andrew.cmu.edu", "context": "Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu"}, {"question": "What is the Email of LTI staff Casey Walker?", "answer": "Casey Walker's Email is clwalker@andrew.cmu.edu", "context": "Casey Walker Academic Program Coordinator Email: clwalker@andrew.cmu.edu Phone: +1 412 268 9315"}, {"question": "What is the Phone of LTI staff Emma Thomas?", "answer": "412-268-7812", "context": "Emma Thomas Sponsored Research Administrator Email: emmat@andrew.cmu.edu Office: 5715 Gates & Hillman Centers Phone: 412-268-7812"}, {"question": "Is Kate Schaich kari Yamakawa Mitamura Lab/Associate Data Analyst of LTI?, Please answer yes or no: ", "answer": "No", "context": "Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788"}, {"question": "Please tell me the Office of LTI staff Joan Axelson based on the background information: ", "answer": "Joan Axelson's Office is Manager", "context": "Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517"}, {"question": "How can I contact LTI staff Yukari Yamakawa?", "answer": "Email: yukariy@andrew.cmu.edu Office: 6717 Gates & Hillman Centers Phone: 412-268-6596", "context": "Yukari Yamakawa Mitamura Lab/Associate Data Analyst Email: yukariy@andrew.cmu.edu Office: 6717 Gates & Hillman Centers Phone: 412-268-6596"}, {"question": "Is Nicole Perrotta cole Perrotta Administrative Coordinator of LTI?, Please answer yes or no: ", "answer": "Yes", "context": "Nicole Perrotta Administrative Coordinator Email: nperrott@andrew.cmu.edu"}, {"question": "How can I contact LTI staff Emma Thomas?", "answer": "Email: emmat@andrew.cmu.edu Office: 5715 Gates & Hillman Centers Phone: 412-268-7812", "context": "Emma Thomas Sponsored Research Administrator Email: emmat@andrew.cmu.edu Office: 5715 Gates & Hillman Centers Phone: 412-268-7812"}, {"question": "Please tell me the Email of LTI staff Annette Han based on the background information: ", "answer": "Annette Han's Email is ahan2@andrew.cmu.edu", "context": "Annette Han Rose Lab/Data Science Course Developer Email: ahan2@andrew.cmu.edu"}, {"question": "What is the Email of LTI staff Nicole Perrotta?", "answer": "nperrott@andrew.cmu.edu", "context": "Nicole Perrotta Administrative Coordinator Email: nperrott@andrew.cmu.edu"}, {"question": "Please tell me the Email of LTI staff Daniel Vosler based on the background information: ", "answer": "dvosler@andrew.cmu.edu", "context": "Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu"}, {"question": "How can I contact LTI staff Allison Blair?", "answer": "Email: ablair2@cs.cmu.edu Office: 5715 TCS Hall Phone: 412-268-5489", "context": "Allison Blair Associate Business Manager Email: ablair2@cs.cmu.edu Office: 5715 TCS Hall Phone: 412-268-5489"}, {"question": "Is Caitlin Korpus itlin Korpus Senior Academic Program Coordinator of LTI?, Please answer yes or no: ", "answer": "Yes", "context": "Caitlin Korpus Senior Academic Program Coordinator Email: ckorpus@andrew.cmu.edu Office: 6719 Gates & Hillman Centers Phone: +1 412 268 7096"}, {"question": "Please provide the contact information of LTI staff Susan E.", "answer": "Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591", "context": "Susan E. Holm Mitamura Lab/Sr. Knowledge Engineer Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591"}, {"question": "How can I contact LTI staff Daniel Vosler?", "answer": "Email: dvosler@andrew.cmu.edu", "context": "Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu"}, {"question": "What is the Phone of LTI staff Brianna Eriksen?", "answer": "Brianna Eriksen's Phone is 412-268-4277", "context": "Brianna Eriksen Academic Program Manager Email: bfreema2@andrew.cmu.edu Phone: 412-268-4277"}, {"question": "Please tell me the Phone of LTI staff John Friday based on the background information: ", "answer": "412-268-1810", "context": "John Friday Sr. Administrative Coordinator Email: jfriday@andrew.cmu.edu Office: 5715 Gates & Hillman Centers Phone: 412-268-1810"}, {"question": "What is the Phone of LTI staff Susan E.?", "answer": "412-268-6591", "context": "Susan E. Holm Mitamura Lab/Sr. Knowledge Engineer Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591"}, {"question": "Is Stacey Young umi Maiti Watanabe Lab/Postdoctoral Research Associate of LTI?, Please answer yes or no: ", "answer": "No", "context": "Stacey Young Academic Program Manager \u2014 Ph.D. Email: staceyy@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-2623"}, {"question": "Please provide the contact information of LTI staff Amber Vivis", "answer": "Email: avivis@cs.cmu.edu Office: 357 TCS Hall Phone: 412-268-9998", "context": "Amber Vivis Sr. Academic Program Manager - MSAII Email: avivis@cs.cmu.edu Office: 357 TCS Hall Phone: 412-268-9998"}, {"question": "Please provide the contact information of LTI staff Joan Axelson", "answer": "Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517", "context": "Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517"}, {"question": "Is Wenhe Liu hn Friday Sr. Administrative Coordinator of LTI?, Please answer yes or no: ", "answer": "No", "context": "Wenhe Liu Research Scholar Email: wenhel@andrew.cmu.edu Office: 5403 Gates & Hillman Centers"}, {"question": "How can I contact LTI staff Brianna Eriksen?", "answer": "Email: bfreema2@andrew.cmu.edu Phone: 412-268-4277", "context": "Brianna Eriksen Academic Program Manager Email: bfreema2@andrew.cmu.edu Phone: 412-268-4277"}, {"question": "What is the Phone of LTI staff Kate Schaich?", "answer": "Kate Schaich's Phone is 412-268-4788", "context": "Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788"}, {"question": "Is Zaid Sheikh i-Qi Cheng Hauptmann Lab/Postdoctoral Research Associate of LTI?, Please answer yes or no: ", "answer": "No", "context": "Zaid Sheikh Neubig Lab/Senior Systems/Software Engineer Email: zsheikh@andrew.cmu.edu Office: 6416 Gates & Hillman Centers Phone:"}, {"question": "What is the Email of LTI staff Joan Axelson?", "answer": "jaxelson@andrew.cmu.edu", "context": "Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517"}, {"question": "Please provide the contact information of LTI staff Annette Han", "answer": "Email: ahan2@andrew.cmu.edu", "context": "Annette Han Rose Lab/Data Science Course Developer Email: ahan2@andrew.cmu.edu"}, {"question": "Please provide the contact information of LTI staff Kira Sullivan", "answer": "Email: kiras@andrew.cmu.edu Phone: 412-268-8737", "context": "Kira Sullivan Administrative Coordinator Email: kiras@andrew.cmu.edu Phone: 412-268-8737"}, {"question": "Please tell me the Phone of LTI staff Yukari Yamakawa based on the background information: ", "answer": "Yukari Yamakawa's Phone is 412-268-6596", "context": "Yukari Yamakawa Mitamura Lab/Associate Data Analyst Email: yukariy@andrew.cmu.edu Office: 6717 Gates & Hillman Centers Phone: 412-268-6596"}, {"question": "Please tell me the Email of LTI staff Robin Hammer based on the background information: ", "answer": "robinham@andrew.cmu.edu", "context": "Robin Hammer Levin Lab/Project Coordinator Email: robinham@andrew.cmu.edu"}, {"question": "How can I contact LTI staff Susan E.?", "answer": "Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591", "context": "Susan E. Holm Mitamura Lab/Sr. Knowledge Engineer Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591"}, {"question": "What is the Email of LTI staff Zhi-Qi Cheng?", "answer": "Zhi-Qi Cheng's Email is zhiqic@andrew.cmu.edu", "context": "Zhi-Qi Cheng Hauptmann Lab/Postdoctoral Research Associate Email: zhiqic@andrew.cmu.edu"}, {"question": "Is Jessica Maguire san E. Holm Mitamura Lab/Sr. Knowledge Engineer of LTI?, Please answer yes or no: ", "answer": "No", "context": "Jessica Maguire Assistant to the Institute Director Email: jlm4@andrew.cmu.edu Office: 5721 Gates & Hillman Centers Phone: 412-268-5480"}, {"question": "Is Susan E. san E. Holm Mitamura Lab/Sr. Knowledge Engineer of LTI?, Please answer yes or no: ", "answer": "Yes", "context": "Susan E. Holm Mitamura Lab/Sr. Knowledge Engineer Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591"}, {"question": "Is Vicente Malave cente Malave Data Science Course Developer of LTI?, Please answer yes or no: ", "answer": "Yes", "context": "Vicente Malave Data Science Course Developer Email: vlm@andrew.cmu.edu"}, {"question": "What is the Email of LTI staff Julie Nys?", "answer": "Julie Nys's Email is jnys@andrew.cmu.edu", "context": "Julie Nys Employment Processes Manager Email: jnys@andrew.cmu.edu Office: 5405 TCS Hall Phone: 412-268-3515"}, {"question": "Is Casey Walker nnifer Clark Administrative Coordinator of LTI?, Please answer yes or no: ", "answer": "No", "context": "Casey Walker Academic Program Coordinator Email: clwalker@andrew.cmu.edu Phone: +1 412 268 9315"}, {"question": "How can I contact LTI staff Daniel Gongaware?", "answer": "Email: dgongawa@cs.cmu.edu Office: 355 TCS Hall Phone: 412-268-6195", "context": "Daniel Gongaware Sponsored Research Administrator Email: dgongawa@cs.cmu.edu Office: 355 TCS Hall Phone: 412-268-6195"}, {"question": "Please tell me the Office of LTI staff Bryan Burtner based on the background information: ", "answer": "Bryan Burtner's Office is 6413 TCS Hall", "context": "Bryan Burtner Associate Business Manager Email: bburtner@cs.cmu.edu Office: 6413 TCS Hall Phone: 412-268-2805"}, {"question": "How can I contact LTI staff Kira Sullivan?", "answer": "Email: kiras@andrew.cmu.edu Phone: 412-268-8737", "context": "Kira Sullivan Administrative Coordinator Email: kiras@andrew.cmu.edu Phone: 412-268-8737"}, {"question": "What is the Email of LTI staff Zhong-Qiu Wang?", "answer": "zhongqiu@andrew.cmu.edu", "context": "Zhong-Qiu Wang Wantanabe Lab/Postdoctoral Research Associate Email: zhongqiu@andrew.cmu.edu Office: 6414 Gates & Hillman Centers Phone: 415-200-8662"}, {"question": "What is the Email of LTI staff Wenhe Liu?", "answer": "Wenhe Liu's Email is wenhel@andrew.cmu.edu", "context": "Wenhe Liu Research Scholar Email: wenhel@andrew.cmu.edu Office: 5403 Gates & Hillman Centers"}, {"question": "What is the Phone of LTI staff Stacey Young?", "answer": "Stacey Young's Phone is 412-268-2623", "context": "Stacey Young Academic Program Manager \u2014 Ph.D. Email: staceyy@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-2623"}, {"question": "Please tell me the Email of LTI staff Joan Axelson based on the background information: ", "answer": "jaxelson@andrew.cmu.edu", "context": "Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517"}, {"question": "Please provide the contact information of LTI staff Jennifer Lucas", "answer": "Email: jmlucas@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-9870", "context": "Jennifer Lucas Academic Program Manager \u2014 MCDS Email: jmlucas@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-9870"}, {"question": "What is the Phone of LTI staff Allison Blair?", "answer": "412-268-5489", "context": "Allison Blair Associate Business Manager Email: ablair2@cs.cmu.edu Office: 5715 TCS Hall Phone: 412-268-5489"}, {"question": "Please provide the contact information of LTI faculty Kemal Oflazer", "answer": "Email: ko@qatar.cmu.edu Office: 1009 Carnegie Mellon - Qatar Campus Phone:", "context": "Kemal Oflazer Teaching Professor of Computer Science Email: ko@qatar.cmu.edu Office: 1009 Carnegie Mellon - Qatar Campus Phone:"}, {"question": "Is Teruko Mitamura ruko Mitamura Research Professor of LTI?, Please answer yes or no: ", "answer": "Yes", "context": "Teruko Mitamura Research Professor Email: teruko@cs.cmu.edu Office: 6711 Gates & Hillman Centers Phone: 412-268-6596 Research Areas: Information Extraction, Summarization and Question Answering, Information Retrieval, Text Mining and Analytics, Language Technologies for Education, Natural Language Processing and Computational Linguistics"}, {"question": "Is Chenyan Xiong i Li Assistant Professor of LTI?, Please answer yes or no: ", "answer": "No", "context": "Chenyan Xiong Associate Professor Email: cx@andrew.cmu.edu Phone: 412-268-7641"}, {"question": "How can I contact LTI faculty Eric Nyberg?", "answer": "Email: ehn@cs.cmu.edu Office: 6715 Gates & Hillman Centers Phone: 412-268-7281", "context": "Eric Nyberg Professor Email: ehn@cs.cmu.edu Office: 6715 Gates & Hillman Centers Phone: 412-268-7281 Research Areas: Information Extraction, Summarization and Question Answering, Information Retrieval, Text Mining and Analytics, Language Technologies for Education"}, {"question": "Please provide the contact information of LTI faculty Bhiksha Ramakrishnan", "answer": "Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826", "context": "Bhiksha Ramakrishnan Professor Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826 Research Areas: Machine Learning, Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing, Privacy"}, {"question": "Is Bhiksha Ramakrishnan iksha Ramakrishnan Professor of LTI?, Please answer yes or no: ", "answer": "Yes", "context": "Bhiksha Ramakrishnan Professor Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826 Research Areas: Machine Learning, Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing, Privacy"}, {"question": "What is the Office of LTI faculty Lori Levin?", "answer": "Lori Levin's Office is 5717 Gates & Hillman Centers", "context": "Lori Levin Research Professor Email: lsl@cs.cmu.edu Office: 5717 Gates & Hillman Centers Phone: 412-268-6193 Research Areas: Machine Translation, Natural Language Processing and Computational Linguistics, Corpus Annotation and Resources"}, {"question": "How can I contact LTI faculty Emma Strubell?", "answer": "Email: estrubel@andrew.cmu.edu Office: Gates & Hillman Centers", "context": "Emma Strubell Assistant Professor Email: estrubel@andrew.cmu.edu Office: Gates & Hillman Centers"}, {"question": "What is the Research Areas of LTI faculty Daniel Fried?", "answer": "Daniel Fried's Research Areas is Natural Language Processing: Language and Code, Conversational AI, Intelligent Agents, and Dialogue, Discourse and Pragmatics, Multimodal AI", "context": "Daniel Fried Assistant Professor Email: dfried@andrew.cmu.edu Research Areas: Natural Language Processing: Language and Code, Conversational AI, Intelligent Agents, and Dialogue, Discourse and Pragmatics, Multimodal AI"}, {"question": "How can I contact LTI faculty Robert Frederking?", "answer": "Email: ref@cs.cmu.edu Office: 6515 Gates & Hillman Centers Phone: 412-268-6656", "context": "Robert Frederking Principal Systems Scientist/Associate Dean of Doctoral Programs/MLT Program Director Email: ref@cs.cmu.edu Office: 6515 Gates & Hillman Centers Phone: 412-268-6656"}, {"question": "Please tell me the Phone of LTI faculty Justine Cassell based on the background information: ", "answer": "Justine Cassell's Phone is 412-204-6268", "context": "Justine Cassell Professor (On Leave) Email: jcassell@andrew.cmu.edu Office: 5107 Gates & Hillman Centers Phone: 412-204-6268"}, {"question": "What is the Research Areas of LTI faculty Shinji Watanabe?", "answer": "Shinji Watanabe's Research Areas is Natural Language Processing: Conversational AI, Intelligent Agents, and Dialogue, Speech Processing (ASR, Speech Synthesis): Speech Recognition, Speech Synthesis, Multilingual/Low-Resource Speech Processing, Speech-to-Speech Translation, Speech Enhancement / Robust Speech Processing", "context": "Shinji Watanabe Associate Professor Email: swatanab@andrew.cmu.edu Phone: 412-268-3687 Research Areas: Natural Language Processing: Conversational AI, Intelligent Agents, and Dialogue, Speech Processing (ASR, Speech Synthesis): Speech Recognition, Speech Synthesis, Multilingual/Low-Resource Speech Processing, Speech-to-Speech Translation, Speech Enhancement / Robust Speech Processing"}, {"question": "Please tell me the Email of LTI faculty Alexander Hauptmann based on the background information: ", "answer": "alex@cs.cmu.edu", "context": "Alexander Hauptmann Research Professor Email: alex@cs.cmu.edu Office: 5519 Gates & Hillman Centers Phone: 412-268-1448 Research Areas: Information Extraction, Summarization and Question Answering, Information Retrieval, Text Mining and Analytics, Machine Learning, Multimodal Computing and Interaction"}, {"question": "Please provide the contact information of LTI faculty Eric Nyberg", "answer": "Email: ehn@cs.cmu.edu Office: 6715 Gates & Hillman Centers Phone: 412-268-7281", "context": "Eric Nyberg Professor Email: ehn@cs.cmu.edu Office: 6715 Gates & Hillman Centers Phone: 412-268-7281 Research Areas: Information Extraction, Summarization and Question Answering, Information Retrieval, Text Mining and Analytics, Language Technologies for Education"}, {"question": "Please tell me the Phone of LTI faculty Robert Frederking based on the background information: ", "answer": "412-268-6656", "context": "Robert Frederking Principal Systems Scientist/Associate Dean of Doctoral Programs/MLT Program Director Email: ref@cs.cmu.edu Office: 6515 Gates & Hillman Centers Phone: 412-268-6656"}, {"question": "Please provide the contact information of LTI faculty Rita Singh", "answer": "Email: rsingh@cs.cmu.edu Office: 6703 Gates & Hillman Centers Phone: 412-268-9859", "context": "Rita Singh Associate Research Professor Email: rsingh@cs.cmu.edu Office: 6703 Gates & Hillman Centers Phone: 412-268-9859"}, {"question": "Please tell me the Phone of LTI faculty Robert Frederking based on the background information: ", "answer": "Robert Frederking's Phone is 412-268-6656", "context": "Robert Frederking Principal Systems Scientist/Associate Dean of Doctoral Programs/MLT Program Director Email: ref@cs.cmu.edu Office: 6515 Gates & Hillman Centers Phone: 412-268-6656"}, {"question": "How can I contact LTI faculty Eric P.?", "answer": "Email: epxing@andrew.cmu.edu Office: 8101 Gates & Hillman Centers Phone: 412-268-2559", "context": "Eric P. Xing Professor (On Leave) Email: epxing@andrew.cmu.edu Office: 8101 Gates & Hillman Centers Phone: 412-268-2559"}, {"question": "What is the Email of LTI faculty Eric Nyberg?", "answer": "Eric Nyberg's Email is ehn@cs.cmu.edu", "context": "Eric Nyberg Professor Email: ehn@cs.cmu.edu Office: 6715 Gates & Hillman Centers Phone: 412-268-7281 Research Areas: Information Extraction, Summarization and Question Answering, Information Retrieval, Text Mining and Analytics, Language Technologies for Education"}, {"question": "Is Anatole Gershman natan Bisk Assistant Professor of LTI?, Please answer yes or no: ", "answer": "No", "context": "Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering"}, {"question": "How can I contact LTI faculty Rita Singh?", "answer": "Email: rsingh@cs.cmu.edu Office: 6703 Gates & Hillman Centers Phone: 412-268-9859", "context": "Rita Singh Associate Research Professor Email: rsingh@cs.cmu.edu Office: 6703 Gates & Hillman Centers Phone: 412-268-9859"}, {"question": "Please tell me the Research Areas of LTI faculty Maarten Sap based on the background information: ", "answer": "Fairness and Ethics in Language Technology, Computational Social Science, Discourse and Pragmatics, Conversational AI, Intelligent Agents, and Dialogue", "context": "Maarten Sap Assistant Professor Email: msap2@andrew.cmu.edu Research Areas: Fairness and Ethics in Language Technology, Computational Social Science, Discourse and Pragmatics, Conversational AI, Intelligent Agents, and Dialogue"}, {"question": "What is the Phone of LTI faculty Anatole Gershman?", "answer": "Anatole Gershman's Phone is 412-268-8259", "context": "Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering"}, {"question": "How can I contact LTI faculty Yonatan Bisk?", "answer": "Email: ybisk@cs.cmu.edu Office: Gates & Hillman Centers", "context": "Yonatan Bisk Assistant Professor Email: ybisk@cs.cmu.edu Office: Gates & Hillman Centers Research Areas: Grounding, RoboNLP, Vision and Language, Embodiment, Unsupervised Learning"}, {"question": "What is the Phone of LTI faculty Shinji Watanabe?", "answer": "Shinji Watanabe's Phone is 412-268-3687", "context": "Shinji Watanabe Associate Professor Email: swatanab@andrew.cmu.edu Phone: 412-268-3687 Research Areas: Natural Language Processing: Conversational AI, Intelligent Agents, and Dialogue, Speech Processing (ASR, Speech Synthesis): Speech Recognition, Speech Synthesis, Multilingual/Low-Resource Speech Processing, Speech-to-Speech Translation, Speech Enhancement / Robust Speech Processing"}, {"question": "How can I contact LTI faculty Louis-Philippe Morency?", "answer": "Email: morency@cs.cmu.edu Office: 5411 Gates & Hillman Centers Phone: 412-268-5508", "context": "Louis-Philippe Morency Leonardo Associate Professor of Computer Science Email: morency@cs.cmu.edu Office: 5411 Gates & Hillman Centers Phone: 412-268-5508 Research Areas: Machine Learning, Multimodal Computing and Interaction, Spoken Interfaces and Dialogue Processing"}, {"question": "Please tell me the Phone of LTI faculty Robert Frederking based on the background information: ", "answer": "412-268-6656", "context": "Robert Frederking Principal Systems Scientist/Associate Dean of Doctoral Programs/MLT Program Director Email: ref@cs.cmu.edu Office: 6515 Gates & Hillman Centers Phone: 412-268-6656"}, {"question": "How can I contact LTI faculty Rita Singh?", "answer": "Email: rsingh@cs.cmu.edu Office: 6703 Gates & Hillman Centers Phone: 412-268-9859", "context": "Rita Singh Associate Research Professor Email: rsingh@cs.cmu.edu Office: 6703 Gates & Hillman Centers Phone: 412-268-9859"}, {"question": "How can I contact LTI faculty Maarten Sap?", "answer": "Email: msap2@andrew.cmu.edu", "context": "Maarten Sap Assistant Professor Email: msap2@andrew.cmu.edu Research Areas: Fairness and Ethics in Language Technology, Computational Social Science, Discourse and Pragmatics, Conversational AI, Intelligent Agents, and Dialogue"}, {"question": "What is the Research Areas of LTI faculty Shinji Watanabe?", "answer": "Shinji Watanabe's Research Areas is Natural Language Processing: Conversational AI, Intelligent Agents, and Dialogue, Speech Processing (ASR, Speech Synthesis): Speech Recognition, Speech Synthesis, Multilingual/Low-Resource Speech Processing, Speech-to-Speech Translation, Speech Enhancement / Robust Speech Processing", "context": "Shinji Watanabe Associate Professor Email: swatanab@andrew.cmu.edu Phone: 412-268-3687 Research Areas: Natural Language Processing: Conversational AI, Intelligent Agents, and Dialogue, Speech Processing (ASR, Speech Synthesis): Speech Recognition, Speech Synthesis, Multilingual/Low-Resource Speech Processing, Speech-to-Speech Translation, Speech Enhancement / Robust Speech Processing"}, {"question": "How can I contact LTI faculty Eric P.?", "answer": "Email: epxing@andrew.cmu.edu Office: 8101 Gates & Hillman Centers Phone: 412-268-2559", "context": "Eric P. Xing Professor (On Leave) Email: epxing@andrew.cmu.edu Office: 8101 Gates & Hillman Centers Phone: 412-268-2559"}, {"question": "Please tell me the Research Areas of LTI faculty Shinji Watanabe based on the background information: ", "answer": "Natural Language Processing: Conversational AI, Intelligent Agents, and Dialogue, Speech Processing (ASR, Speech Synthesis): Speech Recognition, Speech Synthesis, Multilingual/Low-Resource Speech Processing, Speech-to-Speech Translation, Speech Enhancement / Robust Speech Processing", "context": "Shinji Watanabe Associate Professor Email: swatanab@andrew.cmu.edu Phone: 412-268-3687 Research Areas: Natural Language Processing: Conversational AI, Intelligent Agents, and Dialogue, Speech Processing (ASR, Speech Synthesis): Speech Recognition, Speech Synthesis, Multilingual/Low-Resource Speech Processing, Speech-to-Speech Translation, Speech Enhancement / Robust Speech Processing"}, {"question": "Is Scott Fahlman i Li Assistant Professor of LTI?, Please answer yes or no: ", "answer": "No", "context": "Scott Fahlman Research Professor Emeritus Email: sef@cs.cmu.edu Office: 6417 Gates & Hillman Centers Phone: 412-268-2575 Research Areas: AI, Knowledge Representation and Reasoning, Natural Language Understanding"}, {"question": "How can I contact LTI faculty Bhiksha Ramakrishnan?", "answer": "Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826", "context": "Bhiksha Ramakrishnan Professor Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826 Research Areas: Machine Learning, Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing, Privacy"}, {"question": "How can I contact LTI faculty Chenyan Xiong?", "answer": "Email: cx@andrew.cmu.edu Phone: 412-268-7641", "context": "Chenyan Xiong Associate Professor Email: cx@andrew.cmu.edu Phone: 412-268-7641"}, {"question": "How can I contact LTI faculty Bhiksha Ramakrishnan?", "answer": "Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826", "context": "Bhiksha Ramakrishnan Professor Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826 Research Areas: Machine Learning, Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing, Privacy"}, {"question": "How can I contact LTI faculty Maarten Sap?", "answer": "Email: msap2@andrew.cmu.edu", "context": "Maarten Sap Assistant Professor Email: msap2@andrew.cmu.edu Research Areas: Fairness and Ethics in Language Technology, Computational Social Science, Discourse and Pragmatics, Conversational AI, Intelligent Agents, and Dialogue"}, {"question": "What is the Office of LTI faculty Justine Cassell?", "answer": "Justine Cassell's Office is 5107 Gates & Hillman Centers", "context": "Justine Cassell Professor (On Leave) Email: jcassell@andrew.cmu.edu Office: 5107 Gates & Hillman Centers Phone: 412-204-6268"}, {"question": "What is the Research Areas of LTI faculty Louis-Philippe Morency?", "answer": "Louis-Philippe Morency's Research Areas is Machine Learning, Multimodal Computing and Interaction, Spoken Interfaces and Dialogue Processing", "context": "Louis-Philippe Morency Leonardo Associate Professor of Computer Science Email: morency@cs.cmu.edu Office: 5411 Gates & Hillman Centers Phone: 412-268-5508 Research Areas: Machine Learning, Multimodal Computing and Interaction, Spoken Interfaces and Dialogue Processing"}, {"question": "What is the Office of LTI faculty Ralf Brown?", "answer": "5711 Gates & Hillman Centers", "context": "Ralf Brown Principal Systems Scientist Email: ralf@andrew.cmu.edu Office: 5711 Gates & Hillman Centers Phone: 412-268-8298 Research Areas: Information Extraction, Summarization and Question Answering, Information Retrieval, Text Mining and Analytics, Machine Translation, Natural Language Processing and Computational Linguistics"}, {"question": "Please tell me the Email of LTI faculty Robert Frederking based on the background information: ", "answer": "Robert Frederking's Email is ref@cs.cmu.edu", "context": "Robert Frederking Principal Systems Scientist/Associate Dean of Doctoral Programs/MLT Program Director Email: ref@cs.cmu.edu Office: 6515 Gates & Hillman Centers Phone: 412-268-6656"}, {"question": "Please provide the contact information of LTI faculty Jamie Callan", "answer": "Email: callan@cs.cmu.edu Office: 5419 Gates & Hillman Centers Phone: 412-268-4525", "context": "Jamie Callan Professor and PhD Program Director Email: callan@cs.cmu.edu Office: 5419 Gates & Hillman Centers Phone: 412-268-4525 Research Areas: Information Retrieval, Text Mining and Analytics"}, {"question": "What is the Email of LTI faculty Bhiksha Ramakrishnan?", "answer": "bhiksha@cs.cmu.edu", "context": "Bhiksha Ramakrishnan Professor Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826 Research Areas: Machine Learning, Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing, Privacy"}, {"question": "What is the Phone of LTI faculty Justine Cassell?", "answer": "412-204-6268", "context": "Justine Cassell Professor (On Leave) Email: jcassell@andrew.cmu.edu Office: 5107 Gates & Hillman Centers Phone: 412-204-6268"}, {"question": "Is Rita Singh lf Brown Principal Systems Scientist of LTI?, Please answer yes or no: ", "answer": "No", "context": "Rita Singh Associate Research Professor Email: rsingh@cs.cmu.edu Office: 6703 Gates & Hillman Centers Phone: 412-268-9859"}, {"question": "Is Maarten Sap iksha Ramakrishnan Professor of LTI?, Please answer yes or no: ", "answer": "No", "context": "Maarten Sap Assistant Professor Email: msap2@andrew.cmu.edu Research Areas: Fairness and Ethics in Language Technology, Computational Social Science, Discourse and Pragmatics, Conversational AI, Intelligent Agents, and Dialogue"}, {"question": "Please tell me the Email of LTI faculty Sean Welleck based on the background information: ", "answer": "swelleck@andrew.cmu.edu", "context": "Sean Welleck Assistant Professor (Starting January 2024) Email: swelleck@andrew.cmu.edu"}, {"question": "Is Louis-Philippe Morency niel Fried Assistant Professor of LTI?, Please answer yes or no: ", "answer": "No", "context": "Louis-Philippe Morency Leonardo Associate Professor of Computer Science Email: morency@cs.cmu.edu Office: 5411 Gates & Hillman Centers Phone: 412-268-5508 Research Areas: Machine Learning, Multimodal Computing and Interaction, Spoken Interfaces and Dialogue Processing"}, {"question": "What is the Office of LTI faculty Anatole Gershman?", "answer": "Anatole Gershman's Office is 6415 Gates & Hillman Centers", "context": "Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering"}, {"question": "How can I contact LTI faculty Robert Frederking?", "answer": "Email: ref@cs.cmu.edu Office: 6515 Gates & Hillman Centers Phone: 412-268-6656", "context": "Robert Frederking Principal Systems Scientist/Associate Dean of Doctoral Programs/MLT Program Director Email: ref@cs.cmu.edu Office: 6515 Gates & Hillman Centers Phone: 412-268-6656"}, {"question": "According to background information, What happens in 2024-06-21 on university calendar?", "answer": "Mini-5 Final Exams", "context": "Summer One_All 2024: Date: 2024-06-21 Day: F Event: Mini-5 Final Exams"}, {"question": "Based on background information, What is the specific event listed for 2024-05-17, in Summer One_All 2024", "answer": "Mini-5 add, audit, & tuition adjustment drop deadline (1)", "context": "Summer One_All 2024: Date: 2024-05-17 Day: F Event: Mini-5 add, audit, & tuition adjustment drop deadline (1)"}, {"question": "According to background information, When is the [Mini-5 Last Day of Classes] in Summer One_All 2024?", "answer": "The event Mini-5 Last Day of Classes happens on 2024-06-20", "context": "Summer One_All 2024: Date: 2024-06-20 Day: Th Event: Mini-5 Last Day of Classes"}, {"question": "Based on background information, What is the specific event listed for 2023-11-27, in Fall 2023", "answer": "The event happening on 2023-11-27 is Mini-2 pass/no pass & withdrawal deadline (2)", "context": "Fall 2023: Date: 2023-11-27 Day: M Event: Mini-2 pass/no pass & withdrawal deadline (2)"}, {"question": "Based on background information, What is the specific event listed for 2023-10-23, in Fall 2023", "answer": "The event happening on 2023-10-23 is Mini-2 Classes Begin", "context": "Fall 2023: Date: 2023-10-23 Day: M Event: Mini-2 Classes Begin "}, {"question": "Based on background information, What is the specific event listed for 2024-01-15, in Spring 2024", "answer": "Martin Luther King Day; No Classes", "context": "Spring 2024: Date: 2024-01-15 Day: M Event: Martin Luther King Day; No Classes"}, {"question": "According to background information, When is the [Make-Up Final Examinations] in Spring 2024?", "answer": "The event Make-Up Final Examinations happens on 2024-05-07", "context": "Spring 2024: Date: 2024-05-07 Day: Tu Event: Make-Up Final Examinations"}, {"question": "According to background information, What happens in 2023-10-13 on university calendar?", "answer": "The event happening on 2023-10-13 is Mini-1 Last Day of Classes", "context": "Fall 2023: Date: 2023-10-13 Day: F Event: Mini-1 Last Day of Classes"}, {"question": "Based on background information, What is the specific event listed for 2024-07-08, in Summer One_All 2024", "answer": "The event happening on 2024-07-08 is Mini-6 drop deadline; withdrawal grade assigned after this date (2)", "context": "Summer One_All 2024: Date: 2024-07-08 Day: M Event: Mini-6 drop deadline; withdrawal grade assigned after this date (2)"}, {"question": "According to background information, When is the [Reading Day] in Spring 2024?", "answer": "2024-05-01", "context": "Spring 2024: Date: 2024-05-01 Day: W Event: Reading Day"}, {"question": "Based on the given information, Does Spring 2024 Mini-3 Faculty Course Evaluations open happen at 2024-01-15", "answer": "No", "context": "Spring 2024: Date: 2024-02-19 Day: M Event: Mini-3 Faculty Course Evaluations open"}, {"question": "According to background information, When is the [Mini-1 Exams] in Fall 2023?", "answer": "2023-10-14", "context": "Fall 2023: Date: 2023-10-14 Day: Sa Event: Mini-1 Exams"}, {"question": "Based on background information, What is the specific event listed for 2024-07-29, in Summer One_All 2024", "answer": "Semester & Mini-6 Faculty Course Evalutations open", "context": "Summer One_All 2024: Date: 2024-07-29 Day: M Event: Semester & Mini-6 Faculty Course Evalutations open "}, {"question": "According to background information, When is the [Mini-1 add, audit & tuition adjustment drop deadline  (1)] in Fall 2023?", "answer": "2023-09-01", "context": "Fall 2023: Date: 2023-09-01 Day: F Event: Mini-1 add, audit & tuition adjustment drop deadline  (1)"}, {"question": "Based on the given information, Does Summer One_All 2024 Independence Day; University Closed & No Classes happen at 2024-05-06", "answer": "No", "context": "Summer One_All 2024: Date: 2024-07-04 Day: Th Event: Independence Day; University Closed & No Classes"}, {"question": "Based on background information, What is the specific event listed for 2023-09-01, in Fall 2023", "answer": "Mini-1 add, audit & tuition adjustment drop deadline  (1)", "context": "Fall 2023: Date: 2023-09-01 Day: F Event: Mini-1 add, audit & tuition adjustment drop deadline  (1)"}, {"question": "Based on background information, What is the specific event listed for 2024-05-27, in Summer One_All 2024", "answer": "Memorial Day; University Closed & No Classes", "context": "Summer One_All 2024: Date: 2024-05-27 Day: M Event: Memorial Day; University Closed & No Classes"}, {"question": "Based on the given information, Does Fall 2023 Mini-2 drop deadline; withdrawal grade assigned after this date (2) happen at 2023-11-15", "answer": "Yes", "context": "Fall 2023: Date: 2023-11-15 Day: W Event: Mini-2 drop deadline; withdrawal grade assigned after this date (2)"}, {"question": "Based on the given information, Does Summer One_All 2024 Mini-5 Faculty Course Evaluations open happen at 2024-06-14", "answer": "Yes", "context": "Summer One_All 2024: Date: 2024-06-14 Day: F Event: Mini-5 Faculty Course Evaluations open"}, {"question": "Based on background information, What is the specific event listed for 2024-05-08, in Spring 2024", "answer": "Graduating Final Grades Due by 4 pm", "context": "Spring 2024: Date: 2024-05-08 Day: W Event: Graduating Final Grades Due by 4 pm"}, {"question": "Based on the given information, Does Summer One_All 2024 Mini-6 First Day of Classes happen at 2024-06-24", "answer": "Yes", "context": "Summer One_All 2024: Date: 2024-06-24 Day: M Event: Mini-6 First Day of Classes"}, {"question": "According to background information, What happens in 2024-07-29 on university calendar?", "answer": "The event happening on 2024-07-29 is Semester & Mini-6 Faculty Course Evalutations open", "context": "Summer One_All 2024: Date: 2024-07-29 Day: M Event: Semester & Mini-6 Faculty Course Evalutations open "}, {"question": "Based on the given information, Does Fall 2023 Mini-1 voucher election deadline (4) happen at 2023-10-13", "answer": "Yes", "context": "Fall 2023: Date: 2023-10-13 Day: F Event: Mini-1 voucher election deadline (4)"}, {"question": "According to background information, When is the [Final Examinations ] in Spring 2024?", "answer": "2024-04-29-2024-04-30", "context": "Spring 2024: Date: 2024-04-29-2024-04-30 Day: M-Tu Event: Final Examinations "}, {"question": "Based on the given information, Does Summer Two 2024 Summer Semester  Two drop deadline; withdrawal grade assigned after this date (2) happen at 2024-07-29", "answer": "No", "context": "Summer Two 2024: Date: 2024-07-08 Day: M Event: Summer Semester  Two drop deadline; withdrawal grade assigned after this date (2)"}, {"question": "According to background information, When is the [Semester pass/no pass & withdrawal deadline (3)] in Summer One_All 2024?", "answer": "The event Semester pass/no pass & withdrawal deadline (3) happens on 2024-07-05", "context": "Summer One_All 2024: Date: 2024-07-05 Day: F Event: Semester pass/no pass & withdrawal deadline (3)"}, {"question": "According to background information, What happens in 2024-05-06 on university calendar?", "answer": "The event happening on 2024-05-06 is Final Examinations", "context": "Spring 2024: Date: 2024-05-06 Day: M Event: Final Examinations"}, {"question": "Based on the given information, Does Summer One_All 2024 Mini-5 pass/no pass & withdrawal deadline (3) happen at 2024-06-24", "answer": "No", "context": "Summer One_All 2024: Date: 2024-06-07 Day: F Event: Mini-5 pass/no pass & withdrawal deadline (3)"}, {"question": "According to background information, When is the [Summer Semester  Two pass/no pass & withdrawal deadline (3)] in Summer Two 2024?", "answer": "The event Summer Semester  Two pass/no pass & withdrawal deadline (3) happens on 2024-07-22", "context": "Summer Two 2024: Date: 2024-07-22 Day: F Event: Summer Semester  Two pass/no pass & withdrawal deadline (3)"}, {"question": "Based on background information, What is the specific event listed for 2024-02-19, in Spring 2024", "answer": "The event happening on 2024-02-19 is Mini-3 Faculty Course Evaluations open", "context": "Spring 2024: Date: 2024-02-19 Day: M Event: Mini-3 Faculty Course Evaluations open"}, {"question": "Based on the given information, Does Spring 2024 Mini-3 voucher deadline (4) happen at 2023-10-13", "answer": "No", "context": "Spring 2024: Date: 2024-03-01 Day: F Event: Mini-3 voucher deadline (4)"}, {"question": "Based on background information, What is the specific event listed for 2024-04-01, in Spring 2024", "answer": "The event happening on 2024-04-01 is Semester pass/no pass & withdrawal deadline (3)", "context": "Spring 2024: Date: 2024-04-01 Day: M Event: Semester pass/no pass & withdrawal deadline (3)"}, {"question": "According to background information, When is the [Mini-5 pass/no pass & withdrawal deadline (3)] in Summer One_All 2024?", "answer": "The event Mini-5 pass/no pass & withdrawal deadline (3) happens on 2024-06-07", "context": "Summer One_All 2024: Date: 2024-06-07 Day: F Event: Mini-5 pass/no pass & withdrawal deadline (3)"}, {"question": "Based on the given information, Does Summer One_All 2024 Mini-5 Faculty Course Evaluations open happen at 2024-06-14", "answer": "Yes", "context": "Summer One_All 2024: Date: 2024-06-14 Day: F Event: Mini-5 Faculty Course Evaluations open"}, {"question": "Based on the given information, Does Spring 2024 Final Grades Due by 4 pm happen at 2023-12-17", "answer": "No", "context": "Spring 2024: Date: 2024-05-14 Day: Tu Event: Final Grades Due by 4 pm"}, {"question": "Based on background information, What is the specific event listed for 2024-06-21, in Summer One_All 2024", "answer": "Mini-5 Final Exams", "context": "Summer One_All 2024: Date: 2024-06-21 Day: F Event: Mini-5 Final Exams"}, {"question": "According to background information, What happens in 2023-12-18 on university calendar?", "answer": "The event happening on 2023-12-18 is Semester & Mini-2 Faculty Course Evaluations close", "context": "Fall 2023: Date: 2023-12-18 Day: M Event: Semester & Mini-2 Faculty Course Evaluations close"}, {"question": "Based on the given information, Does Summer Two 2024 Summer Semester  Two Last Day of Classes happen at 2024-08-01", "answer": "Yes", "context": "Summer Two 2024: Date: 2024-08-01 Day: Th Event: Summer Semester  Two Last Day of Classes"}, {"question": "Based on background information, What is the specific event listed for 2024-05-14, in Spring 2024", "answer": "The event happening on 2024-05-14 is Final Grades Due by 4 pm", "context": "Spring 2024: Date: 2024-05-14 Day: Tu Event: Final Grades Due by 4 pm"}, {"question": "According to background information, When is the [Final Exams ] in Fall 2023?", "answer": "The event Final Exams  happens on 2023-12-11-2023-12-12", "context": "Fall 2023: Date: 2023-12-11-2023-12-12 Day: M-Tu Event: Final Exams "}, {"question": "Based on background information, What is the specific event listed for 2024-07-04, in Summer Two 2024", "answer": "The event happening on 2024-07-04 is Independence Day; University Closed & No Classes", "context": "Summer Two 2024: Date: 2024-07-04 Day: Th Event: Independence Day; University Closed & No Classes"}, {"question": "Based on background information, What is the specific event listed for 2024-02-26, in Spring 2024", "answer": "The event happening on 2024-02-26 is Semester course drop deadline; withdrawal grade assigned after this date (2)", "context": "Spring 2024: Date: 2024-02-26 Day: M Event: Semester course drop deadline; withdrawal grade assigned after this date (2)"}, {"question": "According to background information, When is the [Reading Day] in Spring 2024?", "answer": "2024-05-01", "context": "Spring 2024: Date: 2024-05-01 Day: W Event: Reading Day"}, {"question": "According to background information, When is the [Spring Deans' Lists Posted] in Spring 2024?", "answer": "The event Spring Deans' Lists Posted happens on 2023-05-21", "context": "Spring 2024: Date: 2023-05-21 Day: Tu Event: Spring Deans' Lists Posted"}, {"question": "Based on the given information, Does Fall 2023 Spring 2024 Registration Week happen at 2023-11-13-2023-11-17", "answer": "Yes", "context": "Fall 2023: Date: 2023-11-13-2023-11-17 Day: M-F Event: Spring 2024 Registration Week"}, {"question": "Based on the given information, Does Summer One_All 2024 Semester & Mini-6 Faculty Course Evaluations close happen at 2024-08-02", "answer": "Yes", "context": "Summer One_All 2024: Date: 2024-08-02 Day: F Event: Semester & Mini-6 Faculty Course Evaluations close"}, {"question": "Based on the given information, Does Spring 2024 Mid-Semester & Mini-3 grades due by 4 pm happen at 2024-04-11-2024-04-13", "answer": "No", "context": "Spring 2024: Date: 2024-03-11 Day: M Event: Mid-Semester & Mini-3 grades due by 4 pm"}, {"question": "According to background information, What happens in 2024-05-13 on university calendar?", "answer": "Semester & Mini-5 Classes Begin", "context": "Summer One_All 2024: Date: 2024-05-13 Day: M Event: Semester & Mini-5 Classes Begin"}, {"question": "Based on background information, What is the specific event listed for 2024-06-07, in Summer One_All 2024", "answer": "The event happening on 2024-06-07 is Mini-5 pass/no pass & withdrawal deadline (3)", "context": "Summer One_All 2024: Date: 2024-06-07 Day: F Event: Mini-5 pass/no pass & withdrawal deadline (3)"}, {"question": "Based on the given information, Does Fall 2023 Final Exams  happen at 2023-12-11-2023-12-12", "answer": "Yes", "context": "Fall 2023: Date: 2023-12-11-2023-12-12 Day: M-Tu Event: Final Exams "}, {"question": "When does the class International Finance Recitation Section A4  offered in Sp end?", "answer": "11:45AM", "context": "Spring offering: Course: 45825 Title: International Finance Units: 6.0 Lec/Sec: Section A4 Days: Tuesday, Thursday Begin: 10:00AM End: 11:45AM Bldg/Room: TEP 2118 Location: Pittsburgh, Pennsylvania Instructor(s): Khokher "}, {"question": "At what time does the class Financial Statements and Analysis of Companies Recitation Section C1  offered in Fa begin?", "answer": "11:00AM", "context": "Fall offering: Course: 90723 Title: Financial Statements and Analysis of Companies Units: 6.0 Lec/Sec: Section C1 Days: Monday, Wednesday Begin: 11:00AM End: 12:20PM Bldg/Room: HBH 2008 Location: Pittsburgh, Pennsylvania Instructor(s): Gogolak "}, {"question": "What is the units of course Principles of Imperative Computation offered in Fa?", "answer": "12.0", "context": "Fall offering: Course: 15122 Title: Principles of Imperative Computation Units: 12.0 Lec/Sec: Section M Days: Monday Begin: 12:00PM End: 12:50PM Bldg/Room: GHC CLSTR Location: Pittsburgh, Pennsylvania Instructor(s): Cervesato, Kaynar "}, {"question": "On which days does the class Big Data Science Recitation Section A4  offered in Sp meet?", "answer": "Tuesday, Thursday", "context": "Spring offering: Course: 18788 Title: Big Data Science Units: 6.0 Lec/Sec: Section A4 Days: Tuesday, Thursday Begin: 08:00AM End: 09:20AM Bldg/Room: HH 1107 Location: Pittsburgh, Pennsylvania Instructor(s): McSharry "}, {"question": "Based on the background information, What is the units of course Statistics for IT Managers offered in Fa?", "answer": "The units of course Statistics for IT Managers is 6.0", "context": "Fall offering: Course: 95796 Title: Statistics for IT Managers Units: 6.0 Lec/Sec: Section D1 Days: Tuesday, Thursday Begin: 03:30PM End: 04:50PM Bldg/Room: HBH 1206 Location: Pittsburgh, Pennsylvania Instructor(s): Garin "}, {"question": "When does the class Tap II - Rhythmic Technique/Foundational to Complex Recitation Section B  offered in Fa end?", "answer": "01:20PM", "context": "Fall offering: Course: 54223 Title: Tap II - Rhythmic Technique/Foundational to Complex Units: 2.0 Lec/Sec: Section B Days: Friday Begin: 12:30PM End: 01:20PM Bldg/Room: PCA 306 Location: Pittsburgh, Pennsylvania Instructor(s): Conte "}, {"question": "On which days does the class Orchestration I Recitation Section A  offered in Fa meet?", "answer": "Tuesday, Thursday", "context": "Fall offering: Course: 57257 Title: Orchestration I Units: 6.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 09:00AM End: 09:50AM Bldg/Room: CFA M160 Location: Pittsburgh, Pennsylvania Instructor(s): Marthaler "}, {"question": "On which days does the class Weight Training: Recitation Section B3  offered in Sp meet?", "answer": "Tuesday, Thursday", "context": "Spring offering: Course: 69102 Title: Weight Training: Units: 3.0 Lec/Sec: Section B3 Days: Tuesday, Thursday Begin: 12:30PM End: 01:50PM Bldg/Room: CUC COURTS Location: Pittsburgh, Pennsylvania Instructor(s): Connelly "}, {"question": "Which campus is class Data Analytics for Organizational Impacts Recitation Section A2  offered in Pittsburgh in Fa?", "answer": "Pittsburgh, Pennsylvania", "context": "Fall offering: Course: 94865 Title: Data Analytics for Organizational Impacts Units: 6.0 Lec/Sec: Section A2 Days: Tuesday, Thursday Begin: 03:30PM End: 04:50PM Bldg/Room: HBH 1004 Location: Pittsburgh, Pennsylvania Instructor(s): Escallon Barrios "}, {"question": "Where does the class Accounting and Finance Analytics Recitation Section E4  offered in Sp meet?", "answer": "HBH 1206", "context": "Spring offering: Course: 95719 Title: Accounting and Finance Analytics Units: 6.0 Lec/Sec: Section E4 Days: Tuesday, Thursday Begin: 05:00PM End: 06:20PM Bldg/Room: HBH 1206 Location: Pittsburgh, Pennsylvania Instructor(s): Gogolak "}, {"question": "On which days does the class IDeATe: Soft Fabrication Skills Recitation Section A1  offered in Fa meet?", "answer": "Saturday", "context": "Fall offering: Course: 99352 Title: IDeATe: Soft Fabrication Skills Units: 1.0 Lec/Sec: Section A1 Days: Saturday Begin: 10:00AM End: 03:00PM Bldg/Room: HL 106B Location: Pittsburgh, Pennsylvania Instructor(s): Pinchuk "}, {"question": "Which campus is class Principles of Macroeconomics Recitation Section A  offered in Pittsburgh in Sp?", "answer": "Pittsburgh, Pennsylvania", "context": "Spring offering: Course: 73103 Title: Principles of Macroeconomics Units: 9.0 Lec/Sec: Section A Days: Friday Begin: 10:00AM End: 10:50AM Bldg/Room: TEP 3801 Location: Pittsburgh, Pennsylvania Instructor(s): Zetlin-Jones "}, {"question": "When does the class Funding Early Stage Ventures: Recitation Section A4  offered in Sp end?", "answer": "09:45AM", "context": "Spring offering: Course: 45905 Title: Funding Early Stage Ventures: Units: 6.0 Lec/Sec: Section A4 Days: Tuesday, Thursday Begin: 08:00AM End: 09:45AM Bldg/Room: TEP 2111 Location: Pittsburgh, Pennsylvania Instructor(s): Risch "}, {"question": "When does the class Design Center: Design for Social Innovation Recitation Section A  offered in Sp end?", "answer": "01:20PM", "context": "Spring offering: Course: 51782 Title: Design Center: Design for Social Innovation Units: 12.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 12:00PM End: 01:20PM Bldg/Room: MM 121 Location: Pittsburgh, Pennsylvania Instructor(s): Krishnaswami "}, {"question": "On which days does the class Fairness, Accountability, Transparency, and Ethics in Sociotechnical Recitation Section A  offered in Fa meet?", "answer": "Monday, Wednesday", "context": "Fall offering: Course: 05499 Title: Fairness, Accountability, Transparency, and Ethics in Sociotechnical Units: 12.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 11:00AM End: 12:20PM Bldg/Room: HH B103 Location: Pittsburgh, Pennsylvania Instructor(s): Shen "}, {"question": "When does the class Organizational Behavior Recitation Section W  offered in Fa end?", "answer": "12:45PM", "context": "Fall offering: Course: 70311 Title: Organizational Behavior Units: 9.0 Lec/Sec: Section W Days: Monday, Wednesday Begin: 11:30AM End: 12:45PM Bldg/Room: CMB 1190 Location: Doha, Qatar Instructor(s): Haan "}, {"question": "Who is the instructor(s) of course Software Engineering Management: Recitation Section A2  offered in Fa?", "answer": "Fang", "context": "Fall offering: Course: 49772 Title: Software Engineering Management: Units: 6.0 Lec/Sec: Section A2 Days: Monday, Wednesday Begin: 11:00AM End: 12:20PM Bldg/Room: B23 227 Location: San Jose, California Instructor(s): Fang "}, {"question": "On which days does the class Business Leadership Endeavor II Recitation Section C1  offered in Fa meet?", "answer": "Tuesday", "context": "Fall offering: Course: 70204 Title: Business Leadership Endeavor II Units: 3.0 Lec/Sec: Section C1 Days: Tuesday Begin: 02:00PM End: 03:20PM Bldg/Room: TEP 3808 Location: Pittsburgh, Pennsylvania Instructor(s): Jafry O'Connor "}, {"question": "On which days does the class Acting for Leadership and Communication Recitation Section B4  offered in Sp meet?", "answer": "Monday, Wednesday", "context": "Spring offering: Course: 94801 Title: Acting for Leadership and Communication Units: 6.0 Lec/Sec: Section B4 Days: Monday, Wednesday Begin: 11:00AM End: 12:20PM Bldg/Room: HBH 2009 Location: Pittsburgh, Pennsylvania Instructor(s): Murphy "}, {"question": "On which days does the class Mathematical Models for Consulting Recitation Section W  offered in Fa meet?", "answer": "Sunday, Tuesday", "context": "Fall offering: Course: 70460 Title: Mathematical Models for Consulting Units: 9.0 Lec/Sec: Section W Days: Sunday, Tuesday Begin: 04:00PM End: 05:15PM Bldg/Room: CMB 1031 Location: Doha, Qatar Instructor(s): Safak "}, {"question": "Based on the background information, What is the units of course Special Topics in ICT: offered in Fa?", "answer": "The units of course Special Topics in ICT: is 12.0", "context": "Fall offering: Course: 04800 Title: Special Topics in ICT: Units: 12.0 Lec/Sec: Section G Days: Tuesday, Thursday Begin: 10:00AM End: 11:50AM Bldg/Room: CMR F309 Location: Kigali, Rwanda Instructor(s): Tucker "}, {"question": "Which campus is class Techniques in Quantitative Analysis Recitation Section B  offered in Pittsburgh in Fa?", "answer": "Pittsburgh, Pennsylvania", "context": "Fall offering: Course: 09207 Title: Techniques in Quantitative Analysis Units: 9.0 Lec/Sec: Section B Days: Tuesday Begin: 06:30PM End: 08:50PM Bldg/Room: DH 1302 Location: Pittsburgh, Pennsylvania Instructor(s): Botcha "}, {"question": "Based on the background information, What is the units of course Singers offered in Fa?", "answer": "The units of course Singers is 3.0", "context": "Fall offering: Course: 57822 Title: Singers Units: 3.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 05:30PM End: 06:20PM Bldg/Room: MM 119 Location: Pittsburgh, Pennsylvania Instructor(s): Douglas "}, {"question": "What is the abstract of the paper Neutral Face Learning and Progressive Fusion Synthesis Network for NIR-VIS Face Recognition published by LTI faculty Yiming Yang in 2023?", "answer": "To meet the strong demand for deploying face recognition systems in low-light scenarios, the Near-InfraRed and VISible (NIR-VIS) face recognition task is receiving increasing attention. However, heterogeneous faces have the characteristics of heterogeneity and non-neutrality. Heterogeneity refers to the fact that the matching images are in different modalities, and non-neutrality means that the matching images are significantly different in pose, expression, lighting, etc. Both situations pose challenges for NIR-VIS face matching. To address this problem, we propose a novel Neutral face Learning and Progressive Fusion synthesis (NLPF) network to disentangle the latent attributes of heterogeneous faces and learn neutral face representations. Our approach naturally integrates Identity-related Neutral face Learning (INL) and Attribute Progressive Fusion (APF) into a joint framework. Firstly, INL eliminates modal variations and residual variations by guiding the network to learn homogeneous neutral face feature representations, which tackles the challenge of heterogeneity and non-neutrality by mapping cross-modal images to a common neutral representation subspace. Besides, APF is presented to perform the disentanglement and reintegration of identity-related features, modality-related features and residual features in a progressive fusion manner, which helps to further purify identity-related features. Comprehensive evaluations are carried out on three mainstream NIR-VIS datasets to verify the robustness and effectiveness of the NLPF model. In particular, NLPF has competitive recognition performance on LAMP-HQ, the most challenging NIR-VIS dataset so far.", "context": "Author: Yiming Yang Title: Neutral Face Learning and Progressive Fusion Synthesis Network for NIR-VIS Face Recognition Publication year: 2023 Coauthors: Yiming Yang, Weipeng Hu, Haifeng Hu Abstract: To meet the strong demand for deploying face recognition systems in low-light scenarios, the Near-InfraRed and VISible (NIR-VIS) face recognition task is receiving increasing attention. However, heterogeneous faces have the characteristics of heterogeneity and non-neutrality. Heterogeneity refers to the fact that the matching images are in different modalities, and non-neutrality means that the matching images are significantly different in pose, expression, lighting, etc. Both situations pose challenges for NIR-VIS face matching. To address this problem, we propose a novel Neutral face Learning and Progressive Fusion synthesis (NLPF) network to disentangle the latent attributes of heterogeneous faces and learn neutral face representations. Our approach naturally integrates Identity-related Neutral face Learning (INL) and Attribute Progressive Fusion (APF) into a joint framework. Firstly, INL eliminates modal variations and residual variations by guiding the network to learn homogeneous neutral face feature representations, which tackles the challenge of heterogeneity and non-neutrality by mapping cross-modal images to a common neutral representation subspace. Besides, APF is presented to perform the disentanglement and reintegration of identity-related features, modality-related features and residual features in a progressive fusion manner, which helps to further purify identity-related features. Comprehensive evaluations are carried out on three mainstream NIR-VIS datasets to verify the robustness and effectiveness of the NLPF model. In particular, NLPF has competitive recognition performance on LAMP-HQ, the most challenging NIR-VIS dataset so far."}, {"question": "Who are the coauthors of the paper Policy Representation via Diffusion Probability Model for Reinforcement Learning published by LTI faculty Yiming Yang in 2023?", "answer": "Long Yang, Zhixiong Huang, Fenghao Lei, Yucun Zhong, Yiming Yang, Cong Fang, Shiting Wen, Binbin Zhou, Zhouchen Lin", "context": "Author: Yiming Yang Title: Policy Representation via Diffusion Probability Model for Reinforcement Learning Publication year: 2023 Coauthors: Long Yang, Zhixiong Huang, Fenghao Lei, Yucun Zhong, Yiming Yang, Cong Fang, Shiting Wen, Binbin Zhou, Zhouchen Lin Abstract: Popular reinforcement learning (RL) algorithms tend to produce a unimodal policy distribution, which weakens the expressiveness of complicated policy and decays the ability of exploration. The diffusion probability model is powerful to learn complicated multimodal distributions, which has shown promising and potential applications to RL. In this paper, we formally build a theoretical foundation of policy representation via the diffusion probability model and provide practical implementations of diffusion policy for online model-free RL. Concretely, we character diffusion policy as a stochastic process, which is a new approach to representing a policy. Then we present a convergence guarantee for diffusion policy, which provides a theory to understand the multimodality of diffusion policy. Furthermore, we propose the DIPO which is an implementation for model-free online RL with DIffusion POlicy. To the best of our knowledge, DIPO is the first algorithm to solve model-free online RL problems with the diffusion model. Finally, extensive empirical results show the effectiveness and superiority of DIPO on the standard continuous control Mujoco benchmark."}, {"question": "Who is the author of the LTI paper The Multimodal Information Based Speech Processing (MISP) 2023 Challenge: Audio-Visual Target Speaker Extraction published in 2023?", "answer": "Shinji Watanabe", "context": "Author: Shinji Watanabe Title: The Multimodal Information Based Speech Processing (MISP) 2023 Challenge: Audio-Visual Target Speaker Extraction Publication year: 2023 Coauthors: Shilong Wu, Chenxi Wang, Hang Chen, Yusheng Dai, Chenyue Zhang, Ruoyu Wang, Hongbo Lan, Jun Du, Chin-Hui Lee, Jingdong Chen, Shinji Watanabe, Sabato Marco Siniscalchi, O. Scharenborg, Zhong-Qiu Wang, Jia Pan, Jianqing Gao Abstract: Previous Multimodal Information based Speech Processing (MISP) challenges mainly focused on audio-visual speech recognition (AVSR) with commendable success. However, the most advanced back-end recognition systems often hit performance limits due to the complex acoustic environments. This has prompted a shift in focus towards the Audio-Visual Target Speaker Extraction (AVTSE) task for the MISP 2023 challenge in ICASSP 2024 Signal Processing Grand Challenges. Unlike existing audio-visual speech enhance-ment challenges primarily focused on simulation data, the MISP 2023 challenge uniquely explores how front-end speech processing, combined with visual clues, impacts back-end tasks in real-world scenarios. This pioneering effort aims to set the first benchmark for the AVTSE task, offering fresh insights into enhancing the ac-curacy of back-end speech recognition systems through AVTSE in challenging and real acoustic environments. This paper delivers a thorough overview of the task setting, dataset, and baseline system of the MISP 2023 challenge. It also includes an in-depth analysis of the challenges participants may encounter. The experimental results highlight the demanding nature of this task, and we look forward to the innovative solutions participants will bring forward."}, {"question": "Who is the author of the LTI paper FindAdaptNet: Find and Insert Adapters by Learned Layer Importance published in 2023?", "answer": "Shinji Watanabe", "context": "Author: Shinji Watanabe Title: FindAdaptNet: Find and Insert Adapters by Learned Layer Importance Publication year: 2023 Coauthors: Junwei Huang, Karthik Ganesan, Soumi Maiti, Young Min Kim, Xuankai Chang, Paul Liang, Shinji Watanabe Abstract: Adapters are lightweight bottleneck modules introduced to assist pre-trained self-supervised learning (SSL) models to be customized to new tasks. However, searching the appropriate layers to insert adapters on large models has become difficult due to the large number of possible layers and thus a vast search space (2N possibilities for N layers). In this paper, we propose a technique that achieves automatic insertion of adapters for downstream automatic speech recognition (ASR) and spoken language understanding (SLU) tasks. Our approach is based on two-stage training. First, we train our model for a specific downstream task with additional shallow learnable layers and weight parameters to obtain the weighted summation over the output of each layer in SSL. This training method is established by the SUPERB baseline [1]. This first-stage training determines the most important layers given their respective weights. In the second stage, we proceed to insert adapters to the most important layers, retaining both performance and neural architecture search efficiency. On the CommonVoice dataset[2] we obtain 20.6% absolute improvement in Word Error Rate (WER) on the Welsh language against the conventional method, which inserts the adapter modules into the highest layers without search. In the SLURP SLU task, our method yields 4.0% intent accuracy improvement against the same conventional baseline."}, {"question": "Who is the author of the LTI paper Understanding the Effect of Model Compression on Social Bias in Large Language Models published in 2023?", "answer": "Emma Strubell", "context": "Author: Emma Strubell Title: Understanding the Effect of Model Compression on Social Bias in Large Language Models Publication year: 2023 Coauthors: Gustavo Gon\u00e7alves, Emma Strubell Abstract: Large Language Models (LLMs) trained with self-supervision on vast corpora of web text fit to the social biases of that text. Without intervention, these social biases persist in the model's predictions in downstream tasks, leading to representational harm. Many strategies have been proposed to mitigate the effects of inappropriate social biases learned during pretraining. Simultaneously, methods for model compression have become increasingly popular to reduce the computational burden of LLMs. Despite the popularity and need for both approaches, little work has been done to explore the interplay between these two. We perform a carefully controlled study of the impact of model compression via quantization and knowledge distillation on measures of social bias in LLMs. Longer pretraining and larger models led to higher social bias, and quantization showed a regularizer effect with its best trade-off around 20% of the original pretraining time."}, {"question": "Who are the coauthors of the paper One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning published by LTI faculty Eric P. Xing in 2023?", "answer": "Arnav Chavan, Zhuang Liu, D. Gupta, Eric P. Xing, Zhiqiang Shen", "context": "Author: Eric P. Xing Title: One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning Publication year: 2023 Coauthors: Arnav Chavan, Zhuang Liu, D. Gupta, Eric P. Xing, Zhiqiang Shen Abstract: We present Generalized LoRA (GLoRA), an advanced approach for universal parameter-efficient fine-tuning tasks. Enhancing Low-Rank Adaptation (LoRA), GLoRA employs a generalized prompt module to optimize pre-trained model weights and adjust intermediate activations, providing more flexibility and capability across diverse tasks and datasets. Moreover, GLoRA facilitates efficient parameter adaptation by employing a scalable, modular, layer-wise structure search that learns individual adapter of each layer. Originating from a unified mathematical formulation, GLoRA exhibits strong transfer learning, few-shot learning and domain generalization abilities, as it adapts to new tasks through not only weights but also additional dimensions like activations. Comprehensive experiments demonstrate that GLoRA outperforms all previous methods in natural, specialized, and structured vision benchmarks, achieving superior accuracy with fewer parameters and computations. The proposed method on LLaMA-1 and LLaMA-2 also show considerable enhancements compared to the original LoRA in the language domain. Furthermore, our structural re-parameterization design ensures that GLoRA incurs no extra inference cost, rendering it a practical solution for resource-limited applications. Code and models are available at: https://github.com/Arnav0400/ViT-Slim/tree/master/GLoRA."}, {"question": "Which LTI faculty published the paper Structured Pruning of Self-Supervised Pre-Trained Models for Speech Recognition and Understanding in 2023?", "answer": "Shinji Watanabe", "context": "Author: Shinji Watanabe Title: Structured Pruning of Self-Supervised Pre-Trained Models for Speech Recognition and Understanding Publication year: 2023 Coauthors: Yifan Peng, Kwangyoun Kim, Felix Wu, Prashant Sridhar, Shinji Watanabe Abstract: Self-supervised speech representation learning (SSL) has shown to be effective in various downstream tasks, but SSL models are usually large and slow. Model compression techniques such as pruning aim to reduce the model size and computation without degradation in accuracy. Prior studies focus on the pruning of Transformers; however, speech models not only utilize a stack of Transformer blocks, but also combine a frontend network based on multiple convolutional layers for low-level feature representation learning. This frontend has a small size but a heavy computational cost. In this work, we propose three task-specific structured pruning methods to deal with such heterogeneous networks. Experiments on LibriSpeech and SLURP show that the proposed method is more accurate than the original wav2vec2-base with 10% to 30% less computation, and is able to reduce the computation by 40% to 50% without any degradation."}, {"question": "Who is the author of the LTI paper The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features published in 2023?", "answer": "Rita Singh", "context": "Author: Rita Singh Title: The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features Publication year: 2023 Coauthors: Liao Qu, X. Zou, Xiang Li, Yandong Wen, Rita Singh, B. Raj Abstract: This work unveils the enigmatic link between phonemes and facial features. Traditional studies on voice-face correlations typically involve using a long period of voice input, including generating face images from voices and reconstructing 3D face meshes from voices. However, in situations like voice-based crimes, the available voice evidence may be short and limited. Additionally, from a physiological perspective, each segment of speech -- phoneme -- corresponds to different types of airflow and movements in the face. Therefore, it is advantageous to discover the hidden link between phonemes and face attributes. In this paper, we propose an analysis pipeline to help us explore the voice-face relationship in a fine-grained manner, i.e., phonemes v.s. facial anthropometric measurements (AM). We build an estimator for each phoneme-AM pair and evaluate the correlation through hypothesis testing. Our results indicate that AMs are more predictable from vowels compared to consonants, particularly with plosives. Additionally, we observe that if a specific AM exhibits more movement during phoneme pronunciation, it is more predictable. Our findings support those in physiology regarding correlation and lay the groundwork for future research on speech-face multimodal learning."}, {"question": "What is the abstract of the paper Core loss analysis of soft magnetic composite under non-sinusoidal excitation based on finite element models published by LTI faculty Yiming Yang in 2023?", "answer": "Due to the effect of higher harmonics on magnetic properties under actual complex operating conditions, the accurate calculation of core losses of soft magnetic composites (SMC) is complicated. First, this paper improves the existing SMC model by introducing a correction factor to correct the hysteresis loss coefficient so that the model can consider the local variation characteristics of the magnetic density waveform and then calculate the core loss under different harmonic excitation. Then, the influence of skin effect and inhomogeneous flux density within the ring sample model is analyzed. Finally, to validate the improved model, it is compared with other models in the reference based on experimental measurements, respectively. The results show that the core loss calculated by the improved model is closer to the experimental results under different harmonic excitations. In addition, the applicability of the improved SMC model under triangular and square wave excitations is also verified by the derivation of the equations.", "context": "Author: Yiming Yang Title: Core loss analysis of soft magnetic composite under non-sinusoidal excitation based on finite element models Publication year: 2023 Coauthors: Lei Zhao, Chengcheng Liu, Youhua H. Wang, Yiming Yang Abstract: Due to the effect of higher harmonics on magnetic properties under actual complex operating conditions, the accurate calculation of core losses of soft magnetic composites (SMC) is complicated. First, this paper improves the existing SMC model by introducing a correction factor to correct the hysteresis loss coefficient so that the model can consider the local variation characteristics of the magnetic density waveform and then calculate the core loss under different harmonic excitation. Then, the influence of skin effect and inhomogeneous flux density within the ring sample model is analyzed. Finally, to validate the improved model, it is compared with other models in the reference based on experimental measurements, respectively. The results show that the core loss calculated by the improved model is closer to the experimental results under different harmonic excitations. In addition, the applicability of the improved SMC model under triangular and square wave excitations is also verified by the derivation of the equations."}, {"question": "Who is the author of the LTI paper AutoReply: Detecting Nonsense in Dialogue with Discriminative Replies published in 2023?", "answer": "Daniel Fried", "context": "Author: Daniel Fried Title: AutoReply: Detecting Nonsense in Dialogue with Discriminative Replies Publication year: 2023 Coauthors: Weiyan Shi, Emily Dinan, Adi Renduchintala, Daniel Fried, Athul Paul Jacob, Zhou Yu, Mike Lewis "}, {"question": "Who is the author of the LTI paper A Gold Standard Dataset for the Reviewer Assignment Problem published in 2023?", "answer": "Graham Neubig", "context": "Author: Graham Neubig Title: A Gold Standard Dataset for the Reviewer Assignment Problem Publication year: 2023 Coauthors: Ivan Stelmakh, J. Wieting, Graham Neubig, Nihar B. Shah Abstract: Many peer-review venues are either using or looking to use algorithms to assign submissions to reviewers. The crux of such automated approaches is the notion of the\"similarity score\"--a numerical estimate of the expertise of a reviewer in reviewing a paper--and many algorithms have been proposed to compute these scores. However, these algorithms have not been subjected to a principled comparison, making it difficult for stakeholders to choose the algorithm in an evidence-based manner. The key challenge in comparing existing algorithms and developing better algorithms is the lack of the publicly available gold-standard data that would be needed to perform reproducible research. We address this challenge by collecting a novel dataset of similarity scores that we release to the research community. Our dataset consists of 477 self-reported expertise scores provided by 58 researchers who evaluated their expertise in reviewing papers they have read previously. We use this data to compare several popular algorithms employed in computer science conferences and come up with recommendations for stakeholders. Our main findings are as follows. First, all algorithms make a non-trivial amount of error. For the task of ordering two papers in terms of their relevance for a reviewer, the error rates range from 12%-30% in easy cases to 36%-43% in hard cases, highlighting the vital need for more research on the similarity-computation problem. Second, most existing algorithms are designed to work with titles and abstracts of papers, and in this regime the Specter+MFR algorithm performs best. Third, to improve performance, it may be important to develop modern deep-learning based algorithms that can make use of the full texts of papers: the classical TD-IDF algorithm enhanced with full texts of papers is on par with the deep-learning based Specter+MFR that cannot make use of this information."}, {"question": "Who is the author of the LTI paper Evaluating Multilingual Speech Translation under Realistic Conditions with Resegmentation and Terminology published in 2023?", "answer": "Mona T. Diab", "context": "Author: Mona T. Diab Title: Evaluating Multilingual Speech Translation under Realistic Conditions with Resegmentation and Terminology Publication year: 2023 Coauthors: Elizabeth Salesky, Kareem Darwish, Mohamed Al-Badrashiny, Mona T. Diab, J. Niehues Abstract: We present the ACL 60/60 evaluation sets for multilingual translation of ACL 2022 technical presentations into 10 target languages. This dataset enables further research into multilingual speech translation under realistic recording conditions with unsegmented audio and domain-specific terminology, applying NLP tools to text and speech in the technical domain, and evaluating and improving model robustness to diverse speaker demographics."}, {"question": "What is the abstract of the paper I3D: Transformer Architectures with Input-Dependent Dynamic Depth for Speech Recognition published by LTI faculty Shinji Watanabe in 2023?", "answer": "Transformer-based end-to-end speech recognition has achieved great success. However, the large footprint and computational overhead make it difficult to deploy these models in some real-world applications. Model compression techniques can reduce the model size and speed up inference, but the compressed model has a fixed architecture which might be suboptimal. We propose a novel Transformer encoder with Input-Dependent Dynamic Depth (I3D) to achieve strong performance-efficiency trade-offs. With a similar number of layers at inference time, I3D-based models outperform the vanilla Transformer and the static pruned model via iterative layer pruning. We also present interesting analysis on the gate probabilities and the input-dependency, which helps us better understand deep encoders.", "context": "Author: Shinji Watanabe Title: I3D: Transformer Architectures with Input-Dependent Dynamic Depth for Speech Recognition Publication year: 2023 Coauthors: Yifan Peng, Jaesong Lee, Shinji Watanabe Abstract: Transformer-based end-to-end speech recognition has achieved great success. However, the large footprint and computational overhead make it difficult to deploy these models in some real-world applications. Model compression techniques can reduce the model size and speed up inference, but the compressed model has a fixed architecture which might be suboptimal. We propose a novel Transformer encoder with Input-Dependent Dynamic Depth (I3D) to achieve strong performance-efficiency trade-offs. With a similar number of layers at inference time, I3D-based models outperform the vanilla Transformer and the static pruned model via iterative layer pruning. We also present interesting analysis on the gate probabilities and the input-dependency, which helps us better understand deep encoders."}, {"question": "What is the abstract of the paper ActiveRAG: Revealing the Treasures of Knowledge via Active Learning published by LTI faculty Chenyan Xiong in 2024?", "answer": "Retrieval Augmented Generation (RAG) has introduced a new paradigm for Large Language Models (LLMs), aiding in the resolution of knowledge-intensive tasks. However, current RAG models position LLMs as passive knowledge receptors, thereby restricting their capacity for learning and comprehending external knowledge. In this paper, we present ActiveRAG, an innovative RAG framework that shifts from passive knowledge acquisition to an active learning mechanism. This approach utilizes the Knowledge Construction mechanism to develop a deeper understanding of external knowledge by associating it with previously acquired or memorized knowledge. Subsequently, it designs the Cognitive Nexus mechanism to incorporate the outcomes from both chains of thought and knowledge construction, thereby calibrating the intrinsic cognition of LLMs. Our experimental results demonstrate that ActiveRAG surpasses previous RAG models, achieving a 5% improvement on question-answering datasets. All data and codes are available at https://github.com/OpenMatch/ActiveRAG.", "context": "Author: Chenyan Xiong Title: ActiveRAG: Revealing the Treasures of Knowledge via Active Learning Publication year: 2024 Coauthors: Zhipeng Xu, Zhenghao Liu, Yibin Liu, Chenyan Xiong, Yukun Yan, Shuo Wang, Shi Yu, Zhiyuan Liu, Ge Yu Abstract: Retrieval Augmented Generation (RAG) has introduced a new paradigm for Large Language Models (LLMs), aiding in the resolution of knowledge-intensive tasks. However, current RAG models position LLMs as passive knowledge receptors, thereby restricting their capacity for learning and comprehending external knowledge. In this paper, we present ActiveRAG, an innovative RAG framework that shifts from passive knowledge acquisition to an active learning mechanism. This approach utilizes the Knowledge Construction mechanism to develop a deeper understanding of external knowledge by associating it with previously acquired or memorized knowledge. Subsequently, it designs the Cognitive Nexus mechanism to incorporate the outcomes from both chains of thought and knowledge construction, thereby calibrating the intrinsic cognition of LLMs. Our experimental results demonstrate that ActiveRAG surpasses previous RAG models, achieving a 5% improvement on question-answering datasets. All data and codes are available at https://github.com/OpenMatch/ActiveRAG."}, {"question": "Who is the author of the LTI paper Distributionally Robust Unsupervised Dense Retrieval Training on Web Graphs published in 2023?", "answer": "Chenyan Xiong", "context": "Author: Chenyan Xiong Title: Distributionally Robust Unsupervised Dense Retrieval Training on Web Graphs Publication year: 2023 Coauthors: Peixuan Han, Zhenghao Liu, Zhiyuan Liu, Chenyan Xiong Abstract: This paper introduces Web-DRO, an unsupervised dense retrieval model, which clusters documents based on web structures and reweights the groups during contrastive training. Specifically, we first leverage web graph links and contrastively train an embedding model for clustering anchor-document pairs. Then we use Group Distributional Robust Optimization to reweight different clusters of anchor-document pairs, which guides the model to assign more weights to the group with higher contrastive loss and pay more attention to the worst case during training. Our experiments on MS MARCO and BEIR show that our model, Web-DRO, significantly improves the retrieval effectiveness in unsupervised scenarios. A comparison of clustering techniques shows that training on the web graph combining URL information reaches optimal performance on clustering. Further analysis confirms that group weights are stable and valid, indicating consistent model preferences as well as effective up-weighting of valuable groups and down-weighting of uninformative ones. The code of this paper can be obtained from https://github.com/OpenMatch/Web-DRO."}, {"question": "Which LTI faculty published the paper Are aligned neural networks adversarially aligned? in 2023?", "answer": "Daphne Ippolito", "context": "Author: Daphne Ippolito Title: Are aligned neural networks adversarially aligned? Publication year: 2023 Coauthors: Nicholas Carlini, Milad Nasr, Christopher A. Choquette-Choo, Matthew Jagielski, Irena Gao, Anas Awadalla, Pang Wei Koh, Daphne Ippolito, Katherine Lee, Florian Tram\u00e8r, Ludwig Schmidt Abstract: Large language models are now tuned to align with the goals of their creators, namely to be\"helpful and harmless.\"These models should respond helpfully to user questions, but refuse to answer requests that could cause harm. However, adversarial users can construct inputs which circumvent attempts at alignment. In this work, we study to what extent these models remain aligned, even when interacting with an adversarial user who constructs worst-case inputs (adversarial examples). These inputs are designed to cause the model to emit harmful content that would otherwise be prohibited. We show that existing NLP-based optimization attacks are insufficiently powerful to reliably attack aligned text models: even when current NLP-based attacks fail, we can find adversarial inputs with brute force. As a result, the failure of current attacks should not be seen as proof that aligned text models remain aligned under adversarial inputs. However the recent trend in large-scale ML models is multimodal models that allow users to provide images that influence the text that is generated. We show these models can be easily attacked, i.e., induced to perform arbitrary un-aligned behavior through adversarial perturbation of the input image. We conjecture that improved NLP attacks may demonstrate this same level of adversarial control over text-only models."}, {"question": "Which LTI faculty published the paper Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large Language Models in 2024?", "answer": "Eric Xing", "context": "Author: Eric Xing Title: Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large Language Models Publication year: 2024 Coauthors: Loka Li, Guan-Hong Chen, Yusheng Su, Zhenhao Chen, Yixuan Zhang, Eric Xing, Kun Zhang Abstract: The recent success of Large Language Models (LLMs) has catalyzed an increasing interest in their self-correction capabilities. This paper presents a comprehensive investigation into the intrinsic self-correction of LLMs, attempting to address the ongoing debate about its feasibility. Our research has identified an important latent factor - the ``confidence'' of LLMs - during the self-correction process. Overlooking this factor may cause the models to over-criticize themselves, resulting in unreliable conclusions regarding the efficacy of self-correction. We have experimentally observed that LLMs possess the capability to understand the ``confidence'' in their own responses. It motivates us to develop an ``If-or-Else'' (IoE) prompting framework, designed to guide LLMs in assessing their own ``confidence'', facilitating intrinsic self-corrections. We conduct extensive experiments and demonstrate that our IoE-based Prompt can achieve a consistent improvement regarding the accuracy of self-corrected responses over the initial answers. Our study not only sheds light on the underlying factors affecting self-correction in LLMs, but also introduces a practical framework that utilizes the IoE prompting principle to efficiently improve self-correction capabilities with ``confidence''. The code is available at \\url{https://github.com/MBZUAI-CLeaR/IoE-Prompting.git}."}, {"question": "What is the abstract of the paper Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers published by LTI faculty Yiming Yang in 2023?", "answer": "We propose a new class of linear Transformers called FourierLearner-Transformers (FLTs), which incorporate a wide range of relative positional encoding mechanisms (RPEs). These include regular RPE techniques applied for nongeometric data, as well as novel RPEs operating on the sequences of tokens embedded in higher-dimensional Euclidean spaces (e.g. point clouds). FLTs construct the optimal RPE mechanism implicitly by learning its spectral representation. As opposed to other architectures combining efficient low-rank linear attention with RPEs, FLTs remain practical in terms of their memory usage and do not require additional assumptions about the structure of the RPE-mask. FLTs allow also for applying certain structural inductive bias techniques to specify masking strategies, e.g. they provide a way to learn the so-called local RPEs introduced in this paper and providing accuracy gains as compared with several other linear Transformers for language modeling. We also thoroughly tested FLTs on other data modalities and tasks, such as: image classification and 3D molecular modeling. For 3D-data FLTs are, to the best of our knowledge, the first Transformers architectures providing RPE-enhanced linear attention.", "context": "Author: Yiming Yang Title: Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers Publication year: 2023 Coauthors: K. Choromanski, Shanda Li, Valerii Likhosherstov, Kumar Avinava Dubey, Shengjie Luo, Di He, Yiming Yang, Tam\u00e1s Sarl\u00f3s, Thomas Weingarten, Adrian Weller Abstract: We propose a new class of linear Transformers called FourierLearner-Transformers (FLTs), which incorporate a wide range of relative positional encoding mechanisms (RPEs). These include regular RPE techniques applied for nongeometric data, as well as novel RPEs operating on the sequences of tokens embedded in higher-dimensional Euclidean spaces (e.g. point clouds). FLTs construct the optimal RPE mechanism implicitly by learning its spectral representation. As opposed to other architectures combining efficient low-rank linear attention with RPEs, FLTs remain practical in terms of their memory usage and do not require additional assumptions about the structure of the RPE-mask. FLTs allow also for applying certain structural inductive bias techniques to specify masking strategies, e.g. they provide a way to learn the so-called local RPEs introduced in this paper and providing accuracy gains as compared with several other linear Transformers for language modeling. We also thoroughly tested FLTs on other data modalities and tasks, such as: image classification and 3D molecular modeling. For 3D-data FLTs are, to the best of our knowledge, the first Transformers architectures providing RPE-enhanced linear attention."}, {"question": "Who is the author of the LTI paper Unsupervised Dense Retrieval Training with Web Anchors published in 2023?", "answer": "Chenyan Xiong", "context": "Author: Chenyan Xiong Title: Unsupervised Dense Retrieval Training with Web Anchors Publication year: 2023 Coauthors: Yiqing Xie, X. Liu, Chenyan Xiong Abstract: In this work, we present an unsupervised retrieval method with contrastive learning on web anchors. The anchor text describes the content that is referenced from the linked page. This shows similarities to search queries that aim to retrieve pertinent information from relevant documents. Based on their commonalities, we train an unsupervised dense retriever, Anchor-DR, with a contrastive learning task that matches the anchor text and the linked document. To filter out uninformative anchors (such as \"homepage\" or other functional anchors), we present a novel filtering technique to only select anchors that contain similar types of information as search queries. Experiments show that Anchor-DR outperforms state-of-the-art methods on unsupervised dense retrieval by a large margin (e.g., by 5.3% NDCG@10 on MSMARCO). The gain of our method is especially significant for search and question answering tasks. Our analysis further reveals that the pattern of anchor-document pairs is similar to that of search query-document pairs. Code available at https://github.com/Veronicium/AnchorDR."}, {"question": "Who is the author of the LTI paper Faith and Fate: Limits of Transformers on Compositionality published in 2023?", "answer": "S. Welleck", "context": "Author: S. Welleck Title: Faith and Fate: Limits of Transformers on Compositionality Publication year: 2023 Coauthors: Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jian, Bill Yuchen Lin, Peter West, Chandra Bhagavatula, Ronan Le Bras, Jena D. Hwang, Soumya Sanyal, S. Welleck, Xiang Ren, Allyson Ettinger, Za\u00efd Harchaoui, Yejin Choi Abstract: Transformer large language models (LLMs) have sparked admiration for their exceptional performance on tasks that demand intricate multi-step reasoning. Yet, these models simultaneously show failures on surprisingly trivial problems. This begs the question: Are these errors incidental, or do they signal more substantial limitations? In an attempt to demystify transformer LLMs, we investigate the limits of these models across three representative compositional tasks -- multi-digit multiplication, logic grid puzzles, and a classic dynamic programming problem. These tasks require breaking problems down into sub-steps and synthesizing these steps into a precise answer. We formulate compositional tasks as computation graphs to systematically quantify the level of complexity, and break down reasoning steps into intermediate sub-procedures. Our empirical findings suggest that transformer LLMs solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, without necessarily developing systematic problem-solving skills. To round off our empirical study, we provide theoretical arguments on abstract multi-step reasoning problems that highlight how autoregressive generations' performance can rapidly decay with\\,increased\\,task\\,complexity."}, {"question": "Which LTI faculty published the paper Active Retrieval Augmented Generation in 2023?", "answer": "Graham Neubig", "context": "Author: Graham Neubig Title: Active Retrieval Augmented Generation Publication year: 2023 Coauthors: Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig Abstract: Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE."}, {"question": "Who are the coauthors of the paper The DARPA Wikidata Overlay: Wikidata as an ontology for natural language processing published by LTI faculty A. Gershman in 2023?", "answer": "Elizabeth Spaulding, Kathryn Conger, A. Gershman, Rosario Uceda-Sosa, S. Brown, James Pustejovsky, Peter Anick, Martha Palmer", "context": "Author: A. Gershman Title: The DARPA Wikidata Overlay: Wikidata as an ontology for natural language processing Publication year: 2023 Coauthors: Elizabeth Spaulding, Kathryn Conger, A. Gershman, Rosario Uceda-Sosa, S. Brown, James Pustejovsky, Peter Anick, Martha Palmer Abstract: With 102,530,067 items currently in its crowd-sourced knowledge base, Wikidata provides NLP practitioners a unique and powerful resource for inference and reasoning over real-world entities. However, because Wikidata is very entity focused, events and actions are often labeled with eventive nouns (e.g., the process of diagnosing a person\u2019s illness is labeled \u201cdiagnosis\u201d), and the typical participants in an event are not described or linked to that event concept (e.g., the medical professional or patient). Motivated by a need for an adaptable, comprehensive, domain-flexible ontology for information extraction, including identifying the roles entities are playing in an event, we present a curated subset of Wikidata in which events have been enriched with PropBank roles. To enable richer narrative understanding between events from Wikidata concepts, we have also provided a comprehensive mapping from temporal Qnodes and Pnodes to the Allen Interval Temporal Logic relations."}, {"question": "What is the abstract of the paper Exploration of Efficient End-to-End ASR using Discretized Input from Self-Supervised Learning published by LTI faculty Shinji Watanabe in 2023?", "answer": "Self-supervised learning (SSL) of speech has shown impressive results in speech-related tasks, particularly in automatic speech recognition (ASR). While most methods employ the output of intermediate layers of the SSL model as real-valued features for downstream tasks, there is potential in exploring alternative approaches that use discretized token sequences. This approach offers benefits such as lower storage requirements and the ability to apply techniques from natural language processing. In this paper, we propose a new protocol that utilizes discretized token sequences in ASR tasks, which includes de-duplication and sub-word modeling to enhance the input sequence. It reduces computational cost by decreasing the length of the sequence. Our experiments on the LibriSpeech dataset demonstrate that our proposed protocol performs competitively with conventional ASR systems using continuous input features, while reducing computational and storage costs.", "context": "Author: Shinji Watanabe Title: Exploration of Efficient End-to-End ASR using Discretized Input from Self-Supervised Learning Publication year: 2023 Coauthors: Xuankai Chang, Brian Yan, Yuya Fujita, Takashi Maekaku, Shinji Watanabe Abstract: Self-supervised learning (SSL) of speech has shown impressive results in speech-related tasks, particularly in automatic speech recognition (ASR). While most methods employ the output of intermediate layers of the SSL model as real-valued features for downstream tasks, there is potential in exploring alternative approaches that use discretized token sequences. This approach offers benefits such as lower storage requirements and the ability to apply techniques from natural language processing. In this paper, we propose a new protocol that utilizes discretized token sequences in ASR tasks, which includes de-duplication and sub-word modeling to enhance the input sequence. It reduces computational cost by decreasing the length of the sequence. Our experiments on the LibriSpeech dataset demonstrate that our proposed protocol performs competitively with conventional ASR systems using continuous input features, while reducing computational and storage costs."}, {"question": "What is the abstract of the paper Data-efficient Active Learning for Structured Prediction with Partial Annotation and Self-Training published by LTI faculty Emma Strubell in 2023?", "answer": "In this work we propose a pragmatic method that reduces the annotation cost for structured label spaces using active learning. Our approach leverages partial annotation, which reduces labeling costs for structured outputs by selecting only the most informative sub-structures for annotation. We also utilize self-training to incorporate the current model's automatic predictions as pseudo-labels for un-annotated sub-structures. A key challenge in effectively combining partial annotation with self-training to reduce annotation cost is determining which sub-structures to select to label. To address this challenge, we adopt an error estimator to adaptively decide the partial selection ratio according to the current model's capability. In evaluations spanning four structured prediction tasks, we show that our combination of partial annotation and self-training using an adaptive selection ratio reduces annotation cost over strong full annotation baselines under a fair comparison scheme that takes reading time into consideration.", "context": "Author: Emma Strubell Title: Data-efficient Active Learning for Structured Prediction with Partial Annotation and Self-Training Publication year: 2023 Coauthors: Zhisong Zhang, Emma Strubell, E. Hovy Abstract: In this work we propose a pragmatic method that reduces the annotation cost for structured label spaces using active learning. Our approach leverages partial annotation, which reduces labeling costs for structured outputs by selecting only the most informative sub-structures for annotation. We also utilize self-training to incorporate the current model's automatic predictions as pseudo-labels for un-annotated sub-structures. A key challenge in effectively combining partial annotation with self-training to reduce annotation cost is determining which sub-structures to select to label. To address this challenge, we adopt an error estimator to adaptively decide the partial selection ratio according to the current model's capability. In evaluations spanning four structured prediction tasks, we show that our combination of partial annotation and self-training using an adaptive selection ratio reduces annotation cost over strong full annotation baselines under a fair comparison scheme that takes reading time into consideration."}, {"question": "Which LTI faculty published the paper Improving Massively Multilingual ASR with Auxiliary CTC Objectives in 2023?", "answer": "Shinji Watanabe", "context": "Author: Shinji Watanabe Title: Improving Massively Multilingual ASR with Auxiliary CTC Objectives Publication year: 2023 Coauthors: William Chen, Brian Yan, Jiatong Shi, Yifan Peng, Soumi Maiti, Shinji Watanabe Abstract: Multilingual Automatic Speech Recognition (ASR) models have extended the usability of speech technologies to a wide variety of languages. With how many languages these models have to handle, however, a key to understanding their imbalanced performance across different languages is to examine if the model actually knows which language it should transcribe. In this paper, we introduce our work on improving performance on FLEURS, a 102-language open ASR benchmark, by conditioning the entire model on language identity (LID). We investigate techniques inspired from recent Connectionist Temporal Classification (CTC) studies to help the model handle the large number of languages, conditioning on the LID predictions of auxiliary tasks. Our experimental results demonstrate the effectiveness of our technique over standard CTC/Attention-based hybrid models. Furthermore, our state-of-the-art systems using self-supervised models with the Conformer architecture improve over the results of prior work on FLEURS by a relative 28.4% CER. Trained models are reproducible recipes are available at https://github.com/espnet/espnet/tree/master/egs2/fleurs/asr1."}, {"question": "Who are the coauthors of the paper LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning published by LTI faculty Eric P. Xing in 2023?", "answer": "Han Guo, P. Greengard, Eric P. Xing, Yoon Kim", "context": "Author: Eric P. Xing Title: LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning Publication year: 2023 Coauthors: Han Guo, P. Greengard, Eric P. Xing, Yoon Kim Abstract: We propose a simple approach for memory-efficient adaptation of pretrained language models. Our approach uses an iterative algorithm to decompose each pretrained matrix into a high-precision low-rank component and a memory-efficient quantized component. During finetuning, the quantized component remains fixed and only the low-rank component is updated. We present an integer linear programming formulation of the quantization component which enables dynamic configuration of quantization parameters (e.g., bit-width, block size) for each matrix given an overall target memory budget. We further explore a data-aware version of the algorithm which uses an approximation of the Fisher information matrix to weight the reconstruction objective during matrix decomposition. Experiments on finetuning RoBERTa and LLaMA-2 (7B and 70B) demonstrate that our low-rank plus quantized matrix decomposition approach (LQ-LoRA) outperforms strong QLoRA and GPTQ-LoRA baselines and enables aggressive quantization to sub-3 bits with only minor performance degradations. When finetuned on a language modeling calibration dataset, LQ-LoRA can also be used for model compression; in this setting our 2.75-bit LLaMA-2-70B model (which has 2.85 bits on average when including the low-rank components and requires 27GB of GPU memory) performs respectably compared to the 16-bit baseline."}, {"question": "Who are the coauthors of the paper A Self-enhancement Approach for Domain-specific Chatbot Training via Knowledge Mining and Digest published by LTI faculty Yiming Yang in 2023?", "answer": "Ruohong Zhang, Luyu Gao, Chen Zheng, Zhen Fan, Guokun Lai, Zheng Zhang, Fangzhou Ai, Yiming Yang, Hongxia Yang", "context": "Author: Yiming Yang Title: A Self-enhancement Approach for Domain-specific Chatbot Training via Knowledge Mining and Digest Publication year: 2023 Coauthors: Ruohong Zhang, Luyu Gao, Chen Zheng, Zhen Fan, Guokun Lai, Zheng Zhang, Fangzhou Ai, Yiming Yang, Hongxia Yang Abstract: Large Language Models (LLMs), despite their great power in language generation, often encounter challenges when dealing with intricate and knowledge-demanding queries in specific domains. This paper introduces a novel approach to enhance LLMs by effectively extracting the relevant knowledge from domain-specific textual sources, and the adaptive training of a chatbot with domain-specific inquiries. Our two-step approach starts from training a knowledge miner, namely LLMiner, which autonomously extracts Question-Answer pairs from relevant documents through a chain-of-thought reasoning process. Subsequently, we blend the mined QA pairs with a conversational dataset to fine-tune the LLM as a chatbot, thereby enriching its domain-specific expertise and conversational capabilities. We also developed a new evaluation benchmark which comprises four domain-specific text corpora and associated human-crafted QA pairs for testing. Our model shows remarkable performance improvement over generally aligned LLM and surpasses domain-adapted models directly fine-tuned on domain corpus. In particular, LLMiner achieves this with minimal human intervention, requiring only 600 seed instances, thereby providing a pathway towards self-improvement of LLMs through model-synthesized training data."}, {"question": "Which LTI faculty published the paper Distributionally Robust Unsupervised Dense Retrieval Training on Web Graphs in 2023?", "answer": "Chenyan Xiong", "context": "Author: Chenyan Xiong Title: Distributionally Robust Unsupervised Dense Retrieval Training on Web Graphs Publication year: 2023 Coauthors: Peixuan Han, Zhenghao Liu, Zhiyuan Liu, Chenyan Xiong Abstract: This paper introduces Web-DRO, an unsupervised dense retrieval model, which clusters documents based on web structures and reweights the groups during contrastive training. Specifically, we first leverage web graph links and contrastively train an embedding model for clustering anchor-document pairs. Then we use Group Distributional Robust Optimization to reweight different clusters of anchor-document pairs, which guides the model to assign more weights to the group with higher contrastive loss and pay more attention to the worst case during training. Our experiments on MS MARCO and BEIR show that our model, Web-DRO, significantly improves the retrieval effectiveness in unsupervised scenarios. A comparison of clustering techniques shows that training on the web graph combining URL information reaches optimal performance on clustering. Further analysis confirms that group weights are stable and valid, indicating consistent model preferences as well as effective up-weighting of valuable groups and down-weighting of uninformative ones. The code of this paper can be obtained from https://github.com/OpenMatch/Web-DRO."}, {"question": "Who is the author of the LTI paper Multilingual TTS Accent Impressions for Accented ASR published in 2023?", "answer": "David R. Mortensen", "context": "Author: David R. Mortensen Title: Multilingual TTS Accent Impressions for Accented ASR Publication year: 2023 Coauthors: G. Karakasidis, Nathaniel R. Robinson, Yaroslav Getman, Atieno Ogayo, Ragheb Al-Ghezi, Ananya Ayasi, Shinji Watanabe, David R. Mortensen, M. Kurimo "}, {"question": "What is the abstract of the paper Transformed Protoform Reconstruction published by LTI faculty David R. Mortensen in 2023?", "answer": "Protoform reconstruction is the task of inferring what morphemes or words appeared like in the ancestral languages of a set of daughter languages. Meloni et al (2021) achieved the state-of-the-art on Latin protoform reconstruction with an RNN-based encoder-decoder with attention model. We update their model with the state-of-the-art seq2seq model: the Transformer. Our model outperforms their model on a suite of different metrics on two different datasets: their Romance data of 8,000 cognates spanning 5 languages and a Chinese dataset (Hou 2004) of 800+ cognates spanning 39 varieties. We also probe our model for potential phylogenetic signal contained in the model. Our code is publicly available at https://github.com/cmu-llab/acl-2023.", "context": "Author: David R. Mortensen Title: Transformed Protoform Reconstruction Publication year: 2023 Coauthors: Young Min Kim, Kalvin Chang, Chenxuan Cui, David R. Mortensen Abstract: Protoform reconstruction is the task of inferring what morphemes or words appeared like in the ancestral languages of a set of daughter languages. Meloni et al (2021) achieved the state-of-the-art on Latin protoform reconstruction with an RNN-based encoder-decoder with attention model. We update their model with the state-of-the-art seq2seq model: the Transformer. Our model outperforms their model on a suite of different metrics on two different datasets: their Romance data of 8,000 cognates spanning 5 languages and a Chinese dataset (Hou 2004) of 800+ cognates spanning 39 varieties. We also probe our model for potential phylogenetic signal contained in the model. Our code is publicly available at https://github.com/cmu-llab/acl-2023."}, {"question": "Who is the author of the LTI paper Learning to Ask Questions for Zero-shot Dialogue State Tracking published in 2023?", "answer": "Alexander I. Rudnicky", "context": "Author: Alexander I. Rudnicky Title: Learning to Ask Questions for Zero-shot Dialogue State Tracking Publication year: 2023 Coauthors: Diogo Tavares, David Semedo, Alexander I. Rudnicky, Jo\u00e3o Magalh\u00e3es Abstract: We present a method for performing zero-shot Dialogue State Tracking (DST) by casting the task as a learning-to-ask-questions framework. The framework learns to pair the best question generation (QG) strategy with in-domain question answering (QA) methods to extract slot values from a dialogue without any human intervention. A novel self-supervised QA pretraining step using in-domain data is essential to learn the structure without requiring any slot-filling annotations. Moreover, we show that QG methods need to be aligned with the same grammatical person used in the dialogue. Empirical evaluation on the MultiWOZ 2.1 dataset demonstrates that our approach, when used alongside robust QA models, outperforms existing zero-shot methods in the challenging task of zero-shot cross domain adaptation-given a comparable amount of domain knowledge during data creation. Finally, we analyze the impact of the types of questions used, and demonstrate that the algorithmic approach outperforms template-based question generation."}, {"question": "Who are the coauthors of the paper The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation published by LTI faculty Graham Neubig in 2023?", "answer": "Patrick Fernandes, Daniel Deutsch, M. Finkelstein, Parker Riley, Andr\u00e9 F. T. Martins, Graham Neubig, Ankush Garg, J. Clark, Markus Freitag, Orhan Firat", "context": "Author: Graham Neubig Title: The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation Publication year: 2023 Coauthors: Patrick Fernandes, Daniel Deutsch, M. Finkelstein, Parker Riley, Andr\u00e9 F. T. Martins, Graham Neubig, Ankush Garg, J. Clark, Markus Freitag, Orhan Firat Abstract: Automatic evaluation of machine translation (MT) is a critical tool driving the rapid iterative development of MT systems. While considerable progress has been made on estimating a single scalar quality score, current metrics lack the informativeness of more detailed schemes that annotate individual errors, such as Multidimensional Quality Metrics (MQM). In this paper, we help fill this gap by proposing AutoMQM, a prompting technique which leverages the reasoning and in-context learning capabilities of large language models (LLMs) and asks them to identify and categorize errors in translations. We start by evaluating recent LLMs, such as PaLM and PaLM-2, through simple score prediction prompting, and we study the impact of labeled data through in-context learning and finetuning. We then evaluate AutoMQM with PaLM-2 models, and we find that it improves performance compared to just prompting for scores (with particularly large gains for larger models) while providing interpretability through error spans that align with human annotations."}, {"question": "What is the abstract of the paper Multi-Channel Speaker Extraction with Adversarial Training: The Wavlab Submission to The Clarity ICASSP 2023 Grand Challenge published by LTI faculty Shinji Watanabe in 2023?", "answer": "In this work we detail our submission to the Clarity ICASSP 2023 grand challenge, in which participants have to develop a strong target speech enhancement system for hearing-aid (HA) devices in noisy-reverberant environments. Our system builds on our previous submission at the Second Clarity Enhancement Challenge (CEC2): iNeuBe-X, which consists in an iterative neural/conventional beamforming enhancement pipeline, guided by an enrollment utterance from the target speaker. This model, which won by a large margin the CEC2, is an extension of the state-of-the-art TF-GridNet model for multi-channel, streamable target-speaker speech enhancement. Here, this approach is extended and further improved by leveraging generative adversarial training, which we show proves especially useful when the training data is limited. Using only the official 6k training scenes data, our best model achieves 0.80 hearing-aid speech perception index (HASPI) and 0.41 hearing-aid speech quality index (HASQI) scores on the synthetic evaluation set. However, our model generalized poorly on the semi-real evaluation set. This highlights the fact that our community should focus more on real-world evaluation and less on fully synthetic datasets.", "context": "Author: Shinji Watanabe Title: Multi-Channel Speaker Extraction with Adversarial Training: The Wavlab Submission to The Clarity ICASSP 2023 Grand Challenge Publication year: 2023 Coauthors: Samuele Cornell, Zhongqiu Wang, Yoshiki Masuyama, Shinji Watanabe, Manuel Pariente, Nobutaka Ono, S. Squartini Abstract: In this work we detail our submission to the Clarity ICASSP 2023 grand challenge, in which participants have to develop a strong target speech enhancement system for hearing-aid (HA) devices in noisy-reverberant environments. Our system builds on our previous submission at the Second Clarity Enhancement Challenge (CEC2): iNeuBe-X, which consists in an iterative neural/conventional beamforming enhancement pipeline, guided by an enrollment utterance from the target speaker. This model, which won by a large margin the CEC2, is an extension of the state-of-the-art TF-GridNet model for multi-channel, streamable target-speaker speech enhancement. Here, this approach is extended and further improved by leveraging generative adversarial training, which we show proves especially useful when the training data is limited. Using only the official 6k training scenes data, our best model achieves 0.80 hearing-aid speech perception index (HASPI) and 0.41 hearing-aid speech quality index (HASQI) scores on the synthetic evaluation set. However, our model generalized poorly on the semi-real evaluation set. This highlights the fact that our community should focus more on real-world evaluation and less on fully synthetic datasets."}, {"question": "Who is the author of the LTI paper Completing Visual Objects via Bridging Generation and Segmentation published in 2023?", "answer": "Rita Singh", "context": "Author: Rita Singh Title: Completing Visual Objects via Bridging Generation and Segmentation Publication year: 2023 Coauthors: Xiang Li, Yinpeng Chen, Chung-Ching Lin, Rita Singh, Bhiksha Raj, Zicheng Liu Abstract: This paper presents a novel approach to object completion, with the primary goal of reconstructing a complete object from its partially visible components. Our method, named MaskComp, delineates the completion process through iterative stages of generation and segmentation. In each iteration, the object mask is provided as an additional condition to boost image generation, and, in return, the generated images can lead to a more accurate mask by fusing the segmentation of images. We demonstrate that the combination of one generation and one segmentation stage effectively functions as a mask denoiser. Through alternation between the generation and segmentation stages, the partial object mask is progressively refined, providing precise shape guidance and yielding superior object completion results. Our experiments demonstrate the superiority of MaskComp over existing approaches, e.g., ControlNet and Stable Diffusion, establishing it as an effective solution for object completion."}, {"question": "Which LTI faculty published the paper An In-depth Look at Gemini's Language Abilities in 2023?", "answer": "Graham Neubig", "context": "Author: Graham Neubig Title: An In-depth Look at Gemini's Language Abilities Publication year: 2023 Coauthors: Syeda Nahida Akter, Zichun Yu, Aashiq Muhamed, Tianyue Ou, Alex Bauerle, \u00c1ngel Alexander Cabrera, Krish Dholakia, Chenyan Xiong, Graham Neubig Abstract: The recently released Google Gemini class of models are the first to comprehensively report results that rival the OpenAI GPT series across a wide variety of tasks. In this paper, we do an in-depth exploration of Gemini's language abilities, making two contributions. First, we provide a third-party, objective comparison of the abilities of the OpenAI GPT and Google Gemini models with reproducible code and fully transparent results. Second, we take a closer look at the results, identifying areas where one of the two model classes excels. We perform this analysis over 10 datasets testing a variety of language abilities, including reasoning, answering knowledge-based questions, solving math problems, translating between languages, generating code, and acting as instruction-following agents. From this analysis, we find that Gemini Pro achieves accuracy that is close but slightly inferior to the corresponding GPT 3.5 Turbo on all tasks that we benchmarked. We further provide explanations for some of this under-performance, including failures in mathematical reasoning with many digits, sensitivity to multiple-choice answer ordering, aggressive content filtering, and others. We also identify areas where Gemini demonstrates comparably high performance, including generation into non-English languages, and handling longer and more complex reasoning chains. Code and data for reproduction can be found at https://github.com/neulab/gemini-benchmark"}, {"question": "What is the abstract of the paper ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages published by LTI faculty Graham Neubig in 2023?", "answer": "Large language models (LLMs) implicitly learn to perform a range of language tasks, including machine translation (MT). Previous studies explore aspects of LLMs\u2019 MT capabilities. However, there exist a wide variety of languages for which recent LLM MT performance has never before been evaluated. Without published experimental evidence on the matter, it is difficult for speakers of the world\u2019s diverse languages to know how and whether they can use LLMs for their languages. We present the first experimental evidence for an expansive set of 204 languages, along with MT cost analysis, using the FLORES-200 benchmark. Trends reveal that GPT models approach or exceed traditional MT model performance for some high-resource languages (HRLs) but consistently lag for low-resource languages (LRLs), under-performing traditional MT for 84.1% of languages we covered. Our analysis reveals that a language\u2019s resource level is the most important feature in determining ChatGPT\u2019s relative ability to translate it, and suggests that ChatGPT is especially disadvantaged for LRLs and African languages.", "context": "Author: Graham Neubig Title: ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages Publication year: 2023 Coauthors: Nathaniel R. Robinson, Perez Ogayo, David R. Mortensen, Graham Neubig Abstract: Large language models (LLMs) implicitly learn to perform a range of language tasks, including machine translation (MT). Previous studies explore aspects of LLMs\u2019 MT capabilities. However, there exist a wide variety of languages for which recent LLM MT performance has never before been evaluated. Without published experimental evidence on the matter, it is difficult for speakers of the world\u2019s diverse languages to know how and whether they can use LLMs for their languages. We present the first experimental evidence for an expansive set of 204 languages, along with MT cost analysis, using the FLORES-200 benchmark. Trends reveal that GPT models approach or exceed traditional MT model performance for some high-resource languages (HRLs) but consistently lag for low-resource languages (LRLs), under-performing traditional MT for 84.1% of languages we covered. Our analysis reveals that a language\u2019s resource level is the most important feature in determining ChatGPT\u2019s relative ability to translate it, and suggests that ChatGPT is especially disadvantaged for LRLs and African languages."}, {"question": "Who is the author of the LTI paper Energy and Carbon Considerations of Fine-Tuning BERT published in 2023?", "answer": "Emma Strubell", "context": "Author: Emma Strubell Title: Energy and Carbon Considerations of Fine-Tuning BERT Publication year: 2023 Coauthors: Xiaorong Wang, Clara Na, Emma Strubell, Sorelle A. Friedler, Sasha Luccioni Abstract: Despite the popularity of the `pre-train then fine-tune' paradigm in the NLP community, existing work quantifying energy costs and associated carbon emissions has largely focused on language model pre-training. Although a single pre-training run draws substantially more energy than fine-tuning, fine-tuning is performed more frequently by many more individual actors, and thus must be accounted for when considering the energy and carbon footprint of NLP. In order to better characterize the role of fine-tuning in the landscape of energy and carbon emissions in NLP, we perform a careful empirical study of the computational costs of fine-tuning across tasks, datasets, hardware infrastructure and measurement modalities. Our experimental results allow us to place fine-tuning energy and carbon costs into perspective with respect to pre-training and inference, and outline recommendations to NLP researchers and practitioners who wish to improve their fine-tuning energy efficiency."}, {"question": "Who are the coauthors of the paper Cross-Modal Fine-Tuning: Align then Refine published by LTI faculty Graham Neubig in 2023?", "answer": "Junhong Shen, Liam Li, L. Dery, Corey Staten, M. Khodak, Graham Neubig, Ameet Talwalkar", "context": "Author: Graham Neubig Title: Cross-Modal Fine-Tuning: Align then Refine Publication year: 2023 Coauthors: Junhong Shen, Liam Li, L. Dery, Corey Staten, M. Khodak, Graham Neubig, Ameet Talwalkar Abstract: Fine-tuning large-scale pretrained models has led to tremendous progress in well-studied modalities such as vision and NLP. However, similar gains have not been observed in many other modalities due to a lack of relevant pretrained models. In this work, we propose ORCA, a general cross-modal fine-tuning framework that extends the applicability of a single large-scale pretrained model to diverse modalities. ORCA adapts to a target task via an align-then-refine workflow: given the target input, ORCA first learns an embedding network that aligns the embedded feature distribution with the pretraining modality. The pretrained model is then fine-tuned on the embedded data to exploit the knowledge shared across modalities. Through extensive experiments, we show that ORCA obtains state-of-the-art results on 3 benchmarks containing over 60 datasets from 12 modalities, outperforming a wide range of hand-designed, AutoML, general-purpose, and task-specific methods. We highlight the importance of data alignment via a series of ablation studies and demonstrate ORCA's utility in data-limited regimes."}, {"question": "Which LTI faculty published the paper KEEC: Embed to Control on An Equivariant Geometry in 2023?", "answer": "Yiming Yang", "context": "Author: Yiming Yang Title: KEEC: Embed to Control on An Equivariant Geometry Publication year: 2023 Coauthors: Xiaoyuan Cheng, Yiming Yang, Wei Jiang, Yukun Hu Abstract: This paper investigates how representation learning can enable optimal control in unknown and complex dynamics, such as chaotic and non-linear systems, without relying on prior domain knowledge of the dynamics. The core idea is to establish an equivariant geometry that is diffeomorphic to the manifold defined by a dynamical system and to perform optimal control within this corresponding geometry, which is a non-trivial task. To address this challenge, Koopman Embed to Equivariant Control (KEEC) is proposed for model learning and control. Inspired by Lie theory, KEEC begins by learning a non-linear dynamical system defined on a manifold and embedding trajectories into a Lie group. Subsequently, KEEC formulates an equivariant value function equation in reinforcement learning on the equivariant geometry, ensuring an invariant effect as the value function on the original manifold. By deriving analytical-form optimal actions on the equivariant value function, KEEC theoretically achieves quadratic convergence for the optimal equivariant value function by leveraging the differential information on the equivariant geometry. The effectiveness of KEEC is demonstrated in challenging dynamical systems, including chaotic ones like Lorenz-63. Notably, our results show that isometric functions, which maintain the compactness and completeness of geometry while preserving metric and differential information, consistently outperform loss functions lacking these characteristics."}, {"question": "Who are the coauthors of the paper Reducing Barriers to Self-Supervised Learning: HuBERT Pre-training with Academic Compute published by LTI faculty Shinji Watanabe in 2023?", "answer": "William Chen, Xuankai Chang, Yifan Peng, Zhaoheng Ni, Soumi Maiti, Shinji Watanabe", "context": "Author: Shinji Watanabe Title: Reducing Barriers to Self-Supervised Learning: HuBERT Pre-training with Academic Compute Publication year: 2023 Coauthors: William Chen, Xuankai Chang, Yifan Peng, Zhaoheng Ni, Soumi Maiti, Shinji Watanabe Abstract: Self-supervised learning (SSL) has led to great strides in speech processing. However, the resources needed to train these models has become prohibitively large as they continue to scale. Currently, only a few groups with substantial resources are capable of creating SSL models, which harms reproducibility. In this work, we optimize HuBERT SSL to fit in academic constraints. We reproduce HuBERT independently from the original implementation, with no performance loss. Our code and training optimizations make SSL feasible with only 8 GPUs, instead of the 32 used in the original work. We also explore a semi-supervised route, using an ASR model to skip the first pre-training iteration. Within one iteration of pre-training, our models improve over HuBERT on several tasks. Furthermore, our HuBERT Large variant requires only 8 GPUs, achieving similar performance to the original trained on 128. As our contribution to the community, all models, configurations, and code are made open-source in ESPnet."}, {"question": "Who are the coauthors of the paper Reasoning about the Unseen for Efficient Outdoor Object Navigation published by LTI faculty Yonatan Bisk in 2023?", "answer": "Quanting Xie, Tianyi Zhang, Kedi Xu, M. Johnson-Roberson, Yonatan Bisk", "context": "Author: Yonatan Bisk Title: Reasoning about the Unseen for Efficient Outdoor Object Navigation Publication year: 2023 Coauthors: Quanting Xie, Tianyi Zhang, Kedi Xu, M. Johnson-Roberson, Yonatan Bisk Abstract: Robots should exist anywhere humans do: indoors, outdoors, and even unmapped environments. In contrast, the focus of recent advancements in Object Goal Navigation(OGN) has targeted navigating in indoor environments by leveraging spatial and semantic cues that do not generalize outdoors. While these contributions provide valuable insights into indoor scenarios, the broader spectrum of real-world robotic applications often extends to outdoor settings. As we transition to the vast and complex terrains of outdoor environments, new challenges emerge. Unlike the structured layouts found indoors, outdoor environments lack clear spatial delineations and are riddled with inherent semantic ambiguities. Despite this, humans navigate with ease because we can reason about the unseen. We introduce a new task OUTDOOR, a new mechanism for Large Language Models (LLMs) to accurately hallucinate possible futures, and a new computationally aware success metric for pushing research forward in this more complex domain. Additionally, we show impressive results on both a simulated drone and physical quadruped in outdoor environments. Our agent has no premapping and our formalism outperforms naive LLM-based approaches"}, {"question": "Who are the coauthors of the paper Leveraging body pose estimation for gesture recognition in human-robot interaction using synthetic data published by LTI faculty Alexander Hauptmann in 2023?", "answer": "Xiaoyu Zhu, Celso de Melo, Alexander Hauptmann", "context": "Author: Alexander Hauptmann Title: Leveraging body pose estimation for gesture recognition in human-robot interaction using synthetic data Publication year: 2023 Coauthors: Xiaoyu Zhu, Celso de Melo, Alexander Hauptmann Abstract: Effectively recognizing human gestures from variant viewpoints plays a fundamental role in the successful collaboration between humans and robots. Deep learning approaches have achieved promising performance in gesture recognition. However, they are usually data-hungry and require large-scale labeled data, which are not usually accessible in a practical setting. Synthetic data, on the other hand, can be easily obtained from simulators with fine-grained annotations and variant modalities. Existing state-of-the-art approaches have shown promising results using synthetic data, but there is still a large performance gap between the models trained on synthetic data and real data. To learn domain-invariant feature representations, we propose a novel approach which jointly takes RGB videos and 3D meshes as inputs to perform robust action recognition. We empirically validate our model on the RoCoG-v2 dataset, which consists of a variety of real and synthetic videos of gestures from the ground and air perspectives. We show that our model trained on synthetic data can outperform state-of-the-art models under the same training setting and models trained on real data."}, {"question": "Which LTI faculty published the paper The DARPA Wikidata Overlay: Wikidata as an ontology for natural language processing in 2023?", "answer": "A. Gershman", "context": "Author: A. Gershman Title: The DARPA Wikidata Overlay: Wikidata as an ontology for natural language processing Publication year: 2023 Coauthors: Elizabeth Spaulding, Kathryn Conger, A. Gershman, Rosario Uceda-Sosa, S. Brown, James Pustejovsky, Peter Anick, Martha Palmer Abstract: With 102,530,067 items currently in its crowd-sourced knowledge base, Wikidata provides NLP practitioners a unique and powerful resource for inference and reasoning over real-world entities. However, because Wikidata is very entity focused, events and actions are often labeled with eventive nouns (e.g., the process of diagnosing a person\u2019s illness is labeled \u201cdiagnosis\u201d), and the typical participants in an event are not described or linked to that event concept (e.g., the medical professional or patient). Motivated by a need for an adaptable, comprehensive, domain-flexible ontology for information extraction, including identifying the roles entities are playing in an event, we present a curated subset of Wikidata in which events have been enriched with PropBank roles. To enable richer narrative understanding between events from Wikidata concepts, we have also provided a comprehensive mapping from temporal Qnodes and Pnodes to the Allen Interval Temporal Logic relations."}, {"question": "Who is the author of the LTI paper Divergences between Language Models and Human Brains published in 2023?", "answer": "Graham Neubig", "context": "Author: Graham Neubig Title: Divergences between Language Models and Human Brains Publication year: 2023 Coauthors: Yuchen Zhou, Emmy Liu, Graham Neubig, Leila Wehbe Abstract: Do machines and humans process language in similar ways? Recent research has hinted in the affirmative, finding that brain signals can be effectively predicted using the internal representations of language models (LMs). Although such results are thought to reflect shared computational principles between LMs and human brains, there are also clear differences in how LMs and humans represent and use language. In this work, we systematically explore the divergences between human and machine language processing by examining the differences between LM representations and human brain responses to language as measured by Magnetoencephalography (MEG) across two datasets in which subjects read and listened to narrative stories. Using a data-driven approach, we identify two domains that are not captured well by LMs: social/emotional intelligence and physical commonsense. We then validate these domains with human behavioral experiments and show that fine-tuning LMs on these domains can improve their alignment with human brain responses."}, {"question": "What is the abstract of the paper Active Retrieval Augmented Generation published by LTI faculty Jamie Callan in 2023?", "answer": "Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.", "context": "Author: Jamie Callan Title: Active Retrieval Augmented Generation Publication year: 2023 Coauthors: Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig Abstract: Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE."}, {"question": "Who is the author of the LTI paper RedCoast: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU/TPUs published in 2023?", "answer": "Eric P. Xing", "context": "Author: Eric P. Xing Title: RedCoast: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU/TPUs Publication year: 2023 Coauthors: Bowen Tan, Yun Zhu, Lijuan Liu, Hongyi Wang, Yonghao Zhuang, Jindong Chen, Eric P. Xing, Zhiting Hu Abstract: The recent progress of AI can be largely attributed to large language models (LLMs). However, their escalating memory requirements introduce challenges for machine learning (ML) researchers and engineers. Addressing this requires developers to partition a large model to distribute it across multiple GPUs or TPUs. This necessitates considerable coding and intricate configuration efforts with existing model parallel tools, such as Megatron-LM, DeepSpeed, and Alpa. These tools require users' expertise in machine learning systems (MLSys), creating a bottleneck in LLM development, particularly for developers without MLSys background. In this work, we present RedCoast(Redco), a lightweight and user-friendly tool crafted to automate distributed training and inference for LLMs, as well as to simplify ML pipeline development. The design of Redco emphasizes two key aspects. Firstly, to automate model parallism, our study identifies two straightforward rules to generate tensor parallel strategies for any given LLM. Integrating these rules into Redco facilitates effortless distributed LLM training and inference, eliminating the need of additional coding or complex configurations. We demonstrate the effectiveness by applying Redco on a set of LLM architectures, such as GPT-J, LLaMA, T5, and OPT, up to the size of 66B. Secondly, we propose a mechanism that allows for the customization of diverse ML pipelines through the definition of merely three functions, avoiding redundant and formulaic code like multi-host related processing. This mechanism proves adaptable across a spectrum of ML algorithms, from foundational language modeling to complex algorithms like meta-learning and reinforcement learning. Consequently, Redco implementations exhibit much fewer code lines compared to their official counterparts."}, {"question": "Who are the coauthors of the paper Analysis of Volatile Components in Dried Fruits and Branch Exudates of Schisandra chinensis with Different Fruit Colors Using GC-IMS Technology published by LTI faculty Yiming Yang in 2023?", "answer": "Yiping Yan, Wenpeng Lu, Taiping Tian, Nan Shu, Yiming Yang, Shutian Fan, Xianyan Han, Yunhua Ge, Peilei Xu", "context": "Author: Yiming Yang Title: Analysis of Volatile Components in Dried Fruits and Branch Exudates of Schisandra chinensis with Different Fruit Colors Using GC-IMS Technology Publication year: 2023 Coauthors: Yiping Yan, Wenpeng Lu, Taiping Tian, Nan Shu, Yiming Yang, Shutian Fan, Xianyan Han, Yunhua Ge, Peilei Xu Abstract: To investigate the volatile components of Schisandra chinensis (Turcz.) Bail (commonly known as northern Schisandra) of different colors and to explore their similarities and differences, to identify the main flavor substances in the volatile components of the branch exudates of northern schisandra, and finally to establish a fingerprint map of the volatile components of the dried fruits and branch exudates of northern Schisandra of different colors, we used GC-IMS technology to analyze the volatile components of the dried fruits and branch exudates of three different colors of northern Schisandra and established a fingerprint spectra. The results showed that a total of 60 different volatile chemical components were identified in the branch exudates and dried fruits of Schisandra. The components of germplasm resources with different fruit colors were significantly different. The ion mobility spectrum and OPLS-DA results showed that white and yellow fruits were more similar compared to red fruits. The volatile components in dried fruits were significantly higher than those in branch exudates. After VIP (variable importance in projection) screening, 41 key volatile substances in dried fruits and 30 key volatile substances in branch exudates were obtained. After screening by odor activity value (OAV), there were 24 volatile components greater than 1 in both dried fruits and branch exudates. The most important contributing volatile substance was 3-methyl-butanal, and the most important contributing volatile substance in white fruit was (E)-2-hexenal."}, {"question": "Who is the author of the LTI paper RedCoast: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU/TPUs published in 2023?", "answer": "Eric P. Xing", "context": "Author: Eric P. Xing Title: RedCoast: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU/TPUs Publication year: 2023 Coauthors: Bowen Tan, Yun Zhu, Lijuan Liu, Hongyi Wang, Yonghao Zhuang, Jindong Chen, Eric P. Xing, Zhiting Hu Abstract: The recent progress of AI can be largely attributed to large language models (LLMs). However, their escalating memory requirements introduce challenges for machine learning (ML) researchers and engineers. Addressing this requires developers to partition a large model to distribute it across multiple GPUs or TPUs. This necessitates considerable coding and intricate configuration efforts with existing model parallel tools, such as Megatron-LM, DeepSpeed, and Alpa. These tools require users' expertise in machine learning systems (MLSys), creating a bottleneck in LLM development, particularly for developers without MLSys background. In this work, we present RedCoast(Redco), a lightweight and user-friendly tool crafted to automate distributed training and inference for LLMs, as well as to simplify ML pipeline development. The design of Redco emphasizes two key aspects. Firstly, to automate model parallism, our study identifies two straightforward rules to generate tensor parallel strategies for any given LLM. Integrating these rules into Redco facilitates effortless distributed LLM training and inference, eliminating the need of additional coding or complex configurations. We demonstrate the effectiveness by applying Redco on a set of LLM architectures, such as GPT-J, LLaMA, T5, and OPT, up to the size of 66B. Secondly, we propose a mechanism that allows for the customization of diverse ML pipelines through the definition of merely three functions, avoiding redundant and formulaic code like multi-host related processing. This mechanism proves adaptable across a spectrum of ML algorithms, from foundational language modeling to complex algorithms like meta-learning and reinforcement learning. Consequently, Redco implementations exhibit much fewer code lines compared to their official counterparts."}, {"question": "What is the abstract of the paper Prompt2Model: Generating Deployable Models from Natural Language Instructions published by LTI faculty Graham Neubig in 2023?", "answer": "Large language models (LLMs) enable system builders today to create competent NLP systems through prompting, where they only need to describe the task in natural language and provide a few examples. However, in other ways, LLMs are a step backward from traditional special-purpose NLP models; they require extensive computational resources for deployment and can be gated behind APIs. In this paper, we propose Prompt2Model, a general-purpose method that takes a natural language task description like the prompts provided to LLMs, and uses it to train a special-purpose model that is conducive to deployment. This is done through a multi-step process of retrieval of existing datasets and pretrained models, dataset generation using LLMs, and supervised fine-tuning on these retrieved and generated datasets. Over three tasks, we demonstrate that given the same few-shot prompt as input, Prompt2Model trains models that outperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20% while being up to 700 times smaller. We also show that this data can be used to obtain reliable performance estimates of model performance, enabling model developers to assess model reliability before deployment. Prompt2Model is available open-source at https://github.com/neulab/prompt2model.", "context": "Author: Graham Neubig Title: Prompt2Model: Generating Deployable Models from Natural Language Instructions Publication year: 2023 Coauthors: Vijay Viswanathan, Chenyang Zhao, Amanda Bertsch, Tongshuang Sherry Wu, Graham Neubig Abstract: Large language models (LLMs) enable system builders today to create competent NLP systems through prompting, where they only need to describe the task in natural language and provide a few examples. However, in other ways, LLMs are a step backward from traditional special-purpose NLP models; they require extensive computational resources for deployment and can be gated behind APIs. In this paper, we propose Prompt2Model, a general-purpose method that takes a natural language task description like the prompts provided to LLMs, and uses it to train a special-purpose model that is conducive to deployment. This is done through a multi-step process of retrieval of existing datasets and pretrained models, dataset generation using LLMs, and supervised fine-tuning on these retrieved and generated datasets. Over three tasks, we demonstrate that given the same few-shot prompt as input, Prompt2Model trains models that outperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20% while being up to 700 times smaller. We also show that this data can be used to obtain reliable performance estimates of model performance, enabling model developers to assess model reliability before deployment. Prompt2Model is available open-source at https://github.com/neulab/prompt2model."}]