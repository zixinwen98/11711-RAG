[
    {
        "question": "Is Jessica Maguire ssica Maguire Assistant to the Institute Director of LTI?, Please answer yes or no: ",
        "context": "Jessica Maguire Assistant to the Institute Director Email: jlm4@andrew.cmu.edu Office: 5721 Gates & Hillman Centers Phone: 412-268-5480",
        "retrieved_doc": "Nicole Perrotta Administrative Coordinator Email: nperrott@andrew.cmu.edu|Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu|Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu",
        "retrieved_relevant_part": false,
        "model_answer": ":  Based on the provided context, I cannot determine if Jessica Maguire exists or not, as there is no mention of her email address or role within LTI in the context. Therefore, it's impossible to answer the question with a definitive yes or no",
        "actual_answer": "Yes",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "How can I contact LTI staff Vicente Malave?",
        "context": "Vicente Malave Data Science Course Developer Email: vlm@andrew.cmu.edu",
        "retrieved_doc": "Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Daniel Gongaware Sponsored Research Administrator Email: dgongawa@cs.cmu.edu Office: 355 TCS Hall Phone: 412-268-6195|Krista McGuigan Business Director Email: klmcg@cs.cmu.edu Office: 349 TCS Hall Phone: 412-268-1635",
        "retrieved_relevant_part": false,
        "model_answer": ":  Based on the context provided, Vicente Malave is not mentioned with any contact information. Therefore, there is no fact from the context that can be used to answer the question",
        "actual_answer": "Email: vlm@andrew.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Please provide the contact information of LTI staff Susan E.",
        "context": "Susan E. Holm Mitamura Lab/Sr. Knowledge Engineer Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591",
        "retrieved_doc": "Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Nicole Perrotta Administrative Coordinator Email: nperrott@andrew.cmu.edu|Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517",
        "retrieved_relevant_part": false,
        "model_answer": ":  I'm sorry for any confusion, but there is no mention of an \"LTI staff Susan\" in the provided context. The context only mentions Mona Diab as the LTI Director and Tenured Professor, Nicole Perrotta as the Administrative Coordinator, and Joan Axelson as the Office Manager. Their respective contact information has been provided",
        "actual_answer": "Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "How can I contact LTI staff Daniel Vosler?",
        "context": "Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu",
        "retrieved_doc": "Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu|Bryan Burtner Associate Business Manager Email: bburtner@cs.cmu.edu Office: 6413 TCS Hall Phone: 412-268-2805|Yukari Yamakawa Mitamura Lab/Associate Data Analyst Email: yukariy@andrew.cmu.edu Office: 6717 Gates & Hillman Centers Phone: 412-268-6596",
        "retrieved_relevant_part": true,
        "model_answer": ":  You can contact Daniel Vosler, the LTI High Performance Cluster Administrator, via email at dvosler@andrew.cmu.edu",
        "actual_answer": "Email: dvosler@andrew.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0.11,
            0.11,
            0.11
        ],
        "recall_max_min_avg": [
            0.5,
            0.5,
            0.5
        ]
    },
    {
        "question": "Please tell me the Email of LTI staff Daniel Vosler based on the background information: ",
        "context": "Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu",
        "retrieved_doc": "Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu|Zaid Sheikh Neubig Lab/Senior Systems/Software Engineer Email: zsheikh@andrew.cmu.edu Office: 6416 Gates & Hillman Centers Phone:|Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu",
        "retrieved_relevant_part": true,
        "model_answer": ":  The email address for Daniel Vosler, who is an LTI High Performance Cluster Administrator, is dvosler@andrew.cmu.edu",
        "actual_answer": "Daniel Vosler's Email is dvosler@andrew.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0.27,
            0.27,
            0.27
        ],
        "recall_max_min_avg": [
            0.6,
            0.6,
            0.6
        ]
    },
    {
        "question": "What is the Email of LTI staff Casey Walker?",
        "context": "Casey Walker Academic Program Coordinator Email: clwalker@andrew.cmu.edu Phone: +1 412 268 9315",
        "retrieved_doc": "Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu|Jennifer Clark Administrative Coordinator Email: jclark2@andrew.cmu.edu|Nicole Perrotta Administrative Coordinator Email: nperrott@andrew.cmu.edu",
        "retrieved_relevant_part": false,
        "model_answer": ":  I'm sorry, but there is no mention of an \"LTI staff Casey Walker\" or their email address in the provided context. The context only lists the emails for Daniel Vosler, Jennifer Clark, and Nicole Perrotta",
        "actual_answer": "Casey Walker's Email is clwalker@andrew.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0.1,
            0.1,
            0.1
        ],
        "recall_max_min_avg": [
            0.4,
            0.4,
            0.4
        ]
    },
    {
        "question": "What is the Phone of LTI staff Emma Thomas?",
        "context": "Emma Thomas Sponsored Research Administrator Email: emmat@andrew.cmu.edu Office: 5715 Gates & Hillman Centers Phone: 412-268-7812",
        "retrieved_doc": "Emma Thomas Sponsored Research Administrator Email: emmat@andrew.cmu.edu Office: 5715 Gates & Hillman Centers Phone: 412-268-7812|Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517|Jessica Maguire Assistant to the Institute Director Email: jlm4@andrew.cmu.edu Office: 5721 Gates & Hillman Centers Phone: 412-268-5480",
        "retrieved_relevant_part": true,
        "model_answer": ":  The phone number for Emma Thomas, who is a Sponsored Research Administrator at Carnegie Mellon University, is 412-268-7812",
        "actual_answer": "412-268-7812",
        "exact_match": false,
        "f1_max_min_avg": [
            0.1,
            0.1,
            0.1
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Is Kate Schaich kari Yamakawa Mitamura Lab/Associate Data Analyst of LTI?, Please answer yes or no: ",
        "context": "Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_doc": "Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu|Zhi-Qi Cheng Hauptmann Lab/Postdoctoral Research Associate Email: zhiqic@andrew.cmu.edu|Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe ",
        "retrieved_relevant_part": false,
        "model_answer": ":  Based on the provided context, I cannot determine if Kate Schaich is an associate data analyst at the LTI lab headed by Kari Yamakawa Mitamura as her name does not appear in the text",
        "actual_answer": "No",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Please tell me the Office of LTI staff Joan Axelson based on the background information: ",
        "context": "Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517",
        "retrieved_doc": "Nicole Perrotta Administrative Coordinator Email: nperrott@andrew.cmu.edu|Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering|Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu",
        "retrieved_relevant_part": false,
        "model_answer": ":  Joan Axelson is not mentioned in the provided context. The context only includes information about Nicole Perrotta, Anatole Gershman, and Daniel Vosler, their roles, emails, phone numbers, and research areas",
        "actual_answer": "Joan Axelson's Office is Manager",
        "exact_match": false,
        "f1_max_min_avg": [
            0.11,
            0.11,
            0.11
        ],
        "recall_max_min_avg": [
            0.4,
            0.4,
            0.4
        ]
    },
    {
        "question": "How can I contact LTI staff Yukari Yamakawa?",
        "context": "Yukari Yamakawa Mitamura Lab/Associate Data Analyst Email: yukariy@andrew.cmu.edu Office: 6717 Gates & Hillman Centers Phone: 412-268-6596",
        "retrieved_doc": "Yukari Yamakawa Mitamura Lab/Associate Data Analyst Email: yukariy@andrew.cmu.edu Office: 6717 Gates & Hillman Centers Phone: 412-268-6596|Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu|Daniel Gongaware Sponsored Research Administrator Email: dgongawa@cs.cmu.edu Office: 355 TCS Hall Phone: 412-268-6195",
        "retrieved_relevant_part": true,
        "model_answer": ":  You can contact Yukari Yamakawa by email at yukariy@andrew.cmu.edu or by phone at 412-268-6596. Her office is located at 6717 Gates & Hillman Centers",
        "actual_answer": "Email: yukariy@andrew.cmu.edu Office: 6717 Gates & Hillman Centers Phone: 412-268-6596",
        "exact_match": false,
        "f1_max_min_avg": [
            0.34,
            0.34,
            0.34
        ],
        "recall_max_min_avg": [
            0.6,
            0.6,
            0.6
        ]
    },
    {
        "question": "Is Nicole Perrotta cole Perrotta Administrative Coordinator of LTI?, Please answer yes or no: ",
        "context": "Nicole Perrotta Administrative Coordinator Email: nperrott@andrew.cmu.edu",
        "retrieved_doc": "Nicole Perrotta Administrative Coordinator Email: nperrott@andrew.cmu.edu|Jennifer Clark Administrative Coordinator Email: jclark2@andrew.cmu.edu|Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu",
        "retrieved_relevant_part": true,
        "model_answer": ":  Based on the context provided, the answer is no. Nicole Perrotta is identified as an Administrative Coordinator, but there is no mention of her being the coordinator for LTI (High Performance Cluster) specifically",
        "actual_answer": "Yes",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "How can I contact LTI staff Emma Thomas?",
        "context": "Emma Thomas Sponsored Research Administrator Email: emmat@andrew.cmu.edu Office: 5715 Gates & Hillman Centers Phone: 412-268-7812",
        "retrieved_doc": "Emma Thomas Sponsored Research Administrator Email: emmat@andrew.cmu.edu Office: 5715 Gates & Hillman Centers Phone: 412-268-7812|Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517|Krista McGuigan Business Director Email: klmcg@cs.cmu.edu Office: 349 TCS Hall Phone: 412-268-1635",
        "retrieved_relevant_part": true,
        "model_answer": ":  You can contact Emma Thomas, the Sponsored Research Administrator at Carnegie Mellon University, through her email address (emmat@andrew.cmu.edu) or phone number (412-268-7812)",
        "actual_answer": "Email: emmat@andrew.cmu.edu Office: 5715 Gates & Hillman Centers Phone: 412-268-7812",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Please tell me the Email of LTI staff Annette Han based on the background information: ",
        "context": "Annette Han Rose Lab/Data Science Course Developer Email: ahan2@andrew.cmu.edu",
        "retrieved_doc": "Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu|Nicole Perrotta Administrative Coordinator Email: nperrott@andrew.cmu.edu|Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu",
        "retrieved_relevant_part": false,
        "model_answer": ":  I'm sorry for any confusion, but Annette Han's email address was not provided in the context. The context only includes the emails of Soumi Maiti Watanabe, Nicole Perrotta, and Daniel Vosler",
        "actual_answer": "Annette Han's Email is ahan2@andrew.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0.11,
            0.11,
            0.11
        ],
        "recall_max_min_avg": [
            0.4,
            0.4,
            0.4
        ]
    },
    {
        "question": "What is the Email of LTI staff Nicole Perrotta?",
        "context": "Nicole Perrotta Administrative Coordinator Email: nperrott@andrew.cmu.edu",
        "retrieved_doc": "Nicole Perrotta Administrative Coordinator Email: nperrott@andrew.cmu.edu|Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu|Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu",
        "retrieved_relevant_part": true,
        "model_answer": ":  The email address for Nicole Perrotta, who is an Administrative Coordinator at LTI (High Performance Cluster) at Carnegie Mellon University, is nperrott@andrew.cmu.edu",
        "actual_answer": "nperrott@andrew.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0.08,
            0.08,
            0.08
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Please tell me the Email of LTI staff Daniel Vosler based on the background information: ",
        "context": "Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu",
        "retrieved_doc": "Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu|Zaid Sheikh Neubig Lab/Senior Systems/Software Engineer Email: zsheikh@andrew.cmu.edu Office: 6416 Gates & Hillman Centers Phone:|Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu",
        "retrieved_relevant_part": true,
        "model_answer": ":  The email address for Daniel Vosler, who is an LTI High Performance Cluster Administrator, is dvosler@andrew.cmu.edu",
        "actual_answer": "dvosler@andrew.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0.11,
            0.11,
            0.11
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "How can I contact LTI staff Allison Blair?",
        "context": "Allison Blair Associate Business Manager Email: ablair2@cs.cmu.edu Office: 5715 TCS Hall Phone: 412-268-5489",
        "retrieved_doc": "Allison Blair Associate Business Manager Email: ablair2@cs.cmu.edu Office: 5715 TCS Hall Phone: 412-268-5489|Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669",
        "retrieved_relevant_part": true,
        "model_answer": ":  You can contact Allison Blair by email at ablair2@cs.cmu.edu or by phone at 412-268-5489",
        "actual_answer": "Email: ablair2@cs.cmu.edu Office: 5715 TCS Hall Phone: 412-268-5489",
        "exact_match": false,
        "f1_max_min_avg": [
            0.17,
            0.17,
            0.17
        ],
        "recall_max_min_avg": [
            0.25,
            0.25,
            0.25
        ]
    },
    {
        "question": "Is Caitlin Korpus itlin Korpus Senior Academic Program Coordinator of LTI?, Please answer yes or no: ",
        "context": "Caitlin Korpus Senior Academic Program Coordinator Email: ckorpus@andrew.cmu.edu Office: 6719 Gates & Hillman Centers Phone: +1 412 268 7096",
        "retrieved_doc": "Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu|Yonatan Bisk Assistant Professor Email: ybisk@cs.cmu.edu Office: Gates & Hillman Centers Research Areas: Grounding, RoboNLP, Vision and Language, Embodiment, Unsupervised Learning|Nicole Perrotta Administrative Coordinator Email: nperrott@andrew.cmu.edu",
        "retrieved_relevant_part": false,
        "model_answer": ":  Based on the provided context, there is no mention of a person named Caitlin Korpus or ITLin Korpus holding the position of Senior Academic Program Coordinator for LTI. Therefore, the answer is no",
        "actual_answer": "Yes",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Please provide the contact information of LTI staff Susan E.",
        "context": "Susan E. Holm Mitamura Lab/Sr. Knowledge Engineer Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591",
        "retrieved_doc": "Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Nicole Perrotta Administrative Coordinator Email: nperrott@andrew.cmu.edu|Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517",
        "retrieved_relevant_part": false,
        "model_answer": ":  I'm sorry for any confusion, but there is no mention of an \"LTI staff Susan\" in the provided context. The context only mentions Mona Diab as the LTI Director and Tenured Professor, Nicole Perrotta as the Administrative Coordinator, and Joan Axelson as the Office Manager. Their respective contact information has been provided",
        "actual_answer": "Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "How can I contact LTI staff Daniel Vosler?",
        "context": "Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu",
        "retrieved_doc": "Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu|Bryan Burtner Associate Business Manager Email: bburtner@cs.cmu.edu Office: 6413 TCS Hall Phone: 412-268-2805|Yukari Yamakawa Mitamura Lab/Associate Data Analyst Email: yukariy@andrew.cmu.edu Office: 6717 Gates & Hillman Centers Phone: 412-268-6596",
        "retrieved_relevant_part": true,
        "model_answer": ":  You can contact Daniel Vosler, the LTI High Performance Cluster Administrator, via email at dvosler@andrew.cmu.edu",
        "actual_answer": "Email: dvosler@andrew.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0.11,
            0.11,
            0.11
        ],
        "recall_max_min_avg": [
            0.5,
            0.5,
            0.5
        ]
    },
    {
        "question": "What is the Phone of LTI staff Brianna Eriksen?",
        "context": "Brianna Eriksen Academic Program Manager Email: bfreema2@andrew.cmu.edu Phone: 412-268-4277",
        "retrieved_doc": "Brianna Eriksen Academic Program Manager Email: bfreema2@andrew.cmu.edu Phone: 412-268-4277|Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": ":  The phone number for Brianna Eriksen is 412-268-4277",
        "actual_answer": "Brianna Eriksen's Phone is 412-268-4277",
        "exact_match": false,
        "f1_max_min_avg": [
            0.43,
            0.43,
            0.43
        ],
        "recall_max_min_avg": [
            0.6,
            0.6,
            0.6
        ]
    },
    {
        "question": "Please tell me the Phone of LTI staff John Friday based on the background information: ",
        "context": "John Friday Sr. Administrative Coordinator Email: jfriday@andrew.cmu.edu Office: 5715 Gates & Hillman Centers Phone: 412-268-1810",
        "retrieved_doc": "Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering|John Friday Sr. Administrative Coordinator Email: jfriday@andrew.cmu.edu Office: 5715 Gates & Hillman Centers Phone: 412-268-1810|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669",
        "retrieved_relevant_part": true,
        "model_answer": ":  The phone number for John Friday, who is an administrative coordinator at LTI (Laboratory for Computer Science) at Carnegie Mellon University, is 412-268-1810",
        "actual_answer": "412-268-1810",
        "exact_match": false,
        "f1_max_min_avg": [
            0.08,
            0.08,
            0.08
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "What is the Phone of LTI staff Susan E.?",
        "context": "Susan E. Holm Mitamura Lab/Sr. Knowledge Engineer Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591",
        "retrieved_doc": "Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517|Susan E. Holm Mitamura Lab/Sr. Knowledge Engineer Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591|Jessica Maguire Assistant to the Institute Director Email: jlm4@andrew.cmu.edu Office: 5721 Gates & Hillman Centers Phone: 412-268-5480",
        "retrieved_relevant_part": true,
        "model_answer": ":  The phone number for Susan E. Holm Mitamura, who is a Sr. Knowledge Engineer at LTI (Learning Technologies Institute), is 412-268-6591",
        "actual_answer": "412-268-6591",
        "exact_match": false,
        "f1_max_min_avg": [
            0.09,
            0.09,
            0.09
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Is Stacey Young umi Maiti Watanabe Lab/Postdoctoral Research Associate of LTI?, Please answer yes or no: ",
        "context": "Stacey Young Academic Program Manager \u2014 Ph.D. Email: staceyy@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-2623",
        "retrieved_doc": "Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu|Zhi-Qi Cheng Hauptmann Lab/Postdoctoral Research Associate Email: zhiqic@andrew.cmu.edu|Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe ",
        "retrieved_relevant_part": false,
        "model_answer": ":  Based on the context provided, Soumi Maiti Watanabe is identified as a Postdoctoral Research Associate in the Watanabe Lab at Carnegie Mellon University (CMU). Therefore, the answer to the question is \"Yes, Soumi Maiti Watanabe is a Postdoctoral Research Associate in the Watanabe Lab at CMU.\" However, there is no mention of Stacey Young in the context, so the question cannot be answered with regard to her",
        "actual_answer": "No",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Please provide the contact information of LTI staff Amber Vivis",
        "context": "Amber Vivis Sr. Academic Program Manager - MSAII Email: avivis@cs.cmu.edu Office: 357 TCS Hall Phone: 412-268-9998",
        "retrieved_doc": "Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Amber Vivis Sr. Academic Program Manager - MSAII Email: avivis@cs.cmu.edu Office: 357 TCS Hall Phone: 412-268-9998|Nicole Perrotta Administrative Coordinator Email: nperrott@andrew.cmu.edu",
        "retrieved_relevant_part": true,
        "model_answer": ":  The contact information for Amber Vivis, who is an Sr. Academic Program Manager at LTI (Learning Technologies Institute) at Carnegie Mellon University, is as follows:",
        "actual_answer": "Email: avivis@cs.cmu.edu Office: 357 TCS Hall Phone: 412-268-9998",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Please provide the contact information of LTI staff Joan Axelson",
        "context": "Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517",
        "retrieved_doc": "Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Krista McGuigan Business Director Email: klmcg@cs.cmu.edu Office: 349 TCS Hall Phone: 412-268-1635",
        "retrieved_relevant_part": true,
        "model_answer": ":  Contact information for Joan Axelson, LTI Staff at Carnegie Mellon University:",
        "actual_answer": "Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Is Wenhe Liu hn Friday Sr. Administrative Coordinator of LTI?, Please answer yes or no: ",
        "context": "Wenhe Liu Research Scholar Email: wenhel@andrew.cmu.edu Office: 5403 Gates & Hillman Centers",
        "retrieved_doc": "Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu|Author: Yiming Yang Title: Distributed cooperative dual closed loop velocity-attitude consensus controller for rendezvous of the underactuated AUV swarm in 3-dimensional space Publication year: 2023 Coauthors: Yu Zhang, W. Zhang, G. Xia, Yiming Yang, Yan-luan Zheng, Peiyu Han |Wenhe Liu Research Scholar Email: wenhel@andrew.cmu.edu Office: 5403 Gates & Hillman Centers",
        "retrieved_relevant_part": true,
        "model_answer": ":  Based on the context provided, Wenhe Liu is identified as a research scholar with an email address \"wenhel@andrew.cmu.edu\" and an office located at \"5403 Gates & Hillman Centers\". The text does not mention any administrative position or title for Wenhe Liu within LTI, such as a Friday Sr. Administrative Coordinator. Therefore, based on the information given in the context, the answer is no",
        "actual_answer": "No",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "How can I contact LTI staff Brianna Eriksen?",
        "context": "Brianna Eriksen Academic Program Manager Email: bfreema2@andrew.cmu.edu Phone: 412-268-4277",
        "retrieved_doc": "Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517|Brianna Eriksen Academic Program Manager Email: bfreema2@andrew.cmu.edu Phone: 412-268-4277|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": ":  You can contact Brianna Eriksen by email at bfreema2@andrew.cmu.edu or by phone at 412-268-4277",
        "actual_answer": "Email: bfreema2@andrew.cmu.edu Phone: 412-268-4277",
        "exact_match": false,
        "f1_max_min_avg": [
            0.21,
            0.21,
            0.21
        ],
        "recall_max_min_avg": [
            0.5,
            0.5,
            0.5
        ]
    },
    {
        "question": "What is the Phone of LTI staff Kate Schaich?",
        "context": "Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_doc": "Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788|Jessica Maguire Assistant to the Institute Director Email: jlm4@andrew.cmu.edu Office: 5721 Gates & Hillman Centers Phone: 412-268-5480|Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517",
        "retrieved_relevant_part": true,
        "model_answer": ":  The phone number for Kate Schaich, who is the LTI Graduate Program Manager, is 412-268-4788",
        "actual_answer": "Kate Schaich's Phone is 412-268-4788",
        "exact_match": false,
        "f1_max_min_avg": [
            0.29,
            0.29,
            0.29
        ],
        "recall_max_min_avg": [
            0.6,
            0.6,
            0.6
        ]
    },
    {
        "question": "Is Zaid Sheikh i-Qi Cheng Hauptmann Lab/Postdoctoral Research Associate of LTI?, Please answer yes or no: ",
        "context": "Zaid Sheikh Neubig Lab/Senior Systems/Software Engineer Email: zsheikh@andrew.cmu.edu Office: 6416 Gates & Hillman Centers Phone:",
        "retrieved_doc": "Zhi-Qi Cheng Hauptmann Lab/Postdoctoral Research Associate Email: zhiqic@andrew.cmu.edu|Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu|Zaid Sheikh Neubig Lab/Senior Systems/Software Engineer Email: zsheikh@andrew.cmu.edu Office: 6416 Gates & Hillman Centers Phone:",
        "retrieved_relevant_part": true,
        "model_answer": ":  No, Zaid Sheikh is not a Postdoctoral Research Associate at the Zhi-Qi Cheng Hauptmann Lab. He is a Senior Systems/Software Engineer at the Neubig Lab",
        "actual_answer": "No",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "What is the Email of LTI staff Joan Axelson?",
        "context": "Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517",
        "retrieved_doc": "Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu|Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517|Nicole Perrotta Administrative Coordinator Email: nperrott@andrew.cmu.edu",
        "retrieved_relevant_part": true,
        "model_answer": ":  The email address for Joan Axelson is jaxelson@andrew.cmu.edu",
        "actual_answer": "jaxelson@andrew.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0.2,
            0.2,
            0.2
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Please provide the contact information of LTI staff Annette Han",
        "context": "Annette Han Rose Lab/Data Science Course Developer Email: ahan2@andrew.cmu.edu",
        "retrieved_doc": "Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Nicole Perrotta Administrative Coordinator Email: nperrott@andrew.cmu.edu|Susan E. Holm Mitamura Lab/Sr. Knowledge Engineer Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591",
        "retrieved_relevant_part": false,
        "model_answer": ":  I'm sorry for any confusion, but Annette Han's contact information was not provided in the context. The context only includes the contact information for Mona Diab, Nicole Perrotta, and Susan E. Holm",
        "actual_answer": "Email: ahan2@andrew.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Please provide the contact information of LTI staff Kira Sullivan",
        "context": "Kira Sullivan Administrative Coordinator Email: kiras@andrew.cmu.edu Phone: 412-268-8737",
        "retrieved_doc": "Kira Sullivan Administrative Coordinator Email: kiras@andrew.cmu.edu Phone: 412-268-8737|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Amber Vivis Sr. Academic Program Manager - MSAII Email: avivis@cs.cmu.edu Office: 357 TCS Hall Phone: 412-268-9998",
        "retrieved_relevant_part": true,
        "model_answer": ":  Contact information for LTI staff member Kira Sullivan:",
        "actual_answer": "Email: kiras@andrew.cmu.edu Phone: 412-268-8737",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Please tell me the Phone of LTI staff Yukari Yamakawa based on the background information: ",
        "context": "Yukari Yamakawa Mitamura Lab/Associate Data Analyst Email: yukariy@andrew.cmu.edu Office: 6717 Gates & Hillman Centers Phone: 412-268-6596",
        "retrieved_doc": "Yukari Yamakawa Mitamura Lab/Associate Data Analyst Email: yukariy@andrew.cmu.edu Office: 6717 Gates & Hillman Centers Phone: 412-268-6596|Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu|Zaid Sheikh Neubig Lab/Senior Systems/Software Engineer Email: zsheikh@andrew.cmu.edu Office: 6416 Gates & Hillman Centers Phone:",
        "retrieved_relevant_part": true,
        "model_answer": ":  The phone number for Yukari Yamakawa, who is an Associate Data Analyst at the Mitamura Lab, is 412-268-6596",
        "actual_answer": "Yukari Yamakawa's Phone is 412-268-6596",
        "exact_match": false,
        "f1_max_min_avg": [
            0.25,
            0.25,
            0.25
        ],
        "recall_max_min_avg": [
            0.6,
            0.6,
            0.6
        ]
    },
    {
        "question": "Please tell me the Email of LTI staff Robin Hammer based on the background information: ",
        "context": "Robin Hammer Levin Lab/Project Coordinator Email: robinham@andrew.cmu.edu",
        "retrieved_doc": "Robin Hammer Levin Lab/Project Coordinator Email: robinham@andrew.cmu.edu|Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu|Zaid Sheikh Neubig Lab/Senior Systems/Software Engineer Email: zsheikh@andrew.cmu.edu Office: 6416 Gates & Hillman Centers Phone:",
        "retrieved_relevant_part": true,
        "model_answer": ":  The email address for Robin Hammer, who is the Lab/Project Coordinator at LTI (Laboratory for Computational Biology and Bioinformatics) at Carnegie Mellon University, is robinham@andrew.cmu.edu",
        "actual_answer": "robinham@andrew.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0.07,
            0.07,
            0.07
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "How can I contact LTI staff Susan E.?",
        "context": "Susan E. Holm Mitamura Lab/Sr. Knowledge Engineer Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591",
        "retrieved_doc": "Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517|Hannah Rodriguez Sponsored Research Administrator Email: hmuczyns@andrew.cmu.edu Office: 6413 TCS Hall Phone: 412-268-3519|Krista McGuigan Business Director Email: klmcg@cs.cmu.edu Office: 349 TCS Hall Phone: 412-268-1635",
        "retrieved_relevant_part": false,
        "model_answer": ":  I'm sorry for the confusion, but there is no staff member named Susan E. mentioned in the context provided. The context only includes the contact information for Joan Axelson, Hannah Rodriguez, and Krista McGuigan",
        "actual_answer": "Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "What is the Email of LTI staff Zhi-Qi Cheng?",
        "context": "Zhi-Qi Cheng Hauptmann Lab/Postdoctoral Research Associate Email: zhiqic@andrew.cmu.edu",
        "retrieved_doc": "Zhi-Qi Cheng Hauptmann Lab/Postdoctoral Research Associate Email: zhiqic@andrew.cmu.edu|Zhong-Qiu Wang Wantanabe Lab/Postdoctoral Research Associate Email: zhongqiu@andrew.cmu.edu Office: 6414 Gates & Hillman Centers Phone: 415-200-8662|Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu",
        "retrieved_relevant_part": true,
        "model_answer": ":  The email address for Zhi-Qi Cheng, who is a Postdoctoral Research Associate at the Hauptmann Lab in Carnegie Mellon University, is zhiqic@andrew.cmu.edu",
        "actual_answer": "Zhi-Qi Cheng's Email is zhiqic@andrew.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0.21,
            0.21,
            0.21
        ],
        "recall_max_min_avg": [
            0.6,
            0.6,
            0.6
        ]
    },
    {
        "question": "Is Jessica Maguire san E. Holm Mitamura Lab/Sr. Knowledge Engineer of LTI?, Please answer yes or no: ",
        "context": "Jessica Maguire Assistant to the Institute Director Email: jlm4@andrew.cmu.edu Office: 5721 Gates & Hillman Centers Phone: 412-268-5480",
        "retrieved_doc": "Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu|Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: David R. Mortensen Title: Multilingual TTS Accent Impressions for Accented ASR Publication year: 2023 Coauthors: G. Karakasidis, Nathaniel R. Robinson, Yaroslav Getman, Atieno Ogayo, Ragheb Al-Ghezi, Ananya Ayasi, Shinji Watanabe, David R. Mortensen, M. Kurimo ",
        "retrieved_relevant_part": false,
        "model_answer": ":  Based on the provided context, I cannot determine if Jessica Maguire is a member of the Soumi Maiti Watanabe Lab or holds the title of Sr. Knowledge Engineer at LTI (Linguistic Technology Inc.). The context does not mention her name or affiliations",
        "actual_answer": "No",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Is Susan E. san E. Holm Mitamura Lab/Sr. Knowledge Engineer of LTI?, Please answer yes or no: ",
        "context": "Susan E. Holm Mitamura Lab/Sr. Knowledge Engineer Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591",
        "retrieved_doc": "Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu|Susan E. Holm Mitamura Lab/Sr. Knowledge Engineer Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591|Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe ",
        "retrieved_relevant_part": true,
        "model_answer": ":  Yes, Susan E. Holm Mitamura is a Sr. Knowledge Engineer at the LTI (Laboratory for Telecommunications and Information Technology) lab at Carnegie Mellon University",
        "actual_answer": "Yes",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Is Vicente Malave cente Malave Data Science Course Developer of LTI?, Please answer yes or no: ",
        "context": "Vicente Malave Data Science Course Developer Email: vlm@andrew.cmu.edu",
        "retrieved_doc": "Vicente Malave Data Science Course Developer Email: vlm@andrew.cmu.edu|Annette Han Rose Lab/Data Science Course Developer Email: ahan2@andrew.cmu.edu|Author: Maarten Sap Title: Don't Take This Out of Context!: On the Need for Contextual Models and Evaluations for Stylistic Rewriting Publication year: 2023 Coauthors: Akhila Yerukola, Xuhui Zhou, Elizabeth Clark, Maarten Sap ",
        "retrieved_relevant_part": true,
        "model_answer": ":  Based on the context provided, Vicente Malave is identified as a Data Science Course Developer for Carnegie Mellon University with an email address of vlm@andrew.cmu.edu. Therefore, the answer to the question is \"Yes, Vicente Malave is a Data Science Course Developer for Carnegie Mellon University.\" It is important to note that the context does not provide any information about his role within LTI (Learning Technology Initiative), so we cannot make any conclusions about that based on this text alone",
        "actual_answer": "Yes",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "What is the Email of LTI staff Julie Nys?",
        "context": "Julie Nys Employment Processes Manager Email: jnys@andrew.cmu.edu Office: 5405 TCS Hall Phone: 412-268-3515",
        "retrieved_doc": "Nicole Perrotta Administrative Coordinator Email: nperrott@andrew.cmu.edu|Jennifer Clark Administrative Coordinator Email: jclark2@andrew.cmu.edu|Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu",
        "retrieved_relevant_part": false,
        "model_answer": ":  I'm sorry, but the context does not provide any information about an LTI staff member named Julie Nys and her email address",
        "actual_answer": "Julie Nys's Email is jnys@andrew.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0.07,
            0.07,
            0.07
        ],
        "recall_max_min_avg": [
            0.2,
            0.2,
            0.2
        ]
    },
    {
        "question": "Is Casey Walker nnifer Clark Administrative Coordinator of LTI?, Please answer yes or no: ",
        "context": "Casey Walker Academic Program Coordinator Email: clwalker@andrew.cmu.edu Phone: +1 412 268 9315",
        "retrieved_doc": "Nicole Perrotta Administrative Coordinator Email: nperrott@andrew.cmu.edu|Jennifer Clark Administrative Coordinator Email: jclark2@andrew.cmu.edu|Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu",
        "retrieved_relevant_part": false,
        "model_answer": ":  Based on the context provided, the answer is no. Jennifer Clark is an Administrative Coordinator, but there is no indication in the context that she is specifically the Administrative Coordinator for LTI",
        "actual_answer": "No",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "How can I contact LTI staff Daniel Gongaware?",
        "context": "Daniel Gongaware Sponsored Research Administrator Email: dgongawa@cs.cmu.edu Office: 355 TCS Hall Phone: 412-268-6195",
        "retrieved_doc": "Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu|Daniel Gongaware Sponsored Research Administrator Email: dgongawa@cs.cmu.edu Office: 355 TCS Hall Phone: 412-268-6195|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669",
        "retrieved_relevant_part": true,
        "model_answer": ":  You can contact Daniel Gongaware by email at dgongawa@cs.cmu.edu or by phone at 412-268-6195",
        "actual_answer": "Email: dgongawa@cs.cmu.edu Office: 355 TCS Hall Phone: 412-268-6195",
        "exact_match": false,
        "f1_max_min_avg": [
            0.17,
            0.17,
            0.17
        ],
        "recall_max_min_avg": [
            0.25,
            0.25,
            0.25
        ]
    },
    {
        "question": "Please tell me the Office of LTI staff Bryan Burtner based on the background information: ",
        "context": "Bryan Burtner Associate Business Manager Email: bburtner@cs.cmu.edu Office: 6413 TCS Hall Phone: 412-268-2805",
        "retrieved_doc": "Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu|Nicole Perrotta Administrative Coordinator Email: nperrott@andrew.cmu.edu|Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering",
        "retrieved_relevant_part": false,
        "model_answer": ":  Based on the provided context, Bryan Burtner's office information is not mentioned. Therefore, I cannot provide an answer to that question with the facts given in the context",
        "actual_answer": "Bryan Burtner's Office is 6413 TCS Hall",
        "exact_match": false,
        "f1_max_min_avg": [
            0.17,
            0.17,
            0.17
        ],
        "recall_max_min_avg": [
            0.43,
            0.43,
            0.43
        ]
    },
    {
        "question": "How can I contact LTI staff Kira Sullivan?",
        "context": "Kira Sullivan Administrative Coordinator Email: kiras@andrew.cmu.edu Phone: 412-268-8737",
        "retrieved_doc": "Kira Sullivan Administrative Coordinator Email: kiras@andrew.cmu.edu Phone: 412-268-8737|Krista McGuigan Business Director Email: klmcg@cs.cmu.edu Office: 349 TCS Hall Phone: 412-268-1635|Amber Vivis Sr. Academic Program Manager - MSAII Email: avivis@cs.cmu.edu Office: 357 TCS Hall Phone: 412-268-9998",
        "retrieved_relevant_part": true,
        "model_answer": ":  You can contact Kira Sullivan by email at kiras@andrew.cmu.edu or by phone at 412-268-8737",
        "actual_answer": "Email: kiras@andrew.cmu.edu Phone: 412-268-8737",
        "exact_match": false,
        "f1_max_min_avg": [
            0.21,
            0.21,
            0.21
        ],
        "recall_max_min_avg": [
            0.5,
            0.5,
            0.5
        ]
    },
    {
        "question": "What is the Email of LTI staff Zhong-Qiu Wang?",
        "context": "Zhong-Qiu Wang Wantanabe Lab/Postdoctoral Research Associate Email: zhongqiu@andrew.cmu.edu Office: 6414 Gates & Hillman Centers Phone: 415-200-8662",
        "retrieved_doc": "Zhi-Qi Cheng Hauptmann Lab/Postdoctoral Research Associate Email: zhiqic@andrew.cmu.edu|Zhong-Qiu Wang Wantanabe Lab/Postdoctoral Research Associate Email: zhongqiu@andrew.cmu.edu Office: 6414 Gates & Hillman Centers Phone: 415-200-8662|Wenhe Liu Research Scholar Email: wenhel@andrew.cmu.edu Office: 5403 Gates & Hillman Centers",
        "retrieved_relevant_part": true,
        "model_answer": ":  The email address for Zhong-Qiu Wang is zhongqiu@andrew.cmu.edu",
        "actual_answer": "zhongqiu@andrew.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0.2,
            0.2,
            0.2
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "What is the Email of LTI staff Wenhe Liu?",
        "context": "Wenhe Liu Research Scholar Email: wenhel@andrew.cmu.edu Office: 5403 Gates & Hillman Centers",
        "retrieved_doc": "Wenhe Liu Research Scholar Email: wenhel@andrew.cmu.edu Office: 5403 Gates & Hillman Centers|Zhi-Qi Cheng Hauptmann Lab/Postdoctoral Research Associate Email: zhiqic@andrew.cmu.edu|Zhong-Qiu Wang Wantanabe Lab/Postdoctoral Research Associate Email: zhongqiu@andrew.cmu.edu Office: 6414 Gates & Hillman Centers Phone: 415-200-8662",
        "retrieved_relevant_part": true,
        "model_answer": ":  The email address for Wenhe Liu is wenhel@andrew.cmu.edu",
        "actual_answer": "Wenhe Liu's Email is wenhel@andrew.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0.43,
            0.43,
            0.43
        ],
        "recall_max_min_avg": [
            0.6,
            0.6,
            0.6
        ]
    },
    {
        "question": "What is the Phone of LTI staff Stacey Young?",
        "context": "Stacey Young Academic Program Manager \u2014 Ph.D. Email: staceyy@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-2623",
        "retrieved_doc": "Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517|Stacey Young Academic Program Manager \u2014 Ph.D. Email: staceyy@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-2623|Jessica Maguire Assistant to the Institute Director Email: jlm4@andrew.cmu.edu Office: 5721 Gates & Hillman Centers Phone: 412-268-5480",
        "retrieved_relevant_part": true,
        "model_answer": ":  The phone number for Stacey Young is 412-268-2623",
        "actual_answer": "Stacey Young's Phone is 412-268-2623",
        "exact_match": false,
        "f1_max_min_avg": [
            0.43,
            0.43,
            0.43
        ],
        "recall_max_min_avg": [
            0.6,
            0.6,
            0.6
        ]
    },
    {
        "question": "Please tell me the Email of LTI staff Joan Axelson based on the background information: ",
        "context": "Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517",
        "retrieved_doc": "Nicole Perrotta Administrative Coordinator Email: nperrott@andrew.cmu.edu|Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu|Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu",
        "retrieved_relevant_part": false,
        "model_answer": ":  I'm sorry for any confusion, but there is no email address provided in the context for Joan Axelson. The context only includes the emails for Nicole Perrotta, Daniel Vosler, and Soumi Maiti",
        "actual_answer": "jaxelson@andrew.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Please provide the contact information of LTI staff Jennifer Lucas",
        "context": "Jennifer Lucas Academic Program Manager \u2014 MCDS Email: jmlucas@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-9870",
        "retrieved_doc": "Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Nicole Perrotta Administrative Coordinator Email: nperrott@andrew.cmu.edu|Krista McGuigan Business Director Email: klmcg@cs.cmu.edu Office: 349 TCS Hall Phone: 412-268-1635",
        "retrieved_relevant_part": false,
        "model_answer": ":  I'm sorry for any confusion, but there is no mention of a staff member named Jennifer Lucas in the provided context. The context only mentions Mona Diab as the LTI Director and Tenured Professor, Nicole Perrotta as the Administrative Coordinator, and Krista McGuigan as the Business Director. Their respective contact information has been provided in the context",
        "actual_answer": "Email: jmlucas@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-9870",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "What is the Phone of LTI staff Allison Blair?",
        "context": "Allison Blair Associate Business Manager Email: ablair2@cs.cmu.edu Office: 5715 TCS Hall Phone: 412-268-5489",
        "retrieved_doc": "Allison Blair Associate Business Manager Email: ablair2@cs.cmu.edu Office: 5715 TCS Hall Phone: 412-268-5489|Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517|Jessica Maguire Assistant to the Institute Director Email: jlm4@andrew.cmu.edu Office: 5721 Gates & Hillman Centers Phone: 412-268-5480",
        "retrieved_relevant_part": true,
        "model_answer": ":  The phone number for Allison Blair is 412-268-5489",
        "actual_answer": "412-268-5489",
        "exact_match": false,
        "f1_max_min_avg": [
            0.2,
            0.2,
            0.2
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Please provide the contact information of LTI faculty Kemal Oflazer",
        "context": "Kemal Oflazer Teaching Professor of Computer Science Email: ko@qatar.cmu.edu Office: 1009 Carnegie Mellon - Qatar Campus Phone:",
        "retrieved_doc": "Kemal Oflazer Teaching Professor of Computer Science Email: ko@qatar.cmu.edu Office: 1009 Carnegie Mellon - Qatar Campus Phone:|Zaid Sheikh Neubig Lab/Senior Systems/Software Engineer Email: zsheikh@andrew.cmu.edu Office: 6416 Gates & Hillman Centers Phone:|Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu",
        "retrieved_relevant_part": true,
        "model_answer": ":  Contact information for Kemal Oflazer, a faculty member at LTI (Laboratory for Telecommunications and Information Technology) at Carnegie Mellon University, can be found below:",
        "actual_answer": "Email: ko@qatar.cmu.edu Office: 1009 Carnegie Mellon - Qatar Campus Phone:",
        "exact_match": false,
        "f1_max_min_avg": [
            0.11,
            0.11,
            0.11
        ],
        "recall_max_min_avg": [
            0.2,
            0.2,
            0.2
        ]
    },
    {
        "question": "Is Teruko Mitamura ruko Mitamura Research Professor of LTI?, Please answer yes or no: ",
        "context": "Teruko Mitamura Research Professor Email: teruko@cs.cmu.edu Office: 6711 Gates & Hillman Centers Phone: 412-268-6596 Research Areas: Information Extraction, Summarization and Question Answering, Information Retrieval, Text Mining and Analytics, Language Technologies for Education, Natural Language Processing and Computational Linguistics",
        "retrieved_doc": "Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu|Author: Shinji Watanabe Title: Unsupervised Data Selection for TTS: Using Arabic Broadcast News as a Case Study Publication year: 2023 Coauthors: Massa Baali, Tomoki Hayashi, Hamdy Mubarak, Soumi Maiti, Shinji Watanabe, W. El-Hajj, Ahmed Ali Abstract: .|Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe ",
        "retrieved_relevant_part": false,
        "model_answer": ":  I am unable to provide an answer to that question based on the given context. Teruko Mitamura's name does not appear in any of the provided publications or authors",
        "actual_answer": "Yes",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Is Chenyan Xiong i Li Assistant Professor of LTI?, Please answer yes or no: ",
        "context": "Chenyan Xiong Associate Professor Email: cx@andrew.cmu.edu Phone: 412-268-7641",
        "retrieved_doc": "Zhi-Qi Cheng Hauptmann Lab/Postdoctoral Research Associate Email: zhiqic@andrew.cmu.edu|Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu|Author: Chenyan Xiong Title: CompleQA: Benchmarking the Impacts of Knowledge Graph Completion Methods on Question Answering Publication year: 2023 Coauthors: Donghan Yu, Yu Gu, Chenyan Xiong, Yiming Yang ",
        "retrieved_relevant_part": false,
        "model_answer": ":  Based on the context provided, Chenyan Xiong is listed as an author of a publication, but there is no mention of him being an assistant professor or holding any title other than \"Chenyan Xiong\" in the context. Therefore, based on the information given, it is not possible to determine whether Chenyan Xiong is an assistant professor or not. The output should be \"Undetermined\"",
        "actual_answer": "No",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "How can I contact LTI faculty Eric Nyberg?",
        "context": "Eric Nyberg Professor Email: ehn@cs.cmu.edu Office: 6715 Gates & Hillman Centers Phone: 412-268-7281 Research Areas: Information Extraction, Summarization and Question Answering, Information Retrieval, Text Mining and Analytics, Language Technologies for Education",
        "retrieved_doc": "Eric P. Xing Professor (On Leave) Email: epxing@andrew.cmu.edu Office: 8101 Gates & Hillman Centers Phone: 412-268-2559|Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering|Alexander Rudnicky Research Professor Emeritus Email: alex.rudnicky@cs.cmu.edu Office: 6511 Gates & Hillman Centers Phone: 412-268-2622 Research Areas: Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing",
        "retrieved_relevant_part": false,
        "model_answer": ":  I'm sorry for any confusion, but Eric Nyberg is not listed among the faculty members provided in the context with their contact information. Therefore, I cannot provide you with his contact details based on the given context",
        "actual_answer": "Email: ehn@cs.cmu.edu Office: 6715 Gates & Hillman Centers Phone: 412-268-7281",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Please provide the contact information of LTI faculty Bhiksha Ramakrishnan",
        "context": "Bhiksha Ramakrishnan Professor Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826 Research Areas: Machine Learning, Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing, Privacy",
        "retrieved_doc": "Bhiksha Ramakrishnan Professor Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826 Research Areas: Machine Learning, Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing, Privacy|Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669",
        "retrieved_relevant_part": true,
        "model_answer": ":  Email: bhiksha@cs.cmu.edu",
        "actual_answer": "Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826",
        "exact_match": false,
        "f1_max_min_avg": [
            0.31,
            0.31,
            0.31
        ],
        "recall_max_min_avg": [
            0.2,
            0.2,
            0.2
        ]
    },
    {
        "question": "Is Bhiksha Ramakrishnan iksha Ramakrishnan Professor of LTI?, Please answer yes or no: ",
        "context": "Bhiksha Ramakrishnan Professor Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826 Research Areas: Machine Learning, Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing, Privacy",
        "retrieved_doc": "Bhiksha Ramakrishnan Professor Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826 Research Areas: Machine Learning, Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing, Privacy|Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu|Yonatan Bisk Assistant Professor Email: ybisk@cs.cmu.edu Office: Gates & Hillman Centers Research Areas: Grounding, RoboNLP, Vision and Language, Embodiment, Unsupervised Learning",
        "retrieved_relevant_part": true,
        "model_answer": ":  Based on the context provided, the correct answer is \"Yes, Bhiksha Ramakrishnan is a Professor at Carnegie Mellon University's Laboratory for Telecommunications and Information Technology (LTI).\" However, the question in the input contained a typo with \"iksha\" instead of \"Bhiksha,\" which is why it may not match exactly with the name in the context. Therefore, it's essential to ensure that the question matches the facts in the context before generating an answer",
        "actual_answer": "Yes",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "What is the Office of LTI faculty Lori Levin?",
        "context": "Lori Levin Research Professor Email: lsl@cs.cmu.edu Office: 5717 Gates & Hillman Centers Phone: 412-268-6193 Research Areas: Machine Translation, Natural Language Processing and Computational Linguistics, Corpus Annotation and Resources",
        "retrieved_doc": "Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu|Nicole Perrotta Administrative Coordinator Email: nperrott@andrew.cmu.edu|Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu",
        "retrieved_relevant_part": false,
        "model_answer": ":  I cannot directly answer that question with the given context as there is no mention of Lori Levin or her office email address in the provided context",
        "actual_answer": "Lori Levin's Office is 5717 Gates & Hillman Centers",
        "exact_match": false,
        "f1_max_min_avg": [
            0.11,
            0.11,
            0.11
        ],
        "recall_max_min_avg": [
            0.22,
            0.22,
            0.22
        ]
    },
    {
        "question": "How can I contact LTI faculty Emma Strubell?",
        "context": "Emma Strubell Assistant Professor Email: estrubel@andrew.cmu.edu Office: Gates & Hillman Centers",
        "retrieved_doc": "Emma Strubell Assistant Professor Email: estrubel@andrew.cmu.edu Office: Gates & Hillman Centers|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788|Justine Cassell Professor (On Leave) Email: jcassell@andrew.cmu.edu Office: 5107 Gates & Hillman Centers Phone: 412-204-6268",
        "retrieved_relevant_part": true,
        "model_answer": ":  You can contact Assistant Professor Emma Strubell at her email address, estrubel@andrew.cmu.edu. Her office is located at the Gates & Hillman Centers",
        "actual_answer": "Email: estrubel@andrew.cmu.edu Office: Gates & Hillman Centers",
        "exact_match": false,
        "f1_max_min_avg": [
            0.27,
            0.27,
            0.27
        ],
        "recall_max_min_avg": [
            0.57,
            0.57,
            0.57
        ]
    },
    {
        "question": "What is the Research Areas of LTI faculty Daniel Fried?",
        "context": "Daniel Fried Assistant Professor Email: dfried@andrew.cmu.edu Research Areas: Natural Language Processing: Language and Code, Conversational AI, Intelligent Agents, and Dialogue, Discourse and Pragmatics, Multimodal AI",
        "retrieved_doc": "Daniel Fried Assistant Professor Email: dfried@andrew.cmu.edu Research Areas: Natural Language Processing: Language and Code, Conversational AI, Intelligent Agents, and Dialogue, Discourse and Pragmatics, Multimodal AI|Graham Neubig Associate Professor Email: gneubig@cs.cmu.edu Office: 5409 Gates & Hillman Centers Research Areas: Machine Translation, Natural Language Processing, Spoken Language Processing, Machine Learning|Yonatan Bisk Assistant Professor Email: ybisk@cs.cmu.edu Office: Gates & Hillman Centers Research Areas: Grounding, RoboNLP, Vision and Language, Embodiment, Unsupervised Learning",
        "retrieved_relevant_part": true,
        "model_answer": ":  Daniel Fried's research areas are Natural Language Processing: Language and Code, Conversational AI, Intelligent Agents, and Dialogue, Discourse and Pragmatics, Multimodal AI",
        "actual_answer": "Daniel Fried's Research Areas is Natural Language Processing: Language and Code, Conversational AI, Intelligent Agents, and Dialogue, Discourse and Pragmatics, Multimodal AI",
        "exact_match": false,
        "f1_max_min_avg": [
            0.71,
            0.71,
            0.71
        ],
        "recall_max_min_avg": [
            0.73,
            0.73,
            0.73
        ]
    },
    {
        "question": "How can I contact LTI faculty Robert Frederking?",
        "context": "Robert Frederking Principal Systems Scientist/Associate Dean of Doctoral Programs/MLT Program Director Email: ref@cs.cmu.edu Office: 6515 Gates & Hillman Centers Phone: 412-268-6656",
        "retrieved_doc": "Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu|Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering|Alexander Rudnicky Research Professor Emeritus Email: alex.rudnicky@cs.cmu.edu Office: 6511 Gates & Hillman Centers Phone: 412-268-2622 Research Areas: Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing",
        "retrieved_relevant_part": false,
        "model_answer": ":  I'm sorry for any confusion, but Robert Frederking's contact information was not provided in the context. The context only includes the email addresses, offices, and research areas of Daniel Vosler, Anatole Gershman, and Alexander Rudnicky",
        "actual_answer": "Email: ref@cs.cmu.edu Office: 6515 Gates & Hillman Centers Phone: 412-268-6656",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Please tell me the Phone of LTI faculty Justine Cassell based on the background information: ",
        "context": "Justine Cassell Professor (On Leave) Email: jcassell@andrew.cmu.edu Office: 5107 Gates & Hillman Centers Phone: 412-204-6268",
        "retrieved_doc": "Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering|Louis-Philippe Morency Leonardo Associate Professor of Computer Science Email: morency@cs.cmu.edu Office: 5411 Gates & Hillman Centers Phone: 412-268-5508 Research Areas: Machine Learning, Multimodal Computing and Interaction, Spoken Interfaces and Dialogue Processing|David Mortensen Assistant Research Professor Email: dmortens@cs.cmu.edu Office: 5707 Gates & Hillman Centers Phone: 412-268-2894 Research Areas: Natural Language Processing and Computational Linguistics, Corpus Annotation and Resources",
        "retrieved_relevant_part": false,
        "model_answer": ":  I'm sorry for any confusion, but Justine Cassell is not mentioned in the provided context. The context only includes information about Anatole Gershman, Louis-Philippe Morency, and David Mortensen, including their emails, offices, and research areas",
        "actual_answer": "Justine Cassell's Phone is 412-204-6268",
        "exact_match": false,
        "f1_max_min_avg": [
            0.1,
            0.1,
            0.1
        ],
        "recall_max_min_avg": [
            0.4,
            0.4,
            0.4
        ]
    },
    {
        "question": "What is the Research Areas of LTI faculty Shinji Watanabe?",
        "context": "Shinji Watanabe Associate Professor Email: swatanab@andrew.cmu.edu Phone: 412-268-3687 Research Areas: Natural Language Processing: Conversational AI, Intelligent Agents, and Dialogue, Speech Processing (ASR, Speech Synthesis): Speech Recognition, Speech Synthesis, Multilingual/Low-Resource Speech Processing, Speech-to-Speech Translation, Speech Enhancement / Robust Speech Processing",
        "retrieved_doc": "Yonatan Bisk Assistant Professor Email: ybisk@cs.cmu.edu Office: Gates & Hillman Centers Research Areas: Grounding, RoboNLP, Vision and Language, Embodiment, Unsupervised Learning|Graham Neubig Associate Professor Email: gneubig@cs.cmu.edu Office: 5409 Gates & Hillman Centers Research Areas: Machine Translation, Natural Language Processing, Spoken Language Processing, Machine Learning|Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu",
        "retrieved_relevant_part": false,
        "model_answer": ":  Shinji Watanabe is not mentioned in the context with his research areas. The context only provides information about Yonatan Bisk and Graham Neubig, including their email addresses, offices, and research areas",
        "actual_answer": "Shinji Watanabe's Research Areas is Natural Language Processing: Conversational AI, Intelligent Agents, and Dialogue, Speech Processing (ASR, Speech Synthesis): Speech Recognition, Speech Synthesis, Multilingual/Low-Resource Speech Processing, Speech-to-Speech Translation, Speech Enhancement / Robust Speech Processing",
        "exact_match": false,
        "f1_max_min_avg": [
            0.09,
            0.09,
            0.09
        ],
        "recall_max_min_avg": [
            0.09,
            0.09,
            0.09
        ]
    },
    {
        "question": "Please tell me the Email of LTI faculty Alexander Hauptmann based on the background information: ",
        "context": "Alexander Hauptmann Research Professor Email: alex@cs.cmu.edu Office: 5519 Gates & Hillman Centers Phone: 412-268-1448 Research Areas: Information Extraction, Summarization and Question Answering, Information Retrieval, Text Mining and Analytics, Machine Learning, Multimodal Computing and Interaction",
        "retrieved_doc": "Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu|Zhi-Qi Cheng Hauptmann Lab/Postdoctoral Research Associate Email: zhiqic@andrew.cmu.edu|Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu",
        "retrieved_relevant_part": false,
        "model_answer": ":  Based on the context provided, Alexander Hauptmann's email address is hauptmann@andrew.cmu.edu as he is mentioned as the head of the Hauptmann Lab where Zhi-Qi Cheng and Soumi Maiti are Postdoctoral Research Associates",
        "actual_answer": "alex@cs.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Please provide the contact information of LTI faculty Eric Nyberg",
        "context": "Eric Nyberg Professor Email: ehn@cs.cmu.edu Office: 6715 Gates & Hillman Centers Phone: 412-268-7281 Research Areas: Information Extraction, Summarization and Question Answering, Information Retrieval, Text Mining and Analytics, Language Technologies for Education",
        "retrieved_doc": "Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering|Alexander Rudnicky Research Professor Emeritus Email: alex.rudnicky@cs.cmu.edu Office: 6511 Gates & Hillman Centers Phone: 412-268-2622 Research Areas: Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing|Eric P. Xing Professor (On Leave) Email: epxing@andrew.cmu.edu Office: 8101 Gates & Hillman Centers Phone: 412-268-2559",
        "retrieved_relevant_part": false,
        "model_answer": ":  I'm sorry for any confusion, but there is no faculty member named \"Eric Nyberg\" mentioned in the provided context. The context only includes information about Anatole Gershman, Alexander Rudnicky, and Eric P. Xing, along with their respective contact details and research areas",
        "actual_answer": "Email: ehn@cs.cmu.edu Office: 6715 Gates & Hillman Centers Phone: 412-268-7281",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Please tell me the Phone of LTI faculty Robert Frederking based on the background information: ",
        "context": "Robert Frederking Principal Systems Scientist/Associate Dean of Doctoral Programs/MLT Program Director Email: ref@cs.cmu.edu Office: 6515 Gates & Hillman Centers Phone: 412-268-6656",
        "retrieved_doc": "Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering|Zaid Sheikh Neubig Lab/Senior Systems/Software Engineer Email: zsheikh@andrew.cmu.edu Office: 6416 Gates & Hillman Centers Phone:|Alexander Rudnicky Research Professor Emeritus Email: alex.rudnicky@cs.cmu.edu Office: 6511 Gates & Hillman Centers Phone: 412-268-2622 Research Areas: Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing",
        "retrieved_relevant_part": false,
        "model_answer": ":  I'm sorry for any confusion, but Robert Frederking was not mentioned in the provided context. The context only includes information about Anatole Gershman, Zaid Sheikh, and Alexander Rudnicky, including their titles, emails, offices, and research areas",
        "actual_answer": "412-268-6656",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Please provide the contact information of LTI faculty Rita Singh",
        "context": "Rita Singh Associate Research Professor Email: rsingh@cs.cmu.edu Office: 6703 Gates & Hillman Centers Phone: 412-268-9859",
        "retrieved_doc": "Rita Singh Associate Research Professor Email: rsingh@cs.cmu.edu Office: 6703 Gates & Hillman Centers Phone: 412-268-9859|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu",
        "retrieved_relevant_part": true,
        "model_answer": ":  The contact information for LTI faculty Rita Singh is: Email: rsingh@cs.cmu.edu, Phone: 412-268-9859",
        "actual_answer": "Email: rsingh@cs.cmu.edu Office: 6703 Gates & Hillman Centers Phone: 412-268-9859",
        "exact_match": false,
        "f1_max_min_avg": [
            0.25,
            0.25,
            0.25
        ],
        "recall_max_min_avg": [
            0.3,
            0.3,
            0.3
        ]
    },
    {
        "question": "Please tell me the Phone of LTI faculty Robert Frederking based on the background information: ",
        "context": "Robert Frederking Principal Systems Scientist/Associate Dean of Doctoral Programs/MLT Program Director Email: ref@cs.cmu.edu Office: 6515 Gates & Hillman Centers Phone: 412-268-6656",
        "retrieved_doc": "Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering|Zaid Sheikh Neubig Lab/Senior Systems/Software Engineer Email: zsheikh@andrew.cmu.edu Office: 6416 Gates & Hillman Centers Phone:|Alexander Rudnicky Research Professor Emeritus Email: alex.rudnicky@cs.cmu.edu Office: 6511 Gates & Hillman Centers Phone: 412-268-2622 Research Areas: Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing",
        "retrieved_relevant_part": false,
        "model_answer": ":  I'm sorry for any confusion, but Robert Frederking was not mentioned in the provided context. The context only includes information about Anatole Gershman, Zaid Sheikh, and Alexander Rudnicky, including their titles, emails, offices, and research areas",
        "actual_answer": "Robert Frederking's Phone is 412-268-6656",
        "exact_match": false,
        "f1_max_min_avg": [
            0.05,
            0.05,
            0.05
        ],
        "recall_max_min_avg": [
            0.2,
            0.2,
            0.2
        ]
    },
    {
        "question": "How can I contact LTI faculty Eric P.?",
        "context": "Eric P. Xing Professor (On Leave) Email: epxing@andrew.cmu.edu Office: 8101 Gates & Hillman Centers Phone: 412-268-2559",
        "retrieved_doc": "Eric P. Xing Professor (On Leave) Email: epxing@andrew.cmu.edu Office: 8101 Gates & Hillman Centers Phone: 412-268-2559|Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering|Alexander Rudnicky Research Professor Emeritus Email: alex.rudnicky@cs.cmu.edu Office: 6511 Gates & Hillman Centers Phone: 412-268-2622 Research Areas: Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing",
        "retrieved_relevant_part": true,
        "model_answer": ":  You can contact Professor Eric P. Xing at his email address epxing@andrew.cmu.edu or call him at his phone number 412-268-2559. His office is located at 8101 Gates & Hillman Centers",
        "actual_answer": "Email: epxing@andrew.cmu.edu Office: 8101 Gates & Hillman Centers Phone: 412-268-2559",
        "exact_match": false,
        "f1_max_min_avg": [
            0.29,
            0.29,
            0.29
        ],
        "recall_max_min_avg": [
            0.6,
            0.6,
            0.6
        ]
    },
    {
        "question": "What is the Email of LTI faculty Eric Nyberg?",
        "context": "Eric Nyberg Professor Email: ehn@cs.cmu.edu Office: 6715 Gates & Hillman Centers Phone: 412-268-7281 Research Areas: Information Extraction, Summarization and Question Answering, Information Retrieval, Text Mining and Analytics, Language Technologies for Education",
        "retrieved_doc": "Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu|Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu|Wenhe Liu Research Scholar Email: wenhel@andrew.cmu.edu Office: 5403 Gates & Hillman Centers",
        "retrieved_relevant_part": false,
        "model_answer": ":  I'm sorry, but the context does not provide the email address for Eric Nyberg. The context only mentions Daniel Vosler, Soumi Maiti, and Wenhe Liu, along with their respective titles and email addresses",
        "actual_answer": "Eric Nyberg's Email is ehn@cs.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0.05,
            0.05,
            0.05
        ],
        "recall_max_min_avg": [
            0.2,
            0.2,
            0.2
        ]
    },
    {
        "question": "Is Anatole Gershman natan Bisk Assistant Professor of LTI?, Please answer yes or no: ",
        "context": "Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering",
        "retrieved_doc": "Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu|Yonatan Bisk Assistant Professor Email: ybisk@cs.cmu.edu Office: Gates & Hillman Centers Research Areas: Grounding, RoboNLP, Vision and Language, Embodiment, Unsupervised Learning|Graham Neubig Associate Professor Email: gneubig@cs.cmu.edu Office: 5409 Gates & Hillman Centers Research Areas: Machine Translation, Natural Language Processing, Spoken Language Processing, Machine Learning",
        "retrieved_relevant_part": false,
        "model_answer": ":  No, Anatole Gershman is not mentioned in the context as being an assistant professor at LTI (Laboratory for Computational Linguistics) or having the email address ybisk@cs.cmu.edu. The context only mentions Yonatan Bisk as an assistant professor with those credentials",
        "actual_answer": "No",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "How can I contact LTI faculty Rita Singh?",
        "context": "Rita Singh Associate Research Professor Email: rsingh@cs.cmu.edu Office: 6703 Gates & Hillman Centers Phone: 412-268-9859",
        "retrieved_doc": "Rita Singh Associate Research Professor Email: rsingh@cs.cmu.edu Office: 6703 Gates & Hillman Centers Phone: 412-268-9859|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Krista McGuigan Business Director Email: klmcg@cs.cmu.edu Office: 349 TCS Hall Phone: 412-268-1635",
        "retrieved_relevant_part": true,
        "model_answer": ":  You can contact Rita Singh, who is an Associate Research Professor at LTI (Language Technologies Institute) at Carnegie Mellon University, using her email address rsingh@cs.cmu.edu or by phone at 412-268-9859",
        "actual_answer": "Email: rsingh@cs.cmu.edu Office: 6703 Gates & Hillman Centers Phone: 412-268-9859",
        "exact_match": false,
        "f1_max_min_avg": [
            0.1,
            0.1,
            0.1
        ],
        "recall_max_min_avg": [
            0.2,
            0.2,
            0.2
        ]
    },
    {
        "question": "Please tell me the Research Areas of LTI faculty Maarten Sap based on the background information: ",
        "context": "Maarten Sap Assistant Professor Email: msap2@andrew.cmu.edu Research Areas: Fairness and Ethics in Language Technology, Computational Social Science, Discourse and Pragmatics, Conversational AI, Intelligent Agents, and Dialogue",
        "retrieved_doc": "Yonatan Bisk Assistant Professor Email: ybisk@cs.cmu.edu Office: Gates & Hillman Centers Research Areas: Grounding, RoboNLP, Vision and Language, Embodiment, Unsupervised Learning|Vicente Malave Data Science Course Developer Email: vlm@andrew.cmu.edu|Graham Neubig Associate Professor Email: gneubig@cs.cmu.edu Office: 5409 Gates & Hillman Centers Research Areas: Machine Translation, Natural Language Processing, Spoken Language Processing, Machine Learning",
        "retrieved_relevant_part": false,
        "model_answer": ":  Based on the provided context, Maarten Sap is not mentioned among the listed faculty members with their research areas. Therefore, I cannot provide an answer to the question using the given context",
        "actual_answer": "Fairness and Ethics in Language Technology, Computational Social Science, Discourse and Pragmatics, Conversational AI, Intelligent Agents, and Dialogue",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "What is the Phone of LTI faculty Anatole Gershman?",
        "context": "Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering",
        "retrieved_doc": "Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering|Susan E. Holm Mitamura Lab/Sr. Knowledge Engineer Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591|Yukari Yamakawa Mitamura Lab/Associate Data Analyst Email: yukariy@andrew.cmu.edu Office: 6717 Gates & Hillman Centers Phone: 412-268-6596",
        "retrieved_relevant_part": true,
        "model_answer": ":  The phone number for Anatole Gershman is 412-268-8259",
        "actual_answer": "Anatole Gershman's Phone is 412-268-8259",
        "exact_match": false,
        "f1_max_min_avg": [
            0.43,
            0.43,
            0.43
        ],
        "recall_max_min_avg": [
            0.6,
            0.6,
            0.6
        ]
    },
    {
        "question": "How can I contact LTI faculty Yonatan Bisk?",
        "context": "Yonatan Bisk Assistant Professor Email: ybisk@cs.cmu.edu Office: Gates & Hillman Centers Research Areas: Grounding, RoboNLP, Vision and Language, Embodiment, Unsupervised Learning",
        "retrieved_doc": "Yukari Yamakawa Mitamura Lab/Associate Data Analyst Email: yukariy@andrew.cmu.edu Office: 6717 Gates & Hillman Centers Phone: 412-268-6596|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Bhiksha Ramakrishnan Professor Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826 Research Areas: Machine Learning, Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing, Privacy",
        "retrieved_relevant_part": false,
        "model_answer": ":  Unfortunately, there is no email or phone number provided for Yonatan Bisk in the context",
        "actual_answer": "Email: ybisk@cs.cmu.edu Office: Gates & Hillman Centers",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "What is the Phone of LTI faculty Shinji Watanabe?",
        "context": "Shinji Watanabe Associate Professor Email: swatanab@andrew.cmu.edu Phone: 412-268-3687 Research Areas: Natural Language Processing: Conversational AI, Intelligent Agents, and Dialogue, Speech Processing (ASR, Speech Synthesis): Speech Recognition, Speech Synthesis, Multilingual/Low-Resource Speech Processing, Speech-to-Speech Translation, Speech Enhancement / Robust Speech Processing",
        "retrieved_doc": "Yukari Yamakawa Mitamura Lab/Associate Data Analyst Email: yukariy@andrew.cmu.edu Office: 6717 Gates & Hillman Centers Phone: 412-268-6596|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Susan E. Holm Mitamura Lab/Sr. Knowledge Engineer Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591",
        "retrieved_relevant_part": false,
        "model_answer": ":  I'm sorry for any confusion, but there is no mention of a faculty member named Shinji Watanabe in the provided context. The context only mentions Yukari Yamakawa Mitamura, Mona Diab, and Susan E. Holm, along with their respective job titles, emails, and phone numbers",
        "actual_answer": "Shinji Watanabe's Phone is 412-268-3687",
        "exact_match": false,
        "f1_max_min_avg": [
            0.08,
            0.08,
            0.08
        ],
        "recall_max_min_avg": [
            0.4,
            0.4,
            0.4
        ]
    },
    {
        "question": "How can I contact LTI faculty Louis-Philippe Morency?",
        "context": "Louis-Philippe Morency Leonardo Associate Professor of Computer Science Email: morency@cs.cmu.edu Office: 5411 Gates & Hillman Centers Phone: 412-268-5508 Research Areas: Machine Learning, Multimodal Computing and Interaction, Spoken Interfaces and Dialogue Processing",
        "retrieved_doc": "Louis-Philippe Morency Leonardo Associate Professor of Computer Science Email: morency@cs.cmu.edu Office: 5411 Gates & Hillman Centers Phone: 412-268-5508 Research Areas: Machine Learning, Multimodal Computing and Interaction, Spoken Interfaces and Dialogue Processing|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering",
        "retrieved_relevant_part": true,
        "model_answer": ":  You can contact Louis-Philippe Morency by email at morency@cs.cmu.edu or by phone at 412-268-5508. His office is located at 5411 Gates & Hillman Centers",
        "actual_answer": "Email: morency@cs.cmu.edu Office: 5411 Gates & Hillman Centers Phone: 412-268-5508",
        "exact_match": false,
        "f1_max_min_avg": [
            0.34,
            0.34,
            0.34
        ],
        "recall_max_min_avg": [
            0.6,
            0.6,
            0.6
        ]
    },
    {
        "question": "Please tell me the Phone of LTI faculty Robert Frederking based on the background information: ",
        "context": "Robert Frederking Principal Systems Scientist/Associate Dean of Doctoral Programs/MLT Program Director Email: ref@cs.cmu.edu Office: 6515 Gates & Hillman Centers Phone: 412-268-6656",
        "retrieved_doc": "Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering|Zaid Sheikh Neubig Lab/Senior Systems/Software Engineer Email: zsheikh@andrew.cmu.edu Office: 6416 Gates & Hillman Centers Phone:|Alexander Rudnicky Research Professor Emeritus Email: alex.rudnicky@cs.cmu.edu Office: 6511 Gates & Hillman Centers Phone: 412-268-2622 Research Areas: Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing",
        "retrieved_relevant_part": false,
        "model_answer": ":  I'm sorry for any confusion, but Robert Frederking was not mentioned in the provided context. The context only includes information about Anatole Gershman, Zaid Sheikh, and Alexander Rudnicky, along with their respective emails, offices, and research areas",
        "actual_answer": "412-268-6656",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "How can I contact LTI faculty Rita Singh?",
        "context": "Rita Singh Associate Research Professor Email: rsingh@cs.cmu.edu Office: 6703 Gates & Hillman Centers Phone: 412-268-9859",
        "retrieved_doc": "Rita Singh Associate Research Professor Email: rsingh@cs.cmu.edu Office: 6703 Gates & Hillman Centers Phone: 412-268-9859|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Krista McGuigan Business Director Email: klmcg@cs.cmu.edu Office: 349 TCS Hall Phone: 412-268-1635",
        "retrieved_relevant_part": true,
        "model_answer": ":  You can contact Rita Singh, who is an Associate Research Professor at LTI (Language Technologies Institute) at Carnegie Mellon University, using her email address rsingh@cs.cmu.edu or by phone at 412-268-9859",
        "actual_answer": "Email: rsingh@cs.cmu.edu Office: 6703 Gates & Hillman Centers Phone: 412-268-9859",
        "exact_match": false,
        "f1_max_min_avg": [
            0.1,
            0.1,
            0.1
        ],
        "recall_max_min_avg": [
            0.2,
            0.2,
            0.2
        ]
    },
    {
        "question": "How can I contact LTI faculty Maarten Sap?",
        "context": "Maarten Sap Assistant Professor Email: msap2@andrew.cmu.edu Research Areas: Fairness and Ethics in Language Technology, Computational Social Science, Discourse and Pragmatics, Conversational AI, Intelligent Agents, and Dialogue",
        "retrieved_doc": "Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering|Brianna Eriksen Academic Program Manager Email: bfreema2@andrew.cmu.edu Phone: 412-268-4277|Jennifer Lucas Academic Program Manager \u2014 MCDS Email: jmlucas@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-9870",
        "retrieved_relevant_part": false,
        "model_answer": ":  Maarten Sap is not mentioned in the context provided. Therefore, there is no information available in the context to answer the question",
        "actual_answer": "Email: msap2@andrew.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "What is the Research Areas of LTI faculty Shinji Watanabe?",
        "context": "Shinji Watanabe Associate Professor Email: swatanab@andrew.cmu.edu Phone: 412-268-3687 Research Areas: Natural Language Processing: Conversational AI, Intelligent Agents, and Dialogue, Speech Processing (ASR, Speech Synthesis): Speech Recognition, Speech Synthesis, Multilingual/Low-Resource Speech Processing, Speech-to-Speech Translation, Speech Enhancement / Robust Speech Processing",
        "retrieved_doc": "Yonatan Bisk Assistant Professor Email: ybisk@cs.cmu.edu Office: Gates & Hillman Centers Research Areas: Grounding, RoboNLP, Vision and Language, Embodiment, Unsupervised Learning|Graham Neubig Associate Professor Email: gneubig@cs.cmu.edu Office: 5409 Gates & Hillman Centers Research Areas: Machine Translation, Natural Language Processing, Spoken Language Processing, Machine Learning|Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu",
        "retrieved_relevant_part": false,
        "model_answer": ":  Shinji Watanabe is not mentioned in the context with his research areas. The context only provides information about Yonatan Bisk and Graham Neubig, including their titles, email addresses, offices, and research areas",
        "actual_answer": "Shinji Watanabe's Research Areas is Natural Language Processing: Conversational AI, Intelligent Agents, and Dialogue, Speech Processing (ASR, Speech Synthesis): Speech Recognition, Speech Synthesis, Multilingual/Low-Resource Speech Processing, Speech-to-Speech Translation, Speech Enhancement / Robust Speech Processing",
        "exact_match": false,
        "f1_max_min_avg": [
            0.09,
            0.09,
            0.09
        ],
        "recall_max_min_avg": [
            0.09,
            0.09,
            0.09
        ]
    },
    {
        "question": "How can I contact LTI faculty Eric P.?",
        "context": "Eric P. Xing Professor (On Leave) Email: epxing@andrew.cmu.edu Office: 8101 Gates & Hillman Centers Phone: 412-268-2559",
        "retrieved_doc": "Eric P. Xing Professor (On Leave) Email: epxing@andrew.cmu.edu Office: 8101 Gates & Hillman Centers Phone: 412-268-2559|Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering|Alexander Rudnicky Research Professor Emeritus Email: alex.rudnicky@cs.cmu.edu Office: 6511 Gates & Hillman Centers Phone: 412-268-2622 Research Areas: Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing",
        "retrieved_relevant_part": true,
        "model_answer": ":  You can contact Professor Eric P. Xing at his email address epxing@andrew.cmu.edu or call him at his phone number 412-268-2559. His office is located at 8101 Gates & Hillman Centers",
        "actual_answer": "Email: epxing@andrew.cmu.edu Office: 8101 Gates & Hillman Centers Phone: 412-268-2559",
        "exact_match": false,
        "f1_max_min_avg": [
            0.29,
            0.29,
            0.29
        ],
        "recall_max_min_avg": [
            0.6,
            0.6,
            0.6
        ]
    },
    {
        "question": "Please tell me the Research Areas of LTI faculty Shinji Watanabe based on the background information: ",
        "context": "Shinji Watanabe Associate Professor Email: swatanab@andrew.cmu.edu Phone: 412-268-3687 Research Areas: Natural Language Processing: Conversational AI, Intelligent Agents, and Dialogue, Speech Processing (ASR, Speech Synthesis): Speech Recognition, Speech Synthesis, Multilingual/Low-Resource Speech Processing, Speech-to-Speech Translation, Speech Enhancement / Robust Speech Processing",
        "retrieved_doc": "Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu|Author: Shinji Watanabe Title: Unsupervised Data Selection for TTS: Using Arabic Broadcast News as a Case Study Publication year: 2023 Coauthors: Massa Baali, Tomoki Hayashi, Hamdy Mubarak, Soumi Maiti, Shinji Watanabe, W. El-Hajj, Ahmed Ali Abstract: .|Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe ",
        "retrieved_relevant_part": false,
        "model_answer": ":  Shinji Watanabe is a faculty member at Carnegie Mellon University's Laboratory for Telecommunications and Information Technology (LTI). His research areas can be identified from the publications listed in the context. In the first publication from 2023, titled \"Unsupervised Data Selection for TTS: Using Arabic Broadcast News as a Case Study,\" he worked on unsupervised data selection for Text-to-Speech (TTS) systems using Arabic broadcast news as a case study, along with co-authors Massa Baali, Tomoki Hayashi, Hamdy Mubarak, and others. In the second publication also from 2023, titled \"Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training,\" he collaborated with Yui Sudo, Muhammad Shakeel, and Yifan Peng on developing time-synchronous one-pass Beam Search for parallel online and offline transducers with dynamic block training. These publications suggest that his research focuses on Natural Language Processing (NLP), Speech Synthesis (Text-to-Speech), and Machine Learning techniques",
        "actual_answer": "Natural Language Processing: Conversational AI, Intelligent Agents, and Dialogue, Speech Processing (ASR, Speech Synthesis): Speech Recognition, Speech Synthesis, Multilingual/Low-Resource Speech Processing, Speech-to-Speech Translation, Speech Enhancement / Robust Speech Processing",
        "exact_match": false,
        "f1_max_min_avg": [
            0.06,
            0.06,
            0.06
        ],
        "recall_max_min_avg": [
            0.17,
            0.17,
            0.17
        ]
    },
    {
        "question": "Is Scott Fahlman i Li Assistant Professor of LTI?, Please answer yes or no: ",
        "context": "Scott Fahlman Research Professor Emeritus Email: sef@cs.cmu.edu Office: 6417 Gates & Hillman Centers Phone: 412-268-2575 Research Areas: AI, Knowledge Representation and Reasoning, Natural Language Understanding",
        "retrieved_doc": "Graham Neubig Associate Professor Email: gneubig@cs.cmu.edu Office: 5409 Gates & Hillman Centers Research Areas: Machine Translation, Natural Language Processing, Spoken Language Processing, Machine Learning|Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu|Emma Strubell Assistant Professor Email: estrubel@andrew.cmu.edu Office: Gates & Hillman Centers",
        "retrieved_relevant_part": false,
        "model_answer": ":  Based on the provided context, I cannot determine if Scott Fahlman is an assistant professor at the Language Technologies Institute (LTI) at Carnegie Mellon University as his name and title were not mentioned in the context",
        "actual_answer": "No",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "How can I contact LTI faculty Bhiksha Ramakrishnan?",
        "context": "Bhiksha Ramakrishnan Professor Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826 Research Areas: Machine Learning, Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing, Privacy",
        "retrieved_doc": "Bhiksha Ramakrishnan Professor Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826 Research Areas: Machine Learning, Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing, Privacy|Rita Singh Associate Research Professor Email: rsingh@cs.cmu.edu Office: 6703 Gates & Hillman Centers Phone: 412-268-9859|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669",
        "retrieved_relevant_part": true,
        "model_answer": ":  You can contact Bhiksha Ramakrishnan, who is a professor at LTI (Language Technologies Institute) at Carnegie Mellon University, using any of the following methods mentioned in the context:",
        "actual_answer": "Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "How can I contact LTI faculty Chenyan Xiong?",
        "context": "Chenyan Xiong Associate Professor Email: cx@andrew.cmu.edu Phone: 412-268-7641",
        "retrieved_doc": "Chenyan Xiong Associate Professor Email: cx@andrew.cmu.edu Phone: 412-268-7641|Zhong-Qiu Wang Wantanabe Lab/Postdoctoral Research Associate Email: zhongqiu@andrew.cmu.edu Office: 6414 Gates & Hillman Centers Phone: 415-200-8662|Yiming Yang Professor Email: yiming@cs.cmu.edu Office: 6717 Gates & Hillman Centers Phone: 412-268-1364",
        "retrieved_relevant_part": true,
        "model_answer": ":  You can contact Chenyan Xiong by email at cx@andrew.cmu.edu or by phone at 412-268-7641",
        "actual_answer": "Email: cx@andrew.cmu.edu Phone: 412-268-7641",
        "exact_match": false,
        "f1_max_min_avg": [
            0.21,
            0.21,
            0.21
        ],
        "recall_max_min_avg": [
            0.5,
            0.5,
            0.5
        ]
    },
    {
        "question": "How can I contact LTI faculty Bhiksha Ramakrishnan?",
        "context": "Bhiksha Ramakrishnan Professor Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826 Research Areas: Machine Learning, Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing, Privacy",
        "retrieved_doc": "Bhiksha Ramakrishnan Professor Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826 Research Areas: Machine Learning, Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing, Privacy|Rita Singh Associate Research Professor Email: rsingh@cs.cmu.edu Office: 6703 Gates & Hillman Centers Phone: 412-268-9859|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669",
        "retrieved_relevant_part": true,
        "model_answer": ":  You can contact Bhiksha Ramakrishnan, who is a professor at LTI (Language Technologies Institute) at Carnegie Mellon University, using any of the following methods mentioned in the context:",
        "actual_answer": "Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "How can I contact LTI faculty Maarten Sap?",
        "context": "Maarten Sap Assistant Professor Email: msap2@andrew.cmu.edu Research Areas: Fairness and Ethics in Language Technology, Computational Social Science, Discourse and Pragmatics, Conversational AI, Intelligent Agents, and Dialogue",
        "retrieved_doc": "Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering|Brianna Eriksen Academic Program Manager Email: bfreema2@andrew.cmu.edu Phone: 412-268-4277|Jennifer Lucas Academic Program Manager \u2014 MCDS Email: jmlucas@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-9870",
        "retrieved_relevant_part": false,
        "model_answer": ":  Maarten Sap is not mentioned in the context provided. Therefore, there is no information available in the context to answer the question",
        "actual_answer": "Email: msap2@andrew.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "What is the Office of LTI faculty Justine Cassell?",
        "context": "Justine Cassell Professor (On Leave) Email: jcassell@andrew.cmu.edu Office: 5107 Gates & Hillman Centers Phone: 412-204-6268",
        "retrieved_doc": "Nicole Perrotta Administrative Coordinator Email: nperrott@andrew.cmu.edu|Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering|Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu",
        "retrieved_relevant_part": false,
        "model_answer": ":  Justine Cassell does not have an office mentioned in the context provided. The context only mentions the offices of Nicole Perrotta and Anatole Gershman, as well as Daniel Vosler's role as LTI High Performance Cluster Administrator but no mention of any office for Justine Cassell",
        "actual_answer": "Justine Cassell's Office is 5107 Gates & Hillman Centers",
        "exact_match": false,
        "f1_max_min_avg": [
            0.04,
            0.04,
            0.04
        ],
        "recall_max_min_avg": [
            0.11,
            0.11,
            0.11
        ]
    },
    {
        "question": "What is the Research Areas of LTI faculty Louis-Philippe Morency?",
        "context": "Louis-Philippe Morency Leonardo Associate Professor of Computer Science Email: morency@cs.cmu.edu Office: 5411 Gates & Hillman Centers Phone: 412-268-5508 Research Areas: Machine Learning, Multimodal Computing and Interaction, Spoken Interfaces and Dialogue Processing",
        "retrieved_doc": "Graham Neubig Associate Professor Email: gneubig@cs.cmu.edu Office: 5409 Gates & Hillman Centers Research Areas: Machine Translation, Natural Language Processing, Spoken Language Processing, Machine Learning|Yonatan Bisk Assistant Professor Email: ybisk@cs.cmu.edu Office: Gates & Hillman Centers Research Areas: Grounding, RoboNLP, Vision and Language, Embodiment, Unsupervised Learning|Maarten Sap Assistant Professor Email: msap2@andrew.cmu.edu Research Areas: Fairness and Ethics in Language Technology, Computational Social Science, Discourse and Pragmatics, Conversational AI, Intelligent Agents, and Dialogue",
        "retrieved_relevant_part": false,
        "model_answer": ":  I apologize for the mistake in the previous response. Based on the context provided, Louis-Philippe Morency is not listed as an LTI faculty member with specified research areas. Therefore, there is no output for this question according to the given context",
        "actual_answer": "Louis-Philippe Morency's Research Areas is Machine Learning, Multimodal Computing and Interaction, Spoken Interfaces and Dialogue Processing",
        "exact_match": false,
        "f1_max_min_avg": [
            0.07,
            0.07,
            0.07
        ],
        "recall_max_min_avg": [
            0.12,
            0.12,
            0.12
        ]
    },
    {
        "question": "What is the Office of LTI faculty Ralf Brown?",
        "context": "Ralf Brown Principal Systems Scientist Email: ralf@andrew.cmu.edu Office: 5711 Gates & Hillman Centers Phone: 412-268-8298 Research Areas: Information Extraction, Summarization and Question Answering, Information Retrieval, Text Mining and Analytics, Machine Translation, Natural Language Processing and Computational Linguistics",
        "retrieved_doc": "of students, faculty, staff and alumni. The Task Force was co-chaired by Director of Athletics Susan Bassett and Dean of Student Affairs Jennifer Church. The mascot selection process included a series of surveys and a university Town Hall meeting.|Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu|Zaid Sheikh Neubig Lab/Senior Systems/Software Engineer Email: zsheikh@andrew.cmu.edu Office: 6416 Gates & Hillman Centers Phone:",
        "retrieved_relevant_part": false,
        "model_answer": ":  Ralf Brown is not mentioned in the context provided with regards to the Office of LTI. The context only mentions Daniel Vosler as the LTI High Performance Cluster Administrator and his contact information",
        "actual_answer": "5711 Gates & Hillman Centers",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Please tell me the Email of LTI faculty Robert Frederking based on the background information: ",
        "context": "Robert Frederking Principal Systems Scientist/Associate Dean of Doctoral Programs/MLT Program Director Email: ref@cs.cmu.edu Office: 6515 Gates & Hillman Centers Phone: 412-268-6656",
        "retrieved_doc": "Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu|Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu|Emma Strubell Assistant Professor Email: estrubel@andrew.cmu.edu Office: Gates & Hillman Centers",
        "retrieved_relevant_part": false,
        "model_answer": ":  I'm sorry for any confusion, but Robert Frederking's email address was not provided in the context. The context only includes the emails of Soumi Maiti Watanabe, Daniel Vosler, and Emma Strubell",
        "actual_answer": "Robert Frederking's Email is ref@cs.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0.11,
            0.11,
            0.11
        ],
        "recall_max_min_avg": [
            0.4,
            0.4,
            0.4
        ]
    },
    {
        "question": "Please provide the contact information of LTI faculty Jamie Callan",
        "context": "Jamie Callan Professor and PhD Program Director Email: callan@cs.cmu.edu Office: 5419 Gates & Hillman Centers Phone: 412-268-4525 Research Areas: Information Retrieval, Text Mining and Analytics",
        "retrieved_doc": "Jamie Callan Professor and PhD Program Director Email: callan@cs.cmu.edu Office: 5419 Gates & Hillman Centers Phone: 412-268-4525 Research Areas: Information Retrieval, Text Mining and Analytics|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering",
        "retrieved_relevant_part": true,
        "model_answer": ":  Contact information for Jamie Callan, who is a faculty member at LTI (Language Technologies Institute) at Carnegie Mellon University, can be found below:",
        "actual_answer": "Email: callan@cs.cmu.edu Office: 5419 Gates & Hillman Centers Phone: 412-268-4525",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "What is the Email of LTI faculty Bhiksha Ramakrishnan?",
        "context": "Bhiksha Ramakrishnan Professor Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826 Research Areas: Machine Learning, Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing, Privacy",
        "retrieved_doc": "Bhiksha Ramakrishnan Professor Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826 Research Areas: Machine Learning, Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing, Privacy|Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu|Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu",
        "retrieved_relevant_part": true,
        "model_answer": ":  The email address for Bhiksha Ramakrishnan, who is a faculty member at LTI (Laboratory for Telecommunications and Information Technology) at Carnegie Mellon University, is bhiksha@cs.cmu.edu",
        "actual_answer": "bhiksha@cs.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0.07,
            0.07,
            0.07
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "What is the Phone of LTI faculty Justine Cassell?",
        "context": "Justine Cassell Professor (On Leave) Email: jcassell@andrew.cmu.edu Office: 5107 Gates & Hillman Centers Phone: 412-204-6268",
        "retrieved_doc": "Justine Cassell Professor (On Leave) Email: jcassell@andrew.cmu.edu Office: 5107 Gates & Hillman Centers Phone: 412-204-6268|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788|Jessica Maguire Assistant to the Institute Director Email: jlm4@andrew.cmu.edu Office: 5721 Gates & Hillman Centers Phone: 412-268-5480",
        "retrieved_relevant_part": true,
        "model_answer": ":  The phone number for Justine Cassell, who is a faculty member at LTI (Learning Technologies Institute) at Carnegie Mellon University, is 412-204-6268",
        "actual_answer": "412-204-6268",
        "exact_match": false,
        "f1_max_min_avg": [
            0.08,
            0.08,
            0.08
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Is Rita Singh lf Brown Principal Systems Scientist of LTI?, Please answer yes or no: ",
        "context": "Rita Singh Associate Research Professor Email: rsingh@cs.cmu.edu Office: 6703 Gates & Hillman Centers Phone: 412-268-9859",
        "retrieved_doc": "Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu|Vicente Malave Data Science Course Developer Email: vlm@andrew.cmu.edu|Zaid Sheikh Neubig Lab/Senior Systems/Software Engineer Email: zsheikh@andrew.cmu.edu Office: 6416 Gates & Hillman Centers Phone:",
        "retrieved_relevant_part": false,
        "model_answer": ":  Based on the provided context, Rita Singh is not mentioned at all. Therefore, it is impossible to determine if she is the Principal Systems Scientist of LTI based on the given information. The context only provides email addresses and job titles for Daniel Vosler, Vicente Malave, and Zaid Sheikh",
        "actual_answer": "No",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Is Maarten Sap iksha Ramakrishnan Professor of LTI?, Please answer yes or no: ",
        "context": "Maarten Sap Assistant Professor Email: msap2@andrew.cmu.edu Research Areas: Fairness and Ethics in Language Technology, Computational Social Science, Discourse and Pragmatics, Conversational AI, Intelligent Agents, and Dialogue",
        "retrieved_doc": "Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu|Yonatan Bisk Assistant Professor Email: ybisk@cs.cmu.edu Office: Gates & Hillman Centers Research Areas: Grounding, RoboNLP, Vision and Language, Embodiment, Unsupervised Learning|Vicente Malave Data Science Course Developer Email: vlm@andrew.cmu.edu",
        "retrieved_relevant_part": false,
        "model_answer": ":  Based on the provided context, Maarten Sap iksha Ramakrishnan is not mentioned at all. Therefore, it is impossible to determine whether he is a professor or not based on this information alone. The context only provides information about Soumi Maiti Watanabe, Yonatan Bisk, and Vicente Malave, along with their respective roles and research areas",
        "actual_answer": "No",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Please tell me the Email of LTI faculty Sean Welleck based on the background information: ",
        "context": "Sean Welleck Assistant Professor (Starting January 2024) Email: swelleck@andrew.cmu.edu",
        "retrieved_doc": "Sean Welleck Assistant Professor (Starting January 2024) Email: swelleck@andrew.cmu.edu|Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu|Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu",
        "retrieved_relevant_part": true,
        "model_answer": ":  The email address for LTI faculty member Sean Welleck is swelleck@andrew.cmu.edu",
        "actual_answer": "swelleck@andrew.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0.15,
            0.15,
            0.15
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Is Louis-Philippe Morency niel Fried Assistant Professor of LTI?, Please answer yes or no: ",
        "context": "Louis-Philippe Morency Leonardo Associate Professor of Computer Science Email: morency@cs.cmu.edu Office: 5411 Gates & Hillman Centers Phone: 412-268-5508 Research Areas: Machine Learning, Multimodal Computing and Interaction, Spoken Interfaces and Dialogue Processing",
        "retrieved_doc": "Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu|Graham Neubig Associate Professor Email: gneubig@cs.cmu.edu Office: 5409 Gates & Hillman Centers Research Areas: Machine Translation, Natural Language Processing, Spoken Language Processing, Machine Learning|Yonatan Bisk Assistant Professor Email: ybisk@cs.cmu.edu Office: Gates & Hillman Centers Research Areas: Grounding, RoboNLP, Vision and Language, Embodiment, Unsupervised Learning",
        "retrieved_relevant_part": false,
        "model_answer": ":  I'm sorry for any confusion, but Louis-Philippe Morency and niel Fried are not mentioned in the provided context. Therefore, I cannot answer the question with certainty based on the information given",
        "actual_answer": "No",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "What is the Office of LTI faculty Anatole Gershman?",
        "context": "Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering",
        "retrieved_doc": "Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering|Graham Neubig Associate Professor Email: gneubig@cs.cmu.edu Office: 5409 Gates & Hillman Centers Research Areas: Machine Translation, Natural Language Processing, Spoken Language Processing, Machine Learning|Zaid Sheikh Neubig Lab/Senior Systems/Software Engineer Email: zsheikh@andrew.cmu.edu Office: 6416 Gates & Hillman Centers Phone:",
        "retrieved_relevant_part": true,
        "model_answer": ":  The office number for Anatole Gershman is 6415 at the Gates & Hillman Centers",
        "actual_answer": "Anatole Gershman's Office is 6415 Gates & Hillman Centers",
        "exact_match": false,
        "f1_max_min_avg": [
            0.58,
            0.58,
            0.58
        ],
        "recall_max_min_avg": [
            0.78,
            0.78,
            0.78
        ]
    },
    {
        "question": "How can I contact LTI faculty Robert Frederking?",
        "context": "Robert Frederking Principal Systems Scientist/Associate Dean of Doctoral Programs/MLT Program Director Email: ref@cs.cmu.edu Office: 6515 Gates & Hillman Centers Phone: 412-268-6656",
        "retrieved_doc": "Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu|Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering|Alexander Rudnicky Research Professor Emeritus Email: alex.rudnicky@cs.cmu.edu Office: 6511 Gates & Hillman Centers Phone: 412-268-2622 Research Areas: Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing",
        "retrieved_relevant_part": false,
        "model_answer": ":  I'm sorry for any confusion, but Robert Frederking's contact information was not provided in the context. The context only includes the email addresses, offices, and research areas of Daniel Vosler, Anatole Gershman, and Alexander Rudnicky",
        "actual_answer": "Email: ref@cs.cmu.edu Office: 6515 Gates & Hillman Centers Phone: 412-268-6656",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "According to background information, What happens in 2024-06-21 on university calendar?",
        "context": "Summer One_All 2024: Date: 2024-06-21 Day: F Event: Mini-5 Final Exams",
        "retrieved_doc": "Spring 2024: Date: 2024-05-06 Day: M Event: Semester & Mini-4 Faculty Course Evaluations close |Fall 2023: Date: 2023-09-04 Day: M Event: Labor Day; No Classes & University Closed|Fall 2023: Date: 2023-12-23-2024-01-02 Day: Sa-Tu Event: Winter Break; University Closed",
        "retrieved_relevant_part": false,
        "model_answer": ":  Based on the provided context, there is no mention of any events or dates specifically occurring on 2024-06-21 in the university calendar",
        "actual_answer": "Mini-5 Final Exams",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Based on background information, What is the specific event listed for 2024-05-17, in Summer One_All 2024",
        "context": "Summer One_All 2024: Date: 2024-05-17 Day: F Event: Mini-5 add, audit, & tuition adjustment drop deadline (1)",
        "retrieved_doc": "Summer One_All 2024: Date: 2024-06-21 Day: F Event: Mini-5 Final Exams|Summer One_All 2024: Date: 2024-06-20 Day: Th Event: Mini-5 Last Day of Classes|Summer One_All 2024: Date: 2024-05-17 Day: F Event: Mini-5 add, audit, & tuition adjustment drop deadline (1)",
        "retrieved_relevant_part": true,
        "model_answer": ":  The specific event listed for 2024-05-17, in Summer One\\_All 2024, is the mini-5 add, audit, & tuition adjustment drop deadline (1)",
        "actual_answer": "Mini-5 add, audit, & tuition adjustment drop deadline (1)",
        "exact_match": false,
        "f1_max_min_avg": [
            0.52,
            0.52,
            0.52
        ],
        "recall_max_min_avg": [
            0.89,
            0.89,
            0.89
        ]
    },
    {
        "question": "According to background information, When is the [Mini-5 Last Day of Classes] in Summer One_All 2024?",
        "context": "Summer One_All 2024: Date: 2024-06-20 Day: Th Event: Mini-5 Last Day of Classes",
        "retrieved_doc": "Summer One_All 2024: Date: 2024-06-20 Day: Th Event: Mini-5 Last Day of Classes|Summer One_All 2024: Date: 2024-06-21 Day: F Event: Mini-5 Faculty Course Evaluations close |Summer One_All 2024: Date: 2024-08-01 Day: Th Event: Semester & Mini-6 Last Day of Classes",
        "retrieved_relevant_part": true,
        "model_answer": ":  The Mini-5 Last Day of Classes in Summer One_All 2024 is on 2024-06-21",
        "actual_answer": "The event Mini-5 Last Day of Classes happens on 2024-06-20",
        "exact_match": false,
        "f1_max_min_avg": [
            0.58,
            0.58,
            0.58
        ],
        "recall_max_min_avg": [
            0.7,
            0.7,
            0.7
        ]
    },
    {
        "question": "Based on background information, What is the specific event listed for 2023-11-27, in Fall 2023",
        "context": "Fall 2023: Date: 2023-11-27 Day: M Event: Mini-2 pass/no pass & withdrawal deadline (2)",
        "retrieved_doc": "Fall 2023: Date: 2023-10-16-2023-10-20 Day: M-F Event: Fall Break; No Classes|Fall 2023: Date: 2023-11-13-2023-11-17 Day: M-F Event: Spring 2024 Registration Week|Fall 2023: Date: 2023-10-13-2023-10-14 Day: F-Sa Event: Family Weekend",
        "retrieved_relevant_part": false,
        "model_answer": ":  There is no specific event listed for 2023-11-27 in the provided context for Fall 2023. The context only mentions events for the dates 2023-10-16 through 2023-10-20 (Fall Break), 2023-11-13 through 2023-11-17 (Spring 2024 Registration Week), and 2023-10-13 and 2023-10-14 (Family Weekend)",
        "actual_answer": "The event happening on 2023-11-27 is Mini-2 pass/no pass & withdrawal deadline (2)",
        "exact_match": false,
        "f1_max_min_avg": [
            0.15,
            0.15,
            0.15
        ],
        "recall_max_min_avg": [
            0.31,
            0.31,
            0.31
        ]
    },
    {
        "question": "Based on background information, What is the specific event listed for 2023-10-23, in Fall 2023",
        "context": "Fall 2023: Date: 2023-10-23 Day: M Event: Mini-2 Classes Begin ",
        "retrieved_doc": "Fall 2023: Date: 2023-10-16-2023-10-20 Day: M-F Event: Fall Break; No Classes|Fall 2023: Date: 2023-10-13-2023-10-14 Day: F-Sa Event: Family Weekend|Fall 2023: Date: 2023-11-13-2023-11-17 Day: M-F Event: Spring 2024 Registration Week",
        "retrieved_relevant_part": false,
        "model_answer": ":  There is no specific event listed for 2023-10-23 in the provided context for Fall 2023. The context only mentions Fall Break from October 16 to October 20 and Family Weekend from October 13 to October 14",
        "actual_answer": "The event happening on 2023-10-23 is Mini-2 Classes Begin",
        "exact_match": false,
        "f1_max_min_avg": [
            0.17,
            0.17,
            0.17
        ],
        "recall_max_min_avg": [
            0.44,
            0.44,
            0.44
        ]
    },
    {
        "question": "Based on background information, What is the specific event listed for 2024-01-15, in Spring 2024",
        "context": "Spring 2024: Date: 2024-01-15 Day: M Event: Martin Luther King Day; No Classes",
        "retrieved_doc": "Spring 2024: Date: 2024-05-04-2024-05-05 Day: Sa-Su Event: Reading Days|Spring 2024: Date: 2024-03-12 Day: T Event: Summer 2024 Registration Opens |Spring 2024: Date: 2024-04-15-2024-04-19 Day: M-F Event: Fall 2024 Registration Week",
        "retrieved_relevant_part": false,
        "model_answer": ":  There is no specific event listed for 2024-01-15 in the provided context for Spring 2024. The events mentioned are \"Reading Days\" from May 4 to May 5, \"Summer 2024 Registration Opens\" on April 15 to April 19, and \"Fall 2024 Registration Week,\" but none of these events fall on January 15",
        "actual_answer": "Martin Luther King Day",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "According to background information, When is the [Make-Up Final Examinations] in Spring 2024?",
        "context": "Spring 2024: Date: 2024-05-07 Day: Tu Event: Make-Up Final Examinations",
        "retrieved_doc": "Fall 2023: Date: 2023-12-18 Day: M Event: Make-Up Final Exams|Spring 2024: Date: 2024-05-07 Day: Tu Event: Make-Up Final Examinations|Spring 2024: Date: 2024-05-06 Day: M Event: Final Examinations",
        "retrieved_relevant_part": true,
        "model_answer": ":  The Make-Up Final Examinations in Spring 2024 take place on 2024-05-07 (Tu)",
        "actual_answer": "The event Make-Up Final Examinations happens on 2024-05-07",
        "exact_match": false,
        "f1_max_min_avg": [
            0.57,
            0.57,
            0.57
        ],
        "recall_max_min_avg": [
            0.75,
            0.75,
            0.75
        ]
    },
    {
        "question": "According to background information, What happens in 2023-10-13 on university calendar?",
        "context": "Fall 2023: Date: 2023-10-13 Day: F Event: Mini-1 Last Day of Classes",
        "retrieved_doc": "Fall 2023: Date: 2023-09-04 Day: M Event: Labor Day; No Classes & University Closed|Fall 2023: Date: 2023-12-23-2024-01-02 Day: Sa-Tu Event: Winter Break; University Closed|Summer Two 2024: Date: 2024-07-04 Day: Th Event: Independence Day; University Closed & No Classes",
        "retrieved_relevant_part": false,
        "model_answer": ":  Based on the provided context, there is no mention of any events or closures on October 13, 2023, in the university calendar",
        "actual_answer": "The event happening on 2023-10-13 is Mini-1 Last Day of Classes",
        "exact_match": false,
        "f1_max_min_avg": [
            0.18,
            0.18,
            0.18
        ],
        "recall_max_min_avg": [
            0.27,
            0.27,
            0.27
        ]
    },
    {
        "question": "Based on background information, What is the specific event listed for 2024-07-08, in Summer One_All 2024",
        "context": "Summer One_All 2024: Date: 2024-07-08 Day: M Event: Mini-6 drop deadline; withdrawal grade assigned after this date (2)",
        "retrieved_doc": "Summer One_All 2024: Date: 2024-06-21 Day: F Event: Mini-5 Final Exams|Summer One_All 2024: Date: 2024-06-28 Day: F Event: Mini-6 add, audit, & tuition adjustment drop deadline (1)|Summer One_All 2024: Date: 2024-07-08 Day: M Event: Mini-6 drop deadline; withdrawal grade assigned after this date (2)",
        "retrieved_relevant_part": true,
        "model_answer": ":  The mini-6 drop deadline is listed for 2024-07-08 in Summer One_All 2024. After this date, a withdrawal grade will be assigned. (Fact 2)",
        "actual_answer": "The event happening on 2024-07-08 is Mini-6 drop deadline",
        "exact_match": false,
        "f1_max_min_avg": [
            0.3,
            0.19,
            0.245
        ],
        "recall_max_min_avg": [
            0.56,
            0.43,
            0.495
        ]
    },
    {
        "question": "According to background information, When is the [Reading Day] in Spring 2024?",
        "context": "Spring 2024: Date: 2024-05-01 Day: W Event: Reading Day",
        "retrieved_doc": "Spring 2024: Date: 2024-05-01 Day: W Event: Reading Day|Fall 2023: Date: 2023-12-13 Day: W Event: Reading Day |Spring 2024: Date: 2024-05-04-2024-05-05 Day: Sa-Su Event: Reading Days",
        "retrieved_relevant_part": true,
        "model_answer": ":  The Reading Days in Spring 2024 take place on Saturday, May 4, and Sunday, May 5",
        "actual_answer": "2024-05-01",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Based on the given information, Does Spring 2024 Mini-3 Faculty Course Evaluations open happen at 2024-01-15",
        "context": "Spring 2024: Date: 2024-02-19 Day: M Event: Mini-3 Faculty Course Evaluations open",
        "retrieved_doc": "Spring 2024: Date: 2024-02-19 Day: M Event: Mini-3 Faculty Course Evaluations open|Fall 2023: Date: 2023-10-02 Day: M Event: Mini-1 Faculty Course Evaluations open|Spring 2024: Date: 2024-03-02 Day: Sa Event: Mini-3 Faculty Course Evaluations close",
        "retrieved_relevant_part": true,
        "model_answer": ":  No, based on the given context, Spring 2024 Mini-3 Faculty Course Evaluations do not open on 2024-01-15. The context states that they open on 2024-02-19",
        "actual_answer": "No",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "According to background information, When is the [Mini-1 Exams] in Fall 2023?",
        "context": "Fall 2023: Date: 2023-10-14 Day: Sa Event: Mini-1 Exams",
        "retrieved_doc": "Fall 2023: Date: 2023-10-14 Day: Sa Event: Mini-1 Exams|Spring 2024: Date: 2024-03-02 Day: Sa Event: Mini-3 Exams|Fall 2023: Date: 2023-12-18 Day: M Event: Make-Up Final Exams",
        "retrieved_relevant_part": true,
        "model_answer": ":  The Mini-1 Exams in Fall 2023 take place on October 14, which is a Saturday",
        "actual_answer": "2023-10-14",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Based on background information, What is the specific event listed for 2024-07-29, in Summer One_All 2024",
        "context": "Summer One_All 2024: Date: 2024-07-29 Day: M Event: Semester & Mini-6 Faculty Course Evalutations open ",
        "retrieved_doc": "Summer One_All 2024: Date: 2024-06-21 Day: F Event: Mini-5 Final Exams|Summer One_All 2024: Date: 2024-06-28 Day: F Event: Mini-6 add, audit, & tuition adjustment drop deadline (1)|Summer One_All 2024: Date: 2024-07-08 Day: M Event: Mini-6 drop deadline; withdrawal grade assigned after this date (2)",
        "retrieved_relevant_part": false,
        "model_answer": ":  There is no specific event listed for 2024-07-29 in the provided context",
        "actual_answer": "Semester & Mini-6 Faculty Course Evalutations open",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "According to background information, When is the [Mini-1 add, audit & tuition adjustment drop deadline  (1)] in Fall 2023?",
        "context": "Fall 2023: Date: 2023-09-01 Day: F Event: Mini-1 add, audit & tuition adjustment drop deadline  (1)",
        "retrieved_doc": "Fall 2023: Date: 2023-09-01 Day: F Event: Mini-1 add, audit & tuition adjustment drop deadline  (1)|Fall 2023: Date: 2023-10-27 Day: F Event: Mini-2 add, audit & tuition adjustment drop deadline (1)|Fall 2023: Date: 2023-09-11 Day: M Event: Semester add, audit & tuition adjustment drop deadline (1)",
        "retrieved_relevant_part": true,
        "model_answer": ":  The Mini-1 add, audit & tuition adjustment drop deadline in Fall 2023 is on September 1, 2023",
        "actual_answer": "2023-09-01",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Based on the given information, Does Summer One_All 2024 Independence Day; University Closed & No Classes happen at 2024-05-06",
        "context": "Summer One_All 2024: Date: 2024-07-04 Day: Th Event: Independence Day; University Closed & No Classes",
        "retrieved_doc": "Summer One_All 2024: Date: 2024-07-04 Day: Th Event: Independence Day; University Closed & No Classes|Summer Two 2024: Date: 2024-07-04 Day: Th Event: Independence Day; University Closed & No Classes|Summer One_All 2024: Date: 2022-06-19 Day: W Event: Juneteenth; University Closed & No Classes",
        "retrieved_relevant_part": true,
        "model_answer": ":  No, according to the given context, Summer One_All 2024 Independence Day; University Closed & No Classes takes place on July 4, 2024, not May 6, 2024",
        "actual_answer": "No",
        "exact_match": false,
        "f1_max_min_avg": [
            0.07,
            0.07,
            0.07
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Based on background information, What is the specific event listed for 2023-09-01, in Fall 2023",
        "context": "Fall 2023: Date: 2023-09-01 Day: F Event: Mini-1 add, audit & tuition adjustment drop deadline  (1)",
        "retrieved_doc": "Fall 2023: Date: 2023-10-16-2023-10-20 Day: M-F Event: Fall Break; No Classes|Spring 2024: Date: 2024-05-04-2024-05-05 Day: Sa-Su Event: Reading Days|Fall 2023: Date: 2023-11-13-2023-11-17 Day: M-F Event: Spring 2024 Registration Week",
        "retrieved_relevant_part": false,
        "model_answer": ":  The context does not provide any information about an event taking place on September 1, 2023, during Fall 2023. Therefore, there is no fact to extract from the context regarding this question",
        "actual_answer": "Mini-1 add, audit & tuition adjustment drop deadline  (1)",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Based on background information, What is the specific event listed for 2024-05-27, in Summer One_All 2024",
        "context": "Summer One_All 2024: Date: 2024-05-27 Day: M Event: Memorial Day; University Closed & No Classes",
        "retrieved_doc": "Summer One_All 2024: Date: 2024-06-21 Day: F Event: Mini-5 Final Exams|Summer One_All 2024: Date: 2024-06-28 Day: F Event: Mini-6 add, audit, & tuition adjustment drop deadline (1)|Summer One_All 2024: Date: 2024-06-20 Day: Th Event: Mini-5 Last Day of Classes",
        "retrieved_relevant_part": false,
        "model_answer": ":  There is no event listed for 2024-05-27 in the provided context for Summer One_All 2024",
        "actual_answer": "Memorial Day",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Based on the given information, Does Fall 2023 Mini-2 drop deadline; withdrawal grade assigned after this date (2) happen at 2023-11-15",
        "context": "Fall 2023: Date: 2023-11-15 Day: W Event: Mini-2 drop deadline; withdrawal grade assigned after this date (2)",
        "retrieved_doc": "Fall 2023: Date: 2023-11-15 Day: W Event: Mini-2 drop deadline; withdrawal grade assigned after this date (2)|Fall 2023: Date: 2023-09-20 Day: W Event: Mini-1 drop deadline; withdrawal grade assigned after this date (2)|Summer One_All 2024: Date: 2024-07-08 Day: M Event: Mini-6 drop deadline; withdrawal grade assigned after this date (2)",
        "retrieved_relevant_part": true,
        "model_answer": ":  Yes, according to the given context, the Fall 2023 Mini-2 drop deadline and the date when a withdrawal grade will be assigned after this date is on 2023-11-15",
        "actual_answer": "Yes",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Based on the given information, Does Summer One_All 2024 Mini-5 Faculty Course Evaluations open happen at 2024-06-14",
        "context": "Summer One_All 2024: Date: 2024-06-14 Day: F Event: Mini-5 Faculty Course Evaluations open",
        "retrieved_doc": "Summer One_All 2024: Date: 2024-06-14 Day: F Event: Mini-5 Faculty Course Evaluations open|Summer One_All 2024: Date: 2024-06-21 Day: F Event: Mini-5 Faculty Course Evaluations close |Summer One_All 2024: Date: 2024-07-29 Day: M Event: Semester & Mini-6 Faculty Course Evalutations open ",
        "retrieved_relevant_part": true,
        "model_answer": ":  Yes, according to the given context, Mini-5 Faculty Course Evaluations open during Summer One_All 2024 on June 14, 2024",
        "actual_answer": "Yes",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Based on background information, What is the specific event listed for 2024-05-08, in Spring 2024",
        "context": "Spring 2024: Date: 2024-05-08 Day: W Event: Graduating Final Grades Due by 4 pm",
        "retrieved_doc": "Spring 2024: Date: 2024-05-04-2024-05-05 Day: Sa-Su Event: Reading Days|Spring 2024: Date: 2024-03-04-2024-03-08 Day: M-F Event: Spring Break; No Classes|Spring 2024: Date: 2024-04-15-2024-04-19 Day: M-F Event: Fall 2024 Registration Week",
        "retrieved_relevant_part": false,
        "model_answer": ":  The specific event listed for 2024-05-08 in Spring 2024 is \"Reading Days.\"",
        "actual_answer": "Graduating Final Grades Due by 4 pm",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Based on the given information, Does Summer One_All 2024 Mini-6 First Day of Classes happen at 2024-06-24",
        "context": "Summer One_All 2024: Date: 2024-06-24 Day: M Event: Mini-6 First Day of Classes",
        "retrieved_doc": "Summer One_All 2024: Date: 2024-06-24 Day: M Event: Mini-6 First Day of Classes|Summer One_All 2024: Date: 2024-05-13 Day: M Event: Semester & Mini-5 Classes Begin|Summer One_All 2024: Date: 2024-06-20 Day: Th Event: Mini-5 Last Day of Classes",
        "retrieved_relevant_part": true,
        "model_answer": ":  Yes, according to the given context, the First Day of Classes for Mini-6 in Summer One_All 2024 is on June 24, 2024",
        "actual_answer": "Yes",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "According to background information, What happens in 2024-07-29 on university calendar?",
        "context": "Summer One_All 2024: Date: 2024-07-29 Day: M Event: Semester & Mini-6 Faculty Course Evalutations open ",
        "retrieved_doc": "Spring 2024: Date: 2024-05-06 Day: M Event: Semester & Mini-4 Faculty Course Evaluations close |Fall 2023: Date: 2023-09-04 Day: M Event: Labor Day; No Classes & University Closed|Summer Two 2024: Date: 2024-07-04 Day: Th Event: Independence Day; University Closed & No Classes",
        "retrieved_relevant_part": false,
        "model_answer": ":  Based on the provided context, there is no mention of any events or happenings on July 29, 2024, in the university calendar",
        "actual_answer": "The event happening on 2024-07-29 is Semester & Mini-6 Faculty Course Evalutations open",
        "exact_match": false,
        "f1_max_min_avg": [
            0.11,
            0.11,
            0.11
        ],
        "recall_max_min_avg": [
            0.15,
            0.15,
            0.15
        ]
    },
    {
        "question": "Based on the given information, Does Fall 2023 Mini-1 voucher election deadline (4) happen at 2023-10-13",
        "context": "Fall 2023: Date: 2023-10-13 Day: F Event: Mini-1 voucher election deadline (4)",
        "retrieved_doc": "Fall 2023: Date: 2023-10-13 Day: F Event: Mini-1 voucher election deadline (4)|Fall 2023: Date: 2023-10-02 Day: M Event: Mini-1 Pass/no pass & withdrawal deadline (3)|Spring 2024: Date: 2024-03-01 Day: F Event: Mini-3 voucher deadline (4)",
        "retrieved_relevant_part": true,
        "model_answer": ":  Yes, according to the given context, the Fall 2023 Mini-1 voucher election deadline (event number 4) does occur on 2023-10-13",
        "actual_answer": "Yes",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "According to background information, When is the [Final Examinations ] in Spring 2024?",
        "context": "Spring 2024: Date: 2024-04-29-2024-04-30 Day: M-Tu Event: Final Examinations ",
        "retrieved_doc": "Spring 2024: Date: 2024-05-06 Day: M Event: Final Examinations|Spring 2024: Date: 2024-05-07 Day: Tu Event: Make-Up Final Examinations|Fall 2023: Date: 2023-12-18 Day: M Event: Make-Up Final Exams",
        "retrieved_relevant_part": false,
        "model_answer": ":  The Final Examinations in Spring 2024 take place on May 6, 2024",
        "actual_answer": "2024-04-29-2024-04-30",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Based on the given information, Does Summer Two 2024 Summer Semester  Two drop deadline; withdrawal grade assigned after this date (2) happen at 2024-07-29",
        "context": "Summer Two 2024: Date: 2024-07-08 Day: M Event: Summer Semester  Two drop deadline; withdrawal grade assigned after this date (2)",
        "retrieved_doc": "Summer Two 2024: Date: 2024-07-08 Day: M Event: Summer Semester  Two drop deadline; withdrawal grade assigned after this date (2)|Summer One_All 2024: Date: 2024-06-18 Day: T Event: Semester course drop deadline; withdrawal grade assigned after this date (2)|Summer Two 2024: Date: 2024-07-22 Day: F Event: Summer Semester  Two pass/no pass & withdrawal deadline (3)",
        "retrieved_relevant_part": true,
        "model_answer": ":  Based on the given context, the Summer Two 2024 Summer Semester Two drop deadline with withdrawal grade assigned after this date is on July 8, 2024. Therefore, the answer to the question is no, as July 29, 2024, is not mentioned in the context as the date for the Summer Two 2024 Summer Semester Two drop deadline or any other relevant event",
        "actual_answer": "No",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "According to background information, When is the [Semester pass/no pass & withdrawal deadline (3)] in Summer One_All 2024?",
        "context": "Summer One_All 2024: Date: 2024-07-05 Day: F Event: Semester pass/no pass & withdrawal deadline (3)",
        "retrieved_doc": "Summer One_All 2024: Date: 2024-07-05 Day: F Event: Semester pass/no pass & withdrawal deadline (3)|Summer Two 2024: Date: 2024-07-22 Day: F Event: Summer Semester  Two pass/no pass & withdrawal deadline (3)|Summer One_All 2024: Date: 2024-06-18 Day: T Event: Semester course drop deadline; withdrawal grade assigned after this date (2)",
        "retrieved_relevant_part": true,
        "model_answer": ":  The semester pass/no pass & withdrawal deadline (3) for Summer One_All 2024 is on July 5, 2024",
        "actual_answer": "The event Semester pass/no pass & withdrawal deadline (3) happens on 2024-07-05",
        "exact_match": false,
        "f1_max_min_avg": [
            0.53,
            0.53,
            0.53
        ],
        "recall_max_min_avg": [
            0.67,
            0.67,
            0.67
        ]
    },
    {
        "question": "According to background information, What happens in 2024-05-06 on university calendar?",
        "context": "Spring 2024: Date: 2024-05-06 Day: M Event: Final Examinations",
        "retrieved_doc": "Spring 2024: Date: 2024-05-06 Day: M Event: Semester & Mini-4 Faculty Course Evaluations close |Fall 2023: Date: 2023-09-04 Day: M Event: Labor Day; No Classes & University Closed|Spring 2024: Date: 2024-04-15 Day: M Event: Semester & Mini-4 Faculty Course Evaluations open ",
        "retrieved_relevant_part": false,
        "model_answer": ":  The faculty course evaluations for Semester & Mini-4 close on May 6, 2024",
        "actual_answer": "The event happening on 2024-05-06 is Final Examinations",
        "exact_match": false,
        "f1_max_min_avg": [
            0.18,
            0.18,
            0.18
        ],
        "recall_max_min_avg": [
            0.25,
            0.25,
            0.25
        ]
    },
    {
        "question": "Based on the given information, Does Summer One_All 2024 Mini-5 pass/no pass & withdrawal deadline (3) happen at 2024-06-24",
        "context": "Summer One_All 2024: Date: 2024-06-07 Day: F Event: Mini-5 pass/no pass & withdrawal deadline (3)",
        "retrieved_doc": "Summer One_All 2024: Date: 2024-06-07 Day: F Event: Mini-5 pass/no pass & withdrawal deadline (3)|Summer One_All 2024: Date: 2024-07-22 Day: F Event: Mini-6 pass/no pass & withdrawal deadline (3)|Fall 2023: Date: 2023-10-02 Day: M Event: Mini-1 Pass/no pass & withdrawal deadline (3)",
        "retrieved_relevant_part": true,
        "model_answer": ":  Based on the given context, the Mini-5 pass/no pass & withdrawal deadline for Summer One_All 2024 is on day F which is June 7, 2024. Therefore, the answer is no, it does not happen on June 24, 2024",
        "actual_answer": "No",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "According to background information, When is the [Summer Semester  Two pass/no pass & withdrawal deadline (3)] in Summer Two 2024?",
        "context": "Summer Two 2024: Date: 2024-07-22 Day: F Event: Summer Semester  Two pass/no pass & withdrawal deadline (3)",
        "retrieved_doc": "Summer Two 2024: Date: 2024-07-22 Day: F Event: Summer Semester  Two pass/no pass & withdrawal deadline (3)|Summer Two 2024: Date: 2024-07-08 Day: M Event: Summer Semester  Two drop deadline; withdrawal grade assigned after this date (2)|Summer One_All 2024: Date: 2024-07-05 Day: F Event: Semester pass/no pass & withdrawal deadline (3)",
        "retrieved_relevant_part": true,
        "model_answer": ":  The pass/no pass & withdrawal deadline for Summer Semester Two 2024 is on July 22, 2024",
        "actual_answer": "The event Summer Semester  Two pass/no pass & withdrawal deadline (3) happens on 2024-07-22",
        "exact_match": false,
        "f1_max_min_avg": [
            0.65,
            0.65,
            0.65
        ],
        "recall_max_min_avg": [
            0.71,
            0.71,
            0.71
        ]
    },
    {
        "question": "Based on background information, What is the specific event listed for 2024-02-19, in Spring 2024",
        "context": "Spring 2024: Date: 2024-02-19 Day: M Event: Mini-3 Faculty Course Evaluations open",
        "retrieved_doc": "Spring 2024: Date: 2024-03-12 Day: T Event: Summer 2024 Registration Opens |Spring 2024: Date: 2024-04-15-2024-04-19 Day: M-F Event: Fall 2024 Registration Week|Spring 2024: Date: 2024-05-04-2024-05-05 Day: Sa-Su Event: Reading Days",
        "retrieved_relevant_part": false,
        "model_answer": ":  There is no specific event listed for 2024-02-19 in the provided context for Spring 2024",
        "actual_answer": "The event happening on 2024-02-19 is Mini-3 Faculty Course Evaluations open",
        "exact_match": false,
        "f1_max_min_avg": [
            0.22,
            0.22,
            0.22
        ],
        "recall_max_min_avg": [
            0.27,
            0.27,
            0.27
        ]
    },
    {
        "question": "Based on the given information, Does Spring 2024 Mini-3 voucher deadline (4) happen at 2023-10-13",
        "context": "Spring 2024: Date: 2024-03-01 Day: F Event: Mini-3 voucher deadline (4)",
        "retrieved_doc": "Spring 2024: Date: 2024-03-01 Day: F Event: Mini-3 voucher deadline (4)|Summer One_All 2024: Date: 2024-06-20 Day: Th Event: Mini-5 voucher deadline (4)|Spring 2024: Date: 2024-04-15 Day: M Event: Mini-4 pass/no pass & withdrawal deadline (3)",
        "retrieved_relevant_part": true,
        "model_answer": ":  No, based on the given context, the Spring 2024 Mini-3 voucher deadline (4) does not occur on 2023-10-13. The context states that the Spring 2024 Mini-3 voucher deadline (4) is on 2024-03-01",
        "actual_answer": "No",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Based on background information, What is the specific event listed for 2024-04-01, in Spring 2024",
        "context": "Spring 2024: Date: 2024-04-01 Day: M Event: Semester pass/no pass & withdrawal deadline (3)",
        "retrieved_doc": "Spring 2024: Date: 2024-05-04-2024-05-05 Day: Sa-Su Event: Reading Days|Spring 2024: Date: 2024-03-12 Day: T Event: Summer 2024 Registration Opens |Spring 2024: Date: 2024-03-04-2024-03-08 Day: M-F Event: Spring Break; No Classes",
        "retrieved_relevant_part": false,
        "model_answer": ":  There is no specific event listed for 2024-04-01 in the provided context",
        "actual_answer": "The event happening on 2024-04-01 is Semester pass/no pass & withdrawal deadline (3)",
        "exact_match": false,
        "f1_max_min_avg": [
            0.23,
            0.23,
            0.23
        ],
        "recall_max_min_avg": [
            0.23,
            0.23,
            0.23
        ]
    },
    {
        "question": "According to background information, When is the [Mini-5 pass/no pass & withdrawal deadline (3)] in Summer One_All 2024?",
        "context": "Summer One_All 2024: Date: 2024-06-07 Day: F Event: Mini-5 pass/no pass & withdrawal deadline (3)",
        "retrieved_doc": "Summer One_All 2024: Date: 2024-06-07 Day: F Event: Mini-5 pass/no pass & withdrawal deadline (3)|Summer One_All 2024: Date: 2024-07-22 Day: F Event: Mini-6 pass/no pass & withdrawal deadline (3)|Fall 2023: Date: 2023-10-02 Day: M Event: Mini-1 Pass/no pass & withdrawal deadline (3)",
        "retrieved_relevant_part": true,
        "model_answer": ":  The Mini-5 pass/no pass & withdrawal deadline (3) in Summer One_All 2024 is on 2024-06-07",
        "actual_answer": "The event Mini-5 pass/no pass & withdrawal deadline (3) happens on 2024-06-07",
        "exact_match": false,
        "f1_max_min_avg": [
            0.71,
            0.71,
            0.71
        ],
        "recall_max_min_avg": [
            0.83,
            0.83,
            0.83
        ]
    },
    {
        "question": "Based on the given information, Does Summer One_All 2024 Mini-5 Faculty Course Evaluations open happen at 2024-06-14",
        "context": "Summer One_All 2024: Date: 2024-06-14 Day: F Event: Mini-5 Faculty Course Evaluations open",
        "retrieved_doc": "Summer One_All 2024: Date: 2024-06-14 Day: F Event: Mini-5 Faculty Course Evaluations open|Summer One_All 2024: Date: 2024-06-21 Day: F Event: Mini-5 Faculty Course Evaluations close |Summer One_All 2024: Date: 2024-07-29 Day: M Event: Semester & Mini-6 Faculty Course Evalutations open ",
        "retrieved_relevant_part": true,
        "model_answer": ":  Yes, according to the given context, Mini-5 Faculty Course Evaluations open during Summer One_All 2024 on June 14, 2024",
        "actual_answer": "Yes",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Based on the given information, Does Spring 2024 Final Grades Due by 4 pm happen at 2023-12-17",
        "context": "Spring 2024: Date: 2024-05-14 Day: Tu Event: Final Grades Due by 4 pm",
        "retrieved_doc": "Fall 2023: Date: 2023-12-20 Day: W Event: Final Grades Due by 4 pm |Spring 2024: Date: 2024-05-08 Day: W Event: Graduating Final Grades Due by 4 pm|Spring 2024: Date: 2024-05-14 Day: Tu Event: Final Grades Due by 4 pm",
        "retrieved_relevant_part": true,
        "model_answer": ":  No, based on the given information, Spring 2024 Final Grades Due by 4 pm does not happen on 2023-12-17. The correct dates for Spring 2024 Final Grades Due by 4 pm are 2024-05-08 and 2024-05-14",
        "actual_answer": "No",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Based on background information, What is the specific event listed for 2024-06-21, in Summer One_All 2024",
        "context": "Summer One_All 2024: Date: 2024-06-21 Day: F Event: Mini-5 Final Exams",
        "retrieved_doc": "Summer One_All 2024: Date: 2024-06-21 Day: F Event: Mini-5 Final Exams|Summer One_All 2024: Date: 2024-06-20 Day: Th Event: Mini-5 Last Day of Classes|Summer One_All 2024: Date: 2024-06-07 Day: F Event: Mini-5 pass/no pass & withdrawal deadline (3)",
        "retrieved_relevant_part": true,
        "model_answer": ":  The Mini-5 Final Exams are scheduled for 2024-06-21 during Summer One_All 2024",
        "actual_answer": "Mini-5 Final Exams",
        "exact_match": false,
        "f1_max_min_avg": [
            0.38,
            0.38,
            0.38
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "According to background information, What happens in 2023-12-18 on university calendar?",
        "context": "Fall 2023: Date: 2023-12-18 Day: M Event: Semester & Mini-2 Faculty Course Evaluations close",
        "retrieved_doc": "Fall 2023: Date: 2023-09-04 Day: M Event: Labor Day; No Classes & University Closed|Fall 2023: Date: 2023-12-23-2024-01-02 Day: Sa-Tu Event: Winter Break; University Closed|Summer Two 2024: Date: 2024-07-04 Day: Th Event: Independence Day; University Closed & No Classes",
        "retrieved_relevant_part": false,
        "model_answer": ":  Based on the provided context, there is no mention of December 18, 2023, in the university calendar. Therefore, according to the background information, nothing specific happens on that date",
        "actual_answer": "The event happening on 2023-12-18 is Semester & Mini-2 Faculty Course Evaluations close",
        "exact_match": false,
        "f1_max_min_avg": [
            0.09,
            0.09,
            0.09
        ],
        "recall_max_min_avg": [
            0.15,
            0.15,
            0.15
        ]
    },
    {
        "question": "Based on the given information, Does Summer Two 2024 Summer Semester  Two Last Day of Classes happen at 2024-08-01",
        "context": "Summer Two 2024: Date: 2024-08-01 Day: Th Event: Summer Semester  Two Last Day of Classes",
        "retrieved_doc": "Summer Two 2024: Date: 2024-08-01 Day: Th Event: Summer Semester  Two Last Day of Classes|Summer Two 2024: Date: 2024-08-02 Day: F Event: Summer Semester  Two Faculty Course Evaluations close |Summer Two 2024: Date: 2024-07-29 Day: M Event: Summer Semester  Two Faculty Course Evaulations open",
        "retrieved_relevant_part": true,
        "model_answer": ":  Yes, according to the given context, the last day of classes for Summer Semester Two 2024 is on August 1, 2024",
        "actual_answer": "Yes",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Based on background information, What is the specific event listed for 2024-05-14, in Spring 2024",
        "context": "Spring 2024: Date: 2024-05-14 Day: Tu Event: Final Grades Due by 4 pm",
        "retrieved_doc": "Spring 2024: Date: 2024-05-04-2024-05-05 Day: Sa-Su Event: Reading Days|Spring 2024: Date: 2024-04-15-2024-04-19 Day: M-F Event: Fall 2024 Registration Week|Spring 2024: Date: 2024-03-04-2024-03-08 Day: M-F Event: Spring Break; No Classes",
        "retrieved_relevant_part": false,
        "model_answer": ":  The context does not provide any information about an event specifically listed for 2024-05-14 during Spring 2024. Therefore, there is no fact to extract from the context for this question",
        "actual_answer": "The event happening on 2024-05-14 is Final Grades Due by 4 pm",
        "exact_match": false,
        "f1_max_min_avg": [
            0.19,
            0.19,
            0.19
        ],
        "recall_max_min_avg": [
            0.33,
            0.33,
            0.33
        ]
    },
    {
        "question": "According to background information, When is the [Final Exams ] in Fall 2023?",
        "context": "Fall 2023: Date: 2023-12-11-2023-12-12 Day: M-Tu Event: Final Exams ",
        "retrieved_doc": "Fall 2023: Date: 2023-12-18 Day: M Event: Make-Up Final Exams|Fall 2023: Date: 2023-12-17 Day: Su Event: Final Exams |Fall 2023: Date: 2023-12-14-2023-12-15 Day: Th-F Event: Final Exams ",
        "retrieved_relevant_part": false,
        "model_answer": ":  The Final Exams in Fall 2023 took place on three different days: December 14-15 (Thursday-Friday), December 17 (Sunday), and December 18 (Monday)",
        "actual_answer": "The event Final Exams  happens on 2023-12-11-2023-12-12",
        "exact_match": false,
        "f1_max_min_avg": [
            0.27,
            0.27,
            0.27
        ],
        "recall_max_min_avg": [
            0.57,
            0.57,
            0.57
        ]
    },
    {
        "question": "Based on background information, What is the specific event listed for 2024-07-04, in Summer Two 2024",
        "context": "Summer Two 2024: Date: 2024-07-04 Day: Th Event: Independence Day; University Closed & No Classes",
        "retrieved_doc": "Summer One_All 2024: Date: 2024-06-21 Day: F Event: Mini-5 Final Exams|Summer One_All 2024: Date: 2024-06-28 Day: F Event: Mini-6 add, audit, & tuition adjustment drop deadline (1)|Summer One_All 2024: Date: 2024-07-08 Day: M Event: Mini-6 drop deadline; withdrawal grade assigned after this date (2)",
        "retrieved_relevant_part": false,
        "model_answer": ":  There is no event listed for 2024-07-04 in the provided context for Summer One_All 2024",
        "actual_answer": "The event happening on 2024-07-04 is Independence Day",
        "exact_match": false,
        "f1_max_min_avg": [
            0.25,
            0,
            0.125
        ],
        "recall_max_min_avg": [
            0.38,
            0,
            0.19
        ]
    },
    {
        "question": "Based on background information, What is the specific event listed for 2024-02-26, in Spring 2024",
        "context": "Spring 2024: Date: 2024-02-26 Day: M Event: Semester course drop deadline; withdrawal grade assigned after this date (2)",
        "retrieved_doc": "Spring 2024: Date: 2024-04-15-2024-04-19 Day: M-F Event: Fall 2024 Registration Week|Spring 2024: Date: 2024-05-04-2024-05-05 Day: Sa-Su Event: Reading Days|Spring 2024: Date: 2024-03-12 Day: T Event: Summer 2024 Registration Opens ",
        "retrieved_relevant_part": false,
        "model_answer": ":  There is no specific event listed for 2024-02-26 in the provided context for Spring 2024",
        "actual_answer": "The event happening on 2024-02-26 is Semester course drop deadline",
        "exact_match": false,
        "f1_max_min_avg": [
            0.23,
            0,
            0.115
        ],
        "recall_max_min_avg": [
            0.3,
            0,
            0.15
        ]
    },
    {
        "question": "According to background information, When is the [Reading Day] in Spring 2024?",
        "context": "Spring 2024: Date: 2024-05-01 Day: W Event: Reading Day",
        "retrieved_doc": "Spring 2024: Date: 2024-05-01 Day: W Event: Reading Day|Fall 2023: Date: 2023-12-13 Day: W Event: Reading Day |Spring 2024: Date: 2024-05-04-2024-05-05 Day: Sa-Su Event: Reading Days",
        "retrieved_relevant_part": true,
        "model_answer": ":  The Reading Days in Spring 2024 take place on Saturday, May 4, and Sunday, May 5",
        "actual_answer": "2024-05-01",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "According to background information, When is the [Spring Deans' Lists Posted] in Spring 2024?",
        "context": "Spring 2024: Date: 2023-05-21 Day: Tu Event: Spring Deans' Lists Posted",
        "retrieved_doc": "Spring 2024: Date: 2023-05-21 Day: Tu Event: Spring Deans' Lists Posted|Fall 2023: Date: 2023-01-08 Day: M Event: Fall Deans' Lists Posted|Summer One_All 2024: Date: 2024-07-29 Day: M Event: Semester & Mini-6 Faculty Course Evalutations open ",
        "retrieved_relevant_part": true,
        "model_answer": ":  The Spring Deans' Lists for Spring 2024 will be posted on May 21, 2024",
        "actual_answer": "The event Spring Deans' Lists Posted happens on 2023-05-21",
        "exact_match": false,
        "f1_max_min_avg": [
            0.42,
            0.42,
            0.42
        ],
        "recall_max_min_avg": [
            0.56,
            0.56,
            0.56
        ]
    },
    {
        "question": "Based on the given information, Does Fall 2023 Spring 2024 Registration Week happen at 2023-11-13-2023-11-17",
        "context": "Fall 2023: Date: 2023-11-13-2023-11-17 Day: M-F Event: Spring 2024 Registration Week",
        "retrieved_doc": "Fall 2023: Date: 2023-11-13-2023-11-17 Day: M-F Event: Spring 2024 Registration Week|Spring 2024: Date: 2024-04-15-2024-04-19 Day: M-F Event: Fall 2024 Registration Week|Spring 2024: Date: 2024-03-12 Day: T Event: Summer 2024 Registration Opens ",
        "retrieved_relevant_part": true,
        "model_answer": ":  Yes, according to the context, Fall 2023 Registration Week for Spring 2024 takes place from November 13, 2023 to November 17, 2023",
        "actual_answer": "Yes",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Based on the given information, Does Summer One_All 2024 Semester & Mini-6 Faculty Course Evaluations close happen at 2024-08-02",
        "context": "Summer One_All 2024: Date: 2024-08-02 Day: F Event: Semester & Mini-6 Faculty Course Evaluations close",
        "retrieved_doc": "Summer One_All 2024: Date: 2024-08-02 Day: F Event: Semester & Mini-6 Faculty Course Evaluations close|Summer One_All 2024: Date: 2024-07-29 Day: M Event: Semester & Mini-6 Faculty Course Evalutations open |Summer One_All 2024: Date: 2024-06-21 Day: F Event: Mini-5 Faculty Course Evaluations close ",
        "retrieved_relevant_part": true,
        "model_answer": ":  Yes, according to the given context, Summer One_All 2024 Semester & Mini-6 Faculty Course Evaluations close on 2024-08-02",
        "actual_answer": "Yes",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Based on the given information, Does Spring 2024 Mid-Semester & Mini-3 grades due by 4 pm happen at 2024-04-11-2024-04-13",
        "context": "Spring 2024: Date: 2024-03-11 Day: M Event: Mid-Semester & Mini-3 grades due by 4 pm",
        "retrieved_doc": "Spring 2024: Date: 2024-03-11 Day: M Event: Mid-Semester & Mini-3 grades due by 4 pm|Fall 2023: Date: 2023-10-23 Day: M Event: Mid-Semester & Mini-1 grades due by 4 pm|Summer One_All 2024: Date: 2024-08-06 Day: T Event: Semester & Mini-6 Final Grades Due by 4 pm",
        "retrieved_relevant_part": true,
        "model_answer": ":  Based on the given context, the Spring 2024 Mid-Semester & Mini-3 grades are due on a day that is not explicitly stated as \"April 11-13\". Therefore, the output should be:",
        "actual_answer": "No",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "According to background information, What happens in 2024-05-13 on university calendar?",
        "context": "Summer One_All 2024: Date: 2024-05-13 Day: M Event: Semester & Mini-5 Classes Begin",
        "retrieved_doc": "Fall 2023: Date: 2023-09-04 Day: M Event: Labor Day; No Classes & University Closed|Spring 2024: Date: 2024-05-06 Day: M Event: Semester & Mini-4 Faculty Course Evaluations close |Fall 2023: Date: 2023-12-23-2024-01-02 Day: Sa-Tu Event: Winter Break; University Closed",
        "retrieved_relevant_part": false,
        "model_answer": ":  Based on the provided context, there is no information about what specifically happens on May 13, 2024, on the university calendar. The context only mentions that faculty course evaluations close from May 6 to May 10, 2024",
        "actual_answer": "Semester & Mini-5 Classes Begin",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Based on background information, What is the specific event listed for 2024-06-07, in Summer One_All 2024",
        "context": "Summer One_All 2024: Date: 2024-06-07 Day: F Event: Mini-5 pass/no pass & withdrawal deadline (3)",
        "retrieved_doc": "Summer One_All 2024: Date: 2024-06-21 Day: F Event: Mini-5 Final Exams|Summer One_All 2024: Date: 2024-06-28 Day: F Event: Mini-6 add, audit, & tuition adjustment drop deadline (1)|Summer One_All 2024: Date: 2024-07-08 Day: M Event: Mini-6 drop deadline; withdrawal grade assigned after this date (2)",
        "retrieved_relevant_part": false,
        "model_answer": ":  There is no specific event listed for 2024-06-07 in the provided context",
        "actual_answer": "The event happening on 2024-06-07 is Mini-5 pass/no pass & withdrawal deadline (3)",
        "exact_match": false,
        "f1_max_min_avg": [
            0.23,
            0.23,
            0.23
        ],
        "recall_max_min_avg": [
            0.23,
            0.23,
            0.23
        ]
    },
    {
        "question": "Based on the given information, Does Fall 2023 Final Exams  happen at 2023-12-11-2023-12-12",
        "context": "Fall 2023: Date: 2023-12-11-2023-12-12 Day: M-Tu Event: Final Exams ",
        "retrieved_doc": "Fall 2023: Date: 2023-12-14-2023-12-15 Day: Th-F Event: Final Exams |Fall 2023: Date: 2023-12-18 Day: M Event: Make-Up Final Exams|Fall 2023: Date: 2023-12-17 Day: Su Event: Final Exams ",
        "retrieved_relevant_part": false,
        "model_answer": ":  No, based on the given information, Fall 2023 Final Exams do not occur on 2023-12-11-2023-12-12. The exams take place on December 14-15 (Thursday-Friday) and December 17 (Sunday). Make-up exams are scheduled for December 18 (Monday)",
        "actual_answer": "Yes",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "When does the class International Finance Recitation Section A4  offered in Sp end?",
        "context": "Spring offering: Course: 45825 Title: International Finance Units: 6.0 Lec/Sec: Section A4 Days: Tuesday, Thursday Begin: 10:00AM End: 11:45AM Bldg/Room: TEP 2118 Location: Pittsburgh, Pennsylvania Instructor(s): Khokher ",
        "retrieved_doc": "Summer 1/Summer All offering: Course: 17996 Title: Societal Computing Independent Study Units: VAR Lec/Sec: Section B Days: To be announced Begin: 05:00PM End: 06:20PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Sadeh |Summer 1/Summer All offering: Course: 17996 Title: Societal Computing Independent Study Units: VAR Lec/Sec: Section M Days: To be announced Begin: 05:00PM End: 06:20PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Vasilescu |Spring offering: Course: 82500 Title: Modern Languages Independent Study Units: VAR Lec/Sec: Section B Days: To be announced Begin: 12:30PM End: 01:50PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Instructor TBA ",
        "retrieved_relevant_part": false,
        "model_answer": ":  The context provided does not contain any information about an \"International Finance Recitation Section A4\" offered in the spring semester. Therefore, there is no fact to extract from the context for this question",
        "actual_answer": "11:45AM",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "At what time does the class Financial Statements and Analysis of Companies Recitation Section C1  offered in Fa begin?",
        "context": "Fall offering: Course: 90723 Title: Financial Statements and Analysis of Companies Units: 6.0 Lec/Sec: Section C1 Days: Monday, Wednesday Begin: 11:00AM End: 12:20PM Bldg/Room: HBH 2008 Location: Pittsburgh, Pennsylvania Instructor(s): Gogolak ",
        "retrieved_doc": "Fall offering: Course: 90723 Title: Financial Statements and Analysis of Companies Units: 6.0 Lec/Sec: Section C1 Days: Monday, Wednesday Begin: 11:00AM End: 12:20PM Bldg/Room: HBH 2008 Location: Pittsburgh, Pennsylvania Instructor(s): Gogolak |Fall offering: Course: 90723 Title: Financial Statements and Analysis of Companies Units: 6.0 Lec/Sec: Section C1 Days: Friday Begin: 02:00PM End: 03:20PM Bldg/Room: HBH 2008 Location: Pittsburgh, Pennsylvania Instructor(s): Gogolak |Fall offering: Course: 19705 Title: Workshop Applied Policy Analysis Units: 6.0 Lec/Sec: Section PP Days: To be announced Begin: 12:30PM End: 01:50PM Bldg/Room: TBA Location: Lisbon, Portugal Instructor(s): Adams ",
        "retrieved_relevant_part": true,
        "model_answer": ":  The Financial Statements and Analysis of Companies Recitation Section C1 begins at 11:00AM according to the context provided",
        "actual_answer": "11:00AM",
        "exact_match": false,
        "f1_max_min_avg": [
            0.1,
            0.1,
            0.1
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "What is the units of course Principles of Imperative Computation offered in Fa?",
        "context": "Fall offering: Course: 15122 Title: Principles of Imperative Computation Units: 12.0 Lec/Sec: Section M Days: Monday Begin: 12:00PM End: 12:50PM Bldg/Room: GHC CLSTR Location: Pittsburgh, Pennsylvania Instructor(s): Cervesato, Kaynar ",
        "retrieved_doc": "Fall offering: Course: 15122 Title: Principles of Imperative Computation Units: 12.0 Lec/Sec: Section F Days: Friday Begin: 02:00PM End: 02:50PM Bldg/Room: PH A18C Location: Pittsburgh, Pennsylvania Instructor(s): Cervesato, Kaynar |Fall offering: Course: 15122 Title: Principles of Imperative Computation Units: 12.0 Lec/Sec: Section I Days: Friday Begin: 05:00PM End: 05:50PM Bldg/Room: PH A18B Location: Pittsburgh, Pennsylvania Instructor(s): Cervesato, Kaynar |Fall offering: Course: 15122 Title: Principles of Imperative Computation Units: 12.0 Lec/Sec: Section G Days: Friday Begin: 03:00PM End: 03:50PM Bldg/Room: PH A18C Location: Pittsburgh, Pennsylvania Instructor(s): Cervesato, Kaynar ",
        "retrieved_relevant_part": false,
        "model_answer": ":  The course \"Principles of Imperative Computation\" offered at the University of Pittsburgh during the fall semester has 12.0 units",
        "actual_answer": "12.0",
        "exact_match": false,
        "f1_max_min_avg": [
            0.1,
            0.1,
            0.1
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "On which days does the class Big Data Science Recitation Section A4  offered in Sp meet?",
        "context": "Spring offering: Course: 18788 Title: Big Data Science Units: 6.0 Lec/Sec: Section A4 Days: Tuesday, Thursday Begin: 08:00AM End: 09:20AM Bldg/Room: HH 1107 Location: Pittsburgh, Pennsylvania Instructor(s): McSharry ",
        "retrieved_doc": "Summer 1/Summer All offering: Course: 17996 Title: Societal Computing Independent Study Units: VAR Lec/Sec: Section B Days: To be announced Begin: 05:00PM End: 06:20PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Sadeh |Summer 1/Summer All offering: Course: 17996 Title: Societal Computing Independent Study Units: VAR Lec/Sec: Section L Days: To be announced Begin: 05:00PM End: 06:20PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Fang |Summer 1/Summer All offering: Course: 17993 Title: Societal Computing Graduate Reading and Research: Units: VAR Lec/Sec: Section Q Days: To be announced Begin: 05:00PM End: 06:20PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Agarwal ",
        "retrieved_relevant_part": false,
        "model_answer": ":  The context provided does not contain any information about a \"Big Data Science Recitation Section A4\" offered in the Spring term or its meeting days. Therefore, there is no fact to extract from the context for this question",
        "actual_answer": "Tuesday, Thursday",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Based on the background information, What is the units of course Statistics for IT Managers offered in Fa?",
        "context": "Fall offering: Course: 95796 Title: Statistics for IT Managers Units: 6.0 Lec/Sec: Section D1 Days: Tuesday, Thursday Begin: 03:30PM End: 04:50PM Bldg/Room: HBH 1206 Location: Pittsburgh, Pennsylvania Instructor(s): Garin ",
        "retrieved_doc": "Fall offering: Course: 46880 Title: Introduction to Probabilty and Statistics Units: 6.0 Lec/Sec: Section M1 Days: Monday Begin: 08:15PM End: 09:30PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Moseley |Fall offering: Course: 70515 Title: Independent Study in Operations Management or Operations Research Units: VAR Lec/Sec: Section A Days: To be announced Begin: 02:00PM End: 03:20PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Hollifield |Fall offering: Course: 67100 Title: Information Systems First Year Colloquium Units: 1.0 Lec/Sec: Section A1 Days: Friday Begin: 11:00AM End: 12:20PM Bldg/Room: DH A302 Location: Pittsburgh, Pennsylvania Instructor(s): Mertz, Lutz ",
        "retrieved_relevant_part": false,
        "model_answer": ":  The context does not provide information about the units of the specific course \"Statistics for IT Managers\" offered in the fall semester. Therefore, an answer cannot be generated based on the given context alone",
        "actual_answer": "The units of course Statistics for IT Managers is 6.0",
        "exact_match": false,
        "f1_max_min_avg": [
            0.27,
            0.27,
            0.27
        ],
        "recall_max_min_avg": [
            0.6,
            0.6,
            0.6
        ]
    },
    {
        "question": "When does the class Tap II - Rhythmic Technique/Foundational to Complex Recitation Section B  offered in Fa end?",
        "context": "Fall offering: Course: 54223 Title: Tap II - Rhythmic Technique/Foundational to Complex Units: 2.0 Lec/Sec: Section B Days: Friday Begin: 12:30PM End: 01:20PM Bldg/Room: PCA 306 Location: Pittsburgh, Pennsylvania Instructor(s): Conte ",
        "retrieved_doc": "Fall offering: Course: 54223 Title: Tap II - Rhythmic Technique/Foundational to Complex Units: 2.0 Lec/Sec: Section B Days: Friday Begin: 12:30PM End: 01:20PM Bldg/Room: PCA 306 Location: Pittsburgh, Pennsylvania Instructor(s): Conte |Fall offering: Course: 54223 Title: Tap II - Rhythmic Technique/Foundational to Complex Units: 2.0 Lec/Sec: Section A Days: Tuesday Begin: 12:30PM End: 01:20PM Bldg/Room: PCA 306 Location: Pittsburgh, Pennsylvania Instructor(s): Conte |Fall offering: Course: 54323 Title: Tap III - Expand Percussive Rhythms Units: 2.0 Lec/Sec: Section B Days: Thursday Begin: 04:30PM End: 05:20PM Bldg/Room: PCA 306 Location: Pittsburgh, Pennsylvania Instructor(s): Cousin ",
        "retrieved_relevant_part": true,
        "model_answer": ":  The class Tap II - Rhythmic Technique/Foundational to Complex Recitation Section B offered in Fall ends at 1:20 PM",
        "actual_answer": "01:20PM",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "On which days does the class Orchestration I Recitation Section A  offered in Fa meet?",
        "context": "Fall offering: Course: 57257 Title: Orchestration I Units: 6.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 09:00AM End: 09:50AM Bldg/Room: CFA M160 Location: Pittsburgh, Pennsylvania Instructor(s): Marthaler ",
        "retrieved_doc": "Spring offering: Course: 57271 Title: Orchestration II Units: 6.0 Lec/Sec: Section A Days: Wednesday, Friday Begin: 10:00AM End: 10:50AM Bldg/Room: HOA 223 Location: Pittsburgh, Pennsylvania Instructor(s): Marthaler |Fall offering: Course: 57257 Title: Orchestration I Units: 6.0 Lec/Sec: Section C Days: Tuesday, Thursday Begin: 11:00AM End: 11:50AM Bldg/Room: HOA 223 Location: Pittsburgh, Pennsylvania Instructor(s): Marthaler |Fall offering: Course: 57257 Title: Orchestration I Units: 6.0 Lec/Sec: Section B Days: Tuesday, Thursday Begin: 10:00AM End: 10:50AM Bldg/Room: CFA M160 Location: Pittsburgh, Pennsylvania Instructor(s): Marthaler ",
        "retrieved_relevant_part": false,
        "model_answer": ":  The context indicates that Orchestration I Recitation Section A is offered on Wednesdays and Fridays",
        "actual_answer": "Tuesday, Thursday",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "On which days does the class Weight Training: Recitation Section B3  offered in Sp meet?",
        "context": "Spring offering: Course: 69102 Title: Weight Training: Units: 3.0 Lec/Sec: Section B3 Days: Tuesday, Thursday Begin: 12:30PM End: 01:50PM Bldg/Room: CUC COURTS Location: Pittsburgh, Pennsylvania Instructor(s): Connelly ",
        "retrieved_doc": "Fall offering: Course: 03120 Title: Bench Skills with Fluorescent Proteins Units: 6.0 Lec/Sec: Section A2 Days: Thursday Begin: 07:00PM End: 10:00PM Bldg/Room: DH 2303 Location: Pittsburgh, Pennsylvania Instructor(s): McGuier |Fall offering: Course: 69102 Title: Weight Training: Units: 3.0 Lec/Sec: Section A2 Days: Monday, Wednesday, Friday Begin: 10:00AM End: 10:50AM Bldg/Room: CUC FITNES Location: Pittsburgh, Pennsylvania Instructor(s): Little, Gauntner |Fall offering: Course: 03120 Title: Bench Skills with Fluorescent Proteins Units: 6.0 Lec/Sec: Section A2 Days: Tuesday Begin: 07:00PM End: 09:00PM Bldg/Room: DH 2303 Location: Pittsburgh, Pennsylvania Instructor(s): McGuier ",
        "retrieved_relevant_part": false,
        "model_answer": ":  Based on the context provided, there is no information about a \"Weight Training: Recitation Section B3\" offered in the fall semester. Therefore, it is not possible to provide an answer to this question using the facts given in the context",
        "actual_answer": "Tuesday, Thursday",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Which campus is class Data Analytics for Organizational Impacts Recitation Section A2  offered in Pittsburgh in Fa?",
        "context": "Fall offering: Course: 94865 Title: Data Analytics for Organizational Impacts Units: 6.0 Lec/Sec: Section A2 Days: Tuesday, Thursday Begin: 03:30PM End: 04:50PM Bldg/Room: HBH 1004 Location: Pittsburgh, Pennsylvania Instructor(s): Escallon Barrios ",
        "retrieved_doc": "Fall offering: Course: 94465 Title: Data Analytics for Organizational Impact Units: 6.0 Lec/Sec: Section A2 Days: Tuesday, Thursday Begin: 03:30PM End: 04:50PM Bldg/Room: HBH 1004 Location: Pittsburgh, Pennsylvania Instructor(s): Escallon Barrios |Fall offering: Course: 94865 Title: Data Analytics for Organizational Impacts Units: 6.0 Lec/Sec: Section A2 Days: Tuesday, Thursday Begin: 03:30PM End: 04:50PM Bldg/Room: HBH 1004 Location: Pittsburgh, Pennsylvania Instructor(s): Escallon Barrios |Fall offering: Course: 99262 Title: ADDvocate: Anti-Discrimination Dialogue for Social Justice Units: VAR Lec/Sec: Section A1 Days: Wednesday Begin: 09:00AM End: 09:50AM Bldg/Room: CMU REMOTE Location: Pittsburgh, Pennsylvania Instructor(s): Ledford, Moufid, Vincent ",
        "retrieved_relevant_part": true,
        "model_answer": ":  The context indicates that Data Analytics for Organizational Impacts Recitation Section A2 is offered at the Pittsburgh campus. Therefore, the output should be \"Pittsburgh.\"",
        "actual_answer": "Pittsburgh, Pennsylvania",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Where does the class Accounting and Finance Analytics Recitation Section E4  offered in Sp meet?",
        "context": "Spring offering: Course: 95719 Title: Accounting and Finance Analytics Units: 6.0 Lec/Sec: Section E4 Days: Tuesday, Thursday Begin: 05:00PM End: 06:20PM Bldg/Room: HBH 1206 Location: Pittsburgh, Pennsylvania Instructor(s): Gogolak ",
        "retrieved_doc": "Spring offering: Course: 95719 Title: Accounting and Finance Analytics Units: 6.0 Lec/Sec: Section E4 Days: Tuesday, Thursday Begin: 05:00PM End: 06:20PM Bldg/Room: HBH 1206 Location: Pittsburgh, Pennsylvania Instructor(s): Gogolak |Fall offering: Course: 95719 Title: Accounting and Finance Analytics Units: 6.0 Lec/Sec: Section E2 Days: Tuesday, Thursday Begin: 09:30AM End: 10:50AM Bldg/Room: HBH 1204 Location: Pittsburgh, Pennsylvania Instructor(s): Gogolak |Fall offering: Course: 95719 Title: Accounting and Finance Analytics Units: 6.0 Lec/Sec: Section B1 Days: Tuesday, Thursday Begin: 09:30AM End: 10:50AM Bldg/Room: HBH 2008 Location: Pittsburgh, Pennsylvania Instructor(s): Gogolak ",
        "retrieved_relevant_part": true,
        "model_answer": ":  The class \"Accounting and Finance Analytics\" Recitation Section E4 offered in the spring meets in room HBH 1206 at Pittsburgh, Pennsylvania. (The context states that the lecture/lab for this section takes place on Tuesdays and Thursdays from 5:00 PM to 6:20 PM in this building and room.)",
        "actual_answer": "HBH 1206",
        "exact_match": false,
        "f1_max_min_avg": [
            0.08,
            0.08,
            0.08
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "On which days does the class IDeATe: Soft Fabrication Skills Recitation Section A1  offered in Fa meet?",
        "context": "Fall offering: Course: 99352 Title: IDeATe: Soft Fabrication Skills Units: 1.0 Lec/Sec: Section A1 Days: Saturday Begin: 10:00AM End: 03:00PM Bldg/Room: HL 106B Location: Pittsburgh, Pennsylvania Instructor(s): Pinchuk ",
        "retrieved_doc": "Fall offering: Course: 99352 Title: IDeATe: Soft Fabrication Skills Units: 1.0 Lec/Sec: Section A1 Days: Saturday Begin: 10:00AM End: 03:00PM Bldg/Room: HL 106B Location: Pittsburgh, Pennsylvania Instructor(s): Pinchuk |Spring offering: Course: 99352 Title: IDeATe: Soft Fabrication Skills Units: 1.0 Lec/Sec: Section A3 Days: Saturday Begin: 10:30AM End: 03:20PM Bldg/Room: HL 106B Location: Pittsburgh, Pennsylvania Instructor(s): Pinchuk |Spring offering: Course: 16224 Title: IDeATe: Re-Crafting Computational Thinking with Soft Technologies Units: 12.0 Lec/Sec: Section A Days: Friday Begin: 12:00PM End: 02:50PM Bldg/Room: CFA 111 Location: Pittsburgh, Pennsylvania Instructor(s): Orta Martinez, Robinson ",
        "retrieved_relevant_part": true,
        "model_answer": ":  The class IDeATe: Soft Fabrication Skills Recitation Section A1 meets on Saturdays. Specifically, it takes place on Saturdays from 10:00 AM to 03:00 PM in room HL 106B at the location in Pittsburgh, Pennsylvania, instructed by Pinchuk",
        "actual_answer": "Saturday",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Which campus is class Principles of Macroeconomics Recitation Section A  offered in Pittsburgh in Sp?",
        "context": "Spring offering: Course: 73103 Title: Principles of Macroeconomics Units: 9.0 Lec/Sec: Section A Days: Friday Begin: 10:00AM End: 10:50AM Bldg/Room: TEP 3801 Location: Pittsburgh, Pennsylvania Instructor(s): Zetlin-Jones ",
        "retrieved_doc": "Fall offering: Course: 73102 Title: Principles of Microeconomics Units: 9.0 Lec/Sec: Section F Days: Friday Begin: 01:00PM End: 01:50PM Bldg/Room: TEP 3808 Location: Pittsburgh, Pennsylvania Instructor(s): Portnykh |Spring offering: Course: 73102 Title: Principles of Microeconomics Units: 9.0 Lec/Sec: Section J Days: Friday Begin: 01:00PM End: 01:50PM Bldg/Room: TEP 3808 Location: Pittsburgh, Pennsylvania Instructor(s): Portnykh |Fall offering: Course: 73102 Title: Principles of Microeconomics Units: 9.0 Lec/Sec: Section L Days: Friday Begin: 02:00PM End: 02:50PM Bldg/Room: TEP 3808 Location: Pittsburgh, Pennsylvania Instructor(s): Portnykh ",
        "retrieved_relevant_part": false,
        "model_answer": ":  Based on the context provided, there is no information about a \"Principles of Macroeconomics\" course or its recitation section offered at the Pittsburgh campus during the spring semester. Therefore, it's not possible to provide an answer to the question using the facts presented in the context",
        "actual_answer": "Pittsburgh, Pennsylvania",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "When does the class Funding Early Stage Ventures: Recitation Section A4  offered in Sp end?",
        "context": "Spring offering: Course: 45905 Title: Funding Early Stage Ventures: Units: 6.0 Lec/Sec: Section A4 Days: Tuesday, Thursday Begin: 08:00AM End: 09:45AM Bldg/Room: TEP 2111 Location: Pittsburgh, Pennsylvania Instructor(s): Risch ",
        "retrieved_doc": "Summer 1/Summer All offering: Course: 48769 Title: Thesis/Project Units: VAR Lec/Sec: Section C Days: To be announced Begin: nan End: nan Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Cardoso Llach |Spring offering: Course: 45905 Title: Funding Early Stage Ventures Units: 6.0 Lec/Sec: Section A4 Days: Thursday Begin: 06:30PM End: 10:00PM Bldg/Room: TEP 2111 Location: Pittsburgh, Pennsylvania Instructor(s): Demmler |Fall offering: Course: 45905 Title: Funding Early Stage Ventures Units: 6.0 Lec/Sec: Section E2 Days: Wednesday Begin: 06:30PM End: 09:50PM Bldg/Room: TEP 2112 Location: Pittsburgh, Pennsylvania Instructor(s): Demmler ",
        "retrieved_relevant_part": false,
        "model_answer": ":  The class \"Funding Early Stage Ventures: Recitation Section A4\" offered in the spring semester begins at 6:30 PM and ends at 10:00 PM",
        "actual_answer": "09:45AM",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "When does the class Design Center: Design for Social Innovation Recitation Section A  offered in Sp end?",
        "context": "Spring offering: Course: 51782 Title: Design Center: Design for Social Innovation Units: 12.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 12:00PM End: 01:20PM Bldg/Room: MM 121 Location: Pittsburgh, Pennsylvania Instructor(s): Krishnaswami ",
        "retrieved_doc": "Summer 1/Summer All offering: Course: 17996 Title: Societal Computing Independent Study Units: VAR Lec/Sec: Section B Days: To be announced Begin: 05:00PM End: 06:20PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Sadeh |Summer 1/Summer All offering: Course: 17996 Title: Societal Computing Independent Study Units: VAR Lec/Sec: Section L Days: To be announced Begin: 05:00PM End: 06:20PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Fang |Summer 1/Summer All offering: Course: 17996 Title: Societal Computing Independent Study Units: VAR Lec/Sec: Section M Days: To be announced Begin: 05:00PM End: 06:20PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Vasilescu ",
        "retrieved_relevant_part": false,
        "model_answer": ":  The context provided does not contain any information about a \"Design Center: Design for Social Innovation Recitation Section A\" course or when it ends. Therefore, the output should be an empty string or null",
        "actual_answer": "01:20PM",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "On which days does the class Fairness, Accountability, Transparency, and Ethics in Sociotechnical Recitation Section A  offered in Fa meet?",
        "context": "Fall offering: Course: 05499 Title: Fairness, Accountability, Transparency, and Ethics in Sociotechnical Units: 12.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 11:00AM End: 12:20PM Bldg/Room: HH B103 Location: Pittsburgh, Pennsylvania Instructor(s): Shen ",
        "retrieved_doc": "Fall offering: Course: 05499 Title: Fairness, Accountability, Transparency, and Ethics in Sociotechnical Units: 12.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 11:00AM End: 12:20PM Bldg/Room: HH B103 Location: Pittsburgh, Pennsylvania Instructor(s): Shen |Fall offering: Course: 05899 Title: Fairness, Accountability, Transparency, and Ethics in Sociotechnical Units: 12.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 11:00AM End: 12:20PM Bldg/Room: HH B103 Location: Pittsburgh, Pennsylvania Instructor(s): Shen |Fall offering: Course: 62104 Title: Design Ethics & Social Justice in Architecture Units: 3.0 Lec/Sec: Section A Days: Tuesday Begin: 12:30PM End: 01:50PM Bldg/Room: CFA 214 Location: Pittsburgh, Pennsylvania Instructor(s): Vavasis, Gutschow ",
        "retrieved_relevant_part": true,
        "model_answer": ":  The class \"Fairness, Accountability, Transparency, and Ethics in Sociotechnical\" Recitation Section A meets on Mondays and Wednesdays",
        "actual_answer": "Monday, Wednesday",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "When does the class Organizational Behavior Recitation Section W  offered in Fa end?",
        "context": "Fall offering: Course: 70311 Title: Organizational Behavior Units: 9.0 Lec/Sec: Section W Days: Monday, Wednesday Begin: 11:30AM End: 12:45PM Bldg/Room: CMB 1190 Location: Doha, Qatar Instructor(s): Haan ",
        "retrieved_doc": "Fall offering: Course: 19469 Title: Behavior, Decision and Policy Units: 9.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 12:30PM End: 01:50PM Bldg/Room: WEH 6403 Location: Pittsburgh, Pennsylvania Instructor(s): Fischhoff |Fall offering: Course: 70504 Title: Independent Study in Organizational Behavior Units: 3-18 Lec/Sec: Section A Days: To be announced Begin: 02:00PM End: 03:20PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Hollifield |Fall offering: Course: 18656 Title: Functional Programming in Practice Units: 12.0 Lec/Sec: Section SV Days: Monday, Wednesday Begin: 03:00PM End: 04:50PM Bldg/Room: B23 109 Location: San Jose, California Instructor(s): Wlodarski ",
        "retrieved_relevant_part": false,
        "model_answer": ":  Based on the context provided, there is no information about a recitation section W for the Organizational Behavior course (Course: 70504) offered in the fall semester. Therefore, it is not possible to provide an answer to the question",
        "actual_answer": "12:45PM",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Who is the instructor(s) of course Software Engineering Management: Recitation Section A2  offered in Fa?",
        "context": "Fall offering: Course: 49772 Title: Software Engineering Management: Units: 6.0 Lec/Sec: Section A2 Days: Monday, Wednesday Begin: 11:00AM End: 12:20PM Bldg/Room: B23 227 Location: San Jose, California Instructor(s): Fang ",
        "retrieved_doc": "Yonatan Bisk Assistant Professor Email: ybisk@cs.cmu.edu Office: Gates & Hillman Centers Research Areas: Grounding, RoboNLP, Vision and Language, Embodiment, Unsupervised Learning|Summer 1/Summer All offering: Course: 48519 Title: Architecture Design Studio: Thesis II/ Independent Project Units: VAR Lec/Sec: Section A Days: To be announced Begin: nan End: nan Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Instructor TBA |Spring offering: Course: 17608 Title: Privacy Engineering Independent Study Units: VAR Lec/Sec: Section A Days: To be announced Begin: 12:30PM End: 01:50PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Bauer, Christin, Cranor, Fanti, Habib, Sadeh, Scanlon ",
        "retrieved_relevant_part": false,
        "model_answer": ":  I apologize for any confusion, but there is no information provided in the context regarding an instructor(s) for a course titled \"Software Engineering Management: Recitation Section A2\" offered in the FA term. Therefore, it's not possible to generate an accurate answer based on the context alone",
        "actual_answer": "Fang",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "On which days does the class Business Leadership Endeavor II Recitation Section C1  offered in Fa meet?",
        "context": "Fall offering: Course: 70204 Title: Business Leadership Endeavor II Units: 3.0 Lec/Sec: Section C1 Days: Tuesday Begin: 02:00PM End: 03:20PM Bldg/Room: TEP 3808 Location: Pittsburgh, Pennsylvania Instructor(s): Jafry O'Connor ",
        "retrieved_doc": "Spring offering: Course: 70204 Title: Business Leadership Endeavor II: Units: 3.0 Lec/Sec: Section B4 Days: Tuesday Begin: 11:00AM End: 12:20PM Bldg/Room: TEP 3808 Location: Pittsburgh, Pennsylvania Instructor(s): Jafry O'Connor |Spring offering: Course: 70304 Title: Business Leadership Endeavor III: Units: 3.0 Lec/Sec: Section B3 Days: Thursday Begin: 11:00AM End: 12:20PM Bldg/Room: TEP 3808 Location: Pittsburgh, Pennsylvania Instructor(s): Jafry O'Connor |Fall offering: Course: 70204 Title: Business Leadership Endeavor II Units: 3.0 Lec/Sec: Section B1 Days: Tuesday Begin: 12:30PM End: 01:50PM Bldg/Room: TEP 3808 Location: Pittsburgh, Pennsylvania Instructor(s): Jafry O'Connor ",
        "retrieved_relevant_part": false,
        "model_answer": ":  Based on the context provided, there is no information about a \"Business Leadership Endeavor II Recitation Section C1\" offered in the fall semester. The context only provides details about lectures (Section A) and recitations (Sections B1, B3, and B4) for two different offerings of the course \"Business Leadership Endeavor II\" (Course 70204), but none of them match the label \"Recitation Section C1\". Therefore, an accurate output would be:",
        "actual_answer": "Tuesday",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "On which days does the class Acting for Leadership and Communication Recitation Section B4  offered in Sp meet?",
        "context": "Spring offering: Course: 94801 Title: Acting for Leadership and Communication Units: 6.0 Lec/Sec: Section B4 Days: Monday, Wednesday Begin: 11:00AM End: 12:20PM Bldg/Room: HBH 2009 Location: Pittsburgh, Pennsylvania Instructor(s): Murphy ",
        "retrieved_doc": "Spring offering: Course: 94801 Title: Acting for Leadership and Communication Units: 6.0 Lec/Sec: Section D3 Days: Monday, Wednesday Begin: 02:00PM End: 03:20PM Bldg/Room: HBH 2008 Location: Pittsburgh, Pennsylvania Instructor(s): Murphy |Spring offering: Course: 94801 Title: Acting for Leadership and Communication Units: 6.0 Lec/Sec: Section B4 Days: Monday, Wednesday Begin: 11:00AM End: 12:20PM Bldg/Room: HBH 2009 Location: Pittsburgh, Pennsylvania Instructor(s): Murphy |Spring offering: Course: 94401 Title: Acting for Leadership and Communication Units: 6.0 Lec/Sec: Section B4 Days: Monday, Wednesday Begin: 11:00AM End: 12:20PM Bldg/Room: HBH 2009 Location: Pittsburgh, Pennsylvania Instructor(s): Murphy ",
        "retrieved_relevant_part": true,
        "model_answer": ":  The class \"Acting for Leadership and Communication\" Recitation Section B4 offered in the spring meets on Mondays and Wednesdays",
        "actual_answer": "Monday, Wednesday",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "On which days does the class Mathematical Models for Consulting Recitation Section W  offered in Fa meet?",
        "context": "Fall offering: Course: 70460 Title: Mathematical Models for Consulting Units: 9.0 Lec/Sec: Section W Days: Sunday, Tuesday Begin: 04:00PM End: 05:15PM Bldg/Room: CMB 1031 Location: Doha, Qatar Instructor(s): Safak ",
        "retrieved_doc": "Fall offering: Course: 90858 Title: How Ideas Become Policy Units: 6.0 Lec/Sec: Section W1 Days: Friday Begin: 09:30AM End: 12:20PM Bldg/Room: TBA Location: Washington, District of Columbia Instructor(s): Mendelson |Fall offering: Course: 19701 Title: Introduction to the Theory and Practice of Policy Analysis Units: 12.0 Lec/Sec: Section PP Days: Tuesday, Thursday Begin: 02:00PM End: 03:20PM Bldg/Room: TBA Location: Lisbon, Portugal Instructor(s): Morgan |Summer 1/Summer All offering: Course: 17996 Title: Societal Computing Independent Study Units: VAR Lec/Sec: Section M Days: To be announced Begin: 05:00PM End: 06:20PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Vasilescu ",
        "retrieved_relevant_part": false,
        "model_answer": ":  The context does not provide information about the days on which the recitation section for Mathematical Models for Consulting (Course: 90858, Section W1) meets. Therefore, an output cannot be generated based on the given context for this question",
        "actual_answer": "Sunday, Tuesday",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Based on the background information, What is the units of course Special Topics in ICT: offered in Fa?",
        "context": "Fall offering: Course: 04800 Title: Special Topics in ICT: Units: 12.0 Lec/Sec: Section G Days: Tuesday, Thursday Begin: 10:00AM End: 11:50AM Bldg/Room: CMR F309 Location: Kigali, Rwanda Instructor(s): Tucker ",
        "retrieved_doc": "Spring offering: Course: 04800 Title: Special Topics in ICT: Units: 12.0 Lec/Sec: Section AF Days: Monday, Wednesday Begin: 04:00PM End: 05:50PM Bldg/Room: CMU REMOTE Location: Kigali, Rwanda Instructor(s): Racz, McSharry |Fall offering: Course: 04801 Title: Special Topics in ICT: Electronic Business and Technology Units: 6.0 Lec/Sec: Section N1 Days: Tuesday, Thursday Begin: 08:00AM End: 09:50AM Bldg/Room: CMR F305 Location: Kigali, Rwanda Instructor(s): Ostheimer |Fall offering: Course: 04801 Title: Special Topics in ICT Mini: Units: 6.0 Lec/Sec: Section K Days: Tuesday, Thursday Begin: 07:00PM End: 08:50PM Bldg/Room: CMU REMOTE Location: Kigali, Rwanda Instructor(s): Wiecha ",
        "retrieved_relevant_part": false,
        "model_answer": ":  The units for Special Topics in ICT: offered in FA (Section AF) is 12.0",
        "actual_answer": "The units of course Special Topics in ICT: is 12.0",
        "exact_match": false,
        "f1_max_min_avg": [
            0.64,
            0.64,
            0.64
        ],
        "recall_max_min_avg": [
            0.8,
            0.8,
            0.8
        ]
    },
    {
        "question": "Which campus is class Techniques in Quantitative Analysis Recitation Section B  offered in Pittsburgh in Fa?",
        "context": "Fall offering: Course: 09207 Title: Techniques in Quantitative Analysis Units: 9.0 Lec/Sec: Section B Days: Tuesday Begin: 06:30PM End: 08:50PM Bldg/Room: DH 1302 Location: Pittsburgh, Pennsylvania Instructor(s): Botcha ",
        "retrieved_doc": "Spring offering: Course: 17608 Title: Privacy Engineering Independent Study Units: VAR Lec/Sec: Section A Days: To be announced Begin: 12:30PM End: 01:50PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Bauer, Christin, Cranor, Fanti, Habib, Sadeh, Scanlon |Spring offering: Course: 05680 Title: Independent Study in HCI - METALS Units: VAR Lec/Sec: Section B Days: To be announced Begin: 02:00PM End: 03:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Bett, Bodnar |Fall offering: Course: 84711 Title: Qualitative Methods in Social Science Research Units: 6.0 Lec/Sec: Section A1 Days: Wednesday Begin: 07:00PM End: 09:50PM Bldg/Room: POS 147 Location: Pittsburgh, Pennsylvania Instructor(s): Morgan ",
        "retrieved_relevant_part": false,
        "model_answer": ":  Based on the context provided, there is no course titled \"Techniques in Quantitative Analysis\" offered in Pittsburgh during the spring semester. Therefore, I cannot provide an answer to the question",
        "actual_answer": "Pittsburgh, Pennsylvania",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Based on the background information, What is the units of course Singers offered in Fa?",
        "context": "Fall offering: Course: 57822 Title: Singers Units: 3.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 05:30PM End: 06:20PM Bldg/Room: MM 119 Location: Pittsburgh, Pennsylvania Instructor(s): Douglas ",
        "retrieved_doc": "Fall offering: Course: 57743 Title: Sonatas/Songs for Collaborative Pianists III Units: 3.0 Lec/Sec: Section A Days: Monday Begin: 06:00PM End: 06:50PM Bldg/Room: CFA ACH Location: Pittsburgh, Pennsylvania Instructor(s): Carver, Rissinger, Kim, Manriquez, Ojeda |Fall offering: Course: 57672 Title: Chamber Music: Woodwind and Mixed Units: 3.0 Lec/Sec: Section B Days: To be announced Begin: 01:00PM End: 01:50PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Sargsyan, Steranka, McGhee |Fall offering: Course: 57417 Title: Major Vocal Performance Ensemble Units: 6.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 04:00PM End: 05:20PM Bldg/Room: MM 119 Location: Pittsburgh, Pennsylvania Instructor(s): Rissinger, Douglas ",
        "retrieved_relevant_part": false,
        "model_answer": ":  The context mentions one course titled \"Major Vocal Performance Ensemble\" with 6.0 units. Therefore, there is one offering of a vocal performance course with a total of 6 units at FA during the fall semester",
        "actual_answer": "The units of course Singers is 3.0",
        "exact_match": false,
        "f1_max_min_avg": [
            0.23,
            0.23,
            0.23
        ],
        "recall_max_min_avg": [
            0.71,
            0.71,
            0.71
        ]
    },
    {
        "question": "What is the abstract of the paper Neutral Face Learning and Progressive Fusion Synthesis Network for NIR-VIS Face Recognition published by LTI faculty Yiming Yang in 2023?",
        "context": "Author: Yiming Yang Title: Neutral Face Learning and Progressive Fusion Synthesis Network for NIR-VIS Face Recognition Publication year: 2023 Coauthors: Yiming Yang, Weipeng Hu, Haifeng Hu Abstract: To meet the strong demand for deploying face recognition systems in low-light scenarios, the Near-InfraRed and VISible (NIR-VIS) face recognition task is receiving increasing attention. However, heterogeneous faces have the characteristics of heterogeneity and non-neutrality. Heterogeneity refers to the fact that the matching images are in different modalities, and non-neutrality means that the matching images are significantly different in pose, expression, lighting, etc. Both situations pose challenges for NIR-VIS face matching. To address this problem, we propose a novel Neutral face Learning and Progressive Fusion synthesis (NLPF) network to disentangle the latent attributes of heterogeneous faces and learn neutral face representations. Our approach naturally integrates Identity-related Neutral face Learning (INL) and Attribute Progressive Fusion (APF) into a joint framework. Firstly, INL eliminates modal variations and residual variations by guiding the network to learn homogeneous neutral face feature representations, which tackles the challenge of heterogeneity and non-neutrality by mapping cross-modal images to a common neutral representation subspace. Besides, APF is presented to perform the disentanglement and reintegration of identity-related features, modality-related features and residual features in a progressive fusion manner, which helps to further purify identity-related features. Comprehensive evaluations are carried out on three mainstream NIR-VIS datasets to verify the robustness and effectiveness of the NLPF model. In particular, NLPF has competitive recognition performance on LAMP-HQ, the most challenging NIR-VIS dataset so far.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Alexander I. Rudnicky Title: Tartan: an LLM Driven SocialBot Publication year: 2023 Coauthors: Liangze Li, Ziyuan Liu, Li-Wei Chen, Ta-Chung Chi, Alexander I. Rudnicky Abstract: We describe our work to date on using large language models (LLMs) in our Alexa Prize Socialbot. Our work has laid the groundwork for looking more closely at using LLMs in a conversational system. We also analyze common patterns in conversations our bot has had with users|Author: Shinji Watanabe Title: E-Branchformer-Based E2E SLU Toward Stop on-Device Challenge Publication year: 2023 Coauthors: Yosuke Kashiwagi, Siddhant Arora, Hayato Futami, Jessica Huynh, Shih-Lun Wu, Yifan Peng, Brian Yan, E. Tsunoo, Shinji Watanabe Abstract: In this paper, we report our team\u2019s study on track 2 of the Spoken Language Understanding Grand Challenge, which is a component of the ICASSP Signal Processing Grand Challenge 2023. The task is intended for on-device processing and involves estimating semantic parse labels from speech using a model with 15 million parameters. We use E2E E-Branchformer-based spoken language understanding model, which is more parameter controllable than cascade models, and reduced the parameter size through sequential distillation and tensor decomposition techniques. On the STOP dataset, we achieved an exact match accuracy of 70.9% under the tight constraint of 15 million parameters.",
        "retrieved_relevant_part": false,
        "model_answer": ":  I apologize for the misunderstanding, but there is no paper titled \"Neutral Face Learning and Progressive Fusion Synthesis Network for NIR-VIS Face Recognition\" mentioned in the provided context by authors Yiming Yang or any other author, nor was it published in the given year 2023. Therefore, I cannot generate an output based on the context",
        "actual_answer": "To meet the strong demand for deploying face recognition systems in low-light scenarios, the Near-InfraRed and VISible (NIR-VIS) face recognition task is receiving increasing attention. However, heterogeneous faces have the characteristics of heterogeneity and non-neutrality. Heterogeneity refers to the fact that the matching images are in different modalities, and non-neutrality means that the matching images are significantly different in pose, expression, lighting, etc. Both situations pose challenges for NIR-VIS face matching. To address this problem, we propose a novel Neutral face Learning and Progressive Fusion synthesis (NLPF) network to disentangle the latent attributes of heterogeneous faces and learn neutral face representations. Our approach naturally integrates Identity-related Neutral face Learning (INL) and Attribute Progressive Fusion (APF) into a joint framework. Firstly, INL eliminates modal variations and residual variations by guiding the network to learn homogeneous neutral face feature representations, which tackles the challenge of heterogeneity and non-neutrality by mapping cross-modal images to a common neutral representation subspace. Besides, APF is presented to perform the disentanglement and reintegration of identity-related features, modality-related features and residual features in a progressive fusion manner, which helps to further purify identity-related features. Comprehensive evaluations are carried out on three mainstream NIR-VIS datasets to verify the robustness and effectiveness of the NLPF model. In particular, NLPF has competitive recognition performance on LAMP-HQ, the most challenging NIR-VIS dataset so far.",
        "exact_match": false,
        "f1_max_min_avg": [
            0.08,
            0.08,
            0.08
        ],
        "recall_max_min_avg": [
            0.05,
            0.05,
            0.05
        ]
    },
    {
        "question": "Who are the coauthors of the paper Policy Representation via Diffusion Probability Model for Reinforcement Learning published by LTI faculty Yiming Yang in 2023?",
        "context": "Author: Yiming Yang Title: Policy Representation via Diffusion Probability Model for Reinforcement Learning Publication year: 2023 Coauthors: Long Yang, Zhixiong Huang, Fenghao Lei, Yucun Zhong, Yiming Yang, Cong Fang, Shiting Wen, Binbin Zhou, Zhouchen Lin Abstract: Popular reinforcement learning (RL) algorithms tend to produce a unimodal policy distribution, which weakens the expressiveness of complicated policy and decays the ability of exploration. The diffusion probability model is powerful to learn complicated multimodal distributions, which has shown promising and potential applications to RL. In this paper, we formally build a theoretical foundation of policy representation via the diffusion probability model and provide practical implementations of diffusion policy for online model-free RL. Concretely, we character diffusion policy as a stochastic process, which is a new approach to representing a policy. Then we present a convergence guarantee for diffusion policy, which provides a theory to understand the multimodality of diffusion policy. Furthermore, we propose the DIPO which is an implementation for model-free online RL with DIffusion POlicy. To the best of our knowledge, DIPO is the first algorithm to solve model-free online RL problems with the diffusion model. Finally, extensive empirical results show the effectiveness and superiority of DIPO on the standard continuous control Mujoco benchmark.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Yiming Yang Title: CompleQA: Benchmarking the Impacts of Knowledge Graph Completion Methods on Question Answering Publication year: 2023 Coauthors: Donghan Yu, Yu Gu, Chenyan Xiong, Yiming Yang |Author: Yiming Yang Title: A Study on Semantic Understanding of Large Language Models from the Perspective of Ambiguity Resolution Publication year: 2023 Coauthors: Shuguang Yang, Feipeng Chen, Yiming Yang, Zude Zhu ",
        "retrieved_relevant_part": false,
        "model_answer": ":  The context does not provide information about the authors or the title of the paper \"Policy Representation via Diffusion Probability Model for Reinforcement Learning\" published by Yiming Yang in 2023. Therefore, it is impossible to generate an output based on the facts presented in the context alone",
        "actual_answer": "Long Yang, Zhixiong Huang, Fenghao Lei, Yucun Zhong, Yiming Yang, Cong Fang, Shiting Wen, Binbin Zhou, Zhouchen Lin",
        "exact_match": false,
        "f1_max_min_avg": [
            0.03,
            0.03,
            0.03
        ],
        "recall_max_min_avg": [
            0.06,
            0.06,
            0.06
        ]
    },
    {
        "question": "Who is the author of the LTI paper The Multimodal Information Based Speech Processing (MISP) 2023 Challenge: Audio-Visual Target Speaker Extraction published in 2023?",
        "context": "Author: Shinji Watanabe Title: The Multimodal Information Based Speech Processing (MISP) 2023 Challenge: Audio-Visual Target Speaker Extraction Publication year: 2023 Coauthors: Shilong Wu, Chenxi Wang, Hang Chen, Yusheng Dai, Chenyue Zhang, Ruoyu Wang, Hongbo Lan, Jun Du, Chin-Hui Lee, Jingdong Chen, Shinji Watanabe, Sabato Marco Siniscalchi, O. Scharenborg, Zhong-Qiu Wang, Jia Pan, Jianqing Gao Abstract: Previous Multimodal Information based Speech Processing (MISP) challenges mainly focused on audio-visual speech recognition (AVSR) with commendable success. However, the most advanced back-end recognition systems often hit performance limits due to the complex acoustic environments. This has prompted a shift in focus towards the Audio-Visual Target Speaker Extraction (AVTSE) task for the MISP 2023 challenge in ICASSP 2024 Signal Processing Grand Challenges. Unlike existing audio-visual speech enhance-ment challenges primarily focused on simulation data, the MISP 2023 challenge uniquely explores how front-end speech processing, combined with visual clues, impacts back-end tasks in real-world scenarios. This pioneering effort aims to set the first benchmark for the AVTSE task, offering fresh insights into enhancing the ac-curacy of back-end speech recognition systems through AVTSE in challenging and real acoustic environments. This paper delivers a thorough overview of the task setting, dataset, and baseline system of the MISP 2023 challenge. It also includes an in-depth analysis of the challenges participants may encounter. The experimental results highlight the demanding nature of this task, and we look forward to the innovative solutions participants will bring forward.",
        "retrieved_doc": "Author: T. Mitamura Title: Language-Agnostic Transformers and Assessing ChatGPT-Based Query Rewriting for Multilingual Document-Grounded QA Publication year: 2023 Coauthors: Srinivas Gowriraj, Soham Dinesh Tiwari, Mitali Potnis, Srijan Bansal, T. Mitamura, Eric Nyberg Abstract: The DialDoc 2023 shared task has expanded the document-grounded dialogue task to encompass multiple languages, despite having limited annotated data. This paper assesses the effectiveness of both language-agnostic and language-aware paradigms for multilingual pre-trained transformer models in a bi-encoder-based dense passage retriever (DPR), concluding that the language-agnostic approach is superior. Additionally, the study investigates the impact of query rewriting techniques using large language models, such as ChatGPT, on multilingual, document-grounded question-answering systems. The experiments conducted demonstrate that, for the examples examined, query rewriting does not enhance performance compared to the original queries. This failure is due to topic switching in final dialogue turns and irrelevant topics being considered for query rewriting.|Author: Mona T. Diab Title: Evaluating Multilingual Speech Translation under Realistic Conditions with Resegmentation and Terminology Publication year: 2023 Coauthors: Elizabeth Salesky, Kareem Darwish, Mohamed Al-Badrashiny, Mona T. Diab, J. Niehues Abstract: We present the ACL 60/60 evaluation sets for multilingual translation of ACL 2022 technical presentations into 10 target languages. This dataset enables further research into multilingual speech translation under realistic recording conditions with unsegmented audio and domain-specific terminology, applying NLP tools to text and speech in the technical domain, and evaluating and improving model robustness to diverse speaker demographics.|Author: Shinji Watanabe Title: Summary on the Multimodal Information Based Speech Processing (MISP) 2022 Challenge Publication year: 2023 Coauthors: Hang Chen, Shilong Wu, Yusheng Dai, Zhe Wang, Jun Du, Chin-Hui Lee, Jingdong Chen, Shinji Watanabe, Sabato Marco Siniscalchi, O. Scharenborg, Diyuan Liu, Baocai Yin, Jia Pan, Jianqing Gao, Cong Liu Abstract: The Multimodal Information based Speech Processing (MISP) 2022 challenge aimed to enhance speech processing performance in harsh acoustic environments by leveraging additional modalities such as video or text. The challenge included two tracks: audio-visual speaker diarization (AVSD) and audio-visual diarization and recognition (AVDR). The training material was based on previous MISP 2021 recordings, but we have accurately synchronized audio and visual data. Additionally, a new evaluation set was provided. This paper gives an overview of the challenge setup, presents the results, and summarizes the effective techniques employed by the participants. We also analyze the current technical challenges and suggest directions for future research in AVSD and AVDR.",
        "retrieved_relevant_part": false,
        "model_answer": ":  Shinji Watanabe",
        "actual_answer": "Shinji Watanabe",
        "exact_match": false,
        "f1_max_min_avg": [
            0.8,
            0.8,
            0.8
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Who is the author of the LTI paper FindAdaptNet: Find and Insert Adapters by Learned Layer Importance published in 2023?",
        "context": "Author: Shinji Watanabe Title: FindAdaptNet: Find and Insert Adapters by Learned Layer Importance Publication year: 2023 Coauthors: Junwei Huang, Karthik Ganesan, Soumi Maiti, Young Min Kim, Xuankai Chang, Paul Liang, Shinji Watanabe Abstract: Adapters are lightweight bottleneck modules introduced to assist pre-trained self-supervised learning (SSL) models to be customized to new tasks. However, searching the appropriate layers to insert adapters on large models has become difficult due to the large number of possible layers and thus a vast search space (2N possibilities for N layers). In this paper, we propose a technique that achieves automatic insertion of adapters for downstream automatic speech recognition (ASR) and spoken language understanding (SLU) tasks. Our approach is based on two-stage training. First, we train our model for a specific downstream task with additional shallow learnable layers and weight parameters to obtain the weighted summation over the output of each layer in SSL. This training method is established by the SUPERB baseline [1]. This first-stage training determines the most important layers given their respective weights. In the second stage, we proceed to insert adapters to the most important layers, retaining both performance and neural architecture search efficiency. On the CommonVoice dataset[2] we obtain 20.6% absolute improvement in Word Error Rate (WER) on the Welsh language against the conventional method, which inserts the adapter modules into the highest layers without search. In the SLURP SLU task, our method yields 4.0% intent accuracy improvement against the same conventional baseline.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Jamie Callan Title: COILcr: Efficient Semantic Matching in Contextualized Exact Match Retrieval Publication year: 2023 Coauthors: Zhen Fan, Luyu Gao, Rohan Jha, Jamie Callan |Author: Alexander I. Rudnicky Title: Tartan: an LLM Driven SocialBot Publication year: 2023 Coauthors: Liangze Li, Ziyuan Liu, Li-Wei Chen, Ta-Chung Chi, Alexander I. Rudnicky Abstract: We describe our work to date on using large language models (LLMs) in our Alexa Prize Socialbot. Our work has laid the groundwork for looking more closely at using LLMs in a conversational system. We also analyze common patterns in conversations our bot has had with users",
        "retrieved_relevant_part": false,
        "model_answer": ":  The context provided does not contain any information about the authors of the paper \"FindAdaptNet\" or its publication year. Therefore, it is not possible to generate an accurate answer based on the context alone",
        "actual_answer": "Shinji Watanabe",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Who is the author of the LTI paper Understanding the Effect of Model Compression on Social Bias in Large Language Models published in 2023?",
        "context": "Author: Emma Strubell Title: Understanding the Effect of Model Compression on Social Bias in Large Language Models Publication year: 2023 Coauthors: Gustavo Gon\u00e7alves, Emma Strubell Abstract: Large Language Models (LLMs) trained with self-supervision on vast corpora of web text fit to the social biases of that text. Without intervention, these social biases persist in the model's predictions in downstream tasks, leading to representational harm. Many strategies have been proposed to mitigate the effects of inappropriate social biases learned during pretraining. Simultaneously, methods for model compression have become increasingly popular to reduce the computational burden of LLMs. Despite the popularity and need for both approaches, little work has been done to explore the interplay between these two. We perform a carefully controlled study of the impact of model compression via quantization and knowledge distillation on measures of social bias in LLMs. Longer pretraining and larger models led to higher social bias, and quantization showed a regularizer effect with its best trade-off around 20% of the original pretraining time.",
        "retrieved_doc": "Author: Emma Strubell Title: Understanding the Effect of Model Compression on Social Bias in Large Language Models Publication year: 2023 Coauthors: Gustavo Gon\u00e7alves, Emma Strubell Abstract: Large Language Models (LLMs) trained with self-supervision on vast corpora of web text fit to the social biases of that text. Without intervention, these social biases persist in the model's predictions in downstream tasks, leading to representational harm. Many strategies have been proposed to mitigate the effects of inappropriate social biases learned during pretraining. Simultaneously, methods for model compression have become increasingly popular to reduce the computational burden of LLMs. Despite the popularity and need for both approaches, little work has been done to explore the interplay between these two. We perform a carefully controlled study of the impact of model compression via quantization and knowledge distillation on measures of social bias in LLMs. Longer pretraining and larger models led to higher social bias, and quantization showed a regularizer effect with its best trade-off around 20% of the original pretraining time.|Author: Alexander I. Rudnicky Title: Tartan: an LLM Driven SocialBot Publication year: 2023 Coauthors: Liangze Li, Ziyuan Liu, Li-Wei Chen, Ta-Chung Chi, Alexander I. Rudnicky Abstract: We describe our work to date on using large language models (LLMs) in our Alexa Prize Socialbot. Our work has laid the groundwork for looking more closely at using LLMs in a conversational system. We also analyze common patterns in conversations our bot has had with users|Author: Jamie Callan Title: COILcr: Efficient Semantic Matching in Contextualized Exact Match Retrieval Publication year: 2023 Coauthors: Zhen Fan, Luyu Gao, Rohan Jha, Jamie Callan ",
        "retrieved_relevant_part": true,
        "model_answer": ":  The authors of the paper \"Understanding the Effect of Model Compression on Social Bias in Large Language Models\" published in 2023 are Emma Strubell and Gustavo Gon\u00e7alves",
        "actual_answer": "Emma Strubell",
        "exact_match": false,
        "f1_max_min_avg": [
            0.13,
            0.13,
            0.13
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Who are the coauthors of the paper One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning published by LTI faculty Eric P. Xing in 2023?",
        "context": "Author: Eric P. Xing Title: One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning Publication year: 2023 Coauthors: Arnav Chavan, Zhuang Liu, D. Gupta, Eric P. Xing, Zhiqiang Shen Abstract: We present Generalized LoRA (GLoRA), an advanced approach for universal parameter-efficient fine-tuning tasks. Enhancing Low-Rank Adaptation (LoRA), GLoRA employs a generalized prompt module to optimize pre-trained model weights and adjust intermediate activations, providing more flexibility and capability across diverse tasks and datasets. Moreover, GLoRA facilitates efficient parameter adaptation by employing a scalable, modular, layer-wise structure search that learns individual adapter of each layer. Originating from a unified mathematical formulation, GLoRA exhibits strong transfer learning, few-shot learning and domain generalization abilities, as it adapts to new tasks through not only weights but also additional dimensions like activations. Comprehensive experiments demonstrate that GLoRA outperforms all previous methods in natural, specialized, and structured vision benchmarks, achieving superior accuracy with fewer parameters and computations. The proposed method on LLaMA-1 and LLaMA-2 also show considerable enhancements compared to the original LoRA in the language domain. Furthermore, our structural re-parameterization design ensures that GLoRA incurs no extra inference cost, rendering it a practical solution for resource-limited applications. Code and models are available at: https://github.com/Arnav0400/ViT-Slim/tree/master/GLoRA.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Jamie Callan Title: COILcr: Efficient Semantic Matching in Contextualized Exact Match Retrieval Publication year: 2023 Coauthors: Zhen Fan, Luyu Gao, Rohan Jha, Jamie Callan |Author: Maarten Sap Title: Don't Take This Out of Context!: On the Need for Contextual Models and Evaluations for Stylistic Rewriting Publication year: 2023 Coauthors: Akhila Yerukola, Xuhui Zhou, Elizabeth Clark, Maarten Sap ",
        "retrieved_relevant_part": false,
        "model_answer": ":  I apologize for the confusion, but there is no paper titled \"One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning\" mentioned in the provided context with an author named Eric P. Xing or publication year 2023. Therefore, it is not possible to provide the names of its coauthors based on the context",
        "actual_answer": "Arnav Chavan, Zhuang Liu, D. Gupta, Eric P. Xing, Zhiqiang Shen",
        "exact_match": false,
        "f1_max_min_avg": [
            0.07,
            0.07,
            0.07
        ],
        "recall_max_min_avg": [
            0.18,
            0.18,
            0.18
        ]
    },
    {
        "question": "Which LTI faculty published the paper Structured Pruning of Self-Supervised Pre-Trained Models for Speech Recognition and Understanding in 2023?",
        "context": "Author: Shinji Watanabe Title: Structured Pruning of Self-Supervised Pre-Trained Models for Speech Recognition and Understanding Publication year: 2023 Coauthors: Yifan Peng, Kwangyoun Kim, Felix Wu, Prashant Sridhar, Shinji Watanabe Abstract: Self-supervised speech representation learning (SSL) has shown to be effective in various downstream tasks, but SSL models are usually large and slow. Model compression techniques such as pruning aim to reduce the model size and computation without degradation in accuracy. Prior studies focus on the pruning of Transformers; however, speech models not only utilize a stack of Transformer blocks, but also combine a frontend network based on multiple convolutional layers for low-level feature representation learning. This frontend has a small size but a heavy computational cost. In this work, we propose three task-specific structured pruning methods to deal with such heterogeneous networks. Experiments on LibriSpeech and SLURP show that the proposed method is more accurate than the original wav2vec2-base with 10% to 30% less computation, and is able to reduce the computation by 40% to 50% without any degradation.",
        "retrieved_doc": "Author: Shinji Watanabe Title: DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models Publication year: 2023 Coauthors: Yifan Peng, Yui Sudo, Muhammad Shakeel, Shinji Watanabe Abstract: Self-supervised learning (SSL) has achieved notable success in many speech processing tasks, but the large model size and heavy computational cost hinder the deployment. Knowledge distillation trains a small student model to mimic the behavior of a large teacher model. However, the student architecture usually needs to be manually designed and will remain fixed during training, which requires prior knowledge and can lead to suboptimal performance. Inspired by recent success of task-specific structured pruning, we propose DPHuBERT, a novel task-agnostic compression method for speech SSL based on joint distillation and pruning. Experiments on SUPERB show that DPHuBERT outperforms pure distillation methods in almost all tasks. Moreover, DPHuBERT requires little training time and performs well with limited training data, making it suitable for resource-constrained applications. Our method can also be applied to various speech SSL models. Our code and models will be publicly available.|Author: Shinji Watanabe Title: Structured Pruning of Self-Supervised Pre-Trained Models for Speech Recognition and Understanding Publication year: 2023 Coauthors: Yifan Peng, Kwangyoun Kim, Felix Wu, Prashant Sridhar, Shinji Watanabe Abstract: Self-supervised speech representation learning (SSL) has shown to be effective in various downstream tasks, but SSL models are usually large and slow. Model compression techniques such as pruning aim to reduce the model size and computation without degradation in accuracy. Prior studies focus on the pruning of Transformers; however, speech models not only utilize a stack of Transformer blocks, but also combine a frontend network based on multiple convolutional layers for low-level feature representation learning. This frontend has a small size but a heavy computational cost. In this work, we propose three task-specific structured pruning methods to deal with such heterogeneous networks. Experiments on LibriSpeech and SLURP show that the proposed method is more accurate than the original wav2vec2-base with 10% to 30% less computation, and is able to reduce the computation by 40% to 50% without any degradation.|Author: Shinji Watanabe Title: Integrating Pretrained ASR and LM to Perform Sequence Generation for Spoken Language Understanding Publication year: 2023 Coauthors: Siddhant Arora, Hayato Futami, Yosuke Kashiwagi, E. Tsunoo, Brian Yan, Shinji Watanabe Abstract: There has been an increased interest in the integration of pretrained speech recognition (ASR) and language models (LM) into the SLU framework. However, prior methods often struggle with a vocabulary mismatch between pretrained models, and LM cannot be directly utilized as they diverge from its NLU formulation. In this study, we propose a three-pass end-to-end (E2E) SLU system that effectively integrates ASR and LM subnetworks into the SLU formulation for sequence generation tasks. In the first pass, our architecture predicts ASR transcripts using the ASR subnetwork. This is followed by the LM subnetwork, which makes an initial SLU prediction. Finally, in the third pass, the deliberation subnetwork conditions on representations from the ASR and LM subnetworks to make the final prediction. Our proposed three-pass SLU system shows improved performance over cascaded and E2E SLU models on two benchmark SLU datasets, SLURP and SLUE, especially on acoustically challenging utterances.",
        "retrieved_relevant_part": true,
        "model_answer": ":  Shinji Watanabe and his coauthors Yifan Peng, Kwangyoun Kim, Felix Wu, and Prashant Sridhar published the paper \"Structured Pruning of Self-Supervised Pre-Trained Models for Speech Recognition and Understanding\" in 2023",
        "actual_answer": "Shinji Watanabe",
        "exact_match": false,
        "f1_max_min_avg": [
            0.12,
            0.12,
            0.12
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Who is the author of the LTI paper The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features published in 2023?",
        "context": "Author: Rita Singh Title: The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features Publication year: 2023 Coauthors: Liao Qu, X. Zou, Xiang Li, Yandong Wen, Rita Singh, B. Raj Abstract: This work unveils the enigmatic link between phonemes and facial features. Traditional studies on voice-face correlations typically involve using a long period of voice input, including generating face images from voices and reconstructing 3D face meshes from voices. However, in situations like voice-based crimes, the available voice evidence may be short and limited. Additionally, from a physiological perspective, each segment of speech -- phoneme -- corresponds to different types of airflow and movements in the face. Therefore, it is advantageous to discover the hidden link between phonemes and face attributes. In this paper, we propose an analysis pipeline to help us explore the voice-face relationship in a fine-grained manner, i.e., phonemes v.s. facial anthropometric measurements (AM). We build an estimator for each phoneme-AM pair and evaluate the correlation through hypothesis testing. Our results indicate that AMs are more predictable from vowels compared to consonants, particularly with plosives. Additionally, we observe that if a specific AM exhibits more movement during phoneme pronunciation, it is more predictable. Our findings support those in physiology regarding correlation and lay the groundwork for future research on speech-face multimodal learning.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Alexander I. Rudnicky Title: Tartan: an LLM Driven SocialBot Publication year: 2023 Coauthors: Liangze Li, Ziyuan Liu, Li-Wei Chen, Ta-Chung Chi, Alexander I. Rudnicky Abstract: We describe our work to date on using large language models (LLMs) in our Alexa Prize Socialbot. Our work has laid the groundwork for looking more closely at using LLMs in a conversational system. We also analyze common patterns in conversations our bot has had with users|Author: Louis-Philippe Morency Title: Reconstructing the neurodynamics of face perception during real world vision in humans using intracranial EEG recordings Publication year: 2023 Coauthors: Arish Alreja, Michael J. Ward, J. A. Colan, Qianli Ma, R. M. Richardson, Louis-Philippe Morency, A. Ghuman ",
        "retrieved_relevant_part": false,
        "model_answer": ":  The context provided does not contain any information about the authors or the title of the paper \"The Hidden Dance of Phonemes and Visage\" or its publication year being 2023. Therefore, it's impossible to generate an accurate answer based on the context alone",
        "actual_answer": "Rita Singh",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "What is the abstract of the paper Core loss analysis of soft magnetic composite under non-sinusoidal excitation based on finite element models published by LTI faculty Yiming Yang in 2023?",
        "context": "Author: Yiming Yang Title: Core loss analysis of soft magnetic composite under non-sinusoidal excitation based on finite element models Publication year: 2023 Coauthors: Lei Zhao, Chengcheng Liu, Youhua H. Wang, Yiming Yang Abstract: Due to the effect of higher harmonics on magnetic properties under actual complex operating conditions, the accurate calculation of core losses of soft magnetic composites (SMC) is complicated. First, this paper improves the existing SMC model by introducing a correction factor to correct the hysteresis loss coefficient so that the model can consider the local variation characteristics of the magnetic density waveform and then calculate the core loss under different harmonic excitation. Then, the influence of skin effect and inhomogeneous flux density within the ring sample model is analyzed. Finally, to validate the improved model, it is compared with other models in the reference based on experimental measurements, respectively. The results show that the core loss calculated by the improved model is closer to the experimental results under different harmonic excitations. In addition, the applicability of the improved SMC model under triangular and square wave excitations is also verified by the derivation of the equations.",
        "retrieved_doc": "Author: Yiming Yang Title: Core loss analysis of soft magnetic composite under non-sinusoidal excitation based on finite element models Publication year: 2023 Coauthors: Lei Zhao, Chengcheng Liu, Youhua H. Wang, Yiming Yang Abstract: Due to the effect of higher harmonics on magnetic properties under actual complex operating conditions, the accurate calculation of core losses of soft magnetic composites (SMC) is complicated. First, this paper improves the existing SMC model by introducing a correction factor to correct the hysteresis loss coefficient so that the model can consider the local variation characteristics of the magnetic density waveform and then calculate the core loss under different harmonic excitation. Then, the influence of skin effect and inhomogeneous flux density within the ring sample model is analyzed. Finally, to validate the improved model, it is compared with other models in the reference based on experimental measurements, respectively. The results show that the core loss calculated by the improved model is closer to the experimental results under different harmonic excitations. In addition, the applicability of the improved SMC model under triangular and square wave excitations is also verified by the derivation of the equations.|Author: A. Rudnicky Title: Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings Publication year: 2023 Coauthors: Ta-Chung Chi, Ting-Han Fan, Li-Wei Chen, A. Rudnicky, P. Ramadge Abstract: The use of positional embeddings in transformer language models is widely accepted. However, recent research has called into question the necessity of such embeddings. We further extend this inquiry by demonstrating that a randomly initialized and frozen transformer language model, devoid of positional embeddings, inherently encodes strong positional information through the shrinkage of self-attention variance. To quantify this variance, we derive the underlying distribution of each step within a transformer layer. Through empirical validation using a fully pretrained model, we show that the variance shrinkage effect still persists after extensive gradient updates. Our findings serve to justify the decision to discard positional embeddings and thus facilitate more efficient pretraining of transformer language models.|Author: Shinji Watanabe Title: E-Branchformer-Based E2E SLU Toward Stop on-Device Challenge Publication year: 2023 Coauthors: Yosuke Kashiwagi, Siddhant Arora, Hayato Futami, Jessica Huynh, Shih-Lun Wu, Yifan Peng, Brian Yan, E. Tsunoo, Shinji Watanabe Abstract: In this paper, we report our team\u2019s study on track 2 of the Spoken Language Understanding Grand Challenge, which is a component of the ICASSP Signal Processing Grand Challenge 2023. The task is intended for on-device processing and involves estimating semantic parse labels from speech using a model with 15 million parameters. We use E2E E-Branchformer-based spoken language understanding model, which is more parameter controllable than cascade models, and reduced the parameter size through sequential distillation and tensor decomposition techniques. On the STOP dataset, we achieved an exact match accuracy of 70.9% under the tight constraint of 15 million parameters.",
        "retrieved_relevant_part": true,
        "model_answer": ":  The abstract of the paper \"Core loss analysis of soft magnetic composite under non-sinusoidal excitation based on finite element models\" by authors Yiming Yang, Lei Zhao, Chengcheng Liu, and Youhua H. Wang, published in 2023, discusses the complexity of accurately calculating core losses of soft magnetic composites due to the effect of higher harmonics on magnetic properties under actual operating conditions. The authors improve the existing SMC model by introducing a correction factor to account for local variation characteristics of the magnetic density waveform and calculate core loss under different harmonic excitations. They analyze the influence of skin effect and inhomogeneous flux density within the ring sample model and compare the improved model with other models based on experimental measurements. The results demonstrate that the core loss calculated by the improved model is closer to experimental results under different harmonic excitations, and its applicability under triangular and square wave excitations is also verified",
        "actual_answer": "Due to the effect of higher harmonics on magnetic properties under actual complex operating conditions, the accurate calculation of core losses of soft magnetic composites (SMC) is complicated. First, this paper improves the existing SMC model by introducing a correction factor to correct the hysteresis loss coefficient so that the model can consider the local variation characteristics of the magnetic density waveform and then calculate the core loss under different harmonic excitation. Then, the influence of skin effect and inhomogeneous flux density within the ring sample model is analyzed. Finally, to validate the improved model, it is compared with other models in the reference based on experimental measurements, respectively. The results show that the core loss calculated by the improved model is closer to the experimental results under different harmonic excitations. In addition, the applicability of the improved SMC model under triangular and square wave excitations is also verified by the derivation of the equations.",
        "exact_match": false,
        "f1_max_min_avg": [
            0.41,
            0.41,
            0.41
        ],
        "recall_max_min_avg": [
            0.41,
            0.41,
            0.41
        ]
    },
    {
        "question": "Who is the author of the LTI paper AutoReply: Detecting Nonsense in Dialogue with Discriminative Replies published in 2023?",
        "context": "Author: Daniel Fried Title: AutoReply: Detecting Nonsense in Dialogue with Discriminative Replies Publication year: 2023 Coauthors: Weiyan Shi, Emily Dinan, Adi Renduchintala, Daniel Fried, Athul Paul Jacob, Zhou Yu, Mike Lewis ",
        "retrieved_doc": "Author: Daniel Fried Title: AutoReply: Detecting Nonsense in Dialogue with Discriminative Replies Publication year: 2023 Coauthors: Weiyan Shi, Emily Dinan, Adi Renduchintala, Daniel Fried, Athul Paul Jacob, Zhou Yu, Mike Lewis |Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Alexander I. Rudnicky Title: Tartan: an LLM Driven SocialBot Publication year: 2023 Coauthors: Liangze Li, Ziyuan Liu, Li-Wei Chen, Ta-Chung Chi, Alexander I. Rudnicky Abstract: We describe our work to date on using large language models (LLMs) in our Alexa Prize Socialbot. Our work has laid the groundwork for looking more closely at using LLMs in a conversational system. We also analyze common patterns in conversations our bot has had with users",
        "retrieved_relevant_part": true,
        "model_answer": ":  The authors of the paper \"AutoReply: Detecting Nonsense in Dialogue with Discriminative Replies,\" published in 2023, are Daniel Fried, Weiyan Shi, Emily Dinan, Adi Renduchintala, Athul Paul Jacob, Zhou Yu, and Mike Lewis",
        "actual_answer": "Daniel Fried",
        "exact_match": false,
        "f1_max_min_avg": [
            0.06,
            0.06,
            0.06
        ],
        "recall_max_min_avg": [
            0.5,
            0.5,
            0.5
        ]
    },
    {
        "question": "Who is the author of the LTI paper A Gold Standard Dataset for the Reviewer Assignment Problem published in 2023?",
        "context": "Author: Graham Neubig Title: A Gold Standard Dataset for the Reviewer Assignment Problem Publication year: 2023 Coauthors: Ivan Stelmakh, J. Wieting, Graham Neubig, Nihar B. Shah Abstract: Many peer-review venues are either using or looking to use algorithms to assign submissions to reviewers. The crux of such automated approaches is the notion of the\"similarity score\"--a numerical estimate of the expertise of a reviewer in reviewing a paper--and many algorithms have been proposed to compute these scores. However, these algorithms have not been subjected to a principled comparison, making it difficult for stakeholders to choose the algorithm in an evidence-based manner. The key challenge in comparing existing algorithms and developing better algorithms is the lack of the publicly available gold-standard data that would be needed to perform reproducible research. We address this challenge by collecting a novel dataset of similarity scores that we release to the research community. Our dataset consists of 477 self-reported expertise scores provided by 58 researchers who evaluated their expertise in reviewing papers they have read previously. We use this data to compare several popular algorithms employed in computer science conferences and come up with recommendations for stakeholders. Our main findings are as follows. First, all algorithms make a non-trivial amount of error. For the task of ordering two papers in terms of their relevance for a reviewer, the error rates range from 12%-30% in easy cases to 36%-43% in hard cases, highlighting the vital need for more research on the similarity-computation problem. Second, most existing algorithms are designed to work with titles and abstracts of papers, and in this regime the Specter+MFR algorithm performs best. Third, to improve performance, it may be important to develop modern deep-learning based algorithms that can make use of the full texts of papers: the classical TD-IDF algorithm enhanced with full texts of papers is on par with the deep-learning based Specter+MFR that cannot make use of this information.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Jamie Callan Title: COILcr: Efficient Semantic Matching in Contextualized Exact Match Retrieval Publication year: 2023 Coauthors: Zhen Fan, Luyu Gao, Rohan Jha, Jamie Callan |Author: Maarten Sap Title: Don't Take This Out of Context!: On the Need for Contextual Models and Evaluations for Stylistic Rewriting Publication year: 2023 Coauthors: Akhila Yerukola, Xuhui Zhou, Elizabeth Clark, Maarten Sap ",
        "retrieved_relevant_part": false,
        "model_answer": ":  I apologize, but none of the provided context mentions an \"LTI paper\" titled \"A Gold Standard Dataset for the Reviewer Assignment Problem\" published in 2023. Therefore, it is not possible to provide an accurate answer based on the context alone",
        "actual_answer": "Graham Neubig",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Who is the author of the LTI paper Evaluating Multilingual Speech Translation under Realistic Conditions with Resegmentation and Terminology published in 2023?",
        "context": "Author: Mona T. Diab Title: Evaluating Multilingual Speech Translation under Realistic Conditions with Resegmentation and Terminology Publication year: 2023 Coauthors: Elizabeth Salesky, Kareem Darwish, Mohamed Al-Badrashiny, Mona T. Diab, J. Niehues Abstract: We present the ACL 60/60 evaluation sets for multilingual translation of ACL 2022 technical presentations into 10 target languages. This dataset enables further research into multilingual speech translation under realistic recording conditions with unsegmented audio and domain-specific terminology, applying NLP tools to text and speech in the technical domain, and evaluating and improving model robustness to diverse speaker demographics.",
        "retrieved_doc": "Author: Mona T. Diab Title: Evaluating Multilingual Speech Translation under Realistic Conditions with Resegmentation and Terminology Publication year: 2023 Coauthors: Elizabeth Salesky, Kareem Darwish, Mohamed Al-Badrashiny, Mona T. Diab, J. Niehues Abstract: We present the ACL 60/60 evaluation sets for multilingual translation of ACL 2022 technical presentations into 10 target languages. This dataset enables further research into multilingual speech translation under realistic recording conditions with unsegmented audio and domain-specific terminology, applying NLP tools to text and speech in the technical domain, and evaluating and improving model robustness to diverse speaker demographics.|Author: T. Mitamura Title: Language-Agnostic Transformers and Assessing ChatGPT-Based Query Rewriting for Multilingual Document-Grounded QA Publication year: 2023 Coauthors: Srinivas Gowriraj, Soham Dinesh Tiwari, Mitali Potnis, Srijan Bansal, T. Mitamura, Eric Nyberg Abstract: The DialDoc 2023 shared task has expanded the document-grounded dialogue task to encompass multiple languages, despite having limited annotated data. This paper assesses the effectiveness of both language-agnostic and language-aware paradigms for multilingual pre-trained transformer models in a bi-encoder-based dense passage retriever (DPR), concluding that the language-agnostic approach is superior. Additionally, the study investigates the impact of query rewriting techniques using large language models, such as ChatGPT, on multilingual, document-grounded question-answering systems. The experiments conducted demonstrate that, for the examples examined, query rewriting does not enhance performance compared to the original queries. This failure is due to topic switching in final dialogue turns and irrelevant topics being considered for query rewriting.|Author: Shinji Watanabe Title: Enhancing End-to-End Conversational Speech Translation Through Target Language Context Utilization Publication year: 2023 Coauthors: A. Hussein, Brian Yan, Antonios Anastasopoulos, Shinji Watanabe, S. Khudanpur Abstract: Incorporating longer context has been shown to benefit machine translation, but the inclusion of context in end-to-end speech translation (E2E-ST) remains under-studied. To bridge this gap, we introduce target language context in E2E-ST, enhancing coherence and overcoming memory constraints of extended audio segments. Additionally, we propose context dropout to ensure robustness to the absence of context, and further improve performance by adding speaker information. Our proposed contextual E2E-ST outperforms the isolated utterance-based E2E-ST approach. Lastly, we demonstrate that in conversational speech, contextual information primarily contributes to capturing context style, as well as resolving anaphora and named entities.",
        "retrieved_relevant_part": true,
        "model_answer": ":  Mona T. Diab",
        "actual_answer": "Mona T. Diab",
        "exact_match": false,
        "f1_max_min_avg": [
            0.86,
            0.86,
            0.86
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "What is the abstract of the paper I3D: Transformer Architectures with Input-Dependent Dynamic Depth for Speech Recognition published by LTI faculty Shinji Watanabe in 2023?",
        "context": "Author: Shinji Watanabe Title: I3D: Transformer Architectures with Input-Dependent Dynamic Depth for Speech Recognition Publication year: 2023 Coauthors: Yifan Peng, Jaesong Lee, Shinji Watanabe Abstract: Transformer-based end-to-end speech recognition has achieved great success. However, the large footprint and computational overhead make it difficult to deploy these models in some real-world applications. Model compression techniques can reduce the model size and speed up inference, but the compressed model has a fixed architecture which might be suboptimal. We propose a novel Transformer encoder with Input-Dependent Dynamic Depth (I3D) to achieve strong performance-efficiency trade-offs. With a similar number of layers at inference time, I3D-based models outperform the vanilla Transformer and the static pruned model via iterative layer pruning. We also present interesting analysis on the gate probabilities and the input-dependency, which helps us better understand deep encoders.",
        "retrieved_doc": "Author: Shinji Watanabe Title: I3D: Transformer Architectures with Input-Dependent Dynamic Depth for Speech Recognition Publication year: 2023 Coauthors: Yifan Peng, Jaesong Lee, Shinji Watanabe Abstract: Transformer-based end-to-end speech recognition has achieved great success. However, the large footprint and computational overhead make it difficult to deploy these models in some real-world applications. Model compression techniques can reduce the model size and speed up inference, but the compressed model has a fixed architecture which might be suboptimal. We propose a novel Transformer encoder with Input-Dependent Dynamic Depth (I3D) to achieve strong performance-efficiency trade-offs. With a similar number of layers at inference time, I3D-based models outperform the vanilla Transformer and the static pruned model via iterative layer pruning. We also present interesting analysis on the gate probabilities and the input-dependency, which helps us better understand deep encoders.|Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Shinji Watanabe Title: A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks Publication year: 2023 Coauthors: Yifan Peng, Kwangyoun Kim, Felix Wu, Brian Yan, Siddhant Arora, William Chen, Jiyang Tang, Suwon Shon, Prashant Sridhar, Shinji Watanabe Abstract: Conformer, a convolution-augmented Transformer variant, has become the de facto encoder architecture for speech processing due to its superior performance in various tasks, including automatic speech recognition (ASR), speech translation (ST) and spoken language understanding (SLU). Recently, a new encoder called E-Branchformer has outperformed Conformer in the LibriSpeech ASR benchmark, making it promising for more general speech applications. This work compares E-Branchformer and Conformer through extensive experiments using different types of end-to-end sequence-to-sequence models. Results demonstrate that E-Branchformer achieves comparable or better performance than Conformer in almost all evaluation sets across 15 ASR, 2 ST, and 3 SLU benchmarks, while being more stable during training. We will release our training configurations and pre-trained models for reproducibility, which can benefit the speech community.",
        "retrieved_relevant_part": true,
        "model_answer": ":  The abstract of the paper \"I3D: Transformer Architectures with Input-Dependent Dynamic Depth for Speech Recognition\" by authors Shinji Watanabe, Yifan Peng, and Jaesong Lee, published in 2023, discusses the challenges of deploying large transformer-based models for speech recognition due to their large footprint and computational overhead. The authors propose a novel Transformer encoder with Input-Dependent Dynamic Depth (I3D) to achieve strong performance-efficiency trade-offs. I3D-based models outperform the vanilla Transformer and static pruned models via iterative layer pruning. The authors also provide analysis on the gate probabilities and input-dependency to better understand deep encoders",
        "actual_answer": "Transformer-based end-to-end speech recognition has achieved great success. However, the large footprint and computational overhead make it difficult to deploy these models in some real-world applications. Model compression techniques can reduce the model size and speed up inference, but the compressed model has a fixed architecture which might be suboptimal. We propose a novel Transformer encoder with Input-Dependent Dynamic Depth (I3D) to achieve strong performance-efficiency trade-offs. With a similar number of layers at inference time, I3D-based models outperform the vanilla Transformer and the static pruned model via iterative layer pruning. We also present interesting analysis on the gate probabilities and the input-dependency, which helps us better understand deep encoders.",
        "exact_match": false,
        "f1_max_min_avg": [
            0.41,
            0.41,
            0.41
        ],
        "recall_max_min_avg": [
            0.39,
            0.39,
            0.39
        ]
    },
    {
        "question": "What is the abstract of the paper ActiveRAG: Revealing the Treasures of Knowledge via Active Learning published by LTI faculty Chenyan Xiong in 2024?",
        "context": "Author: Chenyan Xiong Title: ActiveRAG: Revealing the Treasures of Knowledge via Active Learning Publication year: 2024 Coauthors: Zhipeng Xu, Zhenghao Liu, Yibin Liu, Chenyan Xiong, Yukun Yan, Shuo Wang, Shi Yu, Zhiyuan Liu, Ge Yu Abstract: Retrieval Augmented Generation (RAG) has introduced a new paradigm for Large Language Models (LLMs), aiding in the resolution of knowledge-intensive tasks. However, current RAG models position LLMs as passive knowledge receptors, thereby restricting their capacity for learning and comprehending external knowledge. In this paper, we present ActiveRAG, an innovative RAG framework that shifts from passive knowledge acquisition to an active learning mechanism. This approach utilizes the Knowledge Construction mechanism to develop a deeper understanding of external knowledge by associating it with previously acquired or memorized knowledge. Subsequently, it designs the Cognitive Nexus mechanism to incorporate the outcomes from both chains of thought and knowledge construction, thereby calibrating the intrinsic cognition of LLMs. Our experimental results demonstrate that ActiveRAG surpasses previous RAG models, achieving a 5% improvement on question-answering datasets. All data and codes are available at https://github.com/OpenMatch/ActiveRAG.",
        "retrieved_doc": "Author: Chenyan Xiong Title: CompleQA: Benchmarking the Impacts of Knowledge Graph Completion Methods on Question Answering Publication year: 2023 Coauthors: Donghan Yu, Yu Gu, Chenyan Xiong, Yiming Yang |Author: Yiming Yang Title: CompleQA: Benchmarking the Impacts of Knowledge Graph Completion Methods on Question Answering Publication year: 2023 Coauthors: Donghan Yu, Yu Gu, Chenyan Xiong, Yiming Yang |Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe ",
        "retrieved_relevant_part": false,
        "model_answer": ":  I'm sorry for any confusion, but there seems to be a mistake in the question as the paper \"ActiveRAG: Revealing the Treasures of Knowledge via Active Learning\" published by Chenyan Xiong is not mentioned in the provided context. The context only includes papers published in 2023 by Chenyan Xiong and his coauthors, including \"CompleQA: Benchmarking the Impacts of Knowledge Graph Completion Methods on Question Answering\" and \"Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training\". Therefore, I cannot provide an accurate answer to the question based on the context",
        "actual_answer": "Retrieval Augmented Generation (RAG) has introduced a new paradigm for Large Language Models (LLMs), aiding in the resolution of knowledge-intensive tasks. However, current RAG models position LLMs as passive knowledge receptors, thereby restricting their capacity for learning and comprehending external knowledge. In this paper, we present ActiveRAG, an innovative RAG framework that shifts from passive knowledge acquisition to an active learning mechanism. This approach utilizes the Knowledge Construction mechanism to develop a deeper understanding of external knowledge by associating it with previously acquired or memorized knowledge. Subsequently, it designs the Cognitive Nexus mechanism to incorporate the outcomes from both chains of thought and knowledge construction, thereby calibrating the intrinsic cognition of LLMs. Our experimental results demonstrate that ActiveRAG surpasses previous RAG models, achieving a 5% improvement on question-answering datasets. All data and codes are available at https://github.com/OpenMatch/ActiveRAG.",
        "exact_match": false,
        "f1_max_min_avg": [
            0.11,
            0.11,
            0.11
        ],
        "recall_max_min_avg": [
            0.09,
            0.09,
            0.09
        ]
    },
    {
        "question": "Who is the author of the LTI paper Distributionally Robust Unsupervised Dense Retrieval Training on Web Graphs published in 2023?",
        "context": "Author: Chenyan Xiong Title: Distributionally Robust Unsupervised Dense Retrieval Training on Web Graphs Publication year: 2023 Coauthors: Peixuan Han, Zhenghao Liu, Zhiyuan Liu, Chenyan Xiong Abstract: This paper introduces Web-DRO, an unsupervised dense retrieval model, which clusters documents based on web structures and reweights the groups during contrastive training. Specifically, we first leverage web graph links and contrastively train an embedding model for clustering anchor-document pairs. Then we use Group Distributional Robust Optimization to reweight different clusters of anchor-document pairs, which guides the model to assign more weights to the group with higher contrastive loss and pay more attention to the worst case during training. Our experiments on MS MARCO and BEIR show that our model, Web-DRO, significantly improves the retrieval effectiveness in unsupervised scenarios. A comparison of clustering techniques shows that training on the web graph combining URL information reaches optimal performance on clustering. Further analysis confirms that group weights are stable and valid, indicating consistent model preferences as well as effective up-weighting of valuable groups and down-weighting of uninformative ones. The code of this paper can be obtained from https://github.com/OpenMatch/Web-DRO.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Chenyan Xiong Title: Distributionally Robust Unsupervised Dense Retrieval Training on Web Graphs Publication year: 2023 Coauthors: Peixuan Han, Zhenghao Liu, Zhiyuan Liu, Chenyan Xiong Abstract: This paper introduces Web-DRO, an unsupervised dense retrieval model, which clusters documents based on web structures and reweights the groups during contrastive training. Specifically, we first leverage web graph links and contrastively train an embedding model for clustering anchor-document pairs. Then we use Group Distributional Robust Optimization to reweight different clusters of anchor-document pairs, which guides the model to assign more weights to the group with higher contrastive loss and pay more attention to the worst case during training. Our experiments on MS MARCO and BEIR show that our model, Web-DRO, significantly improves the retrieval effectiveness in unsupervised scenarios. A comparison of clustering techniques shows that training on the web graph combining URL information reaches optimal performance on clustering. Further analysis confirms that group weights are stable and valid, indicating consistent model preferences as well as effective up-weighting of valuable groups and down-weighting of uninformative ones. The code of this paper can be obtained from https://github.com/OpenMatch/Web-DRO.|Author: Jamie Callan Title: COILcr: Efficient Semantic Matching in Contextualized Exact Match Retrieval Publication year: 2023 Coauthors: Zhen Fan, Luyu Gao, Rohan Jha, Jamie Callan ",
        "retrieved_relevant_part": true,
        "model_answer": ":  Chenyan Xiong",
        "actual_answer": "Chenyan Xiong",
        "exact_match": false,
        "f1_max_min_avg": [
            0.8,
            0.8,
            0.8
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Which LTI faculty published the paper Are aligned neural networks adversarially aligned? in 2023?",
        "context": "Author: Daphne Ippolito Title: Are aligned neural networks adversarially aligned? Publication year: 2023 Coauthors: Nicholas Carlini, Milad Nasr, Christopher A. Choquette-Choo, Matthew Jagielski, Irena Gao, Anas Awadalla, Pang Wei Koh, Daphne Ippolito, Katherine Lee, Florian Tram\u00e8r, Ludwig Schmidt Abstract: Large language models are now tuned to align with the goals of their creators, namely to be\"helpful and harmless.\"These models should respond helpfully to user questions, but refuse to answer requests that could cause harm. However, adversarial users can construct inputs which circumvent attempts at alignment. In this work, we study to what extent these models remain aligned, even when interacting with an adversarial user who constructs worst-case inputs (adversarial examples). These inputs are designed to cause the model to emit harmful content that would otherwise be prohibited. We show that existing NLP-based optimization attacks are insufficiently powerful to reliably attack aligned text models: even when current NLP-based attacks fail, we can find adversarial inputs with brute force. As a result, the failure of current attacks should not be seen as proof that aligned text models remain aligned under adversarial inputs. However the recent trend in large-scale ML models is multimodal models that allow users to provide images that influence the text that is generated. We show these models can be easily attacked, i.e., induced to perform arbitrary un-aligned behavior through adversarial perturbation of the input image. We conjecture that improved NLP attacks may demonstrate this same level of adversarial control over text-only models.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Jamie Callan Title: COILcr: Efficient Semantic Matching in Contextualized Exact Match Retrieval Publication year: 2023 Coauthors: Zhen Fan, Luyu Gao, Rohan Jha, Jamie Callan |Author: Shinji Watanabe Title: Challenges of Corporate Alliance CLOMA toward Plastic Litter Publication year: 2023 Coauthors: Shinji Watanabe ",
        "retrieved_relevant_part": false,
        "model_answer": ":  The context provided does not contain any information about a paper titled \"Are aligned neural networks adversarially aligned?\" or its authors or publication year. Therefore, it is impossible to generate an accurate output based on the context alone",
        "actual_answer": "Daphne Ippolito",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Which LTI faculty published the paper Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large Language Models in 2024?",
        "context": "Author: Eric Xing Title: Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large Language Models Publication year: 2024 Coauthors: Loka Li, Guan-Hong Chen, Yusheng Su, Zhenhao Chen, Yixuan Zhang, Eric Xing, Kun Zhang Abstract: The recent success of Large Language Models (LLMs) has catalyzed an increasing interest in their self-correction capabilities. This paper presents a comprehensive investigation into the intrinsic self-correction of LLMs, attempting to address the ongoing debate about its feasibility. Our research has identified an important latent factor - the ``confidence'' of LLMs - during the self-correction process. Overlooking this factor may cause the models to over-criticize themselves, resulting in unreliable conclusions regarding the efficacy of self-correction. We have experimentally observed that LLMs possess the capability to understand the ``confidence'' in their own responses. It motivates us to develop an ``If-or-Else'' (IoE) prompting framework, designed to guide LLMs in assessing their own ``confidence'', facilitating intrinsic self-corrections. We conduct extensive experiments and demonstrate that our IoE-based Prompt can achieve a consistent improvement regarding the accuracy of self-corrected responses over the initial answers. Our study not only sheds light on the underlying factors affecting self-correction in LLMs, but also introduces a practical framework that utilizes the IoE prompting principle to efficiently improve self-correction capabilities with ``confidence''. The code is available at \\url{https://github.com/MBZUAI-CLeaR/IoE-Prompting.git}.",
        "retrieved_doc": "Author: Eric Xing Title: Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large Language Models Publication year: 2024 Coauthors: Loka Li, Guan-Hong Chen, Yusheng Su, Zhenhao Chen, Yixuan Zhang, Eric Xing, Kun Zhang Abstract: The recent success of Large Language Models (LLMs) has catalyzed an increasing interest in their self-correction capabilities. This paper presents a comprehensive investigation into the intrinsic self-correction of LLMs, attempting to address the ongoing debate about its feasibility. Our research has identified an important latent factor - the ``confidence'' of LLMs - during the self-correction process. Overlooking this factor may cause the models to over-criticize themselves, resulting in unreliable conclusions regarding the efficacy of self-correction. We have experimentally observed that LLMs possess the capability to understand the ``confidence'' in their own responses. It motivates us to develop an ``If-or-Else'' (IoE) prompting framework, designed to guide LLMs in assessing their own ``confidence'', facilitating intrinsic self-corrections. We conduct extensive experiments and demonstrate that our IoE-based Prompt can achieve a consistent improvement regarding the accuracy of self-corrected responses over the initial answers. Our study not only sheds light on the underlying factors affecting self-correction in LLMs, but also introduces a practical framework that utilizes the IoE prompting principle to efficiently improve self-correction capabilities with ``confidence''. The code is available at \\url{https://github.com/MBZUAI-CLeaR/IoE-Prompting.git}.|Author: Eric Xing Title: Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large Language Models Publication year: 2024 Coauthors: Loka Li, Guan-Hong Chen, Yusheng Su, Zhenhao Chen, Yixuan Zhang, Eric Xing, Kun Zhang Abstract: The recent success of Large Language Models (LLMs) has catalyzed an increasing interest in their self-correction capabilities. This paper presents a comprehensive investigation into the intrinsic self-correction of LLMs, attempting to address the ongoing debate about its feasibility. Our research has identified an important latent factor - the ``confidence'' of LLMs - during the self-correction process. Overlooking this factor may cause the models to over-criticize themselves, resulting in unreliable conclusions regarding the efficacy of self-correction. We have experimentally observed that LLMs possess the capability to understand the ``confidence'' in their own responses. It motivates us to develop an ``If-or-Else'' (IoE) prompting framework, designed to guide LLMs in assessing their own ``confidence'', facilitating intrinsic self-corrections. We conduct extensive experiments and demonstrate that our IoE-based Prompt can achieve a consistent improvement regarding the accuracy of self-corrected responses over the initial answers. Our study not only sheds light on the underlying factors affecting self-correction in LLMs, but also introduces a practical framework that utilizes the IoE prompting principle to efficiently improve self-correction capabilities with ``confidence''. The code is available at \\url{https://github.com/MBZUAI-CLeaR/IoE-Prompting.git}.|Author: T. Mitamura Title: Language-Agnostic Transformers and Assessing ChatGPT-Based Query Rewriting for Multilingual Document-Grounded QA Publication year: 2023 Coauthors: Srinivas Gowriraj, Soham Dinesh Tiwari, Mitali Potnis, Srijan Bansal, T. Mitamura, Eric Nyberg Abstract: The DialDoc 2023 shared task has expanded the document-grounded dialogue task to encompass multiple languages, despite having limited annotated data. This paper assesses the effectiveness of both language-agnostic and language-aware paradigms for multilingual pre-trained transformer models in a bi-encoder-based dense passage retriever (DPR), concluding that the language-agnostic approach is superior. Additionally, the study investigates the impact of query rewriting techniques using large language models, such as ChatGPT, on multilingual, document-grounded question-answering systems. The experiments conducted demonstrate that, for the examples examined, query rewriting does not enhance performance compared to the original queries. This failure is due to topic switching in final dialogue turns and irrelevant topics being considered for query rewriting.",
        "retrieved_relevant_part": true,
        "model_answer": ":  Eric Xing",
        "actual_answer": "Eric Xing",
        "exact_match": false,
        "f1_max_min_avg": [
            0.8,
            0.8,
            0.8
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "What is the abstract of the paper Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers published by LTI faculty Yiming Yang in 2023?",
        "context": "Author: Yiming Yang Title: Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers Publication year: 2023 Coauthors: K. Choromanski, Shanda Li, Valerii Likhosherstov, Kumar Avinava Dubey, Shengjie Luo, Di He, Yiming Yang, Tam\u00e1s Sarl\u00f3s, Thomas Weingarten, Adrian Weller Abstract: We propose a new class of linear Transformers called FourierLearner-Transformers (FLTs), which incorporate a wide range of relative positional encoding mechanisms (RPEs). These include regular RPE techniques applied for nongeometric data, as well as novel RPEs operating on the sequences of tokens embedded in higher-dimensional Euclidean spaces (e.g. point clouds). FLTs construct the optimal RPE mechanism implicitly by learning its spectral representation. As opposed to other architectures combining efficient low-rank linear attention with RPEs, FLTs remain practical in terms of their memory usage and do not require additional assumptions about the structure of the RPE-mask. FLTs allow also for applying certain structural inductive bias techniques to specify masking strategies, e.g. they provide a way to learn the so-called local RPEs introduced in this paper and providing accuracy gains as compared with several other linear Transformers for language modeling. We also thoroughly tested FLTs on other data modalities and tasks, such as: image classification and 3D molecular modeling. For 3D-data FLTs are, to the best of our knowledge, the first Transformers architectures providing RPE-enhanced linear attention.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Yiming Yang Title: Functional Interpolation for Relative Positions Improves Long Context Transformers Publication year: 2023 Coauthors: Shanda Li, Chong You, Guru Guruganesh, J. Ainslie, Santiago Ontanon, M. Zaheer, Sumit K. Sanghai, Yiming Yang, Sanjiv Kumar, Srinadh Bhojanapalli Abstract: Preventing the performance decay of Transformers on inputs longer than those used for training has been an important challenge in extending the context length of these models. Though the Transformer architecture has fundamentally no limits on the input sequence lengths it can process, the choice of position encoding used during training can limit the performance of these models on longer inputs. We propose a novel functional relative position encoding with progressive interpolation, FIRE, to improve Transformer generalization to longer contexts. We theoretically prove that this can represent some of the popular relative position encodings, such as T5's RPE, Alibi, and Kerple. We next empirically show that FIRE models have better generalization to longer contexts on both zero-shot language modeling and long text benchmarks.|Author: A. Rudnicky Title: Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation Publication year: 2023 Coauthors: Ta-Chung Chi, Ting-Han Fan, A. Rudnicky, P. Ramadge Abstract: Unlike recurrent models, conventional wisdom has it that Transformers cannot perfectly model regular languages. Inspired by the notion of working memory, we propose a new Transformer variant named RegularGPT. With its novel combination of Weight-Sharing, Adaptive-Depth, and Sliding-Dilated-Attention, RegularGPT constructs working memory along the depth dimension, thereby enabling efficient and successful modeling of regular languages such as PARITY. We further test RegularGPT on the task of natural language length extrapolation and surprisingly find that it rediscovers the local windowed attention effect deemed necessary in prior work for length extrapolation.",
        "retrieved_relevant_part": false,
        "model_answer": ":  The abstract you are asking for is not mentioned in the provided context. The context mentions a paper titled \"Functional Interpolation for Relative Positions Improves Long Context Transformers\" authored by Yiming Yang and his colleagues, but it does not provide the abstract for that specific paper",
        "actual_answer": "We propose a new class of linear Transformers called FourierLearner-Transformers (FLTs), which incorporate a wide range of relative positional encoding mechanisms (RPEs). These include regular RPE techniques applied for nongeometric data, as well as novel RPEs operating on the sequences of tokens embedded in higher-dimensional Euclidean spaces (e.g. point clouds). FLTs construct the optimal RPE mechanism implicitly by learning its spectral representation. As opposed to other architectures combining efficient low-rank linear attention with RPEs, FLTs remain practical in terms of their memory usage and do not require additional assumptions about the structure of the RPE-mask. FLTs allow also for applying certain structural inductive bias techniques to specify masking strategies, e.g. they provide a way to learn the so-called local RPEs introduced in this paper and providing accuracy gains as compared with several other linear Transformers for language modeling. We also thoroughly tested FLTs on other data modalities and tasks, such as: image classification and 3D molecular modeling. For 3D-data FLTs are, to the best of our knowledge, the first Transformers architectures providing RPE-enhanced linear attention.",
        "exact_match": false,
        "f1_max_min_avg": [
            0.08,
            0.08,
            0.08
        ],
        "recall_max_min_avg": [
            0.05,
            0.05,
            0.05
        ]
    },
    {
        "question": "Who is the author of the LTI paper Unsupervised Dense Retrieval Training with Web Anchors published in 2023?",
        "context": "Author: Chenyan Xiong Title: Unsupervised Dense Retrieval Training with Web Anchors Publication year: 2023 Coauthors: Yiqing Xie, X. Liu, Chenyan Xiong Abstract: In this work, we present an unsupervised retrieval method with contrastive learning on web anchors. The anchor text describes the content that is referenced from the linked page. This shows similarities to search queries that aim to retrieve pertinent information from relevant documents. Based on their commonalities, we train an unsupervised dense retriever, Anchor-DR, with a contrastive learning task that matches the anchor text and the linked document. To filter out uninformative anchors (such as \"homepage\" or other functional anchors), we present a novel filtering technique to only select anchors that contain similar types of information as search queries. Experiments show that Anchor-DR outperforms state-of-the-art methods on unsupervised dense retrieval by a large margin (e.g., by 5.3% NDCG@10 on MSMARCO). The gain of our method is especially significant for search and question answering tasks. Our analysis further reveals that the pattern of anchor-document pairs is similar to that of search query-document pairs. Code available at https://github.com/Veronicium/AnchorDR.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Chenyan Xiong Title: Unsupervised Dense Retrieval Training with Web Anchors Publication year: 2023 Coauthors: Yiqing Xie, X. Liu, Chenyan Xiong Abstract: In this work, we present an unsupervised retrieval method with contrastive learning on web anchors. The anchor text describes the content that is referenced from the linked page. This shows similarities to search queries that aim to retrieve pertinent information from relevant documents. Based on their commonalities, we train an unsupervised dense retriever, Anchor-DR, with a contrastive learning task that matches the anchor text and the linked document. To filter out uninformative anchors (such as \"homepage\" or other functional anchors), we present a novel filtering technique to only select anchors that contain similar types of information as search queries. Experiments show that Anchor-DR outperforms state-of-the-art methods on unsupervised dense retrieval by a large margin (e.g., by 5.3% NDCG@10 on MSMARCO). The gain of our method is especially significant for search and question answering tasks. Our analysis further reveals that the pattern of anchor-document pairs is similar to that of search query-document pairs. Code available at https://github.com/Veronicium/AnchorDR.|Author: Jamie Callan Title: COILcr: Efficient Semantic Matching in Contextualized Exact Match Retrieval Publication year: 2023 Coauthors: Zhen Fan, Luyu Gao, Rohan Jha, Jamie Callan ",
        "retrieved_relevant_part": true,
        "model_answer": ":  Chenyan Xiong is the author of the LTI paper \"Unsupervised Dense Retrieval Training with Web Anchors\" published in 2023",
        "actual_answer": "Chenyan Xiong",
        "exact_match": false,
        "f1_max_min_avg": [
            0.18,
            0.18,
            0.18
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Who is the author of the LTI paper Faith and Fate: Limits of Transformers on Compositionality published in 2023?",
        "context": "Author: S. Welleck Title: Faith and Fate: Limits of Transformers on Compositionality Publication year: 2023 Coauthors: Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jian, Bill Yuchen Lin, Peter West, Chandra Bhagavatula, Ronan Le Bras, Jena D. Hwang, Soumya Sanyal, S. Welleck, Xiang Ren, Allyson Ettinger, Za\u00efd Harchaoui, Yejin Choi Abstract: Transformer large language models (LLMs) have sparked admiration for their exceptional performance on tasks that demand intricate multi-step reasoning. Yet, these models simultaneously show failures on surprisingly trivial problems. This begs the question: Are these errors incidental, or do they signal more substantial limitations? In an attempt to demystify transformer LLMs, we investigate the limits of these models across three representative compositional tasks -- multi-digit multiplication, logic grid puzzles, and a classic dynamic programming problem. These tasks require breaking problems down into sub-steps and synthesizing these steps into a precise answer. We formulate compositional tasks as computation graphs to systematically quantify the level of complexity, and break down reasoning steps into intermediate sub-procedures. Our empirical findings suggest that transformer LLMs solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, without necessarily developing systematic problem-solving skills. To round off our empirical study, we provide theoretical arguments on abstract multi-step reasoning problems that highlight how autoregressive generations' performance can rapidly decay with\\,increased\\,task\\,complexity.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Shinji Watanabe Title: Challenges of Corporate Alliance CLOMA toward Plastic Litter Publication year: 2023 Coauthors: Shinji Watanabe |Author: Jamie Callan Title: COILcr: Efficient Semantic Matching in Contextualized Exact Match Retrieval Publication year: 2023 Coauthors: Zhen Fan, Luyu Gao, Rohan Jha, Jamie Callan ",
        "retrieved_relevant_part": false,
        "model_answer": ":  The context provided does not contain any information about an LTI paper titled \"Faith and Fate: Limits of Transformers on Compositionality\" published in 2023 by any of the listed authors. Therefore, it is impossible to generate an accurate output based on the given context",
        "actual_answer": "S. Welleck",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Which LTI faculty published the paper Active Retrieval Augmented Generation in 2023?",
        "context": "Author: Graham Neubig Title: Active Retrieval Augmented Generation Publication year: 2023 Coauthors: Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig Abstract: Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Shinji Watanabe Title: Challenges of Corporate Alliance CLOMA toward Plastic Litter Publication year: 2023 Coauthors: Shinji Watanabe |Author: Jamie Callan Title: COILcr: Efficient Semantic Matching in Contextualized Exact Match Retrieval Publication year: 2023 Coauthors: Zhen Fan, Luyu Gao, Rohan Jha, Jamie Callan ",
        "retrieved_relevant_part": false,
        "model_answer": ":  The context does not provide information about a paper titled \"Active Retrieval Augmented Generation\" published by the LTI faculty in 2023. Therefore, it is impossible to generate an accurate output based on the given context",
        "actual_answer": "Graham Neubig",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Who are the coauthors of the paper The DARPA Wikidata Overlay: Wikidata as an ontology for natural language processing published by LTI faculty A. Gershman in 2023?",
        "context": "Author: A. Gershman Title: The DARPA Wikidata Overlay: Wikidata as an ontology for natural language processing Publication year: 2023 Coauthors: Elizabeth Spaulding, Kathryn Conger, A. Gershman, Rosario Uceda-Sosa, S. Brown, James Pustejovsky, Peter Anick, Martha Palmer Abstract: With 102,530,067 items currently in its crowd-sourced knowledge base, Wikidata provides NLP practitioners a unique and powerful resource for inference and reasoning over real-world entities. However, because Wikidata is very entity focused, events and actions are often labeled with eventive nouns (e.g., the process of diagnosing a person\u2019s illness is labeled \u201cdiagnosis\u201d), and the typical participants in an event are not described or linked to that event concept (e.g., the medical professional or patient). Motivated by a need for an adaptable, comprehensive, domain-flexible ontology for information extraction, including identifying the roles entities are playing in an event, we present a curated subset of Wikidata in which events have been enriched with PropBank roles. To enable richer narrative understanding between events from Wikidata concepts, we have also provided a comprehensive mapping from temporal Qnodes and Pnodes to the Allen Interval Temporal Logic relations.",
        "retrieved_doc": "Author: Maarten Sap Title: Don't Take This Out of Context!: On the Need for Contextual Models and Evaluations for Stylistic Rewriting Publication year: 2023 Coauthors: Akhila Yerukola, Xuhui Zhou, Elizabeth Clark, Maarten Sap |Author: A. Gershman Title: The DARPA Wikidata Overlay: Wikidata as an ontology for natural language processing Publication year: 2023 Coauthors: Elizabeth Spaulding, Kathryn Conger, A. Gershman, Rosario Uceda-Sosa, S. Brown, James Pustejovsky, Peter Anick, Martha Palmer Abstract: With 102,530,067 items currently in its crowd-sourced knowledge base, Wikidata provides NLP practitioners a unique and powerful resource for inference and reasoning over real-world entities. However, because Wikidata is very entity focused, events and actions are often labeled with eventive nouns (e.g., the process of diagnosing a person\u2019s illness is labeled \u201cdiagnosis\u201d), and the typical participants in an event are not described or linked to that event concept (e.g., the medical professional or patient). Motivated by a need for an adaptable, comprehensive, domain-flexible ontology for information extraction, including identifying the roles entities are playing in an event, we present a curated subset of Wikidata in which events have been enriched with PropBank roles. To enable richer narrative understanding between events from Wikidata concepts, we have also provided a comprehensive mapping from temporal Qnodes and Pnodes to the Allen Interval Temporal Logic relations.|Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe ",
        "retrieved_relevant_part": true,
        "model_answer": ":  The coauthors of the paper \"The DARPA Wikidata Overlay: Wikidata as an ontology for natural language processing\" published by A. Gershman in 2023 were Elizabeth Spaulding, Kathryn Conger, A. Gershman, Rosario Uceda-Sosa, S. Brown, James Pustejovsky, Peter Anick, and Martha Palmer",
        "actual_answer": "Elizabeth Spaulding, Kathryn Conger, A. Gershman, Rosario Uceda-Sosa, S. Brown, James Pustejovsky, Peter Anick, Martha Palmer",
        "exact_match": false,
        "f1_max_min_avg": [
            0.55,
            0.55,
            0.55
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "What is the abstract of the paper Exploration of Efficient End-to-End ASR using Discretized Input from Self-Supervised Learning published by LTI faculty Shinji Watanabe in 2023?",
        "context": "Author: Shinji Watanabe Title: Exploration of Efficient End-to-End ASR using Discretized Input from Self-Supervised Learning Publication year: 2023 Coauthors: Xuankai Chang, Brian Yan, Yuya Fujita, Takashi Maekaku, Shinji Watanabe Abstract: Self-supervised learning (SSL) of speech has shown impressive results in speech-related tasks, particularly in automatic speech recognition (ASR). While most methods employ the output of intermediate layers of the SSL model as real-valued features for downstream tasks, there is potential in exploring alternative approaches that use discretized token sequences. This approach offers benefits such as lower storage requirements and the ability to apply techniques from natural language processing. In this paper, we propose a new protocol that utilizes discretized token sequences in ASR tasks, which includes de-duplication and sub-word modeling to enhance the input sequence. It reduces computational cost by decreasing the length of the sequence. Our experiments on the LibriSpeech dataset demonstrate that our proposed protocol performs competitively with conventional ASR systems using continuous input features, while reducing computational and storage costs.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Shinji Watanabe Title: E-Branchformer-Based E2E SLU Toward Stop on-Device Challenge Publication year: 2023 Coauthors: Yosuke Kashiwagi, Siddhant Arora, Hayato Futami, Jessica Huynh, Shih-Lun Wu, Yifan Peng, Brian Yan, E. Tsunoo, Shinji Watanabe Abstract: In this paper, we report our team\u2019s study on track 2 of the Spoken Language Understanding Grand Challenge, which is a component of the ICASSP Signal Processing Grand Challenge 2023. The task is intended for on-device processing and involves estimating semantic parse labels from speech using a model with 15 million parameters. We use E2E E-Branchformer-based spoken language understanding model, which is more parameter controllable than cascade models, and reduced the parameter size through sequential distillation and tensor decomposition techniques. On the STOP dataset, we achieved an exact match accuracy of 70.9% under the tight constraint of 15 million parameters.|Author: Shinji Watanabe Title: Exploration of Efficient End-to-End ASR using Discretized Input from Self-Supervised Learning Publication year: 2023 Coauthors: Xuankai Chang, Brian Yan, Yuya Fujita, Takashi Maekaku, Shinji Watanabe Abstract: Self-supervised learning (SSL) of speech has shown impressive results in speech-related tasks, particularly in automatic speech recognition (ASR). While most methods employ the output of intermediate layers of the SSL model as real-valued features for downstream tasks, there is potential in exploring alternative approaches that use discretized token sequences. This approach offers benefits such as lower storage requirements and the ability to apply techniques from natural language processing. In this paper, we propose a new protocol that utilizes discretized token sequences in ASR tasks, which includes de-duplication and sub-word modeling to enhance the input sequence. It reduces computational cost by decreasing the length of the sequence. Our experiments on the LibriSpeech dataset demonstrate that our proposed protocol performs competitively with conventional ASR systems using continuous input features, while reducing computational and storage costs.",
        "retrieved_relevant_part": true,
        "model_answer": ":  The paper \"Exploration of Efficient End-to-End ASR using Discretized Input from Self-Supervised Learning\" by authors Xuankai Chang, Brian Yan, Yuya Fujita, Takashi Maekaku, and Shinji Watanabe, published in 2023, proposes a new protocol for automatic speech recognition (ASR) tasks using discretized token sequences instead of real-valued features from self-supervised learning models. This approach offers benefits such as lower storage requirements and the ability to apply techniques from natural language processing. The experiments conducted on the LibriSpeech dataset show that the proposed protocol performs competitively with conventional ASR systems using continuous input features while reducing computational and storage costs",
        "actual_answer": "Self-supervised learning (SSL) of speech has shown impressive results in speech-related tasks, particularly in automatic speech recognition (ASR). While most methods employ the output of intermediate layers of the SSL model as real-valued features for downstream tasks, there is potential in exploring alternative approaches that use discretized token sequences. This approach offers benefits such as lower storage requirements and the ability to apply techniques from natural language processing. In this paper, we propose a new protocol that utilizes discretized token sequences in ASR tasks, which includes de-duplication and sub-word modeling to enhance the input sequence. It reduces computational cost by decreasing the length of the sequence. Our experiments on the LibriSpeech dataset demonstrate that our proposed protocol performs competitively with conventional ASR systems using continuous input features, while reducing computational and storage costs.",
        "exact_match": false,
        "f1_max_min_avg": [
            0.46,
            0.46,
            0.46
        ],
        "recall_max_min_avg": [
            0.4,
            0.4,
            0.4
        ]
    },
    {
        "question": "What is the abstract of the paper Data-efficient Active Learning for Structured Prediction with Partial Annotation and Self-Training published by LTI faculty Emma Strubell in 2023?",
        "context": "Author: Emma Strubell Title: Data-efficient Active Learning for Structured Prediction with Partial Annotation and Self-Training Publication year: 2023 Coauthors: Zhisong Zhang, Emma Strubell, E. Hovy Abstract: In this work we propose a pragmatic method that reduces the annotation cost for structured label spaces using active learning. Our approach leverages partial annotation, which reduces labeling costs for structured outputs by selecting only the most informative sub-structures for annotation. We also utilize self-training to incorporate the current model's automatic predictions as pseudo-labels for un-annotated sub-structures. A key challenge in effectively combining partial annotation with self-training to reduce annotation cost is determining which sub-structures to select to label. To address this challenge, we adopt an error estimator to adaptively decide the partial selection ratio according to the current model's capability. In evaluations spanning four structured prediction tasks, we show that our combination of partial annotation and self-training using an adaptive selection ratio reduces annotation cost over strong full annotation baselines under a fair comparison scheme that takes reading time into consideration.",
        "retrieved_doc": "Author: Alexander I. Rudnicky Title: Tartan: an LLM Driven SocialBot Publication year: 2023 Coauthors: Liangze Li, Ziyuan Liu, Li-Wei Chen, Ta-Chung Chi, Alexander I. Rudnicky Abstract: We describe our work to date on using large language models (LLMs) in our Alexa Prize Socialbot. Our work has laid the groundwork for looking more closely at using LLMs in a conversational system. We also analyze common patterns in conversations our bot has had with users|Author: David R. Mortensen Title: SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing Publication year: 2023 Coauthors: Taiqi He, Lindia Tjuatja, Nathaniel R. Robinson, Shinji Watanabe, David R. Mortensen, Graham Neubig, L. Levin Abstract: In our submission to the SIGMORPHON 2023 Shared Task on interlinear glossing (IGT), we explore approaches to data augmentation and modeling across seven low-resource languages. For data augmentation, we explore two approaches: creating artificial data from the provided training data and utilizing existing IGT resources in other languages. On the modeling side, we test an enhanced version of the provided token classification baseline as well as a pretrained multilingual seq2seq model. Additionally, we apply post-correction using a dictionary for Gitksan, the language with the smallest amount of data. We find that our token classification models are the best performing, with the highest word-level accuracy for Arapaho and highest morpheme-level accuracy for Gitksan out of all submissions. We also show that data augmentation is an effective strategy, though applying artificial data pretraining has very different effects across both models tested.|Author: Shinji Watanabe Title: SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing Publication year: 2023 Coauthors: Taiqi He, Lindia Tjuatja, Nathaniel R. Robinson, Shinji Watanabe, David R. Mortensen, Graham Neubig, L. Levin Abstract: In our submission to the SIGMORPHON 2023 Shared Task on interlinear glossing (IGT), we explore approaches to data augmentation and modeling across seven low-resource languages. For data augmentation, we explore two approaches: creating artificial data from the provided training data and utilizing existing IGT resources in other languages. On the modeling side, we test an enhanced version of the provided token classification baseline as well as a pretrained multilingual seq2seq model. Additionally, we apply post-correction using a dictionary for Gitksan, the language with the smallest amount of data. We find that our token classification models are the best performing, with the highest word-level accuracy for Arapaho and highest morpheme-level accuracy for Gitksan out of all submissions. We also show that data augmentation is an effective strategy, though applying artificial data pretraining has very different effects across both models tested.",
        "retrieved_relevant_part": false,
        "model_answer": ":  I'm sorry for any confusion, but there is no paper titled \"Data-efficient Active Learning for Structured Prediction with Partial Annotation and Self-Training\" mentioned in the context provided. The context includes papers authored by Alexander I. Rudnicky, David R. Mortensen, and Shinji Watanabe, each with different titles and publication details",
        "actual_answer": "In this work we propose a pragmatic method that reduces the annotation cost for structured label spaces using active learning. Our approach leverages partial annotation, which reduces labeling costs for structured outputs by selecting only the most informative sub-structures for annotation. We also utilize self-training to incorporate the current model's automatic predictions as pseudo-labels for un-annotated sub-structures. A key challenge in effectively combining partial annotation with self-training to reduce annotation cost is determining which sub-structures to select to label. To address this challenge, we adopt an error estimator to adaptively decide the partial selection ratio according to the current model's capability. In evaluations spanning four structured prediction tasks, we show that our combination of partial annotation and self-training using an adaptive selection ratio reduces annotation cost over strong full annotation baselines under a fair comparison scheme that takes reading time into consideration.",
        "exact_match": false,
        "f1_max_min_avg": [
            0.07,
            0.07,
            0.07
        ],
        "recall_max_min_avg": [
            0.05,
            0.05,
            0.05
        ]
    },
    {
        "question": "Which LTI faculty published the paper Improving Massively Multilingual ASR with Auxiliary CTC Objectives in 2023?",
        "context": "Author: Shinji Watanabe Title: Improving Massively Multilingual ASR with Auxiliary CTC Objectives Publication year: 2023 Coauthors: William Chen, Brian Yan, Jiatong Shi, Yifan Peng, Soumi Maiti, Shinji Watanabe Abstract: Multilingual Automatic Speech Recognition (ASR) models have extended the usability of speech technologies to a wide variety of languages. With how many languages these models have to handle, however, a key to understanding their imbalanced performance across different languages is to examine if the model actually knows which language it should transcribe. In this paper, we introduce our work on improving performance on FLEURS, a 102-language open ASR benchmark, by conditioning the entire model on language identity (LID). We investigate techniques inspired from recent Connectionist Temporal Classification (CTC) studies to help the model handle the large number of languages, conditioning on the LID predictions of auxiliary tasks. Our experimental results demonstrate the effectiveness of our technique over standard CTC/Attention-based hybrid models. Furthermore, our state-of-the-art systems using self-supervised models with the Conformer architecture improve over the results of prior work on FLEURS by a relative 28.4% CER. Trained models are reproducible recipes are available at https://github.com/espnet/espnet/tree/master/egs2/fleurs/asr1.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Mona T. Diab Title: Evaluating Multilingual Speech Translation under Realistic Conditions with Resegmentation and Terminology Publication year: 2023 Coauthors: Elizabeth Salesky, Kareem Darwish, Mohamed Al-Badrashiny, Mona T. Diab, J. Niehues Abstract: We present the ACL 60/60 evaluation sets for multilingual translation of ACL 2022 technical presentations into 10 target languages. This dataset enables further research into multilingual speech translation under realistic recording conditions with unsegmented audio and domain-specific terminology, applying NLP tools to text and speech in the technical domain, and evaluating and improving model robustness to diverse speaker demographics.|Author: Louis-Philippe Morency Title: Reconstructing the neurodynamics of face perception during real world vision in humans using intracranial EEG recordings Publication year: 2023 Coauthors: Arish Alreja, Michael J. Ward, J. A. Colan, Qianli Ma, R. M. Richardson, Louis-Philippe Morency, A. Ghuman ",
        "retrieved_relevant_part": false,
        "model_answer": ":  The context does not provide information on any LTI (Laboratory for Information and Language Technology) faculty who published a paper titled \"Improving Massively Multilingual ASR with Auxiliary CTC Objectives\" in 2023. Therefore, no output can be generated based on the provided context",
        "actual_answer": "Shinji Watanabe",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Who are the coauthors of the paper LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning published by LTI faculty Eric P. Xing in 2023?",
        "context": "Author: Eric P. Xing Title: LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning Publication year: 2023 Coauthors: Han Guo, P. Greengard, Eric P. Xing, Yoon Kim Abstract: We propose a simple approach for memory-efficient adaptation of pretrained language models. Our approach uses an iterative algorithm to decompose each pretrained matrix into a high-precision low-rank component and a memory-efficient quantized component. During finetuning, the quantized component remains fixed and only the low-rank component is updated. We present an integer linear programming formulation of the quantization component which enables dynamic configuration of quantization parameters (e.g., bit-width, block size) for each matrix given an overall target memory budget. We further explore a data-aware version of the algorithm which uses an approximation of the Fisher information matrix to weight the reconstruction objective during matrix decomposition. Experiments on finetuning RoBERTa and LLaMA-2 (7B and 70B) demonstrate that our low-rank plus quantized matrix decomposition approach (LQ-LoRA) outperforms strong QLoRA and GPTQ-LoRA baselines and enables aggressive quantization to sub-3 bits with only minor performance degradations. When finetuned on a language modeling calibration dataset, LQ-LoRA can also be used for model compression; in this setting our 2.75-bit LLaMA-2-70B model (which has 2.85 bits on average when including the low-rank components and requires 27GB of GPU memory) performs respectably compared to the 16-bit baseline.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Jamie Callan Title: COILcr: Efficient Semantic Matching in Contextualized Exact Match Retrieval Publication year: 2023 Coauthors: Zhen Fan, Luyu Gao, Rohan Jha, Jamie Callan |Author: Alexander I. Rudnicky Title: Tartan: an LLM Driven SocialBot Publication year: 2023 Coauthors: Liangze Li, Ziyuan Liu, Li-Wei Chen, Ta-Chung Chi, Alexander I. Rudnicky Abstract: We describe our work to date on using large language models (LLMs) in our Alexa Prize Socialbot. Our work has laid the groundwork for looking more closely at using LLMs in a conversational system. We also analyze common patterns in conversations our bot has had with users",
        "retrieved_relevant_part": false,
        "model_answer": ":  I apologize for the confusion, but there is no paper titled \"LQ-LoRA\" mentioned in the provided context. The context only includes papers authored by Shinji Watanabe, Yui Sudo, Muhammad Shakeel, Yifan Peng, Jamie Callan, Zhen Fan, Luyu Gao, Rohan Jha, Liangze Li, Ziyuan Liu, Li-Wei Chen, and Alexander I. Rudnicky, with publication years ranging from 2023 to undisclosed. Therefore, it's not possible to provide the names of the coauthors for the paper you're asking about based on the context alone",
        "actual_answer": "Han Guo, P. Greengard, Eric P. Xing, Yoon Kim",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Who are the coauthors of the paper A Self-enhancement Approach for Domain-specific Chatbot Training via Knowledge Mining and Digest published by LTI faculty Yiming Yang in 2023?",
        "context": "Author: Yiming Yang Title: A Self-enhancement Approach for Domain-specific Chatbot Training via Knowledge Mining and Digest Publication year: 2023 Coauthors: Ruohong Zhang, Luyu Gao, Chen Zheng, Zhen Fan, Guokun Lai, Zheng Zhang, Fangzhou Ai, Yiming Yang, Hongxia Yang Abstract: Large Language Models (LLMs), despite their great power in language generation, often encounter challenges when dealing with intricate and knowledge-demanding queries in specific domains. This paper introduces a novel approach to enhance LLMs by effectively extracting the relevant knowledge from domain-specific textual sources, and the adaptive training of a chatbot with domain-specific inquiries. Our two-step approach starts from training a knowledge miner, namely LLMiner, which autonomously extracts Question-Answer pairs from relevant documents through a chain-of-thought reasoning process. Subsequently, we blend the mined QA pairs with a conversational dataset to fine-tune the LLM as a chatbot, thereby enriching its domain-specific expertise and conversational capabilities. We also developed a new evaluation benchmark which comprises four domain-specific text corpora and associated human-crafted QA pairs for testing. Our model shows remarkable performance improvement over generally aligned LLM and surpasses domain-adapted models directly fine-tuned on domain corpus. In particular, LLMiner achieves this with minimal human intervention, requiring only 600 seed instances, thereby providing a pathway towards self-improvement of LLMs through model-synthesized training data.",
        "retrieved_doc": "Author: Alexander I. Rudnicky Title: Tartan: an LLM Driven SocialBot Publication year: 2023 Coauthors: Liangze Li, Ziyuan Liu, Li-Wei Chen, Ta-Chung Chi, Alexander I. Rudnicky Abstract: We describe our work to date on using large language models (LLMs) in our Alexa Prize Socialbot. Our work has laid the groundwork for looking more closely at using LLMs in a conversational system. We also analyze common patterns in conversations our bot has had with users|Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Yiming Yang Title: A Self-enhancement Approach for Domain-specific Chatbot Training via Knowledge Mining and Digest Publication year: 2023 Coauthors: Ruohong Zhang, Luyu Gao, Chen Zheng, Zhen Fan, Guokun Lai, Zheng Zhang, Fangzhou Ai, Yiming Yang, Hongxia Yang Abstract: Large Language Models (LLMs), despite their great power in language generation, often encounter challenges when dealing with intricate and knowledge-demanding queries in specific domains. This paper introduces a novel approach to enhance LLMs by effectively extracting the relevant knowledge from domain-specific textual sources, and the adaptive training of a chatbot with domain-specific inquiries. Our two-step approach starts from training a knowledge miner, namely LLMiner, which autonomously extracts Question-Answer pairs from relevant documents through a chain-of-thought reasoning process. Subsequently, we blend the mined QA pairs with a conversational dataset to fine-tune the LLM as a chatbot, thereby enriching its domain-specific expertise and conversational capabilities. We also developed a new evaluation benchmark which comprises four domain-specific text corpora and associated human-crafted QA pairs for testing. Our model shows remarkable performance improvement over generally aligned LLM and surpasses domain-adapted models directly fine-tuned on domain corpus. In particular, LLMiner achieves this with minimal human intervention, requiring only 600 seed instances, thereby providing a pathway towards self-improvement of LLMs through model-synthesized training data.",
        "retrieved_relevant_part": true,
        "model_answer": ":  The coauthors of the paper \"A Self-enhancement Approach for Domain-specific Chatbot Training via Knowledge Mining and Digest\" published by Yiming Yang in 2023 are Ruohong Zhang, Luyu Gao, Chen Zheng, Zhen Fan, Guokun Lai, Zheng Zhang, Fangzhou Ai, and Hongxia Yang",
        "actual_answer": "Ruohong Zhang, Luyu Gao, Chen Zheng, Zhen Fan, Guokun Lai, Zheng Zhang, Fangzhou Ai, Yiming Yang, Hongxia Yang",
        "exact_match": false,
        "f1_max_min_avg": [
            0.53,
            0.53,
            0.53
        ],
        "recall_max_min_avg": [
            0.89,
            0.89,
            0.89
        ]
    },
    {
        "question": "Which LTI faculty published the paper Distributionally Robust Unsupervised Dense Retrieval Training on Web Graphs in 2023?",
        "context": "Author: Chenyan Xiong Title: Distributionally Robust Unsupervised Dense Retrieval Training on Web Graphs Publication year: 2023 Coauthors: Peixuan Han, Zhenghao Liu, Zhiyuan Liu, Chenyan Xiong Abstract: This paper introduces Web-DRO, an unsupervised dense retrieval model, which clusters documents based on web structures and reweights the groups during contrastive training. Specifically, we first leverage web graph links and contrastively train an embedding model for clustering anchor-document pairs. Then we use Group Distributional Robust Optimization to reweight different clusters of anchor-document pairs, which guides the model to assign more weights to the group with higher contrastive loss and pay more attention to the worst case during training. Our experiments on MS MARCO and BEIR show that our model, Web-DRO, significantly improves the retrieval effectiveness in unsupervised scenarios. A comparison of clustering techniques shows that training on the web graph combining URL information reaches optimal performance on clustering. Further analysis confirms that group weights are stable and valid, indicating consistent model preferences as well as effective up-weighting of valuable groups and down-weighting of uninformative ones. The code of this paper can be obtained from https://github.com/OpenMatch/Web-DRO.",
        "retrieved_doc": "Author: Chenyan Xiong Title: Distributionally Robust Unsupervised Dense Retrieval Training on Web Graphs Publication year: 2023 Coauthors: Peixuan Han, Zhenghao Liu, Zhiyuan Liu, Chenyan Xiong Abstract: This paper introduces Web-DRO, an unsupervised dense retrieval model, which clusters documents based on web structures and reweights the groups during contrastive training. Specifically, we first leverage web graph links and contrastively train an embedding model for clustering anchor-document pairs. Then we use Group Distributional Robust Optimization to reweight different clusters of anchor-document pairs, which guides the model to assign more weights to the group with higher contrastive loss and pay more attention to the worst case during training. Our experiments on MS MARCO and BEIR show that our model, Web-DRO, significantly improves the retrieval effectiveness in unsupervised scenarios. A comparison of clustering techniques shows that training on the web graph combining URL information reaches optimal performance on clustering. Further analysis confirms that group weights are stable and valid, indicating consistent model preferences as well as effective up-weighting of valuable groups and down-weighting of uninformative ones. The code of this paper can be obtained from https://github.com/OpenMatch/Web-DRO.|Yonatan Bisk Assistant Professor Email: ybisk@cs.cmu.edu Office: Gates & Hillman Centers Research Areas: Grounding, RoboNLP, Vision and Language, Embodiment, Unsupervised Learning|Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe ",
        "retrieved_relevant_part": true,
        "model_answer": ":  Chenyan Xiong published the paper \"Distributionally Robust Unsupervised Dense Retrieval Training on Web Graphs\" in 2023 at Carnegie Mellon University. (The authors listed in the context are Chenyan Xiong, Peixuan Han, Zhenghao Liu, and Zhiyuan Liu.)",
        "actual_answer": "Chenyan Xiong",
        "exact_match": false,
        "f1_max_min_avg": [
            0.1,
            0.1,
            0.1
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Who is the author of the LTI paper Multilingual TTS Accent Impressions for Accented ASR published in 2023?",
        "context": "Author: David R. Mortensen Title: Multilingual TTS Accent Impressions for Accented ASR Publication year: 2023 Coauthors: G. Karakasidis, Nathaniel R. Robinson, Yaroslav Getman, Atieno Ogayo, Ragheb Al-Ghezi, Ananya Ayasi, Shinji Watanabe, David R. Mortensen, M. Kurimo ",
        "retrieved_doc": "Author: David R. Mortensen Title: Multilingual TTS Accent Impressions for Accented ASR Publication year: 2023 Coauthors: G. Karakasidis, Nathaniel R. Robinson, Yaroslav Getman, Atieno Ogayo, Ragheb Al-Ghezi, Ananya Ayasi, Shinji Watanabe, David R. Mortensen, M. Kurimo |Author: Shinji Watanabe Title: Multilingual TTS Accent Impressions for Accented ASR Publication year: 2023 Coauthors: G. Karakasidis, Nathaniel R. Robinson, Yaroslav Getman, Atieno Ogayo, Ragheb Al-Ghezi, Ananya Ayasi, Shinji Watanabe, David R. Mortensen, M. Kurimo |Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe ",
        "retrieved_relevant_part": true,
        "model_answer": ":  The authors of the LTI paper \"Multilingual TTS Accent Impressions for Accented ASR\" published in 2023 are: G. Karakasidis, Nathaniel R. Robinson, Yaroslav Getman, Atieno Ogayo, Ragheb Al-Ghezi, Ananya Ayasi, Shinji Watanabe, and David R. Mortensen",
        "actual_answer": "David R. Mortensen",
        "exact_match": false,
        "f1_max_min_avg": [
            0.15,
            0.15,
            0.15
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "What is the abstract of the paper Transformed Protoform Reconstruction published by LTI faculty David R. Mortensen in 2023?",
        "context": "Author: David R. Mortensen Title: Transformed Protoform Reconstruction Publication year: 2023 Coauthors: Young Min Kim, Kalvin Chang, Chenxuan Cui, David R. Mortensen Abstract: Protoform reconstruction is the task of inferring what morphemes or words appeared like in the ancestral languages of a set of daughter languages. Meloni et al (2021) achieved the state-of-the-art on Latin protoform reconstruction with an RNN-based encoder-decoder with attention model. We update their model with the state-of-the-art seq2seq model: the Transformer. Our model outperforms their model on a suite of different metrics on two different datasets: their Romance data of 8,000 cognates spanning 5 languages and a Chinese dataset (Hou 2004) of 800+ cognates spanning 39 varieties. We also probe our model for potential phylogenetic signal contained in the model. Our code is publicly available at https://github.com/cmu-llab/acl-2023.",
        "retrieved_doc": "Author: Graham Neubig Title: DiffusER: Diffusion via Edit-based Reconstruction Publication year: 2023 Coauthors: Machel Reid, V. Hellendoorn, Graham Neubig |Author: Maarten Sap Title: Don't Take This Out of Context!: On the Need for Contextual Models and Evaluations for Stylistic Rewriting Publication year: 2023 Coauthors: Akhila Yerukola, Xuhui Zhou, Elizabeth Clark, Maarten Sap |Author: David R. Mortensen Title: Transformed Protoform Reconstruction Publication year: 2023 Coauthors: Young Min Kim, Kalvin Chang, Chenxuan Cui, David R. Mortensen Abstract: Protoform reconstruction is the task of inferring what morphemes or words appeared like in the ancestral languages of a set of daughter languages. Meloni et al (2021) achieved the state-of-the-art on Latin protoform reconstruction with an RNN-based encoder-decoder with attention model. We update their model with the state-of-the-art seq2seq model: the Transformer. Our model outperforms their model on a suite of different metrics on two different datasets: their Romance data of 8,000 cognates spanning 5 languages and a Chinese dataset (Hou 2004) of 800+ cognates spanning 39 varieties. We also probe our model for potential phylogenetic signal contained in the model. Our code is publicly available at https://github.com/cmu-llab/acl-2023.",
        "retrieved_relevant_part": true,
        "model_answer": ":  The abstract of the paper \"Transformed Protoform Reconstruction\" by David R. Mortensen, published in 2023, describes the task of protoform reconstruction as the inference of morphemes or words in ancestral languages based on a set of daughter languages. The authors update the state-of-the-art RNN-based encoder-decoder with attention model by Meloni et al (2021) with the Transformer model and achieve better performance on two datasets: a Romance dataset of 8,000 cognates spanning 5 languages and a Chinese dataset (Hou 2004) of 800+ cognates spanning 39 varieties. They also investigate the potential phylogenetic signal present in the model. The code for this research is publicly available at <https://github.com/cmu-llab/acl-2023>",
        "actual_answer": "Protoform reconstruction is the task of inferring what morphemes or words appeared like in the ancestral languages of a set of daughter languages. Meloni et al (2021) achieved the state-of-the-art on Latin protoform reconstruction with an RNN-based encoder-decoder with attention model. We update their model with the state-of-the-art seq2seq model: the Transformer. Our model outperforms their model on a suite of different metrics on two different datasets: their Romance data of 8,000 cognates spanning 5 languages and a Chinese dataset (Hou 2004) of 800+ cognates spanning 39 varieties. We also probe our model for potential phylogenetic signal contained in the model. Our code is publicly available at https://github.com/cmu-llab/acl-2023.",
        "exact_match": false,
        "f1_max_min_avg": [
            0.5,
            0.5,
            0.5
        ],
        "recall_max_min_avg": [
            0.5,
            0.5,
            0.5
        ]
    },
    {
        "question": "Who is the author of the LTI paper Learning to Ask Questions for Zero-shot Dialogue State Tracking published in 2023?",
        "context": "Author: Alexander I. Rudnicky Title: Learning to Ask Questions for Zero-shot Dialogue State Tracking Publication year: 2023 Coauthors: Diogo Tavares, David Semedo, Alexander I. Rudnicky, Jo\u00e3o Magalh\u00e3es Abstract: We present a method for performing zero-shot Dialogue State Tracking (DST) by casting the task as a learning-to-ask-questions framework. The framework learns to pair the best question generation (QG) strategy with in-domain question answering (QA) methods to extract slot values from a dialogue without any human intervention. A novel self-supervised QA pretraining step using in-domain data is essential to learn the structure without requiring any slot-filling annotations. Moreover, we show that QG methods need to be aligned with the same grammatical person used in the dialogue. Empirical evaluation on the MultiWOZ 2.1 dataset demonstrates that our approach, when used alongside robust QA models, outperforms existing zero-shot methods in the challenging task of zero-shot cross domain adaptation-given a comparable amount of domain knowledge during data creation. Finally, we analyze the impact of the types of questions used, and demonstrate that the algorithmic approach outperforms template-based question generation.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Alexander I. Rudnicky Title: Learning to Ask Questions for Zero-shot Dialogue State Tracking Publication year: 2023 Coauthors: Diogo Tavares, David Semedo, Alexander I. Rudnicky, Jo\u00e3o Magalh\u00e3es Abstract: We present a method for performing zero-shot Dialogue State Tracking (DST) by casting the task as a learning-to-ask-questions framework. The framework learns to pair the best question generation (QG) strategy with in-domain question answering (QA) methods to extract slot values from a dialogue without any human intervention. A novel self-supervised QA pretraining step using in-domain data is essential to learn the structure without requiring any slot-filling annotations. Moreover, we show that QG methods need to be aligned with the same grammatical person used in the dialogue. Empirical evaluation on the MultiWOZ 2.1 dataset demonstrates that our approach, when used alongside robust QA models, outperforms existing zero-shot methods in the challenging task of zero-shot cross domain adaptation-given a comparable amount of domain knowledge during data creation. Finally, we analyze the impact of the types of questions used, and demonstrate that the algorithmic approach outperforms template-based question generation.|Author: Jamie Callan Title: COILcr: Efficient Semantic Matching in Contextualized Exact Match Retrieval Publication year: 2023 Coauthors: Zhen Fan, Luyu Gao, Rohan Jha, Jamie Callan ",
        "retrieved_relevant_part": true,
        "model_answer": ":  The authors of the LTI paper \"Learning to Ask Questions for Zero-shot Dialogue State Tracking\" published in 2023 are Diogo Tavares, David Semedo, Alexander I. Rudnicky, and Jo\u00e3o Magalh\u00e3es",
        "actual_answer": "Alexander I. Rudnicky",
        "exact_match": false,
        "f1_max_min_avg": [
            0.12,
            0.12,
            0.12
        ],
        "recall_max_min_avg": [
            0.67,
            0.67,
            0.67
        ]
    },
    {
        "question": "Who are the coauthors of the paper The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation published by LTI faculty Graham Neubig in 2023?",
        "context": "Author: Graham Neubig Title: The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation Publication year: 2023 Coauthors: Patrick Fernandes, Daniel Deutsch, M. Finkelstein, Parker Riley, Andr\u00e9 F. T. Martins, Graham Neubig, Ankush Garg, J. Clark, Markus Freitag, Orhan Firat Abstract: Automatic evaluation of machine translation (MT) is a critical tool driving the rapid iterative development of MT systems. While considerable progress has been made on estimating a single scalar quality score, current metrics lack the informativeness of more detailed schemes that annotate individual errors, such as Multidimensional Quality Metrics (MQM). In this paper, we help fill this gap by proposing AutoMQM, a prompting technique which leverages the reasoning and in-context learning capabilities of large language models (LLMs) and asks them to identify and categorize errors in translations. We start by evaluating recent LLMs, such as PaLM and PaLM-2, through simple score prediction prompting, and we study the impact of labeled data through in-context learning and finetuning. We then evaluate AutoMQM with PaLM-2 models, and we find that it improves performance compared to just prompting for scores (with particularly large gains for larger models) while providing interpretability through error spans that align with human annotations.",
        "retrieved_doc": "Author: Maarten Sap Title: Don't Take This Out of Context!: On the Need for Contextual Models and Evaluations for Stylistic Rewriting Publication year: 2023 Coauthors: Akhila Yerukola, Xuhui Zhou, Elizabeth Clark, Maarten Sap |Author: T. Mitamura Title: Language-Agnostic Transformers and Assessing ChatGPT-Based Query Rewriting for Multilingual Document-Grounded QA Publication year: 2023 Coauthors: Srinivas Gowriraj, Soham Dinesh Tiwari, Mitali Potnis, Srijan Bansal, T. Mitamura, Eric Nyberg Abstract: The DialDoc 2023 shared task has expanded the document-grounded dialogue task to encompass multiple languages, despite having limited annotated data. This paper assesses the effectiveness of both language-agnostic and language-aware paradigms for multilingual pre-trained transformer models in a bi-encoder-based dense passage retriever (DPR), concluding that the language-agnostic approach is superior. Additionally, the study investigates the impact of query rewriting techniques using large language models, such as ChatGPT, on multilingual, document-grounded question-answering systems. The experiments conducted demonstrate that, for the examples examined, query rewriting does not enhance performance compared to the original queries. This failure is due to topic switching in final dialogue turns and irrelevant topics being considered for query rewriting.|Author: Daniel Fried Title: AutoReply: Detecting Nonsense in Dialogue with Discriminative Replies Publication year: 2023 Coauthors: Weiyan Shi, Emily Dinan, Adi Renduchintala, Daniel Fried, Athul Paul Jacob, Zhou Yu, Mike Lewis ",
        "retrieved_relevant_part": false,
        "model_answer": ":  I apologize for the confusion, but the context provided does not contain any information about the authors or the paper \"The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation\" published by Graham Neubig in 2023. Therefore, it is not possible to generate an accurate answer based on the context alone",
        "actual_answer": "Patrick Fernandes, Daniel Deutsch, M. Finkelstein, Parker Riley, Andr\u00e9 F. T. Martins, Graham Neubig, Ankush Garg, J. Clark, Markus Freitag, Orhan Firat",
        "exact_match": false,
        "f1_max_min_avg": [
            0.03,
            0.03,
            0.03
        ],
        "recall_max_min_avg": [
            0.05,
            0.05,
            0.05
        ]
    },
    {
        "question": "What is the abstract of the paper Multi-Channel Speaker Extraction with Adversarial Training: The Wavlab Submission to The Clarity ICASSP 2023 Grand Challenge published by LTI faculty Shinji Watanabe in 2023?",
        "context": "Author: Shinji Watanabe Title: Multi-Channel Speaker Extraction with Adversarial Training: The Wavlab Submission to The Clarity ICASSP 2023 Grand Challenge Publication year: 2023 Coauthors: Samuele Cornell, Zhongqiu Wang, Yoshiki Masuyama, Shinji Watanabe, Manuel Pariente, Nobutaka Ono, S. Squartini Abstract: In this work we detail our submission to the Clarity ICASSP 2023 grand challenge, in which participants have to develop a strong target speech enhancement system for hearing-aid (HA) devices in noisy-reverberant environments. Our system builds on our previous submission at the Second Clarity Enhancement Challenge (CEC2): iNeuBe-X, which consists in an iterative neural/conventional beamforming enhancement pipeline, guided by an enrollment utterance from the target speaker. This model, which won by a large margin the CEC2, is an extension of the state-of-the-art TF-GridNet model for multi-channel, streamable target-speaker speech enhancement. Here, this approach is extended and further improved by leveraging generative adversarial training, which we show proves especially useful when the training data is limited. Using only the official 6k training scenes data, our best model achieves 0.80 hearing-aid speech perception index (HASPI) and 0.41 hearing-aid speech quality index (HASQI) scores on the synthetic evaluation set. However, our model generalized poorly on the semi-real evaluation set. This highlights the fact that our community should focus more on real-world evaluation and less on fully synthetic datasets.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Shinji Watanabe Title: A Study on the Integration of Pipeline and E2E SLU Systems for Spoken Semantic Parsing Toward Stop Quality Challenge Publication year: 2023 Coauthors: Siddhant Arora, Hayato Futami, Shih-Lun Wu, Jessica Huynh, Yifan Peng, Yosuke Kashiwagi, E. Tsunoo, Brian Yan, Shinji Watanabe Abstract: Recently there have been efforts to introduce new benchmark tasks for spoken language understanding (SLU), like semantic parsing. In this paper, we describe our proposed spoken semantic parsing system for the quality track (Track 1) in Spoken Language Understanding Grand Challenge which is part of ICASSP Signal Processing Grand Challenge 2023. We experiment with both end-to-end and pipeline systems for this task. Strong automatic speech recognition (ASR) models like Whisper and pretrained Language models (LM) like BART are utilized inside our SLU framework to boost performance. We also investigate the output level combination of various models to get an exact match accuracy of 80.8, which won the 1st place at the challenge.|Author: Shinji Watanabe Title: Multi-Channel Speaker Extraction with Adversarial Training: The Wavlab Submission to The Clarity ICASSP 2023 Grand Challenge Publication year: 2023 Coauthors: Samuele Cornell, Zhongqiu Wang, Yoshiki Masuyama, Shinji Watanabe, Manuel Pariente, Nobutaka Ono, S. Squartini Abstract: In this work we detail our submission to the Clarity ICASSP 2023 grand challenge, in which participants have to develop a strong target speech enhancement system for hearing-aid (HA) devices in noisy-reverberant environments. Our system builds on our previous submission at the Second Clarity Enhancement Challenge (CEC2): iNeuBe-X, which consists in an iterative neural/conventional beamforming enhancement pipeline, guided by an enrollment utterance from the target speaker. This model, which won by a large margin the CEC2, is an extension of the state-of-the-art TF-GridNet model for multi-channel, streamable target-speaker speech enhancement. Here, this approach is extended and further improved by leveraging generative adversarial training, which we show proves especially useful when the training data is limited. Using only the official 6k training scenes data, our best model achieves 0.80 hearing-aid speech perception index (HASPI) and 0.41 hearing-aid speech quality index (HASQI) scores on the synthetic evaluation set. However, our model generalized poorly on the semi-real evaluation set. This highlights the fact that our community should focus more on real-world evaluation and less on fully synthetic datasets.",
        "retrieved_relevant_part": true,
        "model_answer": ":  The abstract of the paper \"Multi-Channel Speaker Extraction with Adversarial Training: The Wavlab Submission to The Clarity ICASSP 2023 Grand Challenge\" by authors Samuele Cornell, Zhongqiu Wang, Yoshiki Masuyama, Shinji Watanabe, Manuel Pariente, Nobutaka Ono, and S. Squartini, published in 2023, describes their submission to the Clarity ICASSP 2023 grand challenge where they developed a strong target speech enhancement system for hearing-aid devices using an iterative neural/conventional beamforming enhancement pipeline, guided by an enrollment utterance from the target speaker. They extend and improve this approach by leveraging generative adversarial training, achieving 0.80 hearing-aid speech perception index (HASPI) and 0.41 hearing-aid speech quality index (HASQI) scores on the synthetic evaluation set but poor generalization on the semi-real evaluation set. The authors emphasize the importance of focusing more on real-world evaluation and less on fully synthetic datasets",
        "actual_answer": "In this work we detail our submission to the Clarity ICASSP 2023 grand challenge, in which participants have to develop a strong target speech enhancement system for hearing-aid (HA) devices in noisy-reverberant environments. Our system builds on our previous submission at the Second Clarity Enhancement Challenge (CEC2): iNeuBe-X, which consists in an iterative neural/conventional beamforming enhancement pipeline, guided by an enrollment utterance from the target speaker. This model, which won by a large margin the CEC2, is an extension of the state-of-the-art TF-GridNet model for multi-channel, streamable target-speaker speech enhancement. Here, this approach is extended and further improved by leveraging generative adversarial training, which we show proves especially useful when the training data is limited. Using only the official 6k training scenes data, our best model achieves 0.80 hearing-aid speech perception index (HASPI) and 0.41 hearing-aid speech quality index (HASQI) scores on the synthetic evaluation set. However, our model generalized poorly on the semi-real evaluation set. This highlights the fact that our community should focus more on real-world evaluation and less on fully synthetic datasets.",
        "exact_match": false,
        "f1_max_min_avg": [
            0.34,
            0.34,
            0.34
        ],
        "recall_max_min_avg": [
            0.3,
            0.3,
            0.3
        ]
    },
    {
        "question": "Who is the author of the LTI paper Completing Visual Objects via Bridging Generation and Segmentation published in 2023?",
        "context": "Author: Rita Singh Title: Completing Visual Objects via Bridging Generation and Segmentation Publication year: 2023 Coauthors: Xiang Li, Yinpeng Chen, Chung-Ching Lin, Rita Singh, Bhiksha Raj, Zicheng Liu Abstract: This paper presents a novel approach to object completion, with the primary goal of reconstructing a complete object from its partially visible components. Our method, named MaskComp, delineates the completion process through iterative stages of generation and segmentation. In each iteration, the object mask is provided as an additional condition to boost image generation, and, in return, the generated images can lead to a more accurate mask by fusing the segmentation of images. We demonstrate that the combination of one generation and one segmentation stage effectively functions as a mask denoiser. Through alternation between the generation and segmentation stages, the partial object mask is progressively refined, providing precise shape guidance and yielding superior object completion results. Our experiments demonstrate the superiority of MaskComp over existing approaches, e.g., ControlNet and Stable Diffusion, establishing it as an effective solution for object completion.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Alexander I. Rudnicky Title: Tartan: an LLM Driven SocialBot Publication year: 2023 Coauthors: Liangze Li, Ziyuan Liu, Li-Wei Chen, Ta-Chung Chi, Alexander I. Rudnicky Abstract: We describe our work to date on using large language models (LLMs) in our Alexa Prize Socialbot. Our work has laid the groundwork for looking more closely at using LLMs in a conversational system. We also analyze common patterns in conversations our bot has had with users|Author: Maarten Sap Title: Don't Take This Out of Context!: On the Need for Contextual Models and Evaluations for Stylistic Rewriting Publication year: 2023 Coauthors: Akhila Yerukola, Xuhui Zhou, Elizabeth Clark, Maarten Sap ",
        "retrieved_relevant_part": false,
        "model_answer": ":  The context provided does not contain any information about the authors or the title of the paper \"Completing Visual Objects via Bridging Generation and Segmentation\" published in 2023. Therefore, it is impossible to generate an accurate output based on the context alone",
        "actual_answer": "Rita Singh",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Which LTI faculty published the paper An In-depth Look at Gemini's Language Abilities in 2023?",
        "context": "Author: Graham Neubig Title: An In-depth Look at Gemini's Language Abilities Publication year: 2023 Coauthors: Syeda Nahida Akter, Zichun Yu, Aashiq Muhamed, Tianyue Ou, Alex Bauerle, \u00c1ngel Alexander Cabrera, Krish Dholakia, Chenyan Xiong, Graham Neubig Abstract: The recently released Google Gemini class of models are the first to comprehensively report results that rival the OpenAI GPT series across a wide variety of tasks. In this paper, we do an in-depth exploration of Gemini's language abilities, making two contributions. First, we provide a third-party, objective comparison of the abilities of the OpenAI GPT and Google Gemini models with reproducible code and fully transparent results. Second, we take a closer look at the results, identifying areas where one of the two model classes excels. We perform this analysis over 10 datasets testing a variety of language abilities, including reasoning, answering knowledge-based questions, solving math problems, translating between languages, generating code, and acting as instruction-following agents. From this analysis, we find that Gemini Pro achieves accuracy that is close but slightly inferior to the corresponding GPT 3.5 Turbo on all tasks that we benchmarked. We further provide explanations for some of this under-performance, including failures in mathematical reasoning with many digits, sensitivity to multiple-choice answer ordering, aggressive content filtering, and others. We also identify areas where Gemini demonstrates comparably high performance, including generation into non-English languages, and handling longer and more complex reasoning chains. Code and data for reproduction can be found at https://github.com/neulab/gemini-benchmark",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Alexander I. Rudnicky Title: Tartan: an LLM Driven SocialBot Publication year: 2023 Coauthors: Liangze Li, Ziyuan Liu, Li-Wei Chen, Ta-Chung Chi, Alexander I. Rudnicky Abstract: We describe our work to date on using large language models (LLMs) in our Alexa Prize Socialbot. Our work has laid the groundwork for looking more closely at using LLMs in a conversational system. We also analyze common patterns in conversations our bot has had with users|Author: David R. Mortensen Title: Multilingual TTS Accent Impressions for Accented ASR Publication year: 2023 Coauthors: G. Karakasidis, Nathaniel R. Robinson, Yaroslav Getman, Atieno Ogayo, Ragheb Al-Ghezi, Ananya Ayasi, Shinji Watanabe, David R. Mortensen, M. Kurimo ",
        "retrieved_relevant_part": false,
        "model_answer": ":  The context does not provide information about a paper titled \"An In-depth Look at Gemini's Language Abilities\" published by any of the listed authors or institutions in 2023. Therefore, it is impossible to generate an accurate output based on the given context alone",
        "actual_answer": "Graham Neubig",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "What is the abstract of the paper ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages published by LTI faculty Graham Neubig in 2023?",
        "context": "Author: Graham Neubig Title: ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages Publication year: 2023 Coauthors: Nathaniel R. Robinson, Perez Ogayo, David R. Mortensen, Graham Neubig Abstract: Large language models (LLMs) implicitly learn to perform a range of language tasks, including machine translation (MT). Previous studies explore aspects of LLMs\u2019 MT capabilities. However, there exist a wide variety of languages for which recent LLM MT performance has never before been evaluated. Without published experimental evidence on the matter, it is difficult for speakers of the world\u2019s diverse languages to know how and whether they can use LLMs for their languages. We present the first experimental evidence for an expansive set of 204 languages, along with MT cost analysis, using the FLORES-200 benchmark. Trends reveal that GPT models approach or exceed traditional MT model performance for some high-resource languages (HRLs) but consistently lag for low-resource languages (LRLs), under-performing traditional MT for 84.1% of languages we covered. Our analysis reveals that a language\u2019s resource level is the most important feature in determining ChatGPT\u2019s relative ability to translate it, and suggests that ChatGPT is especially disadvantaged for LRLs and African languages.",
        "retrieved_doc": "Author: Graham Neubig Title: ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages Publication year: 2023 Coauthors: Nathaniel R. Robinson, Perez Ogayo, David R. Mortensen, Graham Neubig Abstract: Large language models (LLMs) implicitly learn to perform a range of language tasks, including machine translation (MT). Previous studies explore aspects of LLMs\u2019 MT capabilities. However, there exist a wide variety of languages for which recent LLM MT performance has never before been evaluated. Without published experimental evidence on the matter, it is difficult for speakers of the world\u2019s diverse languages to know how and whether they can use LLMs for their languages. We present the first experimental evidence for an expansive set of 204 languages, along with MT cost analysis, using the FLORES-200 benchmark. Trends reveal that GPT models approach or exceed traditional MT model performance for some high-resource languages (HRLs) but consistently lag for low-resource languages (LRLs), under-performing traditional MT for 84.1% of languages we covered. Our analysis reveals that a language\u2019s resource level is the most important feature in determining ChatGPT\u2019s relative ability to translate it, and suggests that ChatGPT is especially disadvantaged for LRLs and African languages.|Author: David R. Mortensen Title: ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages Publication year: 2023 Coauthors: Nathaniel R. Robinson, Perez Ogayo, David R. Mortensen, Graham Neubig Abstract: Large language models (LLMs) implicitly learn to perform a range of language tasks, including machine translation (MT). Previous studies explore aspects of LLMs\u2019 MT capabilities. However, there exist a wide variety of languages for which recent LLM MT performance has never before been evaluated. Without published experimental evidence on the matter, it is difficult for speakers of the world\u2019s diverse languages to know how and whether they can use LLMs for their languages. We present the first experimental evidence for an expansive set of 204 languages, along with MT cost analysis, using the FLORES-200 benchmark. Trends reveal that GPT models approach or exceed traditional MT model performance for some high-resource languages (HRLs) but consistently lag for low-resource languages (LRLs), under-performing traditional MT for 84.1% of languages we covered. Our analysis reveals that a language\u2019s resource level is the most important feature in determining ChatGPT\u2019s relative ability to translate it, and suggests that ChatGPT is especially disadvantaged for LRLs and African languages.|Author: Shinji Watanabe Title: Findings of the 2023 ML-Superb Challenge: Pre-Training And Evaluation Over More Languages And Beyond Publication year: 2023 Coauthors: Jiatong Shi, William Chen, Dan Berrebbi, Hsiu-Hsuan Wang, Wei-Ping Huang, En-Pei Hu, Ho-Lam Chuang, Xuankai Chang, Yuxun Tang, Shang-Wen Li, Abdelrahman Mohamed, Hung-yi Lee, Shinji Watanabe Abstract: The 2023 Multilingual Speech Universal Performance Benchmark (ML-SUPERB) Challenge expands upon the acclaimed SUPERB framework, emphasizing self-supervised models in multilingual speech recognition and language identification. The challenge comprises a research track focused on applying ML-SUPERB to specific multilingual subjects, a Challenge Track for model submissions, and a New Language Track where language resource researchers can contribute and evaluate their low-resource language data in the context of the latest progress in multilingual speech recognition. The challenge garnered 12 model submissions and 54 language corpora, resulting in a comprehensive benchmark encompassing 154 languages. The findings indicate that merely scaling models is not the definitive solution for multilingual speech tasks, and a variety of speech/voice types present significant challenges in multilingual speech processing.",
        "retrieved_relevant_part": true,
        "model_answer": ":  The abstract of the paper \"ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages\" by Graham Neubig and his co-authors, published in 2023, discusses the evaluation of large language models' machine translation (MT) capabilities for an expansive set of 204 languages using the FLORES-200 benchmark. The study reveals that ChatGPT approaches or exceeds traditional MT model performance for some high-resource languages but lags behind for low-resource languages, under-performing traditional MT for 84.1% of the languages covered. The analysis indicates that a language's resource level is the primary determinant of ChatGPT's relative ability to translate it, suggesting that ChatGPT is particularly disadvantaged for low-resource languages and African languages",
        "actual_answer": "Large language models (LLMs) implicitly learn to perform a range of language tasks, including machine translation (MT). Previous studies explore aspects of LLMs\u2019 MT capabilities. However, there exist a wide variety of languages for which recent LLM MT performance has never before been evaluated. Without published experimental evidence on the matter, it is difficult for speakers of the world\u2019s diverse languages to know how and whether they can use LLMs for their languages. We present the first experimental evidence for an expansive set of 204 languages, along with MT cost analysis, using the FLORES-200 benchmark. Trends reveal that GPT models approach or exceed traditional MT model performance for some high-resource languages (HRLs) but consistently lag for low-resource languages (LRLs), under-performing traditional MT for 84.1% of languages we covered. Our analysis reveals that a language\u2019s resource level is the most important feature in determining ChatGPT\u2019s relative ability to translate it, and suggests that ChatGPT is especially disadvantaged for LRLs and African languages.",
        "exact_match": false,
        "f1_max_min_avg": [
            0.33,
            0.33,
            0.33
        ],
        "recall_max_min_avg": [
            0.28,
            0.28,
            0.28
        ]
    },
    {
        "question": "Who is the author of the LTI paper Energy and Carbon Considerations of Fine-Tuning BERT published in 2023?",
        "context": "Author: Emma Strubell Title: Energy and Carbon Considerations of Fine-Tuning BERT Publication year: 2023 Coauthors: Xiaorong Wang, Clara Na, Emma Strubell, Sorelle A. Friedler, Sasha Luccioni Abstract: Despite the popularity of the `pre-train then fine-tune' paradigm in the NLP community, existing work quantifying energy costs and associated carbon emissions has largely focused on language model pre-training. Although a single pre-training run draws substantially more energy than fine-tuning, fine-tuning is performed more frequently by many more individual actors, and thus must be accounted for when considering the energy and carbon footprint of NLP. In order to better characterize the role of fine-tuning in the landscape of energy and carbon emissions in NLP, we perform a careful empirical study of the computational costs of fine-tuning across tasks, datasets, hardware infrastructure and measurement modalities. Our experimental results allow us to place fine-tuning energy and carbon costs into perspective with respect to pre-training and inference, and outline recommendations to NLP researchers and practitioners who wish to improve their fine-tuning energy efficiency.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Jamie Callan Title: COILcr: Efficient Semantic Matching in Contextualized Exact Match Retrieval Publication year: 2023 Coauthors: Zhen Fan, Luyu Gao, Rohan Jha, Jamie Callan |Author: Emma Strubell Title: Energy and Carbon Considerations of Fine-Tuning BERT Publication year: 2023 Coauthors: Xiaorong Wang, Clara Na, Emma Strubell, Sorelle A. Friedler, Sasha Luccioni Abstract: Despite the popularity of the `pre-train then fine-tune' paradigm in the NLP community, existing work quantifying energy costs and associated carbon emissions has largely focused on language model pre-training. Although a single pre-training run draws substantially more energy than fine-tuning, fine-tuning is performed more frequently by many more individual actors, and thus must be accounted for when considering the energy and carbon footprint of NLP. In order to better characterize the role of fine-tuning in the landscape of energy and carbon emissions in NLP, we perform a careful empirical study of the computational costs of fine-tuning across tasks, datasets, hardware infrastructure and measurement modalities. Our experimental results allow us to place fine-tuning energy and carbon costs into perspective with respect to pre-training and inference, and outline recommendations to NLP researchers and practitioners who wish to improve their fine-tuning energy efficiency.",
        "retrieved_relevant_part": true,
        "model_answer": ":  The authors of the paper \"Energy and Carbon Considerations of Fine-Tuning BERT\" published in 2023 are Xiaorong Wang, Clara Na, Emma Strubell, Sorelle A. Friedler, and Sasha Luccioni",
        "actual_answer": "Emma Strubell",
        "exact_match": false,
        "f1_max_min_avg": [
            0.06,
            0.06,
            0.06
        ],
        "recall_max_min_avg": [
            0.5,
            0.5,
            0.5
        ]
    },
    {
        "question": "Who are the coauthors of the paper Cross-Modal Fine-Tuning: Align then Refine published by LTI faculty Graham Neubig in 2023?",
        "context": "Author: Graham Neubig Title: Cross-Modal Fine-Tuning: Align then Refine Publication year: 2023 Coauthors: Junhong Shen, Liam Li, L. Dery, Corey Staten, M. Khodak, Graham Neubig, Ameet Talwalkar Abstract: Fine-tuning large-scale pretrained models has led to tremendous progress in well-studied modalities such as vision and NLP. However, similar gains have not been observed in many other modalities due to a lack of relevant pretrained models. In this work, we propose ORCA, a general cross-modal fine-tuning framework that extends the applicability of a single large-scale pretrained model to diverse modalities. ORCA adapts to a target task via an align-then-refine workflow: given the target input, ORCA first learns an embedding network that aligns the embedded feature distribution with the pretraining modality. The pretrained model is then fine-tuned on the embedded data to exploit the knowledge shared across modalities. Through extensive experiments, we show that ORCA obtains state-of-the-art results on 3 benchmarks containing over 60 datasets from 12 modalities, outperforming a wide range of hand-designed, AutoML, general-purpose, and task-specific methods. We highlight the importance of data alignment via a series of ablation studies and demonstrate ORCA's utility in data-limited regimes.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Maarten Sap Title: Don't Take This Out of Context!: On the Need for Contextual Models and Evaluations for Stylistic Rewriting Publication year: 2023 Coauthors: Akhila Yerukola, Xuhui Zhou, Elizabeth Clark, Maarten Sap |Author: Jamie Callan Title: COILcr: Efficient Semantic Matching in Contextualized Exact Match Retrieval Publication year: 2023 Coauthors: Zhen Fan, Luyu Gao, Rohan Jha, Jamie Callan ",
        "retrieved_relevant_part": false,
        "model_answer": ":  The context provided does not contain any information about a paper titled \"Cross-Modal Fine-Tuning\" authored by Graham Neubig published in 2023. Therefore, it is not possible to provide an answer based on the context alone",
        "actual_answer": "Junhong Shen, Liam Li, L. Dery, Corey Staten, M. Khodak, Graham Neubig, Ameet Talwalkar",
        "exact_match": false,
        "f1_max_min_avg": [
            0.04,
            0.04,
            0.04
        ],
        "recall_max_min_avg": [
            0.07,
            0.07,
            0.07
        ]
    },
    {
        "question": "Which LTI faculty published the paper KEEC: Embed to Control on An Equivariant Geometry in 2023?",
        "context": "Author: Yiming Yang Title: KEEC: Embed to Control on An Equivariant Geometry Publication year: 2023 Coauthors: Xiaoyuan Cheng, Yiming Yang, Wei Jiang, Yukun Hu Abstract: This paper investigates how representation learning can enable optimal control in unknown and complex dynamics, such as chaotic and non-linear systems, without relying on prior domain knowledge of the dynamics. The core idea is to establish an equivariant geometry that is diffeomorphic to the manifold defined by a dynamical system and to perform optimal control within this corresponding geometry, which is a non-trivial task. To address this challenge, Koopman Embed to Equivariant Control (KEEC) is proposed for model learning and control. Inspired by Lie theory, KEEC begins by learning a non-linear dynamical system defined on a manifold and embedding trajectories into a Lie group. Subsequently, KEEC formulates an equivariant value function equation in reinforcement learning on the equivariant geometry, ensuring an invariant effect as the value function on the original manifold. By deriving analytical-form optimal actions on the equivariant value function, KEEC theoretically achieves quadratic convergence for the optimal equivariant value function by leveraging the differential information on the equivariant geometry. The effectiveness of KEEC is demonstrated in challenging dynamical systems, including chaotic ones like Lorenz-63. Notably, our results show that isometric functions, which maintain the compactness and completeness of geometry while preserving metric and differential information, consistently outperform loss functions lacking these characteristics.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Yonatan Bisk Assistant Professor Email: ybisk@cs.cmu.edu Office: Gates & Hillman Centers Research Areas: Grounding, RoboNLP, Vision and Language, Embodiment, Unsupervised Learning|Author: Alexander Hauptmann Title: Hyperbolic vs Euclidean Embeddings in Few-Shot Learning: Two Sides of the Same Coin Publication year: 2023 Coauthors: Gabriel Moreira, Manuel Marques, J. Costeira, Alexander Hauptmann Abstract: Recent research in representation learning has shown that hierarchical data lends itself to low-dimensional and highly informative representations in hyperbolic space. However, even if hyperbolic embeddings have gathered attention in image recognition, their optimization is prone to numerical hurdles. Further, it remains unclear which applications stand to benefit the most from the implicit bias imposed by hyperbolicity, when compared to traditional Euclidean features. In this paper, we focus on prototypical hyperbolic neural networks. In particular, the tendency of hyperbolic embeddings to converge to the boundary of the Poincar\\'e ball in high dimensions and the effect this has on few-shot classification. We show that the best few-shot results are attained for hyperbolic embeddings at a common hyperbolic radius. In contrast to prior benchmark results, we demonstrate that better performance can be achieved by a fixed-radius encoder equipped with the Euclidean metric, regardless of the embedding dimension.",
        "retrieved_relevant_part": false,
        "model_answer": ":  The context provided does not contain any information about a paper titled \"KEEC: Embed to Control on An Equivariant Geometry\" published by an LTI (Laboratory for Telecommunications and Information Technology) faculty member in 2023. Therefore, no output can be generated based on the context alone",
        "actual_answer": "Yiming Yang",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Who are the coauthors of the paper Reducing Barriers to Self-Supervised Learning: HuBERT Pre-training with Academic Compute published by LTI faculty Shinji Watanabe in 2023?",
        "context": "Author: Shinji Watanabe Title: Reducing Barriers to Self-Supervised Learning: HuBERT Pre-training with Academic Compute Publication year: 2023 Coauthors: William Chen, Xuankai Chang, Yifan Peng, Zhaoheng Ni, Soumi Maiti, Shinji Watanabe Abstract: Self-supervised learning (SSL) has led to great strides in speech processing. However, the resources needed to train these models has become prohibitively large as they continue to scale. Currently, only a few groups with substantial resources are capable of creating SSL models, which harms reproducibility. In this work, we optimize HuBERT SSL to fit in academic constraints. We reproduce HuBERT independently from the original implementation, with no performance loss. Our code and training optimizations make SSL feasible with only 8 GPUs, instead of the 32 used in the original work. We also explore a semi-supervised route, using an ASR model to skip the first pre-training iteration. Within one iteration of pre-training, our models improve over HuBERT on several tasks. Furthermore, our HuBERT Large variant requires only 8 GPUs, achieving similar performance to the original trained on 128. As our contribution to the community, all models, configurations, and code are made open-source in ESPnet.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Yonatan Bisk Assistant Professor Email: ybisk@cs.cmu.edu Office: Gates & Hillman Centers Research Areas: Grounding, RoboNLP, Vision and Language, Embodiment, Unsupervised Learning|Author: Shinji Watanabe Title: Challenges of Corporate Alliance CLOMA toward Plastic Litter Publication year: 2023 Coauthors: Shinji Watanabe ",
        "retrieved_relevant_part": false,
        "model_answer": ":  Based on the context provided, there is no paper titled \"Reducing Barriers to Self-Supervised Learning: HuBERT Pre-training\" mentioned. The given context includes two papers authored by Shinji Watanabe, one in collaboration with Yui Sudo, Muhammad Shakeel, Yifan Peng, and another solo publication. Therefore, the output should be an empty list or null since the requested information is not present in the context",
        "actual_answer": "William Chen, Xuankai Chang, Yifan Peng, Zhaoheng Ni, Soumi Maiti, Shinji Watanabe",
        "exact_match": false,
        "f1_max_min_avg": [
            0.08,
            0.08,
            0.08
        ],
        "recall_max_min_avg": [
            0.25,
            0.25,
            0.25
        ]
    },
    {
        "question": "Who are the coauthors of the paper Reasoning about the Unseen for Efficient Outdoor Object Navigation published by LTI faculty Yonatan Bisk in 2023?",
        "context": "Author: Yonatan Bisk Title: Reasoning about the Unseen for Efficient Outdoor Object Navigation Publication year: 2023 Coauthors: Quanting Xie, Tianyi Zhang, Kedi Xu, M. Johnson-Roberson, Yonatan Bisk Abstract: Robots should exist anywhere humans do: indoors, outdoors, and even unmapped environments. In contrast, the focus of recent advancements in Object Goal Navigation(OGN) has targeted navigating in indoor environments by leveraging spatial and semantic cues that do not generalize outdoors. While these contributions provide valuable insights into indoor scenarios, the broader spectrum of real-world robotic applications often extends to outdoor settings. As we transition to the vast and complex terrains of outdoor environments, new challenges emerge. Unlike the structured layouts found indoors, outdoor environments lack clear spatial delineations and are riddled with inherent semantic ambiguities. Despite this, humans navigate with ease because we can reason about the unseen. We introduce a new task OUTDOOR, a new mechanism for Large Language Models (LLMs) to accurately hallucinate possible futures, and a new computationally aware success metric for pushing research forward in this more complex domain. Additionally, we show impressive results on both a simulated drone and physical quadruped in outdoor environments. Our agent has no premapping and our formalism outperforms naive LLM-based approaches",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Louis-Philippe Morency Title: Reconstructing the neurodynamics of face perception during real world vision in humans using intracranial EEG recordings Publication year: 2023 Coauthors: Arish Alreja, Michael J. Ward, J. A. Colan, Qianli Ma, R. M. Richardson, Louis-Philippe Morency, A. Ghuman |Author: Maarten Sap Title: Don't Take This Out of Context!: On the Need for Contextual Models and Evaluations for Stylistic Rewriting Publication year: 2023 Coauthors: Akhila Yerukola, Xuhui Zhou, Elizabeth Clark, Maarten Sap ",
        "retrieved_relevant_part": false,
        "model_answer": ":  I apologize for the confusion, but there is no paper titled \"Reasoning about the Unseen for Efficient Outdoor Object Navigation\" mentioned in the provided context with an author named Yonatan Bisk or publication year 2023. The context only includes papers authored by Shinji Watanabe, Louis-Philippe Morency, and Maarten Sap, along with their respective coauthors",
        "actual_answer": "Quanting Xie, Tianyi Zhang, Kedi Xu, M. Johnson-Roberson, Yonatan Bisk",
        "exact_match": false,
        "f1_max_min_avg": [
            0.06,
            0.06,
            0.06
        ],
        "recall_max_min_avg": [
            0.2,
            0.2,
            0.2
        ]
    },
    {
        "question": "Who are the coauthors of the paper Leveraging body pose estimation for gesture recognition in human-robot interaction using synthetic data published by LTI faculty Alexander Hauptmann in 2023?",
        "context": "Author: Alexander Hauptmann Title: Leveraging body pose estimation for gesture recognition in human-robot interaction using synthetic data Publication year: 2023 Coauthors: Xiaoyu Zhu, Celso de Melo, Alexander Hauptmann Abstract: Effectively recognizing human gestures from variant viewpoints plays a fundamental role in the successful collaboration between humans and robots. Deep learning approaches have achieved promising performance in gesture recognition. However, they are usually data-hungry and require large-scale labeled data, which are not usually accessible in a practical setting. Synthetic data, on the other hand, can be easily obtained from simulators with fine-grained annotations and variant modalities. Existing state-of-the-art approaches have shown promising results using synthetic data, but there is still a large performance gap between the models trained on synthetic data and real data. To learn domain-invariant feature representations, we propose a novel approach which jointly takes RGB videos and 3D meshes as inputs to perform robust action recognition. We empirically validate our model on the RoCoG-v2 dataset, which consists of a variety of real and synthetic videos of gestures from the ground and air perspectives. We show that our model trained on synthetic data can outperform state-of-the-art models under the same training setting and models trained on real data.",
        "retrieved_doc": "Author: Louis-Philippe Morency Title: Reconstructing the neurodynamics of face perception during real world vision in humans using intracranial EEG recordings Publication year: 2023 Coauthors: Arish Alreja, Michael J. Ward, J. A. Colan, Qianli Ma, R. M. Richardson, Louis-Philippe Morency, A. Ghuman |Author: Shinji Watanabe Title: Speaker-Independent Acoustic-to-Articulatory Speech Inversion Publication year: 2023 Coauthors: Peter Wu, Li-Wei Chen, Cheol Jun Cho, Shinji Watanabe, L. Goldstein, A. Black, G. Anumanchipalli Abstract: To build speech processing methods that can handle speech as naturally as humans, researchers have explored multiple ways of building an invertible mapping from speech to an interpretable space. The articulatory space is a promising inversion target, since this space captures the mechanics of speech production. To this end, we build an acoustic-to-articulatory inversion (AAI) model that leverages autoregression, adversarial training, and self supervision to generalize to unseen speakers. Our approach obtains 0.784 correlation on an electromagnetic articulography (EMA) dataset, improving the state-of-the-art by 12.5%. Additionally, we show the interpretability of these representations through directly com-paring the behavior of estimated representations with speech production behavior. Finally, we propose a resynthesis-based AAI evaluation metric that does not rely on articulatory labels, demonstrating its efficacy with an 18-speaker dataset.|Author: Alexander Hauptmann Title: Leveraging body pose estimation for gesture recognition in human-robot interaction using synthetic data Publication year: 2023 Coauthors: Xiaoyu Zhu, Celso de Melo, Alexander Hauptmann Abstract: Effectively recognizing human gestures from variant viewpoints plays a fundamental role in the successful collaboration between humans and robots. Deep learning approaches have achieved promising performance in gesture recognition. However, they are usually data-hungry and require large-scale labeled data, which are not usually accessible in a practical setting. Synthetic data, on the other hand, can be easily obtained from simulators with fine-grained annotations and variant modalities. Existing state-of-the-art approaches have shown promising results using synthetic data, but there is still a large performance gap between the models trained on synthetic data and real data. To learn domain-invariant feature representations, we propose a novel approach which jointly takes RGB videos and 3D meshes as inputs to perform robust action recognition. We empirically validate our model on the RoCoG-v2 dataset, which consists of a variety of real and synthetic videos of gestures from the ground and air perspectives. We show that our model trained on synthetic data can outperform state-of-the-art models under the same training setting and models trained on real data.",
        "retrieved_relevant_part": true,
        "model_answer": ":  Xiaoyu Zhu and Celso de Melo were the coauthors of the paper \"Leveraging body pose estimation for gesture recognition in human-robot interaction using synthetic data\" published by Alexander Hauptmann in 2023",
        "actual_answer": "Xiaoyu Zhu, Celso de Melo, Alexander Hauptmann",
        "exact_match": false,
        "f1_max_min_avg": [
            0.26,
            0.26,
            0.26
        ],
        "recall_max_min_avg": [
            0.71,
            0.71,
            0.71
        ]
    },
    {
        "question": "Which LTI faculty published the paper The DARPA Wikidata Overlay: Wikidata as an ontology for natural language processing in 2023?",
        "context": "Author: A. Gershman Title: The DARPA Wikidata Overlay: Wikidata as an ontology for natural language processing Publication year: 2023 Coauthors: Elizabeth Spaulding, Kathryn Conger, A. Gershman, Rosario Uceda-Sosa, S. Brown, James Pustejovsky, Peter Anick, Martha Palmer Abstract: With 102,530,067 items currently in its crowd-sourced knowledge base, Wikidata provides NLP practitioners a unique and powerful resource for inference and reasoning over real-world entities. However, because Wikidata is very entity focused, events and actions are often labeled with eventive nouns (e.g., the process of diagnosing a person\u2019s illness is labeled \u201cdiagnosis\u201d), and the typical participants in an event are not described or linked to that event concept (e.g., the medical professional or patient). Motivated by a need for an adaptable, comprehensive, domain-flexible ontology for information extraction, including identifying the roles entities are playing in an event, we present a curated subset of Wikidata in which events have been enriched with PropBank roles. To enable richer narrative understanding between events from Wikidata concepts, we have also provided a comprehensive mapping from temporal Qnodes and Pnodes to the Allen Interval Temporal Logic relations.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Yonatan Bisk Assistant Professor Email: ybisk@cs.cmu.edu Office: Gates & Hillman Centers Research Areas: Grounding, RoboNLP, Vision and Language, Embodiment, Unsupervised Learning|Author: A. Gershman Title: The DARPA Wikidata Overlay: Wikidata as an ontology for natural language processing Publication year: 2023 Coauthors: Elizabeth Spaulding, Kathryn Conger, A. Gershman, Rosario Uceda-Sosa, S. Brown, James Pustejovsky, Peter Anick, Martha Palmer Abstract: With 102,530,067 items currently in its crowd-sourced knowledge base, Wikidata provides NLP practitioners a unique and powerful resource for inference and reasoning over real-world entities. However, because Wikidata is very entity focused, events and actions are often labeled with eventive nouns (e.g., the process of diagnosing a person\u2019s illness is labeled \u201cdiagnosis\u201d), and the typical participants in an event are not described or linked to that event concept (e.g., the medical professional or patient). Motivated by a need for an adaptable, comprehensive, domain-flexible ontology for information extraction, including identifying the roles entities are playing in an event, we present a curated subset of Wikidata in which events have been enriched with PropBank roles. To enable richer narrative understanding between events from Wikidata concepts, we have also provided a comprehensive mapping from temporal Qnodes and Pnodes to the Allen Interval Temporal Logic relations.",
        "retrieved_relevant_part": true,
        "model_answer": ":  A. Gershman",
        "actual_answer": "A. Gershman",
        "exact_match": false,
        "f1_max_min_avg": [
            0.8,
            0.8,
            0.8
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Who is the author of the LTI paper Divergences between Language Models and Human Brains published in 2023?",
        "context": "Author: Graham Neubig Title: Divergences between Language Models and Human Brains Publication year: 2023 Coauthors: Yuchen Zhou, Emmy Liu, Graham Neubig, Leila Wehbe Abstract: Do machines and humans process language in similar ways? Recent research has hinted in the affirmative, finding that brain signals can be effectively predicted using the internal representations of language models (LMs). Although such results are thought to reflect shared computational principles between LMs and human brains, there are also clear differences in how LMs and humans represent and use language. In this work, we systematically explore the divergences between human and machine language processing by examining the differences between LM representations and human brain responses to language as measured by Magnetoencephalography (MEG) across two datasets in which subjects read and listened to narrative stories. Using a data-driven approach, we identify two domains that are not captured well by LMs: social/emotional intelligence and physical commonsense. We then validate these domains with human behavioral experiments and show that fine-tuning LMs on these domains can improve their alignment with human brain responses.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Louis-Philippe Morency Title: Reconstructing the neurodynamics of face perception during real world vision in humans using intracranial EEG recordings Publication year: 2023 Coauthors: Arish Alreja, Michael J. Ward, J. A. Colan, Qianli Ma, R. M. Richardson, Louis-Philippe Morency, A. Ghuman |Author: Graham Neubig Title: Divergences between Language Models and Human Brains Publication year: 2023 Coauthors: Yuchen Zhou, Emmy Liu, Graham Neubig, Leila Wehbe Abstract: Do machines and humans process language in similar ways? Recent research has hinted in the affirmative, finding that brain signals can be effectively predicted using the internal representations of language models (LMs). Although such results are thought to reflect shared computational principles between LMs and human brains, there are also clear differences in how LMs and humans represent and use language. In this work, we systematically explore the divergences between human and machine language processing by examining the differences between LM representations and human brain responses to language as measured by Magnetoencephalography (MEG) across two datasets in which subjects read and listened to narrative stories. Using a data-driven approach, we identify two domains that are not captured well by LMs: social/emotional intelligence and physical commonsense. We then validate these domains with human behavioral experiments and show that fine-tuning LMs on these domains can improve their alignment with human brain responses.",
        "retrieved_relevant_part": true,
        "model_answer": ":  The authors of the LTI paper \"Divergences between Language Models and Human Brains\" published in 2023 are Graham Neubig, Yuchen Zhou, Emmy Liu, and Leila Wehbe",
        "actual_answer": "Graham Neubig",
        "exact_match": false,
        "f1_max_min_avg": [
            0.07,
            0.07,
            0.07
        ],
        "recall_max_min_avg": [
            0.5,
            0.5,
            0.5
        ]
    },
    {
        "question": "What is the abstract of the paper Active Retrieval Augmented Generation published by LTI faculty Jamie Callan in 2023?",
        "context": "Author: Jamie Callan Title: Active Retrieval Augmented Generation Publication year: 2023 Coauthors: Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig Abstract: Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.",
        "retrieved_doc": "Author: Jamie Callan Title: COILcr: Efficient Semantic Matching in Contextualized Exact Match Retrieval Publication year: 2023 Coauthors: Zhen Fan, Luyu Gao, Rohan Jha, Jamie Callan |Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Maarten Sap Title: Don't Take This Out of Context!: On the Need for Contextual Models and Evaluations for Stylistic Rewriting Publication year: 2023 Coauthors: Akhila Yerukola, Xuhui Zhou, Elizabeth Clark, Maarten Sap ",
        "retrieved_relevant_part": false,
        "model_answer": ":  I apologize for any confusion, but there is no paper titled \"Active Retrieval Augmented Generation\" mentioned in the provided context that was authored by Jamie Callan or any other author listed in the context in the year 2023. Therefore, it is not possible to provide an accurate abstract for that paper based on the information given",
        "actual_answer": "Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.",
        "exact_match": false,
        "f1_max_min_avg": [
            0.1,
            0.1,
            0.1
        ],
        "recall_max_min_avg": [
            0.07,
            0.07,
            0.07
        ]
    },
    {
        "question": "Who is the author of the LTI paper RedCoast: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU/TPUs published in 2023?",
        "context": "Author: Eric P. Xing Title: RedCoast: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU/TPUs Publication year: 2023 Coauthors: Bowen Tan, Yun Zhu, Lijuan Liu, Hongyi Wang, Yonghao Zhuang, Jindong Chen, Eric P. Xing, Zhiting Hu Abstract: The recent progress of AI can be largely attributed to large language models (LLMs). However, their escalating memory requirements introduce challenges for machine learning (ML) researchers and engineers. Addressing this requires developers to partition a large model to distribute it across multiple GPUs or TPUs. This necessitates considerable coding and intricate configuration efforts with existing model parallel tools, such as Megatron-LM, DeepSpeed, and Alpa. These tools require users' expertise in machine learning systems (MLSys), creating a bottleneck in LLM development, particularly for developers without MLSys background. In this work, we present RedCoast(Redco), a lightweight and user-friendly tool crafted to automate distributed training and inference for LLMs, as well as to simplify ML pipeline development. The design of Redco emphasizes two key aspects. Firstly, to automate model parallism, our study identifies two straightforward rules to generate tensor parallel strategies for any given LLM. Integrating these rules into Redco facilitates effortless distributed LLM training and inference, eliminating the need of additional coding or complex configurations. We demonstrate the effectiveness by applying Redco on a set of LLM architectures, such as GPT-J, LLaMA, T5, and OPT, up to the size of 66B. Secondly, we propose a mechanism that allows for the customization of diverse ML pipelines through the definition of merely three functions, avoiding redundant and formulaic code like multi-host related processing. This mechanism proves adaptable across a spectrum of ML algorithms, from foundational language modeling to complex algorithms like meta-learning and reinforcement learning. Consequently, Redco implementations exhibit much fewer code lines compared to their official counterparts.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Alexander I. Rudnicky Title: Tartan: an LLM Driven SocialBot Publication year: 2023 Coauthors: Liangze Li, Ziyuan Liu, Li-Wei Chen, Ta-Chung Chi, Alexander I. Rudnicky Abstract: We describe our work to date on using large language models (LLMs) in our Alexa Prize Socialbot. Our work has laid the groundwork for looking more closely at using LLMs in a conversational system. We also analyze common patterns in conversations our bot has had with users|Author: Shinji Watanabe Title: DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models Publication year: 2023 Coauthors: Yifan Peng, Yui Sudo, Muhammad Shakeel, Shinji Watanabe Abstract: Self-supervised learning (SSL) has achieved notable success in many speech processing tasks, but the large model size and heavy computational cost hinder the deployment. Knowledge distillation trains a small student model to mimic the behavior of a large teacher model. However, the student architecture usually needs to be manually designed and will remain fixed during training, which requires prior knowledge and can lead to suboptimal performance. Inspired by recent success of task-specific structured pruning, we propose DPHuBERT, a novel task-agnostic compression method for speech SSL based on joint distillation and pruning. Experiments on SUPERB show that DPHuBERT outperforms pure distillation methods in almost all tasks. Moreover, DPHuBERT requires little training time and performs well with limited training data, making it suitable for resource-constrained applications. Our method can also be applied to various speech SSL models. Our code and models will be publicly available.",
        "retrieved_relevant_part": false,
        "model_answer": ":  The context provided does not contain any information about the authors of the paper \"RedCoast: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU/TPUs\" published in 2023. Therefore, it is impossible to generate an accurate output based on the context alone",
        "actual_answer": "Eric P. Xing",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Who are the coauthors of the paper Analysis of Volatile Components in Dried Fruits and Branch Exudates of Schisandra chinensis with Different Fruit Colors Using GC-IMS Technology published by LTI faculty Yiming Yang in 2023?",
        "context": "Author: Yiming Yang Title: Analysis of Volatile Components in Dried Fruits and Branch Exudates of Schisandra chinensis with Different Fruit Colors Using GC-IMS Technology Publication year: 2023 Coauthors: Yiping Yan, Wenpeng Lu, Taiping Tian, Nan Shu, Yiming Yang, Shutian Fan, Xianyan Han, Yunhua Ge, Peilei Xu Abstract: To investigate the volatile components of Schisandra chinensis (Turcz.) Bail (commonly known as northern Schisandra) of different colors and to explore their similarities and differences, to identify the main flavor substances in the volatile components of the branch exudates of northern schisandra, and finally to establish a fingerprint map of the volatile components of the dried fruits and branch exudates of northern Schisandra of different colors, we used GC-IMS technology to analyze the volatile components of the dried fruits and branch exudates of three different colors of northern Schisandra and established a fingerprint spectra. The results showed that a total of 60 different volatile chemical components were identified in the branch exudates and dried fruits of Schisandra. The components of germplasm resources with different fruit colors were significantly different. The ion mobility spectrum and OPLS-DA results showed that white and yellow fruits were more similar compared to red fruits. The volatile components in dried fruits were significantly higher than those in branch exudates. After VIP (variable importance in projection) screening, 41 key volatile substances in dried fruits and 30 key volatile substances in branch exudates were obtained. After screening by odor activity value (OAV), there were 24 volatile components greater than 1 in both dried fruits and branch exudates. The most important contributing volatile substance was 3-methyl-butanal, and the most important contributing volatile substance in white fruit was (E)-2-hexenal.",
        "retrieved_doc": "Author: Eric P. Xing Title: Redco: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU/TPUs Publication year: 2023 Coauthors: Bowen Tan, Yun Zhu, Lijuan Liu, Hongyi Wang, Yonghao Zhuang, Jindong Chen, Eric P. Xing, Zhiting Hu Abstract: .|Author: Eric P. Xing Title: Redco: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU/TPUs Publication year: 2023 Coauthors: Bowen Tan, Yun Zhu, Lijuan Liu, Hongyi Wang, Yonghao Zhuang, Jindong Chen, Eric P. Xing, Zhiting Hu Abstract: .|Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe ",
        "retrieved_relevant_part": false,
        "model_answer": ":  The context provided does not contain any information about the authors or the title of the paper \"Analysis of Volatile Components in Dried Fruits and Branch Exudates of Schisandra chinensis with Different Fruit Colors Using GC-IMS Technology\" published by LTI faculty Yiming Yang in 2023. Therefore, it is not possible to extract the coauthors' names from the context",
        "actual_answer": "Yiping Yan, Wenpeng Lu, Taiping Tian, Nan Shu, Yiming Yang, Shutian Fan, Xianyan Han, Yunhua Ge, Peilei Xu",
        "exact_match": false,
        "f1_max_min_avg": [
            0.03,
            0.03,
            0.03
        ],
        "recall_max_min_avg": [
            0.06,
            0.06,
            0.06
        ]
    },
    {
        "question": "Who is the author of the LTI paper RedCoast: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU/TPUs published in 2023?",
        "context": "Author: Eric P. Xing Title: RedCoast: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU/TPUs Publication year: 2023 Coauthors: Bowen Tan, Yun Zhu, Lijuan Liu, Hongyi Wang, Yonghao Zhuang, Jindong Chen, Eric P. Xing, Zhiting Hu Abstract: The recent progress of AI can be largely attributed to large language models (LLMs). However, their escalating memory requirements introduce challenges for machine learning (ML) researchers and engineers. Addressing this requires developers to partition a large model to distribute it across multiple GPUs or TPUs. This necessitates considerable coding and intricate configuration efforts with existing model parallel tools, such as Megatron-LM, DeepSpeed, and Alpa. These tools require users' expertise in machine learning systems (MLSys), creating a bottleneck in LLM development, particularly for developers without MLSys background. In this work, we present RedCoast(Redco), a lightweight and user-friendly tool crafted to automate distributed training and inference for LLMs, as well as to simplify ML pipeline development. The design of Redco emphasizes two key aspects. Firstly, to automate model parallism, our study identifies two straightforward rules to generate tensor parallel strategies for any given LLM. Integrating these rules into Redco facilitates effortless distributed LLM training and inference, eliminating the need of additional coding or complex configurations. We demonstrate the effectiveness by applying Redco on a set of LLM architectures, such as GPT-J, LLaMA, T5, and OPT, up to the size of 66B. Secondly, we propose a mechanism that allows for the customization of diverse ML pipelines through the definition of merely three functions, avoiding redundant and formulaic code like multi-host related processing. This mechanism proves adaptable across a spectrum of ML algorithms, from foundational language modeling to complex algorithms like meta-learning and reinforcement learning. Consequently, Redco implementations exhibit much fewer code lines compared to their official counterparts.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Alexander I. Rudnicky Title: Tartan: an LLM Driven SocialBot Publication year: 2023 Coauthors: Liangze Li, Ziyuan Liu, Li-Wei Chen, Ta-Chung Chi, Alexander I. Rudnicky Abstract: We describe our work to date on using large language models (LLMs) in our Alexa Prize Socialbot. Our work has laid the groundwork for looking more closely at using LLMs in a conversational system. We also analyze common patterns in conversations our bot has had with users|Author: Shinji Watanabe Title: DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models Publication year: 2023 Coauthors: Yifan Peng, Yui Sudo, Muhammad Shakeel, Shinji Watanabe Abstract: Self-supervised learning (SSL) has achieved notable success in many speech processing tasks, but the large model size and heavy computational cost hinder the deployment. Knowledge distillation trains a small student model to mimic the behavior of a large teacher model. However, the student architecture usually needs to be manually designed and will remain fixed during training, which requires prior knowledge and can lead to suboptimal performance. Inspired by recent success of task-specific structured pruning, we propose DPHuBERT, a novel task-agnostic compression method for speech SSL based on joint distillation and pruning. Experiments on SUPERB show that DPHuBERT outperforms pure distillation methods in almost all tasks. Moreover, DPHuBERT requires little training time and performs well with limited training data, making it suitable for resource-constrained applications. Our method can also be applied to various speech SSL models. Our code and models will be publicly available.",
        "retrieved_relevant_part": false,
        "model_answer": ":  The context provided does not contain any information about the authors of the paper \"RedCoast: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU/TPUs\" published in 2023. Therefore, it is impossible to generate an accurate output based on the context alone",
        "actual_answer": "Eric P. Xing",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "What is the abstract of the paper Prompt2Model: Generating Deployable Models from Natural Language Instructions published by LTI faculty Graham Neubig in 2023?",
        "context": "Author: Graham Neubig Title: Prompt2Model: Generating Deployable Models from Natural Language Instructions Publication year: 2023 Coauthors: Vijay Viswanathan, Chenyang Zhao, Amanda Bertsch, Tongshuang Sherry Wu, Graham Neubig Abstract: Large language models (LLMs) enable system builders today to create competent NLP systems through prompting, where they only need to describe the task in natural language and provide a few examples. However, in other ways, LLMs are a step backward from traditional special-purpose NLP models; they require extensive computational resources for deployment and can be gated behind APIs. In this paper, we propose Prompt2Model, a general-purpose method that takes a natural language task description like the prompts provided to LLMs, and uses it to train a special-purpose model that is conducive to deployment. This is done through a multi-step process of retrieval of existing datasets and pretrained models, dataset generation using LLMs, and supervised fine-tuning on these retrieved and generated datasets. Over three tasks, we demonstrate that given the same few-shot prompt as input, Prompt2Model trains models that outperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20% while being up to 700 times smaller. We also show that this data can be used to obtain reliable performance estimates of model performance, enabling model developers to assess model reliability before deployment. Prompt2Model is available open-source at https://github.com/neulab/prompt2model.",
        "retrieved_doc": "Author: Maarten Sap Title: Don't Take This Out of Context!: On the Need for Contextual Models and Evaluations for Stylistic Rewriting Publication year: 2023 Coauthors: Akhila Yerukola, Xuhui Zhou, Elizabeth Clark, Maarten Sap |Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Alexander I. Rudnicky Title: Tartan: an LLM Driven SocialBot Publication year: 2023 Coauthors: Liangze Li, Ziyuan Liu, Li-Wei Chen, Ta-Chung Chi, Alexander I. Rudnicky Abstract: We describe our work to date on using large language models (LLMs) in our Alexa Prize Socialbot. Our work has laid the groundwork for looking more closely at using LLMs in a conversational system. We also analyze common patterns in conversations our bot has had with users",
        "retrieved_relevant_part": false,
        "model_answer": ":  I apologize for the confusion, but there is no paper titled \"Prompt2Model: Generating Deployable Models from Natural Language Instructions\" mentioned in the provided context. The context only includes information about three different papers, all published in 2023, and their authors, titles, publication years, and coauthors",
        "actual_answer": "Large language models (LLMs) enable system builders today to create competent NLP systems through prompting, where they only need to describe the task in natural language and provide a few examples. However, in other ways, LLMs are a step backward from traditional special-purpose NLP models",
        "exact_match": false,
        "f1_max_min_avg": [
            0.11,
            0.06,
            0.08499999999999999
        ],
        "recall_max_min_avg": [
            0.11,
            0.04,
            0.075
        ]
    },
    {
        "f1": 0.1284684684684685,
        "recall": 0.28905405405405404,
        "retrieval_acc": 0.49099099099099097
    }
]