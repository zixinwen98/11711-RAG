[
    {
        "question": "Is Jessica Maguire ssica Maguire Assistant to the Institute Director of LTI?, Please answer yes or no: ",
        "context": "Jessica Maguire Assistant to the Institute Director Email: jlm4@andrew.cmu.edu Office: 5721 Gates & Hillman Centers Phone: 412-268-5480",
        "retrieved_doc": "Nicole Perrotta Administrative Coordinator Email: nperrott@andrew.cmu.edu|Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu|and alumni. The Task Force was co-chaired by Director of Athletics Susan Bassett and Dean of Student Affairs Jennifer Church.",
        "retrieved_relevant_part": false,
        "model_answer": "Yes, Jessica Maguire is an assistant to the Institute Director of LTI.<|endoftext|>",
        "actual_answer": "Yes",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "How can I contact LTI staff Vicente Malave?",
        "context": "Vicente Malave Data Science Course Developer Email: vlm@andrew.cmu.edu",
        "retrieved_doc": "Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Daniel Gongaware Sponsored Research Administrator Email: dgongawa@cs.cmu.edu Office: 355 TCS Hall Phone: 412-268-6195|Krista McGuigan Business Director Email: klmcg@cs.cmu.edu Office: 349 TCS Hall Phone: 412-268-1635",
        "retrieved_relevant_part": false,
        "model_answer": "You can contact LTI staff Vicente Malave via email at vmalave@andrew.cmu.edu or by phone at 412-268-3669.<|endoftext|>",
        "actual_answer": "Email: vlm@andrew.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Please provide the contact information of LTI staff Susan E.",
        "context": "Susan E. Holm Mitamura Lab/Sr. Knowledge Engineer Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591",
        "retrieved_doc": "Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Nicole Perrotta Administrative Coordinator Email: nperrott@andrew.cmu.edu|Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517",
        "retrieved_relevant_part": false,
        "model_answer": "Susan E. can be contacted via email at suee@andrew.cmu.edu or by phone at 412-268-3669.<|endoftext|>",
        "actual_answer": "Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "How can I contact LTI staff Daniel Vosler?",
        "context": "Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu",
        "retrieved_doc": "Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu|Bryan Burtner Associate Business Manager Email: bburtner@cs.cmu.edu Office: 6413 TCS Hall Phone: 412-268-2805|Yukari Yamakawa Mitamura Lab/Associate Data Analyst Email: yukariy@andrew.cmu.edu Office: 6717 Gates & Hillman Centers Phone: 412-268-6596",
        "retrieved_relevant_part": true,
        "model_answer": "You can contact LTI staff Daniel Vosler via email at dvosler@andrew.cmu.edu or by phone at 412-268-2805.<|endoftext|>",
        "actual_answer": "Email: dvosler@andrew.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0.11,
            0.11,
            0.11
        ],
        "recall_max_min_avg": [
            0.5,
            0.5,
            0.5
        ]
    },
    {
        "question": "Please tell me the Email of LTI staff Daniel Vosler based on the background information: ",
        "context": "Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu",
        "retrieved_doc": "Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu|Zaid Sheikh Neubig Lab/Senior Systems/Software Engineer Email: zsheikh@andrew.cmu.edu Office: 6416 Gates & Hillman Centers Phone:|Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu",
        "retrieved_relevant_part": true,
        "model_answer": "dvosler@andrew.cmu.edu",
        "actual_answer": "Daniel Vosler's Email is dvosler@andrew.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0.33,
            0.33,
            0.33
        ],
        "recall_max_min_avg": [
            0.2,
            0.2,
            0.2
        ]
    },
    {
        "question": "What is the Email of LTI staff Casey Walker?",
        "context": "Casey Walker Academic Program Coordinator Email: clwalker@andrew.cmu.edu Phone: +1 412 268 9315",
        "retrieved_doc": "Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu|Jennifer Clark Administrative Coordinator Email: jclark2@andrew.cmu.edu|Nicole Perrotta Administrative Coordinator Email: nperrott@andrew.cmu.edu",
        "retrieved_relevant_part": false,
        "model_answer": "cwalker@andrew.cmu.edu",
        "actual_answer": "Casey Walker's Email is clwalker@andrew.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "What is the Phone of LTI staff Emma Thomas?",
        "context": "Emma Thomas Sponsored Research Administrator Email: emmat@andrew.cmu.edu Office: 5715 Gates & Hillman Centers Phone: 412-268-7812",
        "retrieved_doc": "Emma Thomas Sponsored Research Administrator Email: emmat@andrew.cmu.edu Office: 5715 Gates & Hillman Centers Phone: 412-268-7812|Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517|Jessica Maguire Assistant to the Institute Director Email: jlm4@andrew.cmu.edu Office: 5721 Gates & Hillman Centers Phone: 412-268-5480",
        "retrieved_relevant_part": true,
        "model_answer": "The Phone of LTI staff Emma Thomas is 412-268-7812.<|endoftext|>",
        "actual_answer": "412-268-7812",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Is Kate Schaich kari Yamakawa Mitamura Lab/Associate Data Analyst of LTI?, Please answer yes or no: ",
        "context": "Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_doc": "Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu|Zhi-Qi Cheng Hauptmann Lab/Postdoctoral Research Associate Email: zhiqic@andrew.cmu.edu|Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe ",
        "retrieved_relevant_part": false,
        "model_answer": "Yes",
        "actual_answer": "No",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Please tell me the Office of LTI staff Joan Axelson based on the background information: ",
        "context": "Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517",
        "retrieved_doc": "Nicole Perrotta Administrative Coordinator Email: nperrott@andrew.cmu.edu|Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering|Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu",
        "retrieved_relevant_part": false,
        "model_answer": "The Office of LTI staff Joan Axelson is located in the Gates & Hillman Centers.<|endoftext|>",
        "actual_answer": "Joan Axelson's Office is Manager",
        "exact_match": false,
        "f1_max_min_avg": [
            0.3,
            0.3,
            0.3
        ],
        "recall_max_min_avg": [
            0.6,
            0.6,
            0.6
        ]
    },
    {
        "question": "How can I contact LTI staff Yukari Yamakawa?",
        "context": "Yukari Yamakawa Mitamura Lab/Associate Data Analyst Email: yukariy@andrew.cmu.edu Office: 6717 Gates & Hillman Centers Phone: 412-268-6596",
        "retrieved_doc": "Yukari Yamakawa Mitamura Lab/Associate Data Analyst Email: yukariy@andrew.cmu.edu Office: 6717 Gates & Hillman Centers Phone: 412-268-6596|Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu|Daniel Gongaware Sponsored Research Administrator Email: dgongawa@cs.cmu.edu Office: 355 TCS Hall Phone: 412-268-6195",
        "retrieved_relevant_part": true,
        "model_answer": "You can contact LTI staff Yukari Yamakawa via email at yukariy@andrew.cmu.edu or by phone at 412-268-6596.<|endoftext|>",
        "actual_answer": "Email: yukariy@andrew.cmu.edu Office: 6717 Gates & Hillman Centers Phone: 412-268-6596",
        "exact_match": false,
        "f1_max_min_avg": [
            0.08,
            0.08,
            0.08
        ],
        "recall_max_min_avg": [
            0.1,
            0.1,
            0.1
        ]
    },
    {
        "question": "Is Nicole Perrotta cole Perrotta Administrative Coordinator of LTI?, Please answer yes or no: ",
        "context": "Nicole Perrotta Administrative Coordinator Email: nperrott@andrew.cmu.edu",
        "retrieved_doc": "Nicole Perrotta Administrative Coordinator Email: nperrott@andrew.cmu.edu|Jennifer Clark Administrative Coordinator Email: jclark2@andrew.cmu.edu|Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu",
        "retrieved_relevant_part": true,
        "model_answer": "Yes, Nicole Perrotta is the Administrative Coordinator of LTI",
        "actual_answer": "Yes",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "How can I contact LTI staff Emma Thomas?",
        "context": "Emma Thomas Sponsored Research Administrator Email: emmat@andrew.cmu.edu Office: 5715 Gates & Hillman Centers Phone: 412-268-7812",
        "retrieved_doc": "Emma Thomas Sponsored Research Administrator Email: emmat@andrew.cmu.edu Office: 5715 Gates & Hillman Centers Phone: 412-268-7812|Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517|Krista McGuigan Business Director Email: klmcg@cs.cmu.edu Office: 349 TCS Hall Phone: 412-268-1635",
        "retrieved_relevant_part": true,
        "model_answer": "Emma Thomas' email address is emmat@andrew.cmu.edu and her phone number is 412-268-7812",
        "actual_answer": "Email: emmat@andrew.cmu.edu Office: 5715 Gates & Hillman Centers Phone: 412-268-7812",
        "exact_match": false,
        "f1_max_min_avg": [
            0.18,
            0.18,
            0.18
        ],
        "recall_max_min_avg": [
            0.2,
            0.2,
            0.2
        ]
    },
    {
        "question": "Please tell me the Email of LTI staff Annette Han based on the background information: ",
        "context": "Annette Han Rose Lab/Data Science Course Developer Email: ahan2@andrew.cmu.edu",
        "retrieved_doc": "Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu|Nicole Perrotta Administrative Coordinator Email: nperrott@andrew.cmu.edu|Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu",
        "retrieved_relevant_part": false,
        "model_answer": "annette@andrew.cmu.edu",
        "actual_answer": "Annette Han's Email is ahan2@andrew.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "What is the Email of LTI staff Nicole Perrotta?",
        "context": "Nicole Perrotta Administrative Coordinator Email: nperrott@andrew.cmu.edu",
        "retrieved_doc": "Nicole Perrotta Administrative Coordinator Email: nperrott@andrew.cmu.edu|Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu|Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu",
        "retrieved_relevant_part": true,
        "model_answer": "nperrott@andrew.cmu.edu",
        "actual_answer": "nperrott@andrew.cmu.edu",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Please tell me the Email of LTI staff Daniel Vosler based on the background information: ",
        "context": "Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu",
        "retrieved_doc": "Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu|Zaid Sheikh Neubig Lab/Senior Systems/Software Engineer Email: zsheikh@andrew.cmu.edu Office: 6416 Gates & Hillman Centers Phone:|Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu",
        "retrieved_relevant_part": true,
        "model_answer": "dvosler@andrew.cmu.edu",
        "actual_answer": "dvosler@andrew.cmu.edu",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "How can I contact LTI staff Allison Blair?",
        "context": "Allison Blair Associate Business Manager Email: ablair2@cs.cmu.edu Office: 5715 TCS Hall Phone: 412-268-5489",
        "retrieved_doc": "Allison Blair Associate Business Manager Email: ablair2@cs.cmu.edu Office: 5715 TCS Hall Phone: 412-268-5489|Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669",
        "retrieved_relevant_part": true,
        "model_answer": "You can contact LTI staff Allison Blair via email at ablair2@cs.cmu.edu or by phone at 412-268-5489",
        "actual_answer": "Email: ablair2@cs.cmu.edu Office: 5715 TCS Hall Phone: 412-268-5489",
        "exact_match": false,
        "f1_max_min_avg": [
            0.17,
            0.17,
            0.17
        ],
        "recall_max_min_avg": [
            0.25,
            0.25,
            0.25
        ]
    },
    {
        "question": "Is Caitlin Korpus itlin Korpus Senior Academic Program Coordinator of LTI?, Please answer yes or no: ",
        "context": "Caitlin Korpus Senior Academic Program Coordinator Email: ckorpus@andrew.cmu.edu Office: 6719 Gates & Hillman Centers Phone: +1 412 268 7096",
        "retrieved_doc": "Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu|Yonatan Bisk Assistant Professor Email: ybisk@cs.cmu.edu Office: Gates & Hillman Centers Research Areas: Grounding, RoboNLP, Vision and Language, Embodiment, Unsupervised Learning|Nicole Perrotta Administrative Coordinator Email: nperrott@andrew.cmu.edu",
        "retrieved_relevant_part": false,
        "model_answer": "Yes",
        "actual_answer": "Yes",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Please provide the contact information of LTI staff Susan E.",
        "context": "Susan E. Holm Mitamura Lab/Sr. Knowledge Engineer Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591",
        "retrieved_doc": "Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Nicole Perrotta Administrative Coordinator Email: nperrott@andrew.cmu.edu|Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517",
        "retrieved_relevant_part": false,
        "model_answer": "Susan E. can be contacted via email at suee@andrew.cmu.edu or by phone at 412-268-3669.<|endoftext|>",
        "actual_answer": "Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "How can I contact LTI staff Daniel Vosler?",
        "context": "Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu",
        "retrieved_doc": "Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu|Bryan Burtner Associate Business Manager Email: bburtner@cs.cmu.edu Office: 6413 TCS Hall Phone: 412-268-2805|Yukari Yamakawa Mitamura Lab/Associate Data Analyst Email: yukariy@andrew.cmu.edu Office: 6717 Gates & Hillman Centers Phone: 412-268-6596",
        "retrieved_relevant_part": true,
        "model_answer": "You can contact LTI staff Daniel Vosler via email at dvosler@andrew.cmu.edu or by phone at 412-268-2805.<|endoftext|>",
        "actual_answer": "Email: dvosler@andrew.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0.11,
            0.11,
            0.11
        ],
        "recall_max_min_avg": [
            0.5,
            0.5,
            0.5
        ]
    },
    {
        "question": "What is the Phone of LTI staff Brianna Eriksen?",
        "context": "Brianna Eriksen Academic Program Manager Email: bfreema2@andrew.cmu.edu Phone: 412-268-4277",
        "retrieved_doc": "Brianna Eriksen Academic Program Manager Email: bfreema2@andrew.cmu.edu Phone: 412-268-4277|Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": "The Phone of LTI staff Brianna Eriksen is 412-268-4788.<|endoftext|>",
        "actual_answer": "Brianna Eriksen's Phone is 412-268-4277",
        "exact_match": false,
        "f1_max_min_avg": [
            0.43,
            0.43,
            0.43
        ],
        "recall_max_min_avg": [
            0.6,
            0.6,
            0.6
        ]
    },
    {
        "question": "Please tell me the Phone of LTI staff John Friday based on the background information: ",
        "context": "John Friday Sr. Administrative Coordinator Email: jfriday@andrew.cmu.edu Office: 5715 Gates & Hillman Centers Phone: 412-268-1810",
        "retrieved_doc": "Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering|John Friday Sr. Administrative Coordinator Email: jfriday@andrew.cmu.edu Office: 5715 Gates & Hillman Centers Phone: 412-268-1810|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669",
        "retrieved_relevant_part": true,
        "model_answer": "412-268-8259",
        "actual_answer": "412-268-1810",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "What is the Phone of LTI staff Susan E.?",
        "context": "Susan E. Holm Mitamura Lab/Sr. Knowledge Engineer Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591",
        "retrieved_doc": "Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517|Susan E. Holm Mitamura Lab/Sr. Knowledge Engineer Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591|Jessica Maguire Assistant to the Institute Director Email: jlm4@andrew.cmu.edu Office: 5721 Gates & Hillman Centers Phone: 412-268-5480",
        "retrieved_relevant_part": true,
        "model_answer": "The phone number for Susan E. is 412-268-7517",
        "actual_answer": "412-268-6591",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Is Stacey Young umi Maiti Watanabe Lab/Postdoctoral Research Associate of LTI?, Please answer yes or no: ",
        "context": "Stacey Young Academic Program Manager \u2014 Ph.D. Email: staceyy@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-2623",
        "retrieved_doc": "Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu|Zhi-Qi Cheng Hauptmann Lab/Postdoctoral Research Associate Email: zhiqic@andrew.cmu.edu|Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe ",
        "retrieved_relevant_part": false,
        "model_answer": "Yes",
        "actual_answer": "No",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Please provide the contact information of LTI staff Amber Vivis",
        "context": "Amber Vivis Sr. Academic Program Manager - MSAII Email: avivis@cs.cmu.edu Office: 357 TCS Hall Phone: 412-268-9998",
        "retrieved_doc": "Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Amber Vivis Sr. Academic Program Manager - MSAII Email: avivis@cs.cmu.edu Office: 357 TCS Hall Phone: 412-268-9998|Nicole Perrotta Administrative Coordinator Email: nperrott@andrew.cmu.edu",
        "retrieved_relevant_part": true,
        "model_answer": "Amber Vivis' email address is avivis@cs.cmu.edu, phone number is 412-268-3669, and office location is in TCS Hall.<|endoftext|>",
        "actual_answer": "Email: avivis@cs.cmu.edu Office: 357 TCS Hall Phone: 412-268-9998",
        "exact_match": false,
        "f1_max_min_avg": [
            0.08,
            0.08,
            0.08
        ],
        "recall_max_min_avg": [
            0.12,
            0.12,
            0.12
        ]
    },
    {
        "question": "Please provide the contact information of LTI staff Joan Axelson",
        "context": "Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517",
        "retrieved_doc": "Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Krista McGuigan Business Director Email: klmcg@cs.cmu.edu Office: 349 TCS Hall Phone: 412-268-1635",
        "retrieved_relevant_part": true,
        "model_answer": "Joan Axelson's email address is jaxelson@andrew.cmu.edu, and her phone number is 412-268-7517",
        "actual_answer": "Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517",
        "exact_match": false,
        "f1_max_min_avg": [
            0.12,
            0.12,
            0.12
        ],
        "recall_max_min_avg": [
            0.25,
            0.25,
            0.25
        ]
    },
    {
        "question": "Is Wenhe Liu hn Friday Sr. Administrative Coordinator of LTI?, Please answer yes or no: ",
        "context": "Wenhe Liu Research Scholar Email: wenhel@andrew.cmu.edu Office: 5403 Gates & Hillman Centers",
        "retrieved_doc": "Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu|Author: Yiming Yang Title: Distributed cooperative dual closed loop velocity-attitude consensus controller for rendezvous of the underactuated AUV swarm in 3-dimensional space Publication year: 2023 Coauthors: Yu Zhang, W. Zhang, G. Xia, Yiming Yang, Yan-luan Zheng, Peiyu Han |Wenhe Liu Research Scholar Email: wenhel@andrew.cmu.edu Office: 5403 Gates & Hillman Centers",
        "retrieved_relevant_part": true,
        "model_answer": "Yes",
        "actual_answer": "No",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "How can I contact LTI staff Brianna Eriksen?",
        "context": "Brianna Eriksen Academic Program Manager Email: bfreema2@andrew.cmu.edu Phone: 412-268-4277",
        "retrieved_doc": "Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517|Brianna Eriksen Academic Program Manager Email: bfreema2@andrew.cmu.edu Phone: 412-268-4277|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": "You can contact Brianna Eriksen via email at bfreema2@andrew.cmu.edu or by phone at 412-268-4277.<|endoftext|>",
        "actual_answer": "Email: bfreema2@andrew.cmu.edu Phone: 412-268-4277",
        "exact_match": false,
        "f1_max_min_avg": [
            0.11,
            0.11,
            0.11
        ],
        "recall_max_min_avg": [
            0.25,
            0.25,
            0.25
        ]
    },
    {
        "question": "What is the Phone of LTI staff Kate Schaich?",
        "context": "Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_doc": "Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788|Jessica Maguire Assistant to the Institute Director Email: jlm4@andrew.cmu.edu Office: 5721 Gates & Hillman Centers Phone: 412-268-5480|Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517",
        "retrieved_relevant_part": true,
        "model_answer": "The Phone of LTI staff Kate Schaich is 412-268-4788.<|endoftext|>",
        "actual_answer": "Kate Schaich's Phone is 412-268-4788",
        "exact_match": false,
        "f1_max_min_avg": [
            0.43,
            0.43,
            0.43
        ],
        "recall_max_min_avg": [
            0.6,
            0.6,
            0.6
        ]
    },
    {
        "question": "Is Zaid Sheikh i-Qi Cheng Hauptmann Lab/Postdoctoral Research Associate of LTI?, Please answer yes or no: ",
        "context": "Zaid Sheikh Neubig Lab/Senior Systems/Software Engineer Email: zsheikh@andrew.cmu.edu Office: 6416 Gates & Hillman Centers Phone:",
        "retrieved_doc": "Zhi-Qi Cheng Hauptmann Lab/Postdoctoral Research Associate Email: zhiqic@andrew.cmu.edu|Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu|Zaid Sheikh Neubig Lab/Senior Systems/Software Engineer Email: zsheikh@andrew.cmu.edu Office: 6416 Gates & Hillman Centers Phone:",
        "retrieved_relevant_part": true,
        "model_answer": "Yes, Zaid Sheikh is an employee of Carnegie Mellon University and holds a postdoctoral research associate position at the LTI.<|endoftext|>",
        "actual_answer": "No",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "What is the Email of LTI staff Joan Axelson?",
        "context": "Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517",
        "retrieved_doc": "Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu|Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517|Nicole Perrotta Administrative Coordinator Email: nperrott@andrew.cmu.edu",
        "retrieved_relevant_part": true,
        "model_answer": "jaxelson@andrew.cmu.edu",
        "actual_answer": "jaxelson@andrew.cmu.edu",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Please provide the contact information of LTI staff Annette Han",
        "context": "Annette Han Rose Lab/Data Science Course Developer Email: ahan2@andrew.cmu.edu",
        "retrieved_doc": "Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Nicole Perrotta Administrative Coordinator Email: nperrott@andrew.cmu.edu|Susan E. Holm Mitamura Lab/Sr. Knowledge Engineer Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591",
        "retrieved_relevant_part": false,
        "model_answer": "Annette Han's email address is annette@andrew.cmu.edu, phone number is 412-268-3669, and office location is in Wean Hall.<|endoftext|>",
        "actual_answer": "Email: ahan2@andrew.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Please provide the contact information of LTI staff Kira Sullivan",
        "context": "Kira Sullivan Administrative Coordinator Email: kiras@andrew.cmu.edu Phone: 412-268-8737",
        "retrieved_doc": "Kira Sullivan Administrative Coordinator Email: kiras@andrew.cmu.edu Phone: 412-268-8737|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Amber Vivis Sr. Academic Program Manager - MSAII Email: avivis@cs.cmu.edu Office: 357 TCS Hall Phone: 412-268-9998",
        "retrieved_relevant_part": true,
        "model_answer": "Kira Sullivan's email address is kiras@andrew.cmu.edu, phone number is 412-268-8737, and office location is in TCS Hall.<|endoftext|>",
        "actual_answer": "Email: kiras@andrew.cmu.edu Phone: 412-268-8737",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Please tell me the Phone of LTI staff Yukari Yamakawa based on the background information: ",
        "context": "Yukari Yamakawa Mitamura Lab/Associate Data Analyst Email: yukariy@andrew.cmu.edu Office: 6717 Gates & Hillman Centers Phone: 412-268-6596",
        "retrieved_doc": "Yukari Yamakawa Mitamura Lab/Associate Data Analyst Email: yukariy@andrew.cmu.edu Office: 6717 Gates & Hillman Centers Phone: 412-268-6596|Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu|Zaid Sheikh Neubig Lab/Senior Systems/Software Engineer Email: zsheikh@andrew.cmu.edu Office: 6416 Gates & Hillman Centers Phone:",
        "retrieved_relevant_part": true,
        "model_answer": "The phone number for Yukari Yamakawa is 412-268-6596.<|endoftext|>",
        "actual_answer": "Yukari Yamakawa's Phone is 412-268-6596",
        "exact_match": false,
        "f1_max_min_avg": [
            0.31,
            0.31,
            0.31
        ],
        "recall_max_min_avg": [
            0.4,
            0.4,
            0.4
        ]
    },
    {
        "question": "Please tell me the Email of LTI staff Robin Hammer based on the background information: ",
        "context": "Robin Hammer Levin Lab/Project Coordinator Email: robinham@andrew.cmu.edu",
        "retrieved_doc": "Robin Hammer Levin Lab/Project Coordinator Email: robinham@andrew.cmu.edu|Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu|Zaid Sheikh Neubig Lab/Senior Systems/Software Engineer Email: zsheikh@andrew.cmu.edu Office: 6416 Gates & Hillman Centers Phone:",
        "retrieved_relevant_part": true,
        "model_answer": "robinham@andrew.cmu.edu",
        "actual_answer": "robinham@andrew.cmu.edu",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "How can I contact LTI staff Susan E.?",
        "context": "Susan E. Holm Mitamura Lab/Sr. Knowledge Engineer Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591",
        "retrieved_doc": "Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517|Hannah Rodriguez Sponsored Research Administrator Email: hmuczyns@andrew.cmu.edu Office: 6413 TCS Hall Phone: 412-268-3519|Krista McGuigan Business Director Email: klmcg@cs.cmu.edu Office: 349 TCS Hall Phone: 412-268-1635",
        "retrieved_relevant_part": false,
        "model_answer": "You can contact LTI staff Susan E. via email at saxelson@andrew.cmu.edu or by phone at 412-268-7517",
        "actual_answer": "Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "What is the Email of LTI staff Zhi-Qi Cheng?",
        "context": "Zhi-Qi Cheng Hauptmann Lab/Postdoctoral Research Associate Email: zhiqic@andrew.cmu.edu",
        "retrieved_doc": "Zhi-Qi Cheng Hauptmann Lab/Postdoctoral Research Associate Email: zhiqic@andrew.cmu.edu|Zhong-Qiu Wang Wantanabe Lab/Postdoctoral Research Associate Email: zhongqiu@andrew.cmu.edu Office: 6414 Gates & Hillman Centers Phone: 415-200-8662|Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu",
        "retrieved_relevant_part": true,
        "model_answer": "zhiqic@andrew.cmu.edu",
        "actual_answer": "Zhi-Qi Cheng's Email is zhiqic@andrew.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0.33,
            0.33,
            0.33
        ],
        "recall_max_min_avg": [
            0.2,
            0.2,
            0.2
        ]
    },
    {
        "question": "Is Jessica Maguire san E. Holm Mitamura Lab/Sr. Knowledge Engineer of LTI?, Please answer yes or no: ",
        "context": "Jessica Maguire Assistant to the Institute Director Email: jlm4@andrew.cmu.edu Office: 5721 Gates & Hillman Centers Phone: 412-268-5480",
        "retrieved_doc": "Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu|Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: David R. Mortensen Title: Multilingual TTS Accent Impressions for Accented ASR Publication year: 2023 Coauthors: G. Karakasidis, Nathaniel R. Robinson, Yaroslav Getman, Atieno Ogayo, Ragheb Al-Ghezi, Ananya Ayasi, Shinji Watanabe, David R. Mortensen, M. Kurimo ",
        "retrieved_relevant_part": false,
        "model_answer": "Yes",
        "actual_answer": "No",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Is Susan E. san E. Holm Mitamura Lab/Sr. Knowledge Engineer of LTI?, Please answer yes or no: ",
        "context": "Susan E. Holm Mitamura Lab/Sr. Knowledge Engineer Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591",
        "retrieved_doc": "Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu|Susan E. Holm Mitamura Lab/Sr. Knowledge Engineer Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591|Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe ",
        "retrieved_relevant_part": true,
        "model_answer": "Yes",
        "actual_answer": "Yes",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Is Vicente Malave cente Malave Data Science Course Developer of LTI?, Please answer yes or no: ",
        "context": "Vicente Malave Data Science Course Developer Email: vlm@andrew.cmu.edu",
        "retrieved_doc": "Vicente Malave Data Science Course Developer Email: vlm@andrew.cmu.edu|Annette Han Rose Lab/Data Science Course Developer Email: ahan2@andrew.cmu.edu|Author: Maarten Sap Title: Don't Take This Out of Context!: On the Need for Contextual Models and Evaluations for Stylistic Rewriting Publication year: 2023 Coauthors: Akhila Yerukola, Xuhui Zhou, Elizabeth Clark, Maarten Sap ",
        "retrieved_relevant_part": true,
        "model_answer": "Yes",
        "actual_answer": "Yes",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "What is the Email of LTI staff Julie Nys?",
        "context": "Julie Nys Employment Processes Manager Email: jnys@andrew.cmu.edu Office: 5405 TCS Hall Phone: 412-268-3515",
        "retrieved_doc": "Nicole Perrotta Administrative Coordinator Email: nperrott@andrew.cmu.edu|Jennifer Clark Administrative Coordinator Email: jclark2@andrew.cmu.edu|Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu",
        "retrieved_relevant_part": false,
        "model_answer": "jnys@andrew.cmu.edu",
        "actual_answer": "Julie Nys's Email is jnys@andrew.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0.33,
            0.33,
            0.33
        ],
        "recall_max_min_avg": [
            0.2,
            0.2,
            0.2
        ]
    },
    {
        "question": "Is Casey Walker nnifer Clark Administrative Coordinator of LTI?, Please answer yes or no: ",
        "context": "Casey Walker Academic Program Coordinator Email: clwalker@andrew.cmu.edu Phone: +1 412 268 9315",
        "retrieved_doc": "Nicole Perrotta Administrative Coordinator Email: nperrott@andrew.cmu.edu|Jennifer Clark Administrative Coordinator Email: jclark2@andrew.cmu.edu|Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu",
        "retrieved_relevant_part": false,
        "model_answer": "No, Casey Walker is not an administrative coordinator of LTI.<|endoftext|>",
        "actual_answer": "No",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "How can I contact LTI staff Daniel Gongaware?",
        "context": "Daniel Gongaware Sponsored Research Administrator Email: dgongawa@cs.cmu.edu Office: 355 TCS Hall Phone: 412-268-6195",
        "retrieved_doc": "Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu|Daniel Gongaware Sponsored Research Administrator Email: dgongawa@cs.cmu.edu Office: 355 TCS Hall Phone: 412-268-6195|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669",
        "retrieved_relevant_part": true,
        "model_answer": "You can contact LTI staff Daniel Gongaware via email at dgongawa@cs.cmu.edu or by phone at 412-268-6195.<|endoftext|>",
        "actual_answer": "Email: dgongawa@cs.cmu.edu Office: 355 TCS Hall Phone: 412-268-6195",
        "exact_match": false,
        "f1_max_min_avg": [
            0.08,
            0.08,
            0.08
        ],
        "recall_max_min_avg": [
            0.12,
            0.12,
            0.12
        ]
    },
    {
        "question": "Please tell me the Office of LTI staff Bryan Burtner based on the background information: ",
        "context": "Bryan Burtner Associate Business Manager Email: bburtner@cs.cmu.edu Office: 6413 TCS Hall Phone: 412-268-2805",
        "retrieved_doc": "Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu|Nicole Perrotta Administrative Coordinator Email: nperrott@andrew.cmu.edu|Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering",
        "retrieved_relevant_part": false,
        "model_answer": "The Office of LTI staff Bryan Burtner is located in the Gates & Hillman Centers.<|endoftext|>",
        "actual_answer": "Bryan Burtner's Office is 6413 TCS Hall",
        "exact_match": false,
        "f1_max_min_avg": [
            0.27,
            0.27,
            0.27
        ],
        "recall_max_min_avg": [
            0.43,
            0.43,
            0.43
        ]
    },
    {
        "question": "How can I contact LTI staff Kira Sullivan?",
        "context": "Kira Sullivan Administrative Coordinator Email: kiras@andrew.cmu.edu Phone: 412-268-8737",
        "retrieved_doc": "Kira Sullivan Administrative Coordinator Email: kiras@andrew.cmu.edu Phone: 412-268-8737|Krista McGuigan Business Director Email: klmcg@cs.cmu.edu Office: 349 TCS Hall Phone: 412-268-1635|Amber Vivis Sr. Academic Program Manager - MSAII Email: avivis@cs.cmu.edu Office: 357 TCS Hall Phone: 412-268-9998",
        "retrieved_relevant_part": true,
        "model_answer": "You can contact LTI staff Kira Sullivan via email at kiras@andrew.cmu.edu or by phone at 412-268-8737",
        "actual_answer": "Email: kiras@andrew.cmu.edu Phone: 412-268-8737",
        "exact_match": false,
        "f1_max_min_avg": [
            0.2,
            0.2,
            0.2
        ],
        "recall_max_min_avg": [
            0.5,
            0.5,
            0.5
        ]
    },
    {
        "question": "What is the Email of LTI staff Zhong-Qiu Wang?",
        "context": "Zhong-Qiu Wang Wantanabe Lab/Postdoctoral Research Associate Email: zhongqiu@andrew.cmu.edu Office: 6414 Gates & Hillman Centers Phone: 415-200-8662",
        "retrieved_doc": "Zhi-Qi Cheng Hauptmann Lab/Postdoctoral Research Associate Email: zhiqic@andrew.cmu.edu|Zhong-Qiu Wang Wantanabe Lab/Postdoctoral Research Associate Email: zhongqiu@andrew.cmu.edu Office: 6414 Gates & Hillman Centers Phone: 415-200-8662|Wenhe Liu Research Scholar Email: wenhel@andrew.cmu.edu Office: 5403 Gates & Hillman Centers",
        "retrieved_relevant_part": true,
        "model_answer": "zhongqiu@andrew.cmu.edu",
        "actual_answer": "zhongqiu@andrew.cmu.edu",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "What is the Email of LTI staff Wenhe Liu?",
        "context": "Wenhe Liu Research Scholar Email: wenhel@andrew.cmu.edu Office: 5403 Gates & Hillman Centers",
        "retrieved_doc": "Wenhe Liu Research Scholar Email: wenhel@andrew.cmu.edu Office: 5403 Gates & Hillman Centers|Zhi-Qi Cheng Hauptmann Lab/Postdoctoral Research Associate Email: zhiqic@andrew.cmu.edu|Zhong-Qiu Wang Wantanabe Lab/Postdoctoral Research Associate Email: zhongqiu@andrew.cmu.edu Office: 6414 Gates & Hillman Centers Phone: 415-200-8662",
        "retrieved_relevant_part": true,
        "model_answer": "Wenhe Liu's email address is wenhel@andrew.cmu.edu",
        "actual_answer": "Wenhe Liu's Email is wenhel@andrew.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0.73,
            0.73,
            0.73
        ],
        "recall_max_min_avg": [
            0.8,
            0.8,
            0.8
        ]
    },
    {
        "question": "What is the Phone of LTI staff Stacey Young?",
        "context": "Stacey Young Academic Program Manager \u2014 Ph.D. Email: staceyy@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-2623",
        "retrieved_doc": "Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517|Stacey Young Academic Program Manager \u2014 Ph.D. Email: staceyy@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-2623|Jessica Maguire Assistant to the Institute Director Email: jlm4@andrew.cmu.edu Office: 5721 Gates & Hillman Centers Phone: 412-268-5480",
        "retrieved_relevant_part": true,
        "model_answer": "Stacey Young's phone number is 412-268-7517",
        "actual_answer": "Stacey Young's Phone is 412-268-2623",
        "exact_match": false,
        "f1_max_min_avg": [
            0.55,
            0.55,
            0.55
        ],
        "recall_max_min_avg": [
            0.6,
            0.6,
            0.6
        ]
    },
    {
        "question": "Please tell me the Email of LTI staff Joan Axelson based on the background information: ",
        "context": "Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517",
        "retrieved_doc": "Nicole Perrotta Administrative Coordinator Email: nperrott@andrew.cmu.edu|Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu|Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu",
        "retrieved_relevant_part": false,
        "model_answer": "Joan Axelson's email address is not mentioned in the context, so I cannot provide it.<|endoftext|>",
        "actual_answer": "jaxelson@andrew.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Please provide the contact information of LTI staff Jennifer Lucas",
        "context": "Jennifer Lucas Academic Program Manager \u2014 MCDS Email: jmlucas@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-9870",
        "retrieved_doc": "Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Nicole Perrotta Administrative Coordinator Email: nperrott@andrew.cmu.edu|Krista McGuigan Business Director Email: klmcg@cs.cmu.edu Office: 349 TCS Hall Phone: 412-268-1635",
        "retrieved_relevant_part": false,
        "model_answer": "Jennifer Lucas' email address is jlucas@andrew.cmu.edu, phone number is 412-268-3669, and office location is in TCS Hall.<|endoftext|>",
        "actual_answer": "Email: jmlucas@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-9870",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "What is the Phone of LTI staff Allison Blair?",
        "context": "Allison Blair Associate Business Manager Email: ablair2@cs.cmu.edu Office: 5715 TCS Hall Phone: 412-268-5489",
        "retrieved_doc": "Allison Blair Associate Business Manager Email: ablair2@cs.cmu.edu Office: 5715 TCS Hall Phone: 412-268-5489|Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517|Jessica Maguire Assistant to the Institute Director Email: jlm4@andrew.cmu.edu Office: 5721 Gates & Hillman Centers Phone: 412-268-5480",
        "retrieved_relevant_part": true,
        "model_answer": "412-268-5489",
        "actual_answer": "412-268-5489",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Please provide the contact information of LTI faculty Kemal Oflazer",
        "context": "Kemal Oflazer Teaching Professor of Computer Science Email: ko@qatar.cmu.edu Office: 1009 Carnegie Mellon - Qatar Campus Phone:",
        "retrieved_doc": "Kemal Oflazer Teaching Professor of Computer Science Email: ko@qatar.cmu.edu Office: 1009 Carnegie Mellon - Qatar Campus Phone:|Zaid Sheikh Neubig Lab/Senior Systems/Software Engineer Email: zsheikh@andrew.cmu.edu Office: 6416 Gates & Hillman Centers Phone:|Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu",
        "retrieved_relevant_part": true,
        "model_answer": "Kemal Oflazer's email address is ko@qatar.cmu.edu, office location is at 1009 Carnegie Mellon - Qatar Campus, and phone number is Zaid Sheikh Neubig",
        "actual_answer": "Email: ko@qatar.cmu.edu Office: 1009 Carnegie Mellon - Qatar Campus Phone:",
        "exact_match": false,
        "f1_max_min_avg": [
            0.3,
            0.3,
            0.3
        ],
        "recall_max_min_avg": [
            0.5,
            0.5,
            0.5
        ]
    },
    {
        "question": "Is Teruko Mitamura ruko Mitamura Research Professor of LTI?, Please answer yes or no: ",
        "context": "Teruko Mitamura Research Professor Email: teruko@cs.cmu.edu Office: 6711 Gates & Hillman Centers Phone: 412-268-6596 Research Areas: Information Extraction, Summarization and Question Answering, Information Retrieval, Text Mining and Analytics, Language Technologies for Education, Natural Language Processing and Computational Linguistics",
        "retrieved_doc": "Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu|Author: Shinji Watanabe Title: Unsupervised Data Selection for TTS: Using Arabic Broadcast News as a Case Study Publication year: 2023 Coauthors: Massa Baali, Tomoki Hayashi, Hamdy Mubarak, Soumi Maiti, Shinji Watanabe, W. El-Hajj, Ahmed Ali Abstract: .|Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe ",
        "retrieved_relevant_part": false,
        "model_answer": "Yes",
        "actual_answer": "Yes",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Is Chenyan Xiong i Li Assistant Professor of LTI?, Please answer yes or no: ",
        "context": "Chenyan Xiong Associate Professor Email: cx@andrew.cmu.edu Phone: 412-268-7641",
        "retrieved_doc": "Zhi-Qi Cheng Hauptmann Lab/Postdoctoral Research Associate Email: zhiqic@andrew.cmu.edu|Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu|Author: Chenyan Xiong Title: CompleQA: Benchmarking the Impacts of Knowledge Graph Completion Methods on Question Answering Publication year: 2023 Coauthors: Donghan Yu, Yu Gu, Chenyan Xiong, Yiming Yang ",
        "retrieved_relevant_part": false,
        "model_answer": "No",
        "actual_answer": "No",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "How can I contact LTI faculty Eric Nyberg?",
        "context": "Eric Nyberg Professor Email: ehn@cs.cmu.edu Office: 6715 Gates & Hillman Centers Phone: 412-268-7281 Research Areas: Information Extraction, Summarization and Question Answering, Information Retrieval, Text Mining and Analytics, Language Technologies for Education",
        "retrieved_doc": "Eric P. Xing Professor (On Leave) Email: epxing@andrew.cmu.edu Office: 8101 Gates & Hillman Centers Phone: 412-268-2559|Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering|Alexander Rudnicky Research Professor Emeritus Email: alex.rudnicky@cs.cmu.edu Office: 6511 Gates & Hillman Centers Phone: 412-268-2622 Research Areas: Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing",
        "retrieved_relevant_part": false,
        "model_answer": "You can contact LTI faculty Eric Nyberg via email at nyberg@cs.cmu.edu or by phone at 412-268-2559.<|endoftext|>",
        "actual_answer": "Email: ehn@cs.cmu.edu Office: 6715 Gates & Hillman Centers Phone: 412-268-7281",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Please provide the contact information of LTI faculty Bhiksha Ramakrishnan",
        "context": "Bhiksha Ramakrishnan Professor Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826 Research Areas: Machine Learning, Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing, Privacy",
        "retrieved_doc": "Bhiksha Ramakrishnan Professor Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826 Research Areas: Machine Learning, Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing, Privacy|Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669",
        "retrieved_relevant_part": true,
        "model_answer": "Bhiksha Ramakrishnan's email address is bhiksha@cs.cmu.edu, office location is at 6705 Gates & Hillman Centers, phone number is 412-268-9826",
        "actual_answer": "Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826",
        "exact_match": false,
        "f1_max_min_avg": [
            0.34,
            0.34,
            0.34
        ],
        "recall_max_min_avg": [
            0.5,
            0.5,
            0.5
        ]
    },
    {
        "question": "Is Bhiksha Ramakrishnan iksha Ramakrishnan Professor of LTI?, Please answer yes or no: ",
        "context": "Bhiksha Ramakrishnan Professor Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826 Research Areas: Machine Learning, Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing, Privacy",
        "retrieved_doc": "Bhiksha Ramakrishnan Professor Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826 Research Areas: Machine Learning, Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing, Privacy|Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu|Yonatan Bisk Assistant Professor Email: ybisk@cs.cmu.edu Office: Gates & Hillman Centers Research Areas: Grounding, RoboNLP, Vision and Language, Embodiment, Unsupervised Learning",
        "retrieved_relevant_part": true,
        "model_answer": "Yes",
        "actual_answer": "Yes",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "What is the Office of LTI faculty Lori Levin?",
        "context": "Lori Levin Research Professor Email: lsl@cs.cmu.edu Office: 5717 Gates & Hillman Centers Phone: 412-268-6193 Research Areas: Machine Translation, Natural Language Processing and Computational Linguistics, Corpus Annotation and Resources",
        "retrieved_doc": "and alumni. The Task Force was co-chaired by Director of Athletics Susan Bassett and Dean of Student Affairs Jennifer Church.|Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu|Nicole Perrotta Administrative Coordinator Email: nperrott@andrew.cmu.edu",
        "retrieved_relevant_part": false,
        "model_answer": "The Office of LTI faculty Lori Levin is a department within Carnegie Mellon University that provides support for faculty members in various areas, including research, teaching, and professional development",
        "actual_answer": "Lori Levin's Office is 5717 Gates & Hillman Centers",
        "exact_match": false,
        "f1_max_min_avg": [
            0.16,
            0.16,
            0.16
        ],
        "recall_max_min_avg": [
            0.33,
            0.33,
            0.33
        ]
    },
    {
        "question": "How can I contact LTI faculty Emma Strubell?",
        "context": "Emma Strubell Assistant Professor Email: estrubel@andrew.cmu.edu Office: Gates & Hillman Centers",
        "retrieved_doc": "Emma Strubell Assistant Professor Email: estrubel@andrew.cmu.edu Office: Gates & Hillman Centers|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788|Justine Cassell Professor (On Leave) Email: jcassell@andrew.cmu.edu Office: 5107 Gates & Hillman Centers Phone: 412-204-6268",
        "retrieved_relevant_part": true,
        "model_answer": "You can contact Emma Strubell via email at estrubel@andrew.cmu.edu or by phone at 412-268-4788",
        "actual_answer": "Email: estrubel@andrew.cmu.edu Office: Gates & Hillman Centers",
        "exact_match": false,
        "f1_max_min_avg": [
            0.1,
            0.1,
            0.1
        ],
        "recall_max_min_avg": [
            0.14,
            0.14,
            0.14
        ]
    },
    {
        "question": "What is the Research Areas of LTI faculty Daniel Fried?",
        "context": "Daniel Fried Assistant Professor Email: dfried@andrew.cmu.edu Research Areas: Natural Language Processing: Language and Code, Conversational AI, Intelligent Agents, and Dialogue, Discourse and Pragmatics, Multimodal AI",
        "retrieved_doc": "Daniel Fried Assistant Professor Email: dfried@andrew.cmu.edu Research Areas: Natural Language Processing: Language and Code, Conversational AI, Intelligent Agents, and Dialogue, Discourse and Pragmatics, Multimodal AI|Graham Neubig Associate Professor Email: gneubig@cs.cmu.edu Office: 5409 Gates & Hillman Centers Research Areas: Machine Translation, Natural Language Processing, Spoken Language Processing, Machine Learning|Yonatan Bisk Assistant Professor Email: ybisk@cs.cmu.edu Office: Gates & Hillman Centers Research Areas: Grounding, RoboNLP, Vision and Language, Embodiment, Unsupervised Learning",
        "retrieved_relevant_part": true,
        "model_answer": "Daniel Fried's research areas include natural language processing, conversational AI, intelligent agents and dialogue, discourse and pragmatics, multimodal aigraham, and machine translation",
        "actual_answer": "Daniel Fried's Research Areas is Natural Language Processing: Language and Code, Conversational AI, Intelligent Agents, and Dialogue, Discourse and Pragmatics, Multimodal AI",
        "exact_match": false,
        "f1_max_min_avg": [
            0.18,
            0.18,
            0.18
        ],
        "recall_max_min_avg": [
            0.18,
            0.18,
            0.18
        ]
    },
    {
        "question": "How can I contact LTI faculty Robert Frederking?",
        "context": "Robert Frederking Principal Systems Scientist/Associate Dean of Doctoral Programs/MLT Program Director Email: ref@cs.cmu.edu Office: 6515 Gates & Hillman Centers Phone: 412-268-6656",
        "retrieved_doc": "Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu|Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering|Alexander Rudnicky Research Professor Emeritus Email: alex.rudnicky@cs.cmu.edu Office: 6511 Gates & Hillman Centers Phone: 412-268-2622 Research Areas: Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing",
        "retrieved_relevant_part": false,
        "model_answer": "Robert Frederking's email address is rfederking@cs.cmu.edu.<|endoftext|>",
        "actual_answer": "Email: ref@cs.cmu.edu Office: 6515 Gates & Hillman Centers Phone: 412-268-6656",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Please tell me the Phone of LTI faculty Justine Cassell based on the background information: ",
        "context": "Justine Cassell Professor (On Leave) Email: jcassell@andrew.cmu.edu Office: 5107 Gates & Hillman Centers Phone: 412-204-6268",
        "retrieved_doc": "Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering|Louis-Philippe Morency Leonardo Associate Professor of Computer Science Email: morency@cs.cmu.edu Office: 5411 Gates & Hillman Centers Phone: 412-268-5508 Research Areas: Machine Learning, Multimodal Computing and Interaction, Spoken Interfaces and Dialogue Processing|David Mortensen Assistant Research Professor Email: dmortens@cs.cmu.edu Office: 5707 Gates & Hillman Centers Phone: 412-268-2894 Research Areas: Natural Language Processing and Computational Linguistics, Corpus Annotation and Resources",
        "retrieved_relevant_part": false,
        "model_answer": "The phone number for Justine Cassell is 412-268-8259.<|endoftext|>",
        "actual_answer": "Justine Cassell's Phone is 412-204-6268",
        "exact_match": false,
        "f1_max_min_avg": [
            0.31,
            0.31,
            0.31
        ],
        "recall_max_min_avg": [
            0.4,
            0.4,
            0.4
        ]
    },
    {
        "question": "What is the Research Areas of LTI faculty Shinji Watanabe?",
        "context": "Shinji Watanabe Associate Professor Email: swatanab@andrew.cmu.edu Phone: 412-268-3687 Research Areas: Natural Language Processing: Conversational AI, Intelligent Agents, and Dialogue, Speech Processing (ASR, Speech Synthesis): Speech Recognition, Speech Synthesis, Multilingual/Low-Resource Speech Processing, Speech-to-Speech Translation, Speech Enhancement / Robust Speech Processing",
        "retrieved_doc": "Yonatan Bisk Assistant Professor Email: ybisk@cs.cmu.edu Office: Gates & Hillman Centers Research Areas: Grounding, RoboNLP, Vision and Language, Embodiment, Unsupervised Learning|Graham Neubig Associate Professor Email: gneubig@cs.cmu.edu Office: 5409 Gates & Hillman Centers Research Areas: Machine Translation, Natural Language Processing, Spoken Language Processing, Machine Learning|Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu",
        "retrieved_relevant_part": false,
        "model_answer": "The Research Areas of LTI faculty Shinji Watanabe include Machine Translation, Natural Language Processing, Spoken Language Processing, and Machine Learning.<|endoftext|>",
        "actual_answer": "Shinji Watanabe's Research Areas is Natural Language Processing: Conversational AI, Intelligent Agents, and Dialogue, Speech Processing (ASR, Speech Synthesis): Speech Recognition, Speech Synthesis, Multilingual/Low-Resource Speech Processing, Speech-to-Speech Translation, Speech Enhancement / Robust Speech Processing",
        "exact_match": false,
        "f1_max_min_avg": [
            0.3,
            0.3,
            0.3
        ],
        "recall_max_min_avg": [
            0.24,
            0.24,
            0.24
        ]
    },
    {
        "question": "Please tell me the Email of LTI faculty Alexander Hauptmann based on the background information: ",
        "context": "Alexander Hauptmann Research Professor Email: alex@cs.cmu.edu Office: 5519 Gates & Hillman Centers Phone: 412-268-1448 Research Areas: Information Extraction, Summarization and Question Answering, Information Retrieval, Text Mining and Analytics, Machine Learning, Multimodal Computing and Interaction",
        "retrieved_doc": "Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu|Zhi-Qi Cheng Hauptmann Lab/Postdoctoral Research Associate Email: zhiqic@andrew.cmu.edu|Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu",
        "retrieved_relevant_part": false,
        "model_answer": "zhiqic@andrew.cmu.edu",
        "actual_answer": "alex@cs.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Please provide the contact information of LTI faculty Eric Nyberg",
        "context": "Eric Nyberg Professor Email: ehn@cs.cmu.edu Office: 6715 Gates & Hillman Centers Phone: 412-268-7281 Research Areas: Information Extraction, Summarization and Question Answering, Information Retrieval, Text Mining and Analytics, Language Technologies for Education",
        "retrieved_doc": "Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering|Alexander Rudnicky Research Professor Emeritus Email: alex.rudnicky@cs.cmu.edu Office: 6511 Gates & Hillman Centers Phone: 412-268-2622 Research Areas: Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing|Eric P. Xing Professor (On Leave) Email: epxing@andrew.cmu.edu Office: 8101 Gates & Hillman Centers Phone: 412-268-2559",
        "retrieved_relevant_part": false,
        "model_answer": "Eric Nyberg's email address is enyberg@cs.cmu.edu, office location is at 6415 Gates & Hillman Centers, and phone number is 412-268-8259.<|endoftext|>",
        "actual_answer": "Email: ehn@cs.cmu.edu Office: 6715 Gates & Hillman Centers Phone: 412-268-7281",
        "exact_match": false,
        "f1_max_min_avg": [
            0.2,
            0.2,
            0.2
        ],
        "recall_max_min_avg": [
            0.3,
            0.3,
            0.3
        ]
    },
    {
        "question": "Please tell me the Phone of LTI faculty Robert Frederking based on the background information: ",
        "context": "Robert Frederking Principal Systems Scientist/Associate Dean of Doctoral Programs/MLT Program Director Email: ref@cs.cmu.edu Office: 6515 Gates & Hillman Centers Phone: 412-268-6656",
        "retrieved_doc": "Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering|Zaid Sheikh Neubig Lab/Senior Systems/Software Engineer Email: zsheikh@andrew.cmu.edu Office: 6416 Gates & Hillman Centers Phone:|Alexander Rudnicky Research Professor Emeritus Email: alex.rudnicky@cs.cmu.edu Office: 6511 Gates & Hillman Centers Phone: 412-268-2622 Research Areas: Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing",
        "retrieved_relevant_part": false,
        "model_answer": "The phone number of LTI faculty Robert Frederking is 412-268-8259.<|endoftext|>",
        "actual_answer": "412-268-6656",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Please provide the contact information of LTI faculty Rita Singh",
        "context": "Rita Singh Associate Research Professor Email: rsingh@cs.cmu.edu Office: 6703 Gates & Hillman Centers Phone: 412-268-9859",
        "retrieved_doc": "Rita Singh Associate Research Professor Email: rsingh@cs.cmu.edu Office: 6703 Gates & Hillman Centers Phone: 412-268-9859|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu",
        "retrieved_relevant_part": true,
        "model_answer": "Rita Singh's email address is rsingh@cs.cmu.edu, her office is located at 6703 Gates & Hillman Centers, and her phone number is 412-268-9859",
        "actual_answer": "Email: rsingh@cs.cmu.edu Office: 6703 Gates & Hillman Centers Phone: 412-268-9859",
        "exact_match": false,
        "f1_max_min_avg": [
            0.31,
            0.31,
            0.31
        ],
        "recall_max_min_avg": [
            0.5,
            0.5,
            0.5
        ]
    },
    {
        "question": "Please tell me the Phone of LTI faculty Robert Frederking based on the background information: ",
        "context": "Robert Frederking Principal Systems Scientist/Associate Dean of Doctoral Programs/MLT Program Director Email: ref@cs.cmu.edu Office: 6515 Gates & Hillman Centers Phone: 412-268-6656",
        "retrieved_doc": "Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering|Zaid Sheikh Neubig Lab/Senior Systems/Software Engineer Email: zsheikh@andrew.cmu.edu Office: 6416 Gates & Hillman Centers Phone:|Alexander Rudnicky Research Professor Emeritus Email: alex.rudnicky@cs.cmu.edu Office: 6511 Gates & Hillman Centers Phone: 412-268-2622 Research Areas: Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing",
        "retrieved_relevant_part": false,
        "model_answer": "The phone number of LTI faculty Robert Frederking is 412-268-8259.<|endoftext|>",
        "actual_answer": "Robert Frederking's Phone is 412-268-6656",
        "exact_match": false,
        "f1_max_min_avg": [
            0.27,
            0.27,
            0.27
        ],
        "recall_max_min_avg": [
            0.4,
            0.4,
            0.4
        ]
    },
    {
        "question": "How can I contact LTI faculty Eric P.?",
        "context": "Eric P. Xing Professor (On Leave) Email: epxing@andrew.cmu.edu Office: 8101 Gates & Hillman Centers Phone: 412-268-2559",
        "retrieved_doc": "Eric P. Xing Professor (On Leave) Email: epxing@andrew.cmu.edu Office: 8101 Gates & Hillman Centers Phone: 412-268-2559|Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering|Alexander Rudnicky Research Professor Emeritus Email: alex.rudnicky@cs.cmu.edu Office: 6511 Gates & Hillman Centers Phone: 412-268-2622 Research Areas: Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing",
        "retrieved_relevant_part": true,
        "model_answer": "You can contact LTI faculty Eric P. via email at epxing@andrew.cmu.edu or by phone at 412-268-2559",
        "actual_answer": "Email: epxing@andrew.cmu.edu Office: 8101 Gates & Hillman Centers Phone: 412-268-2559",
        "exact_match": false,
        "f1_max_min_avg": [
            0.15,
            0.15,
            0.15
        ],
        "recall_max_min_avg": [
            0.2,
            0.2,
            0.2
        ]
    },
    {
        "question": "What is the Email of LTI faculty Eric Nyberg?",
        "context": "Eric Nyberg Professor Email: ehn@cs.cmu.edu Office: 6715 Gates & Hillman Centers Phone: 412-268-7281 Research Areas: Information Extraction, Summarization and Question Answering, Information Retrieval, Text Mining and Analytics, Language Technologies for Education",
        "retrieved_doc": "Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu|Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu|Wenhe Liu Research Scholar Email: wenhel@andrew.cmu.edu Office: 5403 Gates & Hillman Centers",
        "retrieved_relevant_part": false,
        "model_answer": "nyberg@andrew.cmu.edu",
        "actual_answer": "Eric Nyberg's Email is ehn@cs.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Is Anatole Gershman natan Bisk Assistant Professor of LTI?, Please answer yes or no: ",
        "context": "Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering",
        "retrieved_doc": "Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu|Yonatan Bisk Assistant Professor Email: ybisk@cs.cmu.edu Office: Gates & Hillman Centers Research Areas: Grounding, RoboNLP, Vision and Language, Embodiment, Unsupervised Learning|Graham Neubig Associate Professor Email: gneubig@cs.cmu.edu Office: 5409 Gates & Hillman Centers Research Areas: Machine Translation, Natural Language Processing, Spoken Language Processing, Machine Learning",
        "retrieved_relevant_part": false,
        "model_answer": "Yes",
        "actual_answer": "No",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "How can I contact LTI faculty Rita Singh?",
        "context": "Rita Singh Associate Research Professor Email: rsingh@cs.cmu.edu Office: 6703 Gates & Hillman Centers Phone: 412-268-9859",
        "retrieved_doc": "Rita Singh Associate Research Professor Email: rsingh@cs.cmu.edu Office: 6703 Gates & Hillman Centers Phone: 412-268-9859|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Krista McGuigan Business Director Email: klmcg@cs.cmu.edu Office: 349 TCS Hall Phone: 412-268-1635",
        "retrieved_relevant_part": true,
        "model_answer": "You can contact Rita Singh at rsingh@cs.cmu.edu or by phone at 412-268-9859",
        "actual_answer": "Email: rsingh@cs.cmu.edu Office: 6703 Gates & Hillman Centers Phone: 412-268-9859",
        "exact_match": false,
        "f1_max_min_avg": [
            0.18,
            0.18,
            0.18
        ],
        "recall_max_min_avg": [
            0.2,
            0.2,
            0.2
        ]
    },
    {
        "question": "Please tell me the Research Areas of LTI faculty Maarten Sap based on the background information: ",
        "context": "Maarten Sap Assistant Professor Email: msap2@andrew.cmu.edu Research Areas: Fairness and Ethics in Language Technology, Computational Social Science, Discourse and Pragmatics, Conversational AI, Intelligent Agents, and Dialogue",
        "retrieved_doc": "Yonatan Bisk Assistant Professor Email: ybisk@cs.cmu.edu Office: Gates & Hillman Centers Research Areas: Grounding, RoboNLP, Vision and Language, Embodiment, Unsupervised Learning|Vicente Malave Data Science Course Developer Email: vlm@andrew.cmu.edu|Graham Neubig Associate Professor Email: gneubig@cs.cmu.edu Office: 5409 Gates & Hillman Centers Research Areas: Machine Translation, Natural Language Processing, Spoken Language Processing, Machine Learning",
        "retrieved_relevant_part": false,
        "model_answer": "The Research Areas of LTI faculty Maarten Sap are not mentioned in the context.<|endoftext|>",
        "actual_answer": "Fairness and Ethics in Language Technology, Computational Social Science, Discourse and Pragmatics, Conversational AI, Intelligent Agents, and Dialogue",
        "exact_match": false,
        "f1_max_min_avg": [
            0.06,
            0.06,
            0.06
        ],
        "recall_max_min_avg": [
            0.06,
            0.06,
            0.06
        ]
    },
    {
        "question": "What is the Phone of LTI faculty Anatole Gershman?",
        "context": "Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering",
        "retrieved_doc": "Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering|Susan E. Holm Mitamura Lab/Sr. Knowledge Engineer Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591|Yukari Yamakawa Mitamura Lab/Associate Data Analyst Email: yukariy@andrew.cmu.edu Office: 6717 Gates & Hillman Centers Phone: 412-268-6596",
        "retrieved_relevant_part": true,
        "model_answer": "The phone number of LTI faculty Anatole Gershman is 412-268-8259.<|endoftext|>",
        "actual_answer": "Anatole Gershman's Phone is 412-268-8259",
        "exact_match": false,
        "f1_max_min_avg": [
            0.27,
            0.27,
            0.27
        ],
        "recall_max_min_avg": [
            0.4,
            0.4,
            0.4
        ]
    },
    {
        "question": "How can I contact LTI faculty Yonatan Bisk?",
        "context": "Yonatan Bisk Assistant Professor Email: ybisk@cs.cmu.edu Office: Gates & Hillman Centers Research Areas: Grounding, RoboNLP, Vision and Language, Embodiment, Unsupervised Learning",
        "retrieved_doc": "Yukari Yamakawa Mitamura Lab/Associate Data Analyst Email: yukariy@andrew.cmu.edu Office: 6717 Gates & Hillman Centers Phone: 412-268-6596|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Bhiksha Ramakrishnan Professor Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826 Research Areas: Machine Learning, Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing, Privacy",
        "retrieved_relevant_part": false,
        "model_answer": "You can contact Yonatan Bisk by emailing him at bhiksha@cs.cmu.edu or by phone at 412-268-9826.<|endoftext|>",
        "actual_answer": "Email: ybisk@cs.cmu.edu Office: Gates & Hillman Centers",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "What is the Phone of LTI faculty Shinji Watanabe?",
        "context": "Shinji Watanabe Associate Professor Email: swatanab@andrew.cmu.edu Phone: 412-268-3687 Research Areas: Natural Language Processing: Conversational AI, Intelligent Agents, and Dialogue, Speech Processing (ASR, Speech Synthesis): Speech Recognition, Speech Synthesis, Multilingual/Low-Resource Speech Processing, Speech-to-Speech Translation, Speech Enhancement / Robust Speech Processing",
        "retrieved_doc": "Yukari Yamakawa Mitamura Lab/Associate Data Analyst Email: yukariy@andrew.cmu.edu Office: 6717 Gates & Hillman Centers Phone: 412-268-6596|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Susan E. Holm Mitamura Lab/Sr. Knowledge Engineer Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591",
        "retrieved_relevant_part": false,
        "model_answer": "The Phone of LTI faculty Shinji Watanabe is 412-268-6596.<|endoftext|>",
        "actual_answer": "Shinji Watanabe's Phone is 412-268-3687",
        "exact_match": false,
        "f1_max_min_avg": [
            0.43,
            0.43,
            0.43
        ],
        "recall_max_min_avg": [
            0.6,
            0.6,
            0.6
        ]
    },
    {
        "question": "How can I contact LTI faculty Louis-Philippe Morency?",
        "context": "Louis-Philippe Morency Leonardo Associate Professor of Computer Science Email: morency@cs.cmu.edu Office: 5411 Gates & Hillman Centers Phone: 412-268-5508 Research Areas: Machine Learning, Multimodal Computing and Interaction, Spoken Interfaces and Dialogue Processing",
        "retrieved_doc": "Louis-Philippe Morency Leonardo Associate Professor of Computer Science Email: morency@cs.cmu.edu Office: 5411 Gates & Hillman Centers Phone: 412-268-5508 Research Areas: Machine Learning, Multimodal Computing and Interaction, Spoken Interfaces and Dialogue Processing|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering",
        "retrieved_relevant_part": true,
        "model_answer": "You can contact LTI faculty Louis-Philippe Morency via email at morency@cs.cmu.edu or by phone at 412-268-5508",
        "actual_answer": "Email: morency@cs.cmu.edu Office: 5411 Gates & Hillman Centers Phone: 412-268-5508",
        "exact_match": false,
        "f1_max_min_avg": [
            0.15,
            0.15,
            0.15
        ],
        "recall_max_min_avg": [
            0.2,
            0.2,
            0.2
        ]
    },
    {
        "question": "Please tell me the Phone of LTI faculty Robert Frederking based on the background information: ",
        "context": "Robert Frederking Principal Systems Scientist/Associate Dean of Doctoral Programs/MLT Program Director Email: ref@cs.cmu.edu Office: 6515 Gates & Hillman Centers Phone: 412-268-6656",
        "retrieved_doc": "Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering|Zaid Sheikh Neubig Lab/Senior Systems/Software Engineer Email: zsheikh@andrew.cmu.edu Office: 6416 Gates & Hillman Centers Phone:|Alexander Rudnicky Research Professor Emeritus Email: alex.rudnicky@cs.cmu.edu Office: 6511 Gates & Hillman Centers Phone: 412-268-2622 Research Areas: Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing",
        "retrieved_relevant_part": false,
        "model_answer": "The phone number of LTI faculty Robert Frederking is 412-268-8259.<|endoftext|>",
        "actual_answer": "412-268-6656",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "How can I contact LTI faculty Rita Singh?",
        "context": "Rita Singh Associate Research Professor Email: rsingh@cs.cmu.edu Office: 6703 Gates & Hillman Centers Phone: 412-268-9859",
        "retrieved_doc": "Rita Singh Associate Research Professor Email: rsingh@cs.cmu.edu Office: 6703 Gates & Hillman Centers Phone: 412-268-9859|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Krista McGuigan Business Director Email: klmcg@cs.cmu.edu Office: 349 TCS Hall Phone: 412-268-1635",
        "retrieved_relevant_part": true,
        "model_answer": "You can contact Rita Singh at rsingh@cs.cmu.edu or by phone at 412-268-9859",
        "actual_answer": "Email: rsingh@cs.cmu.edu Office: 6703 Gates & Hillman Centers Phone: 412-268-9859",
        "exact_match": false,
        "f1_max_min_avg": [
            0.18,
            0.18,
            0.18
        ],
        "recall_max_min_avg": [
            0.2,
            0.2,
            0.2
        ]
    },
    {
        "question": "How can I contact LTI faculty Maarten Sap?",
        "context": "Maarten Sap Assistant Professor Email: msap2@andrew.cmu.edu Research Areas: Fairness and Ethics in Language Technology, Computational Social Science, Discourse and Pragmatics, Conversational AI, Intelligent Agents, and Dialogue",
        "retrieved_doc": "Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering|Brianna Eriksen Academic Program Manager Email: bfreema2@andrew.cmu.edu Phone: 412-268-4277|Jennifer Lucas Academic Program Manager \u2014 MCDS Email: jmlucas@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-9870",
        "retrieved_relevant_part": false,
        "model_answer": "Maarten Sap's email address is maarten.sap@cs.cmu.edu and his phone number is 412-268-8259.<|endoftext|>",
        "actual_answer": "Email: msap2@andrew.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "What is the Research Areas of LTI faculty Shinji Watanabe?",
        "context": "Shinji Watanabe Associate Professor Email: swatanab@andrew.cmu.edu Phone: 412-268-3687 Research Areas: Natural Language Processing: Conversational AI, Intelligent Agents, and Dialogue, Speech Processing (ASR, Speech Synthesis): Speech Recognition, Speech Synthesis, Multilingual/Low-Resource Speech Processing, Speech-to-Speech Translation, Speech Enhancement / Robust Speech Processing",
        "retrieved_doc": "Yonatan Bisk Assistant Professor Email: ybisk@cs.cmu.edu Office: Gates & Hillman Centers Research Areas: Grounding, RoboNLP, Vision and Language, Embodiment, Unsupervised Learning|Graham Neubig Associate Professor Email: gneubig@cs.cmu.edu Office: 5409 Gates & Hillman Centers Research Areas: Machine Translation, Natural Language Processing, Spoken Language Processing, Machine Learning|Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu",
        "retrieved_relevant_part": false,
        "model_answer": "The Research Areas of LTI faculty Shinji Watanabe include Machine Translation, Natural Language Processing, Spoken Language Processing, and Machine Learning.<|endoftext|>",
        "actual_answer": "Shinji Watanabe's Research Areas is Natural Language Processing: Conversational AI, Intelligent Agents, and Dialogue, Speech Processing (ASR, Speech Synthesis): Speech Recognition, Speech Synthesis, Multilingual/Low-Resource Speech Processing, Speech-to-Speech Translation, Speech Enhancement / Robust Speech Processing",
        "exact_match": false,
        "f1_max_min_avg": [
            0.3,
            0.3,
            0.3
        ],
        "recall_max_min_avg": [
            0.24,
            0.24,
            0.24
        ]
    },
    {
        "question": "How can I contact LTI faculty Eric P.?",
        "context": "Eric P. Xing Professor (On Leave) Email: epxing@andrew.cmu.edu Office: 8101 Gates & Hillman Centers Phone: 412-268-2559",
        "retrieved_doc": "Eric P. Xing Professor (On Leave) Email: epxing@andrew.cmu.edu Office: 8101 Gates & Hillman Centers Phone: 412-268-2559|Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering|Alexander Rudnicky Research Professor Emeritus Email: alex.rudnicky@cs.cmu.edu Office: 6511 Gates & Hillman Centers Phone: 412-268-2622 Research Areas: Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing",
        "retrieved_relevant_part": true,
        "model_answer": "You can contact LTI faculty Eric P. via email at epxing@andrew.cmu.edu or by phone at 412-268-2559",
        "actual_answer": "Email: epxing@andrew.cmu.edu Office: 8101 Gates & Hillman Centers Phone: 412-268-2559",
        "exact_match": false,
        "f1_max_min_avg": [
            0.15,
            0.15,
            0.15
        ],
        "recall_max_min_avg": [
            0.2,
            0.2,
            0.2
        ]
    },
    {
        "question": "Please tell me the Research Areas of LTI faculty Shinji Watanabe based on the background information: ",
        "context": "Shinji Watanabe Associate Professor Email: swatanab@andrew.cmu.edu Phone: 412-268-3687 Research Areas: Natural Language Processing: Conversational AI, Intelligent Agents, and Dialogue, Speech Processing (ASR, Speech Synthesis): Speech Recognition, Speech Synthesis, Multilingual/Low-Resource Speech Processing, Speech-to-Speech Translation, Speech Enhancement / Robust Speech Processing",
        "retrieved_doc": "Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu|Author: Shinji Watanabe Title: Unsupervised Data Selection for TTS: Using Arabic Broadcast News as a Case Study Publication year: 2023 Coauthors: Massa Baali, Tomoki Hayashi, Hamdy Mubarak, Soumi Maiti, Shinji Watanabe, W. El-Hajj, Ahmed Ali Abstract: .|Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe ",
        "retrieved_relevant_part": false,
        "model_answer": "The Research Areas of LTI faculty Shinji Watanabe are not mentioned in the context.<|endoftext|>",
        "actual_answer": "Natural Language Processing: Conversational AI, Intelligent Agents, and Dialogue, Speech Processing (ASR, Speech Synthesis): Speech Recognition, Speech Synthesis, Multilingual/Low-Resource Speech Processing, Speech-to-Speech Translation, Speech Enhancement / Robust Speech Processing",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Is Scott Fahlman i Li Assistant Professor of LTI?, Please answer yes or no: ",
        "context": "Scott Fahlman Research Professor Emeritus Email: sef@cs.cmu.edu Office: 6417 Gates & Hillman Centers Phone: 412-268-2575 Research Areas: AI, Knowledge Representation and Reasoning, Natural Language Understanding",
        "retrieved_doc": "Graham Neubig Associate Professor Email: gneubig@cs.cmu.edu Office: 5409 Gates & Hillman Centers Research Areas: Machine Translation, Natural Language Processing, Spoken Language Processing, Machine Learning|Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu|Emma Strubell Assistant Professor Email: estrubel@andrew.cmu.edu Office: Gates & Hillman Centers",
        "retrieved_relevant_part": false,
        "model_answer": "Yes",
        "actual_answer": "No",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "How can I contact LTI faculty Bhiksha Ramakrishnan?",
        "context": "Bhiksha Ramakrishnan Professor Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826 Research Areas: Machine Learning, Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing, Privacy",
        "retrieved_doc": "Bhiksha Ramakrishnan Professor Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826 Research Areas: Machine Learning, Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing, Privacy|Rita Singh Associate Research Professor Email: rsingh@cs.cmu.edu Office: 6703 Gates & Hillman Centers Phone: 412-268-9859|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669",
        "retrieved_relevant_part": true,
        "model_answer": "You can contact Bhiksha Ramakrishnan via email at bhiksha@cs.cmu.edu or by phone at 412-268-9826",
        "actual_answer": "Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826",
        "exact_match": false,
        "f1_max_min_avg": [
            0.17,
            0.17,
            0.17
        ],
        "recall_max_min_avg": [
            0.2,
            0.2,
            0.2
        ]
    },
    {
        "question": "How can I contact LTI faculty Chenyan Xiong?",
        "context": "Chenyan Xiong Associate Professor Email: cx@andrew.cmu.edu Phone: 412-268-7641",
        "retrieved_doc": "Chenyan Xiong Associate Professor Email: cx@andrew.cmu.edu Phone: 412-268-7641|Zhong-Qiu Wang Wantanabe Lab/Postdoctoral Research Associate Email: zhongqiu@andrew.cmu.edu Office: 6414 Gates & Hillman Centers Phone: 415-200-8662|Yiming Yang Professor Email: yiming@cs.cmu.edu Office: 6717 Gates & Hillman Centers Phone: 412-268-1364",
        "retrieved_relevant_part": true,
        "model_answer": "Chenyan Xiong's email address is cx@andrew.cmu.edu and her phone number is 412-268-7641",
        "actual_answer": "Email: cx@andrew.cmu.edu Phone: 412-268-7641",
        "exact_match": false,
        "f1_max_min_avg": [
            0.25,
            0.25,
            0.25
        ],
        "recall_max_min_avg": [
            0.5,
            0.5,
            0.5
        ]
    },
    {
        "question": "How can I contact LTI faculty Bhiksha Ramakrishnan?",
        "context": "Bhiksha Ramakrishnan Professor Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826 Research Areas: Machine Learning, Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing, Privacy",
        "retrieved_doc": "Bhiksha Ramakrishnan Professor Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826 Research Areas: Machine Learning, Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing, Privacy|Rita Singh Associate Research Professor Email: rsingh@cs.cmu.edu Office: 6703 Gates & Hillman Centers Phone: 412-268-9859|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669",
        "retrieved_relevant_part": true,
        "model_answer": "You can contact Bhiksha Ramakrishnan via email at bhiksha@cs.cmu.edu or by phone at 412-268-9826",
        "actual_answer": "Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826",
        "exact_match": false,
        "f1_max_min_avg": [
            0.17,
            0.17,
            0.17
        ],
        "recall_max_min_avg": [
            0.2,
            0.2,
            0.2
        ]
    },
    {
        "question": "How can I contact LTI faculty Maarten Sap?",
        "context": "Maarten Sap Assistant Professor Email: msap2@andrew.cmu.edu Research Areas: Fairness and Ethics in Language Technology, Computational Social Science, Discourse and Pragmatics, Conversational AI, Intelligent Agents, and Dialogue",
        "retrieved_doc": "Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering|Brianna Eriksen Academic Program Manager Email: bfreema2@andrew.cmu.edu Phone: 412-268-4277|Jennifer Lucas Academic Program Manager \u2014 MCDS Email: jmlucas@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-9870",
        "retrieved_relevant_part": false,
        "model_answer": "Maarten Sap's email address is maarten.sap@cs.cmu.edu and his phone number is 412-268-8259.<|endoftext|>",
        "actual_answer": "Email: msap2@andrew.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "What is the Office of LTI faculty Justine Cassell?",
        "context": "Justine Cassell Professor (On Leave) Email: jcassell@andrew.cmu.edu Office: 5107 Gates & Hillman Centers Phone: 412-204-6268",
        "retrieved_doc": "and alumni. The Task Force was co-chaired by Director of Athletics Susan Bassett and Dean of Student Affairs Jennifer Church.|Nicole Perrotta Administrative Coordinator Email: nperrott@andrew.cmu.edu|Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering",
        "retrieved_relevant_part": false,
        "model_answer": "The Office of LTI faculty Justine Cassell is located in the Gates & Hillman Centers.<|endoftext|>",
        "actual_answer": "Justine Cassell's Office is 5107 Gates & Hillman Centers",
        "exact_match": false,
        "f1_max_min_avg": [
            0.5,
            0.5,
            0.5
        ],
        "recall_max_min_avg": [
            0.67,
            0.67,
            0.67
        ]
    },
    {
        "question": "What is the Research Areas of LTI faculty Louis-Philippe Morency?",
        "context": "Louis-Philippe Morency Leonardo Associate Professor of Computer Science Email: morency@cs.cmu.edu Office: 5411 Gates & Hillman Centers Phone: 412-268-5508 Research Areas: Machine Learning, Multimodal Computing and Interaction, Spoken Interfaces and Dialogue Processing",
        "retrieved_doc": "Graham Neubig Associate Professor Email: gneubig@cs.cmu.edu Office: 5409 Gates & Hillman Centers Research Areas: Machine Translation, Natural Language Processing, Spoken Language Processing, Machine Learning|Yonatan Bisk Assistant Professor Email: ybisk@cs.cmu.edu Office: Gates & Hillman Centers Research Areas: Grounding, RoboNLP, Vision and Language, Embodiment, Unsupervised Learning|Maarten Sap Assistant Professor Email: msap2@andrew.cmu.edu Research Areas: Fairness and Ethics in Language Technology, Computational Social Science, Discourse and Pragmatics, Conversational AI, Intelligent Agents, and Dialogue",
        "retrieved_relevant_part": false,
        "model_answer": "The Research Areas of LTI faculty Louis-Philippe Morency include Machine Translation, Natural Language Processing, Spoken Language Processing, and Machine Learning",
        "actual_answer": "Louis-Philippe Morency's Research Areas is Machine Learning, Multimodal Computing and Interaction, Spoken Interfaces and Dialogue Processing",
        "exact_match": false,
        "f1_max_min_avg": [
            0.33,
            0.33,
            0.33
        ],
        "recall_max_min_avg": [
            0.38,
            0.38,
            0.38
        ]
    },
    {
        "question": "What is the Office of LTI faculty Ralf Brown?",
        "context": "Ralf Brown Principal Systems Scientist Email: ralf@andrew.cmu.edu Office: 5711 Gates & Hillman Centers Phone: 412-268-8298 Research Areas: Information Extraction, Summarization and Question Answering, Information Retrieval, Text Mining and Analytics, Machine Translation, Natural Language Processing and Computational Linguistics",
        "retrieved_doc": "and alumni. The Task Force was co-chaired by Director of Athletics Susan Bassett and Dean of Student Affairs Jennifer Church.|Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu|Zaid Sheikh Neubig Lab/Senior Systems/Software Engineer Email: zsheikh@andrew.cmu.edu Office: 6416 Gates & Hillman Centers Phone:",
        "retrieved_relevant_part": false,
        "model_answer": "The Office of LTI faculty Ralf Brown is located in the Gates & Hillman Centers.<|endoftext|>",
        "actual_answer": "5711 Gates & Hillman Centers",
        "exact_match": false,
        "f1_max_min_avg": [
            0.3,
            0.3,
            0.3
        ],
        "recall_max_min_avg": [
            0.6,
            0.6,
            0.6
        ]
    },
    {
        "question": "Please tell me the Email of LTI faculty Robert Frederking based on the background information: ",
        "context": "Robert Frederking Principal Systems Scientist/Associate Dean of Doctoral Programs/MLT Program Director Email: ref@cs.cmu.edu Office: 6515 Gates & Hillman Centers Phone: 412-268-6656",
        "retrieved_doc": "Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu|Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu|Emma Strubell Assistant Professor Email: estrubel@andrew.cmu.edu Office: Gates & Hillman Centers",
        "retrieved_relevant_part": false,
        "model_answer": "The email address for LTI faculty Robert Frederking is not mentioned in the context, so I cannot provide it.<|endoftext|>",
        "actual_answer": "Robert Frederking's Email is ref@cs.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0.17,
            0.17,
            0.17
        ],
        "recall_max_min_avg": [
            0.4,
            0.4,
            0.4
        ]
    },
    {
        "question": "Please provide the contact information of LTI faculty Jamie Callan",
        "context": "Jamie Callan Professor and PhD Program Director Email: callan@cs.cmu.edu Office: 5419 Gates & Hillman Centers Phone: 412-268-4525 Research Areas: Information Retrieval, Text Mining and Analytics",
        "retrieved_doc": "Jamie Callan Professor and PhD Program Director Email: callan@cs.cmu.edu Office: 5419 Gates & Hillman Centers Phone: 412-268-4525 Research Areas: Information Retrieval, Text Mining and Analytics|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering",
        "retrieved_relevant_part": true,
        "model_answer": "Jamie Callan's email address is callan@cs.cmu.edu, office location is at 5419 Gates & Hillman Centers, and phone number is 412-268-4525",
        "actual_answer": "Email: callan@cs.cmu.edu Office: 5419 Gates & Hillman Centers Phone: 412-268-4525",
        "exact_match": false,
        "f1_max_min_avg": [
            0.33,
            0.33,
            0.33
        ],
        "recall_max_min_avg": [
            0.5,
            0.5,
            0.5
        ]
    },
    {
        "question": "What is the Email of LTI faculty Bhiksha Ramakrishnan?",
        "context": "Bhiksha Ramakrishnan Professor Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826 Research Areas: Machine Learning, Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing, Privacy",
        "retrieved_doc": "Bhiksha Ramakrishnan Professor Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826 Research Areas: Machine Learning, Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing, Privacy|Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu|Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu",
        "retrieved_relevant_part": true,
        "model_answer": "bhiksha@cs.cmu.edu",
        "actual_answer": "bhiksha@cs.cmu.edu",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "What is the Phone of LTI faculty Justine Cassell?",
        "context": "Justine Cassell Professor (On Leave) Email: jcassell@andrew.cmu.edu Office: 5107 Gates & Hillman Centers Phone: 412-204-6268",
        "retrieved_doc": "Justine Cassell Professor (On Leave) Email: jcassell@andrew.cmu.edu Office: 5107 Gates & Hillman Centers Phone: 412-204-6268|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788|Jessica Maguire Assistant to the Institute Director Email: jlm4@andrew.cmu.edu Office: 5721 Gates & Hillman Centers Phone: 412-268-5480",
        "retrieved_relevant_part": true,
        "model_answer": "The Phone of LTI faculty Justine Cassell is 412-204-6268",
        "actual_answer": "412-204-6268",
        "exact_match": false,
        "f1_max_min_avg": [
            0.2,
            0.2,
            0.2
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Is Rita Singh lf Brown Principal Systems Scientist of LTI?, Please answer yes or no: ",
        "context": "Rita Singh Associate Research Professor Email: rsingh@cs.cmu.edu Office: 6703 Gates & Hillman Centers Phone: 412-268-9859",
        "retrieved_doc": "Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu|Vicente Malave Data Science Course Developer Email: vlm@andrew.cmu.edu|Zaid Sheikh Neubig Lab/Senior Systems/Software Engineer Email: zsheikh@andrew.cmu.edu Office: 6416 Gates & Hillman Centers Phone:",
        "retrieved_relevant_part": false,
        "model_answer": "Yes, Rita Singh is a Principal Systems Scientist at LTI",
        "actual_answer": "No",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Is Maarten Sap iksha Ramakrishnan Professor of LTI?, Please answer yes or no: ",
        "context": "Maarten Sap Assistant Professor Email: msap2@andrew.cmu.edu Research Areas: Fairness and Ethics in Language Technology, Computational Social Science, Discourse and Pragmatics, Conversational AI, Intelligent Agents, and Dialogue",
        "retrieved_doc": "Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu|Yonatan Bisk Assistant Professor Email: ybisk@cs.cmu.edu Office: Gates & Hillman Centers Research Areas: Grounding, RoboNLP, Vision and Language, Embodiment, Unsupervised Learning|Vicente Malave Data Science Course Developer Email: vlm@andrew.cmu.edu",
        "retrieved_relevant_part": false,
        "model_answer": "No",
        "actual_answer": "No",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Please tell me the Email of LTI faculty Sean Welleck based on the background information: ",
        "context": "Sean Welleck Assistant Professor (Starting January 2024) Email: swelleck@andrew.cmu.edu",
        "retrieved_doc": "Sean Welleck Assistant Professor (Starting January 2024) Email: swelleck@andrew.cmu.edu|Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu|Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu",
        "retrieved_relevant_part": true,
        "model_answer": "swelleck@andrew.cmu.edu",
        "actual_answer": "swelleck@andrew.cmu.edu",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Is Louis-Philippe Morency niel Fried Assistant Professor of LTI?, Please answer yes or no: ",
        "context": "Louis-Philippe Morency Leonardo Associate Professor of Computer Science Email: morency@cs.cmu.edu Office: 5411 Gates & Hillman Centers Phone: 412-268-5508 Research Areas: Machine Learning, Multimodal Computing and Interaction, Spoken Interfaces and Dialogue Processing",
        "retrieved_doc": "Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu|Graham Neubig Associate Professor Email: gneubig@cs.cmu.edu Office: 5409 Gates & Hillman Centers Research Areas: Machine Translation, Natural Language Processing, Spoken Language Processing, Machine Learning|Yonatan Bisk Assistant Professor Email: ybisk@cs.cmu.edu Office: Gates & Hillman Centers Research Areas: Grounding, RoboNLP, Vision and Language, Embodiment, Unsupervised Learning",
        "retrieved_relevant_part": false,
        "model_answer": "Yes",
        "actual_answer": "No",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "What is the Office of LTI faculty Anatole Gershman?",
        "context": "Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering",
        "retrieved_doc": "Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering|Graham Neubig Associate Professor Email: gneubig@cs.cmu.edu Office: 5409 Gates & Hillman Centers Research Areas: Machine Translation, Natural Language Processing, Spoken Language Processing, Machine Learning|Zaid Sheikh Neubig Lab/Senior Systems/Software Engineer Email: zsheikh@andrew.cmu.edu Office: 6416 Gates & Hillman Centers Phone:",
        "retrieved_relevant_part": true,
        "model_answer": "The Office of LTI faculty Anatole Gershman is located in the Gates & Hillman Centers",
        "actual_answer": "Anatole Gershman's Office is 6415 Gates & Hillman Centers",
        "exact_match": false,
        "f1_max_min_avg": [
            0.58,
            0.58,
            0.58
        ],
        "recall_max_min_avg": [
            0.78,
            0.78,
            0.78
        ]
    },
    {
        "question": "How can I contact LTI faculty Robert Frederking?",
        "context": "Robert Frederking Principal Systems Scientist/Associate Dean of Doctoral Programs/MLT Program Director Email: ref@cs.cmu.edu Office: 6515 Gates & Hillman Centers Phone: 412-268-6656",
        "retrieved_doc": "Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu|Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering|Alexander Rudnicky Research Professor Emeritus Email: alex.rudnicky@cs.cmu.edu Office: 6511 Gates & Hillman Centers Phone: 412-268-2622 Research Areas: Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing",
        "retrieved_relevant_part": false,
        "model_answer": "Robert Frederking's email address is rfederking@cs.cmu.edu.<|endoftext|>",
        "actual_answer": "Email: ref@cs.cmu.edu Office: 6515 Gates & Hillman Centers Phone: 412-268-6656",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "According to background information, What happens in 2024-06-21 on university calendar?",
        "context": "Summer One_All 2024: Date: 2024-06-21 Day: F Event: Mini-5 Final Exams",
        "retrieved_doc": "Spring 2024: Date: 2024-05-06 Day: M Event: Semester & Mini-4 Faculty Course Evaluations close |Fall 2023: Date: 2023-09-04 Day: M Event: Labor Day; No Classes & University Closed|Fall 2023: Date: 2023-12-23-2024-01-02 Day: Sa-Tu Event: Winter Break; University Closed",
        "retrieved_relevant_part": false,
        "model_answer": "The semester & mini-4 faculty course evaluations close",
        "actual_answer": "Mini-5 Final Exams",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Based on background information, What is the specific event listed for 2024-05-17, in Summer One_All 2024",
        "context": "Summer One_All 2024: Date: 2024-05-17 Day: F Event: Mini-5 add, audit, & tuition adjustment drop deadline (1)",
        "retrieved_doc": "Summer One_All 2024: Date: 2024-06-21 Day: F Event: Mini-5 Final Exams|Summer One_All 2024: Date: 2024-06-20 Day: Th Event: Mini-5 Last Day of Classes|Summer One_All 2024: Date: 2024-05-17 Day: F Event: Mini-5 add, audit, & tuition adjustment drop deadline (1)",
        "retrieved_relevant_part": true,
        "model_answer": "The mini-5 last day of classes.<|endoftext|>",
        "actual_answer": "Mini-5 add, audit, & tuition adjustment drop deadline (1)",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "According to background information, When is the [Mini-5 Last Day of Classes] in Summer One_All 2024?",
        "context": "Summer One_All 2024: Date: 2024-06-20 Day: Th Event: Mini-5 Last Day of Classes",
        "retrieved_doc": "Summer One_All 2024: Date: 2024-06-20 Day: Th Event: Mini-5 Last Day of Classes|Summer One_All 2024: Date: 2024-06-21 Day: F Event: Mini-5 Faculty Course Evaluations close |Summer One_All 2024: Date: 2024-08-01 Day: Th Event: Semester & Mini-6 Last Day of Classes",
        "retrieved_relevant_part": true,
        "model_answer": "The [Mini-5 Last Day of Classes] in Summer One_All 2024 is on June 21",
        "actual_answer": "The event Mini-5 Last Day of Classes happens on 2024-06-20",
        "exact_match": false,
        "f1_max_min_avg": [
            0.42,
            0.42,
            0.42
        ],
        "recall_max_min_avg": [
            0.5,
            0.5,
            0.5
        ]
    },
    {
        "question": "Based on background information, What is the specific event listed for 2023-11-27, in Fall 2023",
        "context": "Fall 2023: Date: 2023-11-27 Day: M Event: Mini-2 pass/no pass & withdrawal deadline (2)",
        "retrieved_doc": "Fall 2023: Date: 2023-10-16-2023-10-20 Day: M-F Event: Fall Break; No Classes|Fall 2023: Date: 2023-11-13-2023-11-17 Day: M-F Event: Spring 2024 Registration Week|Fall 2023: Date: 2023-10-13-2023-10-14 Day: F-Sa Event: Family Weekend",
        "retrieved_relevant_part": false,
        "model_answer": "The specific event for 2023-11-27, in Fall 2023, is \"Family Weekend.\"<|endoftext|>",
        "actual_answer": "The event happening on 2023-11-27 is Mini-2 pass/no pass & withdrawal deadline (2)",
        "exact_match": false,
        "f1_max_min_avg": [
            0.25,
            0.25,
            0.25
        ],
        "recall_max_min_avg": [
            0.23,
            0.23,
            0.23
        ]
    },
    {
        "question": "Based on background information, What is the specific event listed for 2023-10-23, in Fall 2023",
        "context": "Fall 2023: Date: 2023-10-23 Day: M Event: Mini-2 Classes Begin ",
        "retrieved_doc": "Fall 2023: Date: 2023-10-16-2023-10-20 Day: M-F Event: Fall Break; No Classes|Fall 2023: Date: 2023-10-13-2023-10-14 Day: F-Sa Event: Family Weekend|Fall 2023: Date: 2023-11-13-2023-11-17 Day: M-F Event: Spring 2024 Registration Week",
        "retrieved_relevant_part": false,
        "model_answer": "The event for 2023-10-23, in Fall 2023, is called \"Family Weekend.\"",
        "actual_answer": "The event happening on 2023-10-23 is Mini-2 Classes Begin",
        "exact_match": false,
        "f1_max_min_avg": [
            0.3,
            0.3,
            0.3
        ],
        "recall_max_min_avg": [
            0.33,
            0.33,
            0.33
        ]
    },
    {
        "question": "Based on background information, What is the specific event listed for 2024-01-15, in Spring 2024",
        "context": "Spring 2024: Date: 2024-01-15 Day: M Event: Martin Luther King Day; No Classes",
        "retrieved_doc": "Spring 2024: Date: 2024-05-04-2024-05-05 Day: Sa-Su Event: Reading Days|Spring 2024: Date: 2024-03-12 Day: T Event: Summer 2024 Registration Opens |Spring 2024: Date: 2024-04-15-2024-04-19 Day: M-F Event: Fall 2024 Registration Week",
        "retrieved_relevant_part": false,
        "model_answer": "The event for 2024-01-15, in Spring 2024, is \"Fall 2024 Registration Week.\"<|endoftext|>",
        "actual_answer": "Martin Luther King Day",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "According to background information, When is the [Make-Up Final Examinations] in Spring 2024?",
        "context": "Spring 2024: Date: 2024-05-07 Day: Tu Event: Make-Up Final Examinations",
        "retrieved_doc": "Fall 2023: Date: 2023-12-18 Day: M Event: Make-Up Final Exams|Spring 2024: Date: 2024-05-07 Day: Tu Event: Make-Up Final Examinations|Spring 2024: Date: 2024-05-06 Day: M Event: Final Examinations",
        "retrieved_relevant_part": true,
        "model_answer": "The [Make-Up Final Examinations] in Spring 2024 is on December 18",
        "actual_answer": "The event Make-Up Final Examinations happens on 2024-05-07",
        "exact_match": false,
        "f1_max_min_avg": [
            0.32,
            0.32,
            0.32
        ],
        "recall_max_min_avg": [
            0.38,
            0.38,
            0.38
        ]
    },
    {
        "question": "According to background information, What happens in 2023-10-13 on university calendar?",
        "context": "Fall 2023: Date: 2023-10-13 Day: F Event: Mini-1 Last Day of Classes",
        "retrieved_doc": "Fall 2023: Date: 2023-09-04 Day: M Event: Labor Day; No Classes & University Closed|Fall 2023: Date: 2023-12-23-2024-01-02 Day: Sa-Tu Event: Winter Break; University Closed|Summer Two 2024: Date: 2024-07-04 Day: Th Event: Independence Day; University Closed & No Classes",
        "retrieved_relevant_part": false,
        "model_answer": "The university is closed and there are no classes",
        "actual_answer": "The event happening on 2023-10-13 is Mini-1 Last Day of Classes",
        "exact_match": false,
        "f1_max_min_avg": [
            0.2,
            0.2,
            0.2
        ],
        "recall_max_min_avg": [
            0.18,
            0.18,
            0.18
        ]
    },
    {
        "question": "Based on background information, What is the specific event listed for 2024-07-08, in Summer One_All 2024",
        "context": "Summer One_All 2024: Date: 2024-07-08 Day: M Event: Mini-6 drop deadline; withdrawal grade assigned after this date (2)",
        "retrieved_doc": "Summer One_All 2024: Date: 2024-06-21 Day: F Event: Mini-5 Final Exams|Summer One_All 2024: Date: 2024-06-28 Day: F Event: Mini-6 add, audit, & tuition adjustment drop deadline (1)|Summer One_All 2024: Date: 2024-07-08 Day: M Event: Mini-6 drop deadline; withdrawal grade assigned after this date (2)",
        "retrieved_relevant_part": true,
        "model_answer": "The mini-6 drop deadline.<|endoftext|>",
        "actual_answer": "The event happening on 2024-07-08 is Mini-6 drop deadline",
        "exact_match": false,
        "f1_max_min_avg": [
            0.31,
            0,
            0.155
        ],
        "recall_max_min_avg": [
            0.22,
            0,
            0.11
        ]
    },
    {
        "question": "According to background information, When is the [Reading Day] in Spring 2024?",
        "context": "Spring 2024: Date: 2024-05-01 Day: W Event: Reading Day",
        "retrieved_doc": "Spring 2024: Date: 2024-05-01 Day: W Event: Reading Day|Fall 2023: Date: 2023-12-13 Day: W Event: Reading Day |Spring 2024: Date: 2024-05-04-2024-05-05 Day: Sa-Su Event: Reading Days",
        "retrieved_relevant_part": true,
        "model_answer": "The [Reading Day] in Spring 2024 is on May 4 and May 5",
        "actual_answer": "2024-05-01",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Based on the given information, Does Spring 2024 Mini-3 Faculty Course Evaluations open happen at 2024-01-15",
        "context": "Spring 2024: Date: 2024-02-19 Day: M Event: Mini-3 Faculty Course Evaluations open",
        "retrieved_doc": "Spring 2024: Date: 2024-02-19 Day: M Event: Mini-3 Faculty Course Evaluations open|Fall 2023: Date: 2023-10-02 Day: M Event: Mini-1 Faculty Course Evaluations open|Spring 2024: Date: 2024-03-02 Day: Sa Event: Mini-3 Faculty Course Evaluations close",
        "retrieved_relevant_part": true,
        "model_answer": "No",
        "actual_answer": "No",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "According to background information, When is the [Mini-1 Exams] in Fall 2023?",
        "context": "Fall 2023: Date: 2023-10-14 Day: Sa Event: Mini-1 Exams",
        "retrieved_doc": "Fall 2023: Date: 2023-10-14 Day: Sa Event: Mini-1 Exams|Spring 2024: Date: 2024-03-02 Day: Sa Event: Mini-3 Exams|Fall 2023: Date: 2023-12-18 Day: M Event: Make-Up Final Exams",
        "retrieved_relevant_part": true,
        "model_answer": "The mini-1 exams are on October 14 and December 18",
        "actual_answer": "2023-10-14",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Based on background information, What is the specific event listed for 2024-07-29, in Summer One_All 2024",
        "context": "Summer One_All 2024: Date: 2024-07-29 Day: M Event: Semester & Mini-6 Faculty Course Evalutations open ",
        "retrieved_doc": "Summer One_All 2024: Date: 2024-06-21 Day: F Event: Mini-5 Final Exams|Summer One_All 2024: Date: 2024-06-28 Day: F Event: Mini-6 add, audit, & tuition adjustment drop deadline (1)|Summer One_All 2024: Date: 2024-07-08 Day: M Event: Mini-6 drop deadline; withdrawal grade assigned after this date (2)",
        "retrieved_relevant_part": false,
        "model_answer": "The mini-6 drop deadline.<|endoftext|>",
        "actual_answer": "Semester & Mini-6 Faculty Course Evalutations open",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "According to background information, When is the [Mini-1 add, audit & tuition adjustment drop deadline  (1)] in Fall 2023?",
        "context": "Fall 2023: Date: 2023-09-01 Day: F Event: Mini-1 add, audit & tuition adjustment drop deadline  (1)",
        "retrieved_doc": "Fall 2023: Date: 2023-09-01 Day: F Event: Mini-1 add, audit & tuition adjustment drop deadline  (1)|Fall 2023: Date: 2023-10-27 Day: F Event: Mini-2 add, audit & tuition adjustment drop deadline (1)|Fall 2023: Date: 2023-09-11 Day: M Event: Semester add, audit & tuition adjustment drop deadline (1)",
        "retrieved_relevant_part": true,
        "model_answer": "The mini-1 add, audit & tuition adjustment drop deadline for Fall 2023 is on September 1",
        "actual_answer": "2023-09-01",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Based on the given information, Does Summer One_All 2024 Independence Day; University Closed & No Classes happen at 2024-05-06",
        "context": "Summer One_All 2024: Date: 2024-07-04 Day: Th Event: Independence Day; University Closed & No Classes",
        "retrieved_doc": "Summer One_All 2024: Date: 2024-07-04 Day: Th Event: Independence Day; University Closed & No Classes|Summer Two 2024: Date: 2024-07-04 Day: Th Event: Independence Day; University Closed & No Classes|Summer One_All 2024: Date: 2022-06-19 Day: W Event: Juneteenth; University Closed & No Classes",
        "retrieved_relevant_part": true,
        "model_answer": "False",
        "actual_answer": "No",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Based on background information, What is the specific event listed for 2023-09-01, in Fall 2023",
        "context": "Fall 2023: Date: 2023-09-01 Day: F Event: Mini-1 add, audit & tuition adjustment drop deadline  (1)",
        "retrieved_doc": "Fall 2023: Date: 2023-10-16-2023-10-20 Day: M-F Event: Fall Break; No Classes|Spring 2024: Date: 2024-05-04-2024-05-05 Day: Sa-Su Event: Reading Days|Fall 2023: Date: 2023-11-13-2023-11-17 Day: M-F Event: Spring 2024 Registration Week",
        "retrieved_relevant_part": false,
        "model_answer": "The event for 2023-09-01, in Fall 2023, is \"Reading Days.\"",
        "actual_answer": "Mini-1 add, audit & tuition adjustment drop deadline  (1)",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Based on background information, What is the specific event listed for 2024-05-27, in Summer One_All 2024",
        "context": "Summer One_All 2024: Date: 2024-05-27 Day: M Event: Memorial Day; University Closed & No Classes",
        "retrieved_doc": "Summer One_All 2024: Date: 2024-06-21 Day: F Event: Mini-5 Final Exams|Summer One_All 2024: Date: 2024-06-28 Day: F Event: Mini-6 add, audit, & tuition adjustment drop deadline (1)|Summer One_All 2024: Date: 2024-06-20 Day: Th Event: Mini-5 Last Day of Classes",
        "retrieved_relevant_part": false,
        "model_answer": "The mini-5 final exams",
        "actual_answer": "Memorial Day",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Based on the given information, Does Fall 2023 Mini-2 drop deadline; withdrawal grade assigned after this date (2) happen at 2023-11-15",
        "context": "Fall 2023: Date: 2023-11-15 Day: W Event: Mini-2 drop deadline; withdrawal grade assigned after this date (2)",
        "retrieved_doc": "Fall 2023: Date: 2023-11-15 Day: W Event: Mini-2 drop deadline; withdrawal grade assigned after this date (2)|Fall 2023: Date: 2023-09-20 Day: W Event: Mini-1 drop deadline; withdrawal grade assigned after this date (2)|Summer One_All 2024: Date: 2024-07-08 Day: M Event: Mini-6 drop deadline; withdrawal grade assigned after this date (2)",
        "retrieved_relevant_part": true,
        "model_answer": "Yes",
        "actual_answer": "Yes",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Based on the given information, Does Summer One_All 2024 Mini-5 Faculty Course Evaluations open happen at 2024-06-14",
        "context": "Summer One_All 2024: Date: 2024-06-14 Day: F Event: Mini-5 Faculty Course Evaluations open",
        "retrieved_doc": "Summer One_All 2024: Date: 2024-06-14 Day: F Event: Mini-5 Faculty Course Evaluations open|Summer One_All 2024: Date: 2024-06-21 Day: F Event: Mini-5 Faculty Course Evaluations close |Summer One_All 2024: Date: 2024-07-29 Day: M Event: Semester & Mini-6 Faculty Course Evalutations open ",
        "retrieved_relevant_part": true,
        "model_answer": "Yes",
        "actual_answer": "Yes",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Based on background information, What is the specific event listed for 2024-05-08, in Spring 2024",
        "context": "Spring 2024: Date: 2024-05-08 Day: W Event: Graduating Final Grades Due by 4 pm",
        "retrieved_doc": "Spring 2024: Date: 2024-05-04-2024-05-05 Day: Sa-Su Event: Reading Days|Spring 2024: Date: 2024-03-04-2024-03-08 Day: M-F Event: Spring Break; No Classes|Spring 2024: Date: 2024-04-15-2024-04-19 Day: M-F Event: Fall 2024 Registration Week",
        "retrieved_relevant_part": false,
        "model_answer": "The event for May 8, 2024, in Spring 2024 is \"Reading Days.\"",
        "actual_answer": "Graduating Final Grades Due by 4 pm",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Based on the given information, Does Summer One_All 2024 Mini-6 First Day of Classes happen at 2024-06-24",
        "context": "Summer One_All 2024: Date: 2024-06-24 Day: M Event: Mini-6 First Day of Classes",
        "retrieved_doc": "Summer One_All 2024: Date: 2024-06-24 Day: M Event: Mini-6 First Day of Classes|Summer One_All 2024: Date: 2024-05-13 Day: M Event: Semester & Mini-5 Classes Begin|Summer One_All 2024: Date: 2024-06-20 Day: Th Event: Mini-5 Last Day of Classes",
        "retrieved_relevant_part": true,
        "model_answer": "Yes",
        "actual_answer": "Yes",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "According to background information, What happens in 2024-07-29 on university calendar?",
        "context": "Summer One_All 2024: Date: 2024-07-29 Day: M Event: Semester & Mini-6 Faculty Course Evalutations open ",
        "retrieved_doc": "Spring 2024: Date: 2024-05-06 Day: M Event: Semester & Mini-4 Faculty Course Evaluations close |Fall 2023: Date: 2023-09-04 Day: M Event: Labor Day; No Classes & University Closed|Summer Two 2024: Date: 2024-07-04 Day: Th Event: Independence Day; University Closed & No Classes",
        "retrieved_relevant_part": false,
        "model_answer": "The university is closed and there are no classes.<|endoftext|>",
        "actual_answer": "The event happening on 2024-07-29 is Semester & Mini-6 Faculty Course Evalutations open",
        "exact_match": false,
        "f1_max_min_avg": [
            0.18,
            0.18,
            0.18
        ],
        "recall_max_min_avg": [
            0.15,
            0.15,
            0.15
        ]
    },
    {
        "question": "Based on the given information, Does Fall 2023 Mini-1 voucher election deadline (4) happen at 2023-10-13",
        "context": "Fall 2023: Date: 2023-10-13 Day: F Event: Mini-1 voucher election deadline (4)",
        "retrieved_doc": "Fall 2023: Date: 2023-10-13 Day: F Event: Mini-1 voucher election deadline (4)|Fall 2023: Date: 2023-10-02 Day: M Event: Mini-1 Pass/no pass & withdrawal deadline (3)|Spring 2024: Date: 2024-03-01 Day: F Event: Mini-3 voucher deadline (4)",
        "retrieved_relevant_part": true,
        "model_answer": "Yes",
        "actual_answer": "Yes",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "According to background information, When is the [Final Examinations ] in Spring 2024?",
        "context": "Spring 2024: Date: 2024-04-29-2024-04-30 Day: M-Tu Event: Final Examinations ",
        "retrieved_doc": "Spring 2024: Date: 2024-05-06 Day: M Event: Final Examinations|Spring 2024: Date: 2024-05-07 Day: Tu Event: Make-Up Final Examinations|Fall 2023: Date: 2023-12-18 Day: M Event: Make-Up Final Exams",
        "retrieved_relevant_part": false,
        "model_answer": "The [Final Examinations] in Spring 2024 is on May 6 and May 7",
        "actual_answer": "2024-04-29-2024-04-30",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Based on the given information, Does Summer Two 2024 Summer Semester  Two drop deadline; withdrawal grade assigned after this date (2) happen at 2024-07-29",
        "context": "Summer Two 2024: Date: 2024-07-08 Day: M Event: Summer Semester  Two drop deadline; withdrawal grade assigned after this date (2)",
        "retrieved_doc": "Summer Two 2024: Date: 2024-07-08 Day: M Event: Summer Semester  Two drop deadline; withdrawal grade assigned after this date (2)|Summer One_All 2024: Date: 2024-06-18 Day: T Event: Semester course drop deadline; withdrawal grade assigned after this date (2)|Summer Two 2024: Date: 2024-07-22 Day: F Event: Summer Semester  Two pass/no pass & withdrawal deadline (3)",
        "retrieved_relevant_part": true,
        "model_answer": "No",
        "actual_answer": "No",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "According to background information, When is the [Semester pass/no pass & withdrawal deadline (3)] in Summer One_All 2024?",
        "context": "Summer One_All 2024: Date: 2024-07-05 Day: F Event: Semester pass/no pass & withdrawal deadline (3)",
        "retrieved_doc": "Summer One_All 2024: Date: 2024-07-05 Day: F Event: Semester pass/no pass & withdrawal deadline (3)|Summer Two 2024: Date: 2024-07-22 Day: F Event: Summer Semester  Two pass/no pass & withdrawal deadline (3)|Summer One_All 2024: Date: 2024-06-18 Day: T Event: Semester course drop deadline; withdrawal grade assigned after this date (2)",
        "retrieved_relevant_part": true,
        "model_answer": "The semester pass/no pass & withdrawal deadline (3) in Summer One_All 2024 is on July 5",
        "actual_answer": "The event Semester pass/no pass & withdrawal deadline (3) happens on 2024-07-05",
        "exact_match": false,
        "f1_max_min_avg": [
            0.57,
            0.57,
            0.57
        ],
        "recall_max_min_avg": [
            0.67,
            0.67,
            0.67
        ]
    },
    {
        "question": "According to background information, What happens in 2024-05-06 on university calendar?",
        "context": "Spring 2024: Date: 2024-05-06 Day: M Event: Final Examinations",
        "retrieved_doc": "Spring 2024: Date: 2024-05-06 Day: M Event: Semester & Mini-4 Faculty Course Evaluations close |Fall 2023: Date: 2023-09-04 Day: M Event: Labor Day; No Classes & University Closed|Spring 2024: Date: 2024-04-15 Day: M Event: Semester & Mini-4 Faculty Course Evaluations open ",
        "retrieved_relevant_part": false,
        "model_answer": "The semester & mini-4 faculty course evaluations are scheduled for May 6, 2024",
        "actual_answer": "The event happening on 2024-05-06 is Final Examinations",
        "exact_match": false,
        "f1_max_min_avg": [
            0.1,
            0.1,
            0.1
        ],
        "recall_max_min_avg": [
            0.12,
            0.12,
            0.12
        ]
    },
    {
        "question": "Based on the given information, Does Summer One_All 2024 Mini-5 pass/no pass & withdrawal deadline (3) happen at 2024-06-24",
        "context": "Summer One_All 2024: Date: 2024-06-07 Day: F Event: Mini-5 pass/no pass & withdrawal deadline (3)",
        "retrieved_doc": "Summer One_All 2024: Date: 2024-06-07 Day: F Event: Mini-5 pass/no pass & withdrawal deadline (3)|Summer One_All 2024: Date: 2024-07-22 Day: F Event: Mini-6 pass/no pass & withdrawal deadline (3)|Fall 2023: Date: 2023-10-02 Day: M Event: Mini-1 Pass/no pass & withdrawal deadline (3)",
        "retrieved_relevant_part": true,
        "model_answer": "No",
        "actual_answer": "No",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "According to background information, When is the [Summer Semester  Two pass/no pass & withdrawal deadline (3)] in Summer Two 2024?",
        "context": "Summer Two 2024: Date: 2024-07-22 Day: F Event: Summer Semester  Two pass/no pass & withdrawal deadline (3)",
        "retrieved_doc": "Summer Two 2024: Date: 2024-07-22 Day: F Event: Summer Semester  Two pass/no pass & withdrawal deadline (3)|Summer Two 2024: Date: 2024-07-08 Day: M Event: Summer Semester  Two drop deadline; withdrawal grade assigned after this date (2)|Summer One_All 2024: Date: 2024-07-05 Day: F Event: Semester pass/no pass & withdrawal deadline (3)",
        "retrieved_relevant_part": true,
        "model_answer": "The [Summer Semester  Two pass/no pass & withdrawal deadline (3)] in Summer Two 2024 is on July 22",
        "actual_answer": "The event Summer Semester  Two pass/no pass & withdrawal deadline (3) happens on 2024-07-22",
        "exact_match": false,
        "f1_max_min_avg": [
            0.63,
            0.63,
            0.63
        ],
        "recall_max_min_avg": [
            0.71,
            0.71,
            0.71
        ]
    },
    {
        "question": "Based on background information, What is the specific event listed for 2024-02-19, in Spring 2024",
        "context": "Spring 2024: Date: 2024-02-19 Day: M Event: Mini-3 Faculty Course Evaluations open",
        "retrieved_doc": "Spring 2024: Date: 2024-03-12 Day: T Event: Summer 2024 Registration Opens |Spring 2024: Date: 2024-04-15-2024-04-19 Day: M-F Event: Fall 2024 Registration Week|Spring 2024: Date: 2024-05-04-2024-05-05 Day: Sa-Su Event: Reading Days",
        "retrieved_relevant_part": false,
        "model_answer": "The event for 2024-02-19, in Spring 2024, is \"Reading Days.\"<|endoftext|>",
        "actual_answer": "The event happening on 2024-02-19 is Mini-3 Faculty Course Evaluations open",
        "exact_match": false,
        "f1_max_min_avg": [
            0.29,
            0.29,
            0.29
        ],
        "recall_max_min_avg": [
            0.27,
            0.27,
            0.27
        ]
    },
    {
        "question": "Based on the given information, Does Spring 2024 Mini-3 voucher deadline (4) happen at 2023-10-13",
        "context": "Spring 2024: Date: 2024-03-01 Day: F Event: Mini-3 voucher deadline (4)",
        "retrieved_doc": "Spring 2024: Date: 2024-03-01 Day: F Event: Mini-3 voucher deadline (4)|Summer One_All 2024: Date: 2024-06-20 Day: Th Event: Mini-5 voucher deadline (4)|Spring 2024: Date: 2024-04-15 Day: M Event: Mini-4 pass/no pass & withdrawal deadline (3)",
        "retrieved_relevant_part": true,
        "model_answer": "No",
        "actual_answer": "No",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Based on background information, What is the specific event listed for 2024-04-01, in Spring 2024",
        "context": "Spring 2024: Date: 2024-04-01 Day: M Event: Semester pass/no pass & withdrawal deadline (3)",
        "retrieved_doc": "Spring 2024: Date: 2024-05-04-2024-05-05 Day: Sa-Su Event: Reading Days|Spring 2024: Date: 2024-03-12 Day: T Event: Summer 2024 Registration Opens |Spring 2024: Date: 2024-03-04-2024-03-08 Day: M-F Event: Spring Break; No Classes",
        "retrieved_relevant_part": false,
        "model_answer": "The event for 2024-04-01, in Spring 2024, is \"Reading Days.\"",
        "actual_answer": "The event happening on 2024-04-01 is Semester pass/no pass & withdrawal deadline (3)",
        "exact_match": false,
        "f1_max_min_avg": [
            0.26,
            0.26,
            0.26
        ],
        "recall_max_min_avg": [
            0.23,
            0.23,
            0.23
        ]
    },
    {
        "question": "According to background information, When is the [Mini-5 pass/no pass & withdrawal deadline (3)] in Summer One_All 2024?",
        "context": "Summer One_All 2024: Date: 2024-06-07 Day: F Event: Mini-5 pass/no pass & withdrawal deadline (3)",
        "retrieved_doc": "Summer One_All 2024: Date: 2024-06-07 Day: F Event: Mini-5 pass/no pass & withdrawal deadline (3)|Summer One_All 2024: Date: 2024-07-22 Day: F Event: Mini-6 pass/no pass & withdrawal deadline (3)|Fall 2023: Date: 2023-10-02 Day: M Event: Mini-1 Pass/no pass & withdrawal deadline (3)",
        "retrieved_relevant_part": true,
        "model_answer": "The [Mini-5 pass/no pass & withdrawal deadline (3)] in Summer One_All 2024 is on June 7",
        "actual_answer": "The event Mini-5 pass/no pass & withdrawal deadline (3) happens on 2024-06-07",
        "exact_match": false,
        "f1_max_min_avg": [
            0.5,
            0.5,
            0.5
        ],
        "recall_max_min_avg": [
            0.58,
            0.58,
            0.58
        ]
    },
    {
        "question": "Based on the given information, Does Summer One_All 2024 Mini-5 Faculty Course Evaluations open happen at 2024-06-14",
        "context": "Summer One_All 2024: Date: 2024-06-14 Day: F Event: Mini-5 Faculty Course Evaluations open",
        "retrieved_doc": "Summer One_All 2024: Date: 2024-06-14 Day: F Event: Mini-5 Faculty Course Evaluations open|Summer One_All 2024: Date: 2024-06-21 Day: F Event: Mini-5 Faculty Course Evaluations close |Summer One_All 2024: Date: 2024-07-29 Day: M Event: Semester & Mini-6 Faculty Course Evalutations open ",
        "retrieved_relevant_part": true,
        "model_answer": "Yes",
        "actual_answer": "Yes",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Based on the given information, Does Spring 2024 Final Grades Due by 4 pm happen at 2023-12-17",
        "context": "Spring 2024: Date: 2024-05-14 Day: Tu Event: Final Grades Due by 4 pm",
        "retrieved_doc": "Fall 2023: Date: 2023-12-20 Day: W Event: Final Grades Due by 4 pm |Spring 2024: Date: 2024-05-08 Day: W Event: Graduating Final Grades Due by 4 pm|Spring 2024: Date: 2024-05-14 Day: Tu Event: Final Grades Due by 4 pm",
        "retrieved_relevant_part": true,
        "model_answer": "No",
        "actual_answer": "No",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Based on background information, What is the specific event listed for 2024-06-21, in Summer One_All 2024",
        "context": "Summer One_All 2024: Date: 2024-06-21 Day: F Event: Mini-5 Final Exams",
        "retrieved_doc": "Summer One_All 2024: Date: 2024-06-21 Day: F Event: Mini-5 Final Exams|Summer One_All 2024: Date: 2024-06-20 Day: Th Event: Mini-5 Last Day of Classes|Summer One_All 2024: Date: 2024-06-07 Day: F Event: Mini-5 pass/no pass & withdrawal deadline (3)",
        "retrieved_relevant_part": true,
        "model_answer": "The mini-5 final exams",
        "actual_answer": "Mini-5 Final Exams",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "According to background information, What happens in 2023-12-18 on university calendar?",
        "context": "Fall 2023: Date: 2023-12-18 Day: M Event: Semester & Mini-2 Faculty Course Evaluations close",
        "retrieved_doc": "Fall 2023: Date: 2023-09-04 Day: M Event: Labor Day; No Classes & University Closed|Fall 2023: Date: 2023-12-23-2024-01-02 Day: Sa-Tu Event: Winter Break; University Closed|Summer Two 2024: Date: 2024-07-04 Day: Th Event: Independence Day; University Closed & No Classes",
        "retrieved_relevant_part": false,
        "model_answer": "The university is closed and there are no classes during this period",
        "actual_answer": "The event happening on 2023-12-18 is Semester & Mini-2 Faculty Course Evaluations close",
        "exact_match": false,
        "f1_max_min_avg": [
            0.16,
            0.16,
            0.16
        ],
        "recall_max_min_avg": [
            0.15,
            0.15,
            0.15
        ]
    },
    {
        "question": "Based on the given information, Does Summer Two 2024 Summer Semester  Two Last Day of Classes happen at 2024-08-01",
        "context": "Summer Two 2024: Date: 2024-08-01 Day: Th Event: Summer Semester  Two Last Day of Classes",
        "retrieved_doc": "Summer Two 2024: Date: 2024-08-01 Day: Th Event: Summer Semester  Two Last Day of Classes|Summer Two 2024: Date: 2024-08-02 Day: F Event: Summer Semester  Two Faculty Course Evaluations close |Summer Two 2024: Date: 2024-07-29 Day: M Event: Summer Semester  Two Faculty Course Evaulations open",
        "retrieved_relevant_part": true,
        "model_answer": "Yes",
        "actual_answer": "Yes",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Based on background information, What is the specific event listed for 2024-05-14, in Spring 2024",
        "context": "Spring 2024: Date: 2024-05-14 Day: Tu Event: Final Grades Due by 4 pm",
        "retrieved_doc": "Spring 2024: Date: 2024-05-04-2024-05-05 Day: Sa-Su Event: Reading Days|Spring 2024: Date: 2024-04-15-2024-04-19 Day: M-F Event: Fall 2024 Registration Week|Spring 2024: Date: 2024-03-04-2024-03-08 Day: M-F Event: Spring Break; No Classes",
        "retrieved_relevant_part": false,
        "model_answer": "The event for May 14, 2024, in Spring 2024 is \"Reading Days.\"",
        "actual_answer": "The event happening on 2024-05-14 is Final Grades Due by 4 pm",
        "exact_match": false,
        "f1_max_min_avg": [
            0.25,
            0.25,
            0.25
        ],
        "recall_max_min_avg": [
            0.25,
            0.25,
            0.25
        ]
    },
    {
        "question": "According to background information, When is the [Final Exams ] in Fall 2023?",
        "context": "Fall 2023: Date: 2023-12-11-2023-12-12 Day: M-Tu Event: Final Exams ",
        "retrieved_doc": "Fall 2023: Date: 2023-12-18 Day: M Event: Make-Up Final Exams|Fall 2023: Date: 2023-12-17 Day: Su Event: Final Exams |Fall 2023: Date: 2023-12-14-2023-12-15 Day: Th-F Event: Final Exams ",
        "retrieved_relevant_part": false,
        "model_answer": "The [Final Exams] in Fall 2023 is on December 14-15",
        "actual_answer": "The event Final Exams  happens on 2023-12-11-2023-12-12",
        "exact_match": false,
        "f1_max_min_avg": [
            0.24,
            0.24,
            0.24
        ],
        "recall_max_min_avg": [
            0.29,
            0.29,
            0.29
        ]
    },
    {
        "question": "Based on background information, What is the specific event listed for 2024-07-04, in Summer Two 2024",
        "context": "Summer Two 2024: Date: 2024-07-04 Day: Th Event: Independence Day; University Closed & No Classes",
        "retrieved_doc": "Summer One_All 2024: Date: 2024-06-21 Day: F Event: Mini-5 Final Exams|Summer One_All 2024: Date: 2024-06-28 Day: F Event: Mini-6 add, audit, & tuition adjustment drop deadline (1)|Summer One_All 2024: Date: 2024-07-08 Day: M Event: Mini-6 drop deadline; withdrawal grade assigned after this date (2)",
        "retrieved_relevant_part": false,
        "model_answer": "The mini-6 drop deadline.<|endoftext|>",
        "actual_answer": "The event happening on 2024-07-04 is Independence Day",
        "exact_match": false,
        "f1_max_min_avg": [
            0.17,
            0,
            0.085
        ],
        "recall_max_min_avg": [
            0.12,
            0,
            0.06
        ]
    },
    {
        "question": "Based on background information, What is the specific event listed for 2024-02-26, in Spring 2024",
        "context": "Spring 2024: Date: 2024-02-26 Day: M Event: Semester course drop deadline; withdrawal grade assigned after this date (2)",
        "retrieved_doc": "Spring 2024: Date: 2024-04-15-2024-04-19 Day: M-F Event: Fall 2024 Registration Week|Spring 2024: Date: 2024-05-04-2024-05-05 Day: Sa-Su Event: Reading Days|Spring 2024: Date: 2024-03-12 Day: T Event: Summer 2024 Registration Opens ",
        "retrieved_relevant_part": false,
        "model_answer": "The event for 2024-02-26, in Spring 2024, is \"Reading Days.\"<|endoftext|>",
        "actual_answer": "The event happening on 2024-02-26 is Semester course drop deadline",
        "exact_match": false,
        "f1_max_min_avg": [
            0.3,
            0,
            0.15
        ],
        "recall_max_min_avg": [
            0.3,
            0,
            0.15
        ]
    },
    {
        "question": "According to background information, When is the [Reading Day] in Spring 2024?",
        "context": "Spring 2024: Date: 2024-05-01 Day: W Event: Reading Day",
        "retrieved_doc": "Spring 2024: Date: 2024-05-01 Day: W Event: Reading Day|Fall 2023: Date: 2023-12-13 Day: W Event: Reading Day |Spring 2024: Date: 2024-05-04-2024-05-05 Day: Sa-Su Event: Reading Days",
        "retrieved_relevant_part": true,
        "model_answer": "The [Reading Day] in Spring 2024 is on May 4 and May 5",
        "actual_answer": "2024-05-01",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "According to background information, When is the [Spring Deans' Lists Posted] in Spring 2024?",
        "context": "Spring 2024: Date: 2023-05-21 Day: Tu Event: Spring Deans' Lists Posted",
        "retrieved_doc": "Spring 2024: Date: 2023-05-21 Day: Tu Event: Spring Deans' Lists Posted|Fall 2023: Date: 2023-01-08 Day: M Event: Fall Deans' Lists Posted|Summer One_All 2024: Date: 2024-07-29 Day: M Event: Semester & Mini-6 Faculty Course Evalutations open ",
        "retrieved_relevant_part": true,
        "model_answer": "The [Spring Deans' Lists Posted] in Spring 2024 is on May 21",
        "actual_answer": "The event Spring Deans' Lists Posted happens on 2023-05-21",
        "exact_match": false,
        "f1_max_min_avg": [
            0.48,
            0.48,
            0.48
        ],
        "recall_max_min_avg": [
            0.56,
            0.56,
            0.56
        ]
    },
    {
        "question": "Based on the given information, Does Fall 2023 Spring 2024 Registration Week happen at 2023-11-13-2023-11-17",
        "context": "Fall 2023: Date: 2023-11-13-2023-11-17 Day: M-F Event: Spring 2024 Registration Week",
        "retrieved_doc": "Fall 2023: Date: 2023-11-13-2023-11-17 Day: M-F Event: Spring 2024 Registration Week|Spring 2024: Date: 2024-04-15-2024-04-19 Day: M-F Event: Fall 2024 Registration Week|Spring 2024: Date: 2024-03-12 Day: T Event: Summer 2024 Registration Opens ",
        "retrieved_relevant_part": true,
        "model_answer": "Yes",
        "actual_answer": "Yes",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Based on the given information, Does Summer One_All 2024 Semester & Mini-6 Faculty Course Evaluations close happen at 2024-08-02",
        "context": "Summer One_All 2024: Date: 2024-08-02 Day: F Event: Semester & Mini-6 Faculty Course Evaluations close",
        "retrieved_doc": "Summer One_All 2024: Date: 2024-08-02 Day: F Event: Semester & Mini-6 Faculty Course Evaluations close|Summer One_All 2024: Date: 2024-07-29 Day: M Event: Semester & Mini-6 Faculty Course Evalutations open |Summer One_All 2024: Date: 2024-06-21 Day: F Event: Mini-5 Faculty Course Evaluations close ",
        "retrieved_relevant_part": true,
        "model_answer": "Yes",
        "actual_answer": "Yes",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Based on the given information, Does Spring 2024 Mid-Semester & Mini-3 grades due by 4 pm happen at 2024-04-11-2024-04-13",
        "context": "Spring 2024: Date: 2024-03-11 Day: M Event: Mid-Semester & Mini-3 grades due by 4 pm",
        "retrieved_doc": "Spring 2024: Date: 2024-03-11 Day: M Event: Mid-Semester & Mini-3 grades due by 4 pm|Fall 2023: Date: 2023-10-23 Day: M Event: Mid-Semester & Mini-1 grades due by 4 pm|Summer One_All 2024: Date: 2024-08-06 Day: T Event: Semester & Mini-6 Final Grades Due by 4 pm",
        "retrieved_relevant_part": true,
        "model_answer": "No",
        "actual_answer": "No",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "According to background information, What happens in 2024-05-13 on university calendar?",
        "context": "Summer One_All 2024: Date: 2024-05-13 Day: M Event: Semester & Mini-5 Classes Begin",
        "retrieved_doc": "Fall 2023: Date: 2023-09-04 Day: M Event: Labor Day; No Classes & University Closed|Spring 2024: Date: 2024-05-06 Day: M Event: Semester & Mini-4 Faculty Course Evaluations close |Fall 2023: Date: 2023-12-23-2024-01-02 Day: Sa-Tu Event: Winter Break; University Closed",
        "retrieved_relevant_part": false,
        "model_answer": "The university is closed.<|endoftext|>",
        "actual_answer": "Semester & Mini-5 Classes Begin",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Based on background information, What is the specific event listed for 2024-06-07, in Summer One_All 2024",
        "context": "Summer One_All 2024: Date: 2024-06-07 Day: F Event: Mini-5 pass/no pass & withdrawal deadline (3)",
        "retrieved_doc": "Summer One_All 2024: Date: 2024-06-21 Day: F Event: Mini-5 Final Exams|Summer One_All 2024: Date: 2024-06-28 Day: F Event: Mini-6 add, audit, & tuition adjustment drop deadline (1)|Summer One_All 2024: Date: 2024-07-08 Day: M Event: Mini-6 drop deadline; withdrawal grade assigned after this date (2)",
        "retrieved_relevant_part": false,
        "model_answer": "The mini-5 final exams",
        "actual_answer": "The event happening on 2024-06-07 is Mini-5 pass/no pass & withdrawal deadline (3)",
        "exact_match": false,
        "f1_max_min_avg": [
            0.12,
            0.12,
            0.12
        ],
        "recall_max_min_avg": [
            0.08,
            0.08,
            0.08
        ]
    },
    {
        "question": "Based on the given information, Does Fall 2023 Final Exams  happen at 2023-12-11-2023-12-12",
        "context": "Fall 2023: Date: 2023-12-11-2023-12-12 Day: M-Tu Event: Final Exams ",
        "retrieved_doc": "Fall 2023: Date: 2023-12-14-2023-12-15 Day: Th-F Event: Final Exams |Fall 2023: Date: 2023-12-18 Day: M Event: Make-Up Final Exams|Fall 2023: Date: 2023-12-17 Day: Su Event: Final Exams ",
        "retrieved_relevant_part": false,
        "model_answer": "No",
        "actual_answer": "Yes",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "When does the class International Finance Recitation Section A4  offered in Sp end?",
        "context": "Spring offering: Course: 45825 Title: International Finance Units: 6.0 Lec/Sec: Section A4 Days: Tuesday, Thursday Begin: 10:00AM End: 11:45AM Bldg/Room: TEP 2118 Location: Pittsburgh, Pennsylvania Instructor(s): Khokher ",
        "retrieved_doc": "Summer 1/Summer All offering: Course: 17996 Title: Societal Computing Independent Study Units: VAR Lec/Sec: Section B Days: To be announced Begin: 05:00PM End: 06:20PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Sadeh |Summer 1/Summer All offering: Course: 17996 Title: Societal Computing Independent Study Units: VAR Lec/Sec: Section M Days: To be announced Begin: 05:00PM End: 06:20PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Vasilescu |Spring offering: Course: 82500 Title: Modern Languages Independent Study Units: VAR Lec/Sec: Section B Days: To be announced Begin: 12:30PM End: 01:50PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Instructor TBA ",
        "retrieved_relevant_part": false,
        "model_answer": "The course International Finance Recitation Section A4 is offered in the summer semester",
        "actual_answer": "11:45AM",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "At what time does the class Financial Statements and Analysis of Companies Recitation Section C1  offered in Fa begin?",
        "context": "Fall offering: Course: 90723 Title: Financial Statements and Analysis of Companies Units: 6.0 Lec/Sec: Section C1 Days: Monday, Wednesday Begin: 11:00AM End: 12:20PM Bldg/Room: HBH 2008 Location: Pittsburgh, Pennsylvania Instructor(s): Gogolak ",
        "retrieved_doc": "Fall offering: Course: 90723 Title: Financial Statements and Analysis of Companies Units: 6.0 Lec/Sec: Section C1 Days: Monday, Wednesday Begin: 11:00AM End: 12:20PM Bldg/Room: HBH 2008 Location: Pittsburgh, Pennsylvania Instructor(s): Gogolak |Fall offering: Course: 90723 Title: Financial Statements and Analysis of Companies Units: 6.0 Lec/Sec: Section C1 Days: Friday Begin: 02:00PM End: 03:20PM Bldg/Room: HBH 2008 Location: Pittsburgh, Pennsylvania Instructor(s): Gogolak |Fall offering: Course: 19705 Title: Workshop Applied Policy Analysis Units: 6.0 Lec/Sec: Section PP Days: To be announced Begin: 12:30PM End: 01:50PM Bldg/Room: TBA Location: Lisbon, Portugal Instructor(s): Adams ",
        "retrieved_relevant_part": true,
        "model_answer": "The class Financial Statements and Analysis of Companies Recitation Section C1 begins at 11:00AM",
        "actual_answer": "11:00AM",
        "exact_match": false,
        "f1_max_min_avg": [
            0.13,
            0.13,
            0.13
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "What is the units of course Principles of Imperative Computation offered in Fa?",
        "context": "Fall offering: Course: 15122 Title: Principles of Imperative Computation Units: 12.0 Lec/Sec: Section M Days: Monday Begin: 12:00PM End: 12:50PM Bldg/Room: GHC CLSTR Location: Pittsburgh, Pennsylvania Instructor(s): Cervesato, Kaynar ",
        "retrieved_doc": "Fall offering: Course: 15122 Title: Principles of Imperative Computation Units: 12.0 Lec/Sec: Section F Days: Friday Begin: 02:00PM End: 02:50PM Bldg/Room: PH A18C Location: Pittsburgh, Pennsylvania Instructor(s): Cervesato, Kaynar |Fall offering: Course: 15122 Title: Principles of Imperative Computation Units: 12.0 Lec/Sec: Section I Days: Friday Begin: 05:00PM End: 05:50PM Bldg/Room: PH A18B Location: Pittsburgh, Pennsylvania Instructor(s): Cervesato, Kaynar |Fall offering: Course: 15122 Title: Principles of Imperative Computation Units: 12.0 Lec/Sec: Section G Days: Friday Begin: 03:00PM End: 03:50PM Bldg/Room: PH A18C Location: Pittsburgh, Pennsylvania Instructor(s): Cervesato, Kaynar ",
        "retrieved_relevant_part": false,
        "model_answer": "12.0 units",
        "actual_answer": "12.0",
        "exact_match": false,
        "f1_max_min_avg": [
            0.67,
            0.67,
            0.67
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "On which days does the class Big Data Science Recitation Section A4  offered in Sp meet?",
        "context": "Spring offering: Course: 18788 Title: Big Data Science Units: 6.0 Lec/Sec: Section A4 Days: Tuesday, Thursday Begin: 08:00AM End: 09:20AM Bldg/Room: HH 1107 Location: Pittsburgh, Pennsylvania Instructor(s): McSharry ",
        "retrieved_doc": "Summer 1/Summer All offering: Course: 17996 Title: Societal Computing Independent Study Units: VAR Lec/Sec: Section B Days: To be announced Begin: 05:00PM End: 06:20PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Sadeh |Summer 1/Summer All offering: Course: 17996 Title: Societal Computing Independent Study Units: VAR Lec/Sec: Section L Days: To be announced Begin: 05:00PM End: 06:20PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Fang |Summer 1/Summer All offering: Course: 17993 Title: Societal Computing Graduate Reading and Research: Units: VAR Lec/Sec: Section Q Days: To be announced Begin: 05:00PM End: 06:20PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Agarwal ",
        "retrieved_relevant_part": false,
        "model_answer": "The course is offered on Saturdays and Sundays.<|endoftext|>",
        "actual_answer": "Tuesday, Thursday",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Based on the background information, What is the units of course Statistics for IT Managers offered in Fa?",
        "context": "Fall offering: Course: 95796 Title: Statistics for IT Managers Units: 6.0 Lec/Sec: Section D1 Days: Tuesday, Thursday Begin: 03:30PM End: 04:50PM Bldg/Room: HBH 1206 Location: Pittsburgh, Pennsylvania Instructor(s): Garin ",
        "retrieved_doc": "Fall offering: Course: 46880 Title: Introduction to Probabilty and Statistics Units: 6.0 Lec/Sec: Section M1 Days: Monday Begin: 08:15PM End: 09:30PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Moseley |Fall offering: Course: 70515 Title: Independent Study in Operations Management or Operations Research Units: VAR Lec/Sec: Section A Days: To be announced Begin: 02:00PM End: 03:20PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Hollifield |Fall offering: Course: 67100 Title: Information Systems First Year Colloquium Units: 1.0 Lec/Sec: Section A1 Days: Friday Begin: 11:00AM End: 12:20PM Bldg/Room: DH A302 Location: Pittsburgh, Pennsylvania Instructor(s): Mertz, Lutz ",
        "retrieved_relevant_part": false,
        "model_answer": "6.0 units",
        "actual_answer": "The units of course Statistics for IT Managers is 6.0",
        "exact_match": false,
        "f1_max_min_avg": [
            0.33,
            0.33,
            0.33
        ],
        "recall_max_min_avg": [
            0.2,
            0.2,
            0.2
        ]
    },
    {
        "question": "When does the class Tap II - Rhythmic Technique/Foundational to Complex Recitation Section B  offered in Fa end?",
        "context": "Fall offering: Course: 54223 Title: Tap II - Rhythmic Technique/Foundational to Complex Units: 2.0 Lec/Sec: Section B Days: Friday Begin: 12:30PM End: 01:20PM Bldg/Room: PCA 306 Location: Pittsburgh, Pennsylvania Instructor(s): Conte ",
        "retrieved_doc": "Fall offering: Course: 54223 Title: Tap II - Rhythmic Technique/Foundational to Complex Units: 2.0 Lec/Sec: Section B Days: Friday Begin: 12:30PM End: 01:20PM Bldg/Room: PCA 306 Location: Pittsburgh, Pennsylvania Instructor(s): Conte |Fall offering: Course: 54223 Title: Tap II - Rhythmic Technique/Foundational to Complex Units: 2.0 Lec/Sec: Section A Days: Tuesday Begin: 12:30PM End: 01:20PM Bldg/Room: PCA 306 Location: Pittsburgh, Pennsylvania Instructor(s): Conte |Fall offering: Course: 54323 Title: Tap III - Expand Percussive Rhythms Units: 2.0 Lec/Sec: Section B Days: Thursday Begin: 04:30PM End: 05:20PM Bldg/Room: PCA 306 Location: Pittsburgh, Pennsylvania Instructor(s): Cousin ",
        "retrieved_relevant_part": true,
        "model_answer": "The course is offered in the fall semester",
        "actual_answer": "01:20PM",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "On which days does the class Orchestration I Recitation Section A  offered in Fa meet?",
        "context": "Fall offering: Course: 57257 Title: Orchestration I Units: 6.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 09:00AM End: 09:50AM Bldg/Room: CFA M160 Location: Pittsburgh, Pennsylvania Instructor(s): Marthaler ",
        "retrieved_doc": "Spring offering: Course: 57271 Title: Orchestration II Units: 6.0 Lec/Sec: Section A Days: Wednesday, Friday Begin: 10:00AM End: 10:50AM Bldg/Room: HOA 223 Location: Pittsburgh, Pennsylvania Instructor(s): Marthaler |Fall offering: Course: 57257 Title: Orchestration I Units: 6.0 Lec/Sec: Section C Days: Tuesday, Thursday Begin: 11:00AM End: 11:50AM Bldg/Room: HOA 223 Location: Pittsburgh, Pennsylvania Instructor(s): Marthaler |Fall offering: Course: 57257 Title: Orchestration I Units: 6.0 Lec/Sec: Section B Days: Tuesday, Thursday Begin: 10:00AM End: 10:50AM Bldg/Room: CFA M160 Location: Pittsburgh, Pennsylvania Instructor(s): Marthaler ",
        "retrieved_relevant_part": false,
        "model_answer": "The class Orchestration I Recitation Section A meets on Wednesdays and Fridays",
        "actual_answer": "Tuesday, Thursday",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "On which days does the class Weight Training: Recitation Section B3  offered in Sp meet?",
        "context": "Spring offering: Course: 69102 Title: Weight Training: Units: 3.0 Lec/Sec: Section B3 Days: Tuesday, Thursday Begin: 12:30PM End: 01:50PM Bldg/Room: CUC COURTS Location: Pittsburgh, Pennsylvania Instructor(s): Connelly ",
        "retrieved_doc": "Fall offering: Course: 03120 Title: Bench Skills with Fluorescent Proteins Units: 6.0 Lec/Sec: Section A2 Days: Thursday Begin: 07:00PM End: 10:00PM Bldg/Room: DH 2303 Location: Pittsburgh, Pennsylvania Instructor(s): McGuier |Fall offering: Course: 69102 Title: Weight Training: Units: 3.0 Lec/Sec: Section A2 Days: Monday, Wednesday, Friday Begin: 10:00AM End: 10:50AM Bldg/Room: CUC FITNES Location: Pittsburgh, Pennsylvania Instructor(s): Little, Gauntner |Fall offering: Course: 03120 Title: Bench Skills with Fluorescent Proteins Units: 6.0 Lec/Sec: Section A2 Days: Tuesday Begin: 07:00PM End: 09:00PM Bldg/Room: DH 2303 Location: Pittsburgh, Pennsylvania Instructor(s): McGuier ",
        "retrieved_relevant_part": false,
        "model_answer": "The class meets on Monday, Wednesday, and Friday.<|endoftext|>",
        "actual_answer": "Tuesday, Thursday",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Which campus is class Data Analytics for Organizational Impacts Recitation Section A2  offered in Pittsburgh in Fa?",
        "context": "Fall offering: Course: 94865 Title: Data Analytics for Organizational Impacts Units: 6.0 Lec/Sec: Section A2 Days: Tuesday, Thursday Begin: 03:30PM End: 04:50PM Bldg/Room: HBH 1004 Location: Pittsburgh, Pennsylvania Instructor(s): Escallon Barrios ",
        "retrieved_doc": "Fall offering: Course: 94465 Title: Data Analytics for Organizational Impact Units: 6.0 Lec/Sec: Section A2 Days: Tuesday, Thursday Begin: 03:30PM End: 04:50PM Bldg/Room: HBH 1004 Location: Pittsburgh, Pennsylvania Instructor(s): Escallon Barrios |Fall offering: Course: 94865 Title: Data Analytics for Organizational Impacts Units: 6.0 Lec/Sec: Section A2 Days: Tuesday, Thursday Begin: 03:30PM End: 04:50PM Bldg/Room: HBH 1004 Location: Pittsburgh, Pennsylvania Instructor(s): Escallon Barrios |Fall offering: Course: 99262 Title: ADDvocate: Anti-Discrimination Dialogue for Social Justice Units: VAR Lec/Sec: Section A1 Days: Wednesday Begin: 09:00AM End: 09:50AM Bldg/Room: CMU REMOTE Location: Pittsburgh, Pennsylvania Instructor(s): Ledford, Moufid, Vincent ",
        "retrieved_relevant_part": true,
        "model_answer": "Carnegie Mellon University",
        "actual_answer": "Pittsburgh, Pennsylvania",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Where does the class Accounting and Finance Analytics Recitation Section E4  offered in Sp meet?",
        "context": "Spring offering: Course: 95719 Title: Accounting and Finance Analytics Units: 6.0 Lec/Sec: Section E4 Days: Tuesday, Thursday Begin: 05:00PM End: 06:20PM Bldg/Room: HBH 1206 Location: Pittsburgh, Pennsylvania Instructor(s): Gogolak ",
        "retrieved_doc": "Spring offering: Course: 95719 Title: Accounting and Finance Analytics Units: 6.0 Lec/Sec: Section E4 Days: Tuesday, Thursday Begin: 05:00PM End: 06:20PM Bldg/Room: HBH 1206 Location: Pittsburgh, Pennsylvania Instructor(s): Gogolak |Fall offering: Course: 95719 Title: Accounting and Finance Analytics Units: 6.0 Lec/Sec: Section E2 Days: Tuesday, Thursday Begin: 09:30AM End: 10:50AM Bldg/Room: HBH 1204 Location: Pittsburgh, Pennsylvania Instructor(s): Gogolak |Fall offering: Course: 95719 Title: Accounting and Finance Analytics Units: 6.0 Lec/Sec: Section B1 Days: Tuesday, Thursday Begin: 09:30AM End: 10:50AM Bldg/Room: HBH 2008 Location: Pittsburgh, Pennsylvania Instructor(s): Gogolak ",
        "retrieved_relevant_part": true,
        "model_answer": "The class is held in Building HBH 1206, located in Pittsburgh, Pennsylvania.<|endoftext|>",
        "actual_answer": "HBH 1206",
        "exact_match": false,
        "f1_max_min_avg": [
            0.14,
            0.14,
            0.14
        ],
        "recall_max_min_avg": [
            0.5,
            0.5,
            0.5
        ]
    },
    {
        "question": "On which days does the class IDeATe: Soft Fabrication Skills Recitation Section A1  offered in Fa meet?",
        "context": "Fall offering: Course: 99352 Title: IDeATe: Soft Fabrication Skills Units: 1.0 Lec/Sec: Section A1 Days: Saturday Begin: 10:00AM End: 03:00PM Bldg/Room: HL 106B Location: Pittsburgh, Pennsylvania Instructor(s): Pinchuk ",
        "retrieved_doc": "Fall offering: Course: 99352 Title: IDeATe: Soft Fabrication Skills Units: 1.0 Lec/Sec: Section A1 Days: Saturday Begin: 10:00AM End: 03:00PM Bldg/Room: HL 106B Location: Pittsburgh, Pennsylvania Instructor(s): Pinchuk |Spring offering: Course: 99352 Title: IDeATe: Soft Fabrication Skills Units: 1.0 Lec/Sec: Section A3 Days: Saturday Begin: 10:30AM End: 03:20PM Bldg/Room: HL 106B Location: Pittsburgh, Pennsylvania Instructor(s): Pinchuk |Spring offering: Course: 16224 Title: IDeATe: Re-Crafting Computational Thinking with Soft Technologies Units: 12.0 Lec/Sec: Section A Days: Friday Begin: 12:00PM End: 02:50PM Bldg/Room: CFA 111 Location: Pittsburgh, Pennsylvania Instructor(s): Orta Martinez, Robinson ",
        "retrieved_relevant_part": true,
        "model_answer": "Saturdays",
        "actual_answer": "Saturday",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Which campus is class Principles of Macroeconomics Recitation Section A  offered in Pittsburgh in Sp?",
        "context": "Spring offering: Course: 73103 Title: Principles of Macroeconomics Units: 9.0 Lec/Sec: Section A Days: Friday Begin: 10:00AM End: 10:50AM Bldg/Room: TEP 3801 Location: Pittsburgh, Pennsylvania Instructor(s): Zetlin-Jones ",
        "retrieved_doc": "Fall offering: Course: 73102 Title: Principles of Microeconomics Units: 9.0 Lec/Sec: Section F Days: Friday Begin: 01:00PM End: 01:50PM Bldg/Room: TEP 3808 Location: Pittsburgh, Pennsylvania Instructor(s): Portnykh |Spring offering: Course: 73102 Title: Principles of Microeconomics Units: 9.0 Lec/Sec: Section J Days: Friday Begin: 01:00PM End: 01:50PM Bldg/Room: TEP 3808 Location: Pittsburgh, Pennsylvania Instructor(s): Portnykh |Fall offering: Course: 73102 Title: Principles of Microeconomics Units: 9.0 Lec/Sec: Section L Days: Friday Begin: 02:00PM End: 02:50PM Bldg/Room: TEP 3808 Location: Pittsburgh, Pennsylvania Instructor(s): Portnykh ",
        "retrieved_relevant_part": false,
        "model_answer": "The University of Pittsburgh",
        "actual_answer": "Pittsburgh, Pennsylvania",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "When does the class Funding Early Stage Ventures: Recitation Section A4  offered in Sp end?",
        "context": "Spring offering: Course: 45905 Title: Funding Early Stage Ventures: Units: 6.0 Lec/Sec: Section A4 Days: Tuesday, Thursday Begin: 08:00AM End: 09:45AM Bldg/Room: TEP 2111 Location: Pittsburgh, Pennsylvania Instructor(s): Risch ",
        "retrieved_doc": "Summer 1/Summer All offering: Course: 48769 Title: Thesis/Project Units: VAR Lec/Sec: Section C Days: To be announced Begin: nan End: nan Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Cardoso Llach |Spring offering: Course: 45905 Title: Funding Early Stage Ventures Units: 6.0 Lec/Sec: Section A4 Days: Thursday Begin: 06:30PM End: 10:00PM Bldg/Room: TEP 2111 Location: Pittsburgh, Pennsylvania Instructor(s): Demmler |Fall offering: Course: 45905 Title: Funding Early Stage Ventures Units: 6.0 Lec/Sec: Section E2 Days: Wednesday Begin: 06:30PM End: 09:50PM Bldg/Room: TEP 2112 Location: Pittsburgh, Pennsylvania Instructor(s): Demmler ",
        "retrieved_relevant_part": false,
        "model_answer": "The course is offered in the summer semester",
        "actual_answer": "09:45AM",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "When does the class Design Center: Design for Social Innovation Recitation Section A  offered in Sp end?",
        "context": "Spring offering: Course: 51782 Title: Design Center: Design for Social Innovation Units: 12.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 12:00PM End: 01:20PM Bldg/Room: MM 121 Location: Pittsburgh, Pennsylvania Instructor(s): Krishnaswami ",
        "retrieved_doc": "Summer 1/Summer All offering: Course: 17996 Title: Societal Computing Independent Study Units: VAR Lec/Sec: Section B Days: To be announced Begin: 05:00PM End: 06:20PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Sadeh |Summer 1/Summer All offering: Course: 17996 Title: Societal Computing Independent Study Units: VAR Lec/Sec: Section L Days: To be announced Begin: 05:00PM End: 06:20PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Fang |Summer 1/Summer All offering: Course: 17996 Title: Societal Computing Independent Study Units: VAR Lec/Sec: Section M Days: To be announced Begin: 05:00PM End: 06:20PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Vasilescu ",
        "retrieved_relevant_part": false,
        "model_answer": "The course Design Center: Design for Social Innovation Recitation Section A is offered in the summer semester",
        "actual_answer": "01:20PM",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "On which days does the class Fairness, Accountability, Transparency, and Ethics in Sociotechnical Recitation Section A  offered in Fa meet?",
        "context": "Fall offering: Course: 05499 Title: Fairness, Accountability, Transparency, and Ethics in Sociotechnical Units: 12.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 11:00AM End: 12:20PM Bldg/Room: HH B103 Location: Pittsburgh, Pennsylvania Instructor(s): Shen ",
        "retrieved_doc": "Fall offering: Course: 05499 Title: Fairness, Accountability, Transparency, and Ethics in Sociotechnical Units: 12.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 11:00AM End: 12:20PM Bldg/Room: HH B103 Location: Pittsburgh, Pennsylvania Instructor(s): Shen |Fall offering: Course: 05899 Title: Fairness, Accountability, Transparency, and Ethics in Sociotechnical Units: 12.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 11:00AM End: 12:20PM Bldg/Room: HH B103 Location: Pittsburgh, Pennsylvania Instructor(s): Shen |Fall offering: Course: 62104 Title: Design Ethics & Social Justice in Architecture Units: 3.0 Lec/Sec: Section A Days: Tuesday Begin: 12:30PM End: 01:50PM Bldg/Room: CFA 214 Location: Pittsburgh, Pennsylvania Instructor(s): Vavasis, Gutschow ",
        "retrieved_relevant_part": true,
        "model_answer": "The class meets on Monday and Wednesday",
        "actual_answer": "Monday, Wednesday",
        "exact_match": false,
        "f1_max_min_avg": [
            0.22,
            0.22,
            0.22
        ],
        "recall_max_min_avg": [
            0.5,
            0.5,
            0.5
        ]
    },
    {
        "question": "When does the class Organizational Behavior Recitation Section W  offered in Fa end?",
        "context": "Fall offering: Course: 70311 Title: Organizational Behavior Units: 9.0 Lec/Sec: Section W Days: Monday, Wednesday Begin: 11:30AM End: 12:45PM Bldg/Room: CMB 1190 Location: Doha, Qatar Instructor(s): Haan ",
        "retrieved_doc": "Fall offering: Course: 19469 Title: Behavior, Decision and Policy Units: 9.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 12:30PM End: 01:50PM Bldg/Room: WEH 6403 Location: Pittsburgh, Pennsylvania Instructor(s): Fischhoff |Fall offering: Course: 70504 Title: Independent Study in Organizational Behavior Units: 3-18 Lec/Sec: Section A Days: To be announced Begin: 02:00PM End: 03:20PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Hollifield |Fall offering: Course: 18656 Title: Functional Programming in Practice Units: 12.0 Lec/Sec: Section SV Days: Monday, Wednesday Begin: 03:00PM End: 04:50PM Bldg/Room: B23 109 Location: San Jose, California Instructor(s): Wlodarski ",
        "retrieved_relevant_part": false,
        "model_answer": "The course is offered in the fall semester",
        "actual_answer": "12:45PM",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Who is the instructor(s) of course Software Engineering Management: Recitation Section A2  offered in Fa?",
        "context": "Fall offering: Course: 49772 Title: Software Engineering Management: Units: 6.0 Lec/Sec: Section A2 Days: Monday, Wednesday Begin: 11:00AM End: 12:20PM Bldg/Room: B23 227 Location: San Jose, California Instructor(s): Fang ",
        "retrieved_doc": "Yonatan Bisk Assistant Professor Email: ybisk@cs.cmu.edu Office: Gates & Hillman Centers Research Areas: Grounding, RoboNLP, Vision and Language, Embodiment, Unsupervised Learning|Summer 1/Summer All offering: Course: 48519 Title: Architecture Design Studio: Thesis II/ Independent Project Units: VAR Lec/Sec: Section A Days: To be announced Begin: nan End: nan Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Instructor TBA |Spring offering: Course: 17608 Title: Privacy Engineering Independent Study Units: VAR Lec/Sec: Section A Days: To be announced Begin: 12:30PM End: 01:50PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Bauer, Christin, Cranor, Fanti, Habib, Sadeh, Scanlon ",
        "retrieved_relevant_part": false,
        "model_answer": "The instructor(s) of course Software Engineering Management: Recitation Section A2 is Yonatan Bisk",
        "actual_answer": "Fang",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "On which days does the class Business Leadership Endeavor II Recitation Section C1  offered in Fa meet?",
        "context": "Fall offering: Course: 70204 Title: Business Leadership Endeavor II Units: 3.0 Lec/Sec: Section C1 Days: Tuesday Begin: 02:00PM End: 03:20PM Bldg/Room: TEP 3808 Location: Pittsburgh, Pennsylvania Instructor(s): Jafry O'Connor ",
        "retrieved_doc": "Spring offering: Course: 70204 Title: Business Leadership Endeavor II: Units: 3.0 Lec/Sec: Section B4 Days: Tuesday Begin: 11:00AM End: 12:20PM Bldg/Room: TEP 3808 Location: Pittsburgh, Pennsylvania Instructor(s): Jafry O'Connor |Spring offering: Course: 70304 Title: Business Leadership Endeavor III: Units: 3.0 Lec/Sec: Section B3 Days: Thursday Begin: 11:00AM End: 12:20PM Bldg/Room: TEP 3808 Location: Pittsburgh, Pennsylvania Instructor(s): Jafry O'Connor |Fall offering: Course: 70204 Title: Business Leadership Endeavor II Units: 3.0 Lec/Sec: Section B1 Days: Tuesday Begin: 12:30PM End: 01:50PM Bldg/Room: TEP 3808 Location: Pittsburgh, Pennsylvania Instructor(s): Jafry O'Connor ",
        "retrieved_relevant_part": false,
        "model_answer": "The class Business Leadership Endeavor II Recitation Section C1 meets on Tuesdays and Thursdays.<|endoftext|>",
        "actual_answer": "Tuesday",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "On which days does the class Acting for Leadership and Communication Recitation Section B4  offered in Sp meet?",
        "context": "Spring offering: Course: 94801 Title: Acting for Leadership and Communication Units: 6.0 Lec/Sec: Section B4 Days: Monday, Wednesday Begin: 11:00AM End: 12:20PM Bldg/Room: HBH 2009 Location: Pittsburgh, Pennsylvania Instructor(s): Murphy ",
        "retrieved_doc": "Spring offering: Course: 94801 Title: Acting for Leadership and Communication Units: 6.0 Lec/Sec: Section D3 Days: Monday, Wednesday Begin: 02:00PM End: 03:20PM Bldg/Room: HBH 2008 Location: Pittsburgh, Pennsylvania Instructor(s): Murphy |Spring offering: Course: 94801 Title: Acting for Leadership and Communication Units: 6.0 Lec/Sec: Section B4 Days: Monday, Wednesday Begin: 11:00AM End: 12:20PM Bldg/Room: HBH 2009 Location: Pittsburgh, Pennsylvania Instructor(s): Murphy |Spring offering: Course: 94401 Title: Acting for Leadership and Communication Units: 6.0 Lec/Sec: Section B4 Days: Monday, Wednesday Begin: 11:00AM End: 12:20PM Bldg/Room: HBH 2009 Location: Pittsburgh, Pennsylvania Instructor(s): Murphy ",
        "retrieved_relevant_part": true,
        "model_answer": "The class meets on Mondays and Wednesdays.<|endoftext|>",
        "actual_answer": "Monday, Wednesday",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "On which days does the class Mathematical Models for Consulting Recitation Section W  offered in Fa meet?",
        "context": "Fall offering: Course: 70460 Title: Mathematical Models for Consulting Units: 9.0 Lec/Sec: Section W Days: Sunday, Tuesday Begin: 04:00PM End: 05:15PM Bldg/Room: CMB 1031 Location: Doha, Qatar Instructor(s): Safak ",
        "retrieved_doc": "Fall offering: Course: 90858 Title: How Ideas Become Policy Units: 6.0 Lec/Sec: Section W1 Days: Friday Begin: 09:30AM End: 12:20PM Bldg/Room: TBA Location: Washington, District of Columbia Instructor(s): Mendelson |Fall offering: Course: 19701 Title: Introduction to the Theory and Practice of Policy Analysis Units: 12.0 Lec/Sec: Section PP Days: Tuesday, Thursday Begin: 02:00PM End: 03:20PM Bldg/Room: TBA Location: Lisbon, Portugal Instructor(s): Morgan |Summer 1/Summer All offering: Course: 17996 Title: Societal Computing Independent Study Units: VAR Lec/Sec: Section M Days: To be announced Begin: 05:00PM End: 06:20PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Vasilescu ",
        "retrieved_relevant_part": false,
        "model_answer": "The class meets on Fridays and Tuesdays.<|endoftext|>",
        "actual_answer": "Sunday, Tuesday",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Based on the background information, What is the units of course Special Topics in ICT: offered in Fa?",
        "context": "Fall offering: Course: 04800 Title: Special Topics in ICT: Units: 12.0 Lec/Sec: Section G Days: Tuesday, Thursday Begin: 10:00AM End: 11:50AM Bldg/Room: CMR F309 Location: Kigali, Rwanda Instructor(s): Tucker ",
        "retrieved_doc": "Spring offering: Course: 04800 Title: Special Topics in ICT: Units: 12.0 Lec/Sec: Section AF Days: Monday, Wednesday Begin: 04:00PM End: 05:50PM Bldg/Room: CMU REMOTE Location: Kigali, Rwanda Instructor(s): Racz, McSharry |Fall offering: Course: 04801 Title: Special Topics in ICT: Electronic Business and Technology Units: 6.0 Lec/Sec: Section N1 Days: Tuesday, Thursday Begin: 08:00AM End: 09:50AM Bldg/Room: CMR F305 Location: Kigali, Rwanda Instructor(s): Ostheimer |Fall offering: Course: 04801 Title: Special Topics in ICT Mini: Units: 6.0 Lec/Sec: Section K Days: Tuesday, Thursday Begin: 07:00PM End: 08:50PM Bldg/Room: CMU REMOTE Location: Kigali, Rwanda Instructor(s): Wiecha ",
        "retrieved_relevant_part": false,
        "model_answer": "12.0 units",
        "actual_answer": "The units of course Special Topics in ICT: is 12.0",
        "exact_match": false,
        "f1_max_min_avg": [
            0.33,
            0.33,
            0.33
        ],
        "recall_max_min_avg": [
            0.2,
            0.2,
            0.2
        ]
    },
    {
        "question": "Which campus is class Techniques in Quantitative Analysis Recitation Section B  offered in Pittsburgh in Fa?",
        "context": "Fall offering: Course: 09207 Title: Techniques in Quantitative Analysis Units: 9.0 Lec/Sec: Section B Days: Tuesday Begin: 06:30PM End: 08:50PM Bldg/Room: DH 1302 Location: Pittsburgh, Pennsylvania Instructor(s): Botcha ",
        "retrieved_doc": "Spring offering: Course: 17608 Title: Privacy Engineering Independent Study Units: VAR Lec/Sec: Section A Days: To be announced Begin: 12:30PM End: 01:50PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Bauer, Christin, Cranor, Fanti, Habib, Sadeh, Scanlon |Spring offering: Course: 05680 Title: Independent Study in HCI - METALS Units: VAR Lec/Sec: Section B Days: To be announced Begin: 02:00PM End: 03:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Bett, Bodnar |Fall offering: Course: 84711 Title: Qualitative Methods in Social Science Research Units: 6.0 Lec/Sec: Section A1 Days: Wednesday Begin: 07:00PM End: 09:50PM Bldg/Room: POS 147 Location: Pittsburgh, Pennsylvania Instructor(s): Morgan ",
        "retrieved_relevant_part": false,
        "model_answer": "The University of Pittsburgh",
        "actual_answer": "Pittsburgh, Pennsylvania",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Based on the background information, What is the units of course Singers offered in Fa?",
        "context": "Fall offering: Course: 57822 Title: Singers Units: 3.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 05:30PM End: 06:20PM Bldg/Room: MM 119 Location: Pittsburgh, Pennsylvania Instructor(s): Douglas ",
        "retrieved_doc": "Fall offering: Course: 57743 Title: Sonatas/Songs for Collaborative Pianists III Units: 3.0 Lec/Sec: Section A Days: Monday Begin: 06:00PM End: 06:50PM Bldg/Room: CFA ACH Location: Pittsburgh, Pennsylvania Instructor(s): Carver, Rissinger, Kim, Manriquez, Ojeda |Fall offering: Course: 57672 Title: Chamber Music: Woodwind and Mixed Units: 3.0 Lec/Sec: Section B Days: To be announced Begin: 01:00PM End: 01:50PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Sargsyan, Steranka, McGhee |Fall offering: Course: 57417 Title: Major Vocal Performance Ensemble Units: 6.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 04:00PM End: 05:20PM Bldg/Room: MM 119 Location: Pittsburgh, Pennsylvania Instructor(s): Rissinger, Douglas ",
        "retrieved_relevant_part": false,
        "model_answer": "The units of course Singers offered in Fa are 3.0",
        "actual_answer": "The units of course Singers is 3.0",
        "exact_match": false,
        "f1_max_min_avg": [
            0.71,
            0.71,
            0.71
        ],
        "recall_max_min_avg": [
            0.86,
            0.86,
            0.86
        ]
    },
    {
        "question": "What is the abstract of the paper Neutral Face Learning and Progressive Fusion Synthesis Network for NIR-VIS Face Recognition published by LTI faculty Yiming Yang in 2023?",
        "context": "Author: Yiming Yang Title: Neutral Face Learning and Progressive Fusion Synthesis Network for NIR-VIS Face Recognition Publication year: 2023 Coauthors: Yiming Yang, Weipeng Hu, Haifeng Hu Abstract: To meet the strong demand for deploying face recognition systems in low-light scenarios, the Near-InfraRed and VISible (NIR-VIS) face recognition task is receiving increasing attention. However, heterogeneous faces have the characteristics of heterogeneity and non-neutrality. Heterogeneity refers to the fact that the matching images are in different modalities, and non-neutrality means that the matching images are significantly different in pose, expression, lighting, etc. Both situations pose challenges for NIR-VIS face matching. To address this problem, we propose a novel Neutral face Learning and Progressive Fusion synthesis (NLPF) network to disentangle the latent attributes of heterogeneous faces and learn neutral face representations. Our approach naturally integrates Identity-related Neutral face Learning (INL) and Attribute Progressive Fusion (APF) into a joint framework. Firstly, INL eliminates modal variations and residual variations by guiding the network to learn homogeneous neutral face feature representations, which tackles the challenge of heterogeneity and non-neutrality by mapping cross-modal images to a common neutral representation subspace. Besides, APF is presented to perform the disentanglement and reintegration of identity-related features, modality-related features and residual features in a progressive fusion manner, which helps to further purify identity-related features. Comprehensive evaluations are carried out on three mainstream NIR-VIS datasets to verify the robustness and effectiveness of the NLPF model. In particular, NLPF has competitive recognition performance on LAMP-HQ, the most challenging NIR-VIS dataset so far.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Alexander I. Rudnicky Title: Tartan: an LLM Driven SocialBot Publication year: 2023 Coauthors: Liangze Li, Ziyuan Liu, Li-Wei Chen, Ta-Chung Chi, Alexander I. Rudnicky Abstract: We describe our work to date on using large language models (LLMs) in our Alexa Prize Socialbot. Our work has laid the groundwork for looking more closely at using LLMs in a conversational system. We also analyze common patterns in conversations our bot has had with users|Author: Shinji Watanabe Title: E-Branchformer-Based E2E SLU Toward Stop on-Device Challenge Publication year: 2023 Coauthors: Yosuke Kashiwagi, Siddhant Arora, Hayato Futami, Jessica Huynh, Shih-Lun Wu, Yifan Peng, Brian Yan, E. Tsunoo, Shinji Watanabe Abstract: In this paper, we report our team\u2019s study on track 2 of the Spoken Language Understanding Grand Challenge, which is a component of the ICASSP Signal Processing Grand Challenge 2023. The task is intended for on-device processing and involves estimating semantic parse labels from speech using a model with 15 million parameters. We use E2E E-Branchformer-based spoken language understanding model, which is more parameter controllable than cascade models, and reduced the parameter size through sequential distillation and tensor decomposition techniques. On the STOP dataset, we achieved an exact match accuracy of 70.9% under the tight constraint of 15 million parameters.",
        "retrieved_relevant_part": false,
        "model_answer": "The paper describes a network for recognizing neutral faces using neural networks and deep learning techniques",
        "actual_answer": "To meet the strong demand for deploying face recognition systems in low-light scenarios, the Near-InfraRed and VISible (NIR-VIS) face recognition task is receiving increasing attention. However, heterogeneous faces have the characteristics of heterogeneity and non-neutrality. Heterogeneity refers to the fact that the matching images are in different modalities, and non-neutrality means that the matching images are significantly different in pose, expression, lighting, etc. Both situations pose challenges for NIR-VIS face matching. To address this problem, we propose a novel Neutral face Learning and Progressive Fusion synthesis (NLPF) network to disentangle the latent attributes of heterogeneous faces and learn neutral face representations. Our approach naturally integrates Identity-related Neutral face Learning (INL) and Attribute Progressive Fusion (APF) into a joint framework. Firstly, INL eliminates modal variations and residual variations by guiding the network to learn homogeneous neutral face feature representations, which tackles the challenge of heterogeneity and non-neutrality by mapping cross-modal images to a common neutral representation subspace. Besides, APF is presented to perform the disentanglement and reintegration of identity-related features, modality-related features and residual features in a progressive fusion manner, which helps to further purify identity-related features. Comprehensive evaluations are carried out on three mainstream NIR-VIS datasets to verify the robustness and effectiveness of the NLPF model. In particular, NLPF has competitive recognition performance on LAMP-HQ, the most challenging NIR-VIS dataset so far.",
        "exact_match": false,
        "f1_max_min_avg": [
            0.05,
            0.05,
            0.05
        ],
        "recall_max_min_avg": [
            0.03,
            0.03,
            0.03
        ]
    },
    {
        "question": "Who are the coauthors of the paper Policy Representation via Diffusion Probability Model for Reinforcement Learning published by LTI faculty Yiming Yang in 2023?",
        "context": "Author: Yiming Yang Title: Policy Representation via Diffusion Probability Model for Reinforcement Learning Publication year: 2023 Coauthors: Long Yang, Zhixiong Huang, Fenghao Lei, Yucun Zhong, Yiming Yang, Cong Fang, Shiting Wen, Binbin Zhou, Zhouchen Lin Abstract: Popular reinforcement learning (RL) algorithms tend to produce a unimodal policy distribution, which weakens the expressiveness of complicated policy and decays the ability of exploration. The diffusion probability model is powerful to learn complicated multimodal distributions, which has shown promising and potential applications to RL. In this paper, we formally build a theoretical foundation of policy representation via the diffusion probability model and provide practical implementations of diffusion policy for online model-free RL. Concretely, we character diffusion policy as a stochastic process, which is a new approach to representing a policy. Then we present a convergence guarantee for diffusion policy, which provides a theory to understand the multimodality of diffusion policy. Furthermore, we propose the DIPO which is an implementation for model-free online RL with DIffusion POlicy. To the best of our knowledge, DIPO is the first algorithm to solve model-free online RL problems with the diffusion model. Finally, extensive empirical results show the effectiveness and superiority of DIPO on the standard continuous control Mujoco benchmark.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Yiming Yang Title: CompleQA: Benchmarking the Impacts of Knowledge Graph Completion Methods on Question Answering Publication year: 2023 Coauthors: Donghan Yu, Yu Gu, Chenyan Xiong, Yiming Yang |Author: Yiming Yang Title: A Study on Semantic Understanding of Large Language Models from the Perspective of Ambiguity Resolution Publication year: 2023 Coauthors: Shuguang Yang, Feipeng Chen, Yiming Yang, Zude Zhu ",
        "retrieved_relevant_part": false,
        "model_answer": "The coauthors of the paper are Donghan Yu, Yu Gu, Chenyan Xiong, and Shuguang Yang",
        "actual_answer": "Long Yang, Zhixiong Huang, Fenghao Lei, Yucun Zhong, Yiming Yang, Cong Fang, Shiting Wen, Binbin Zhou, Zhouchen Lin",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Who is the author of the LTI paper The Multimodal Information Based Speech Processing (MISP) 2023 Challenge: Audio-Visual Target Speaker Extraction published in 2023?",
        "context": "Author: Shinji Watanabe Title: The Multimodal Information Based Speech Processing (MISP) 2023 Challenge: Audio-Visual Target Speaker Extraction Publication year: 2023 Coauthors: Shilong Wu, Chenxi Wang, Hang Chen, Yusheng Dai, Chenyue Zhang, Ruoyu Wang, Hongbo Lan, Jun Du, Chin-Hui Lee, Jingdong Chen, Shinji Watanabe, Sabato Marco Siniscalchi, O. Scharenborg, Zhong-Qiu Wang, Jia Pan, Jianqing Gao Abstract: Previous Multimodal Information based Speech Processing (MISP) challenges mainly focused on audio-visual speech recognition (AVSR) with commendable success. However, the most advanced back-end recognition systems often hit performance limits due to the complex acoustic environments. This has prompted a shift in focus towards the Audio-Visual Target Speaker Extraction (AVTSE) task for the MISP 2023 challenge in ICASSP 2024 Signal Processing Grand Challenges. Unlike existing audio-visual speech enhance-ment challenges primarily focused on simulation data, the MISP 2023 challenge uniquely explores how front-end speech processing, combined with visual clues, impacts back-end tasks in real-world scenarios. This pioneering effort aims to set the first benchmark for the AVTSE task, offering fresh insights into enhancing the ac-curacy of back-end speech recognition systems through AVTSE in challenging and real acoustic environments. This paper delivers a thorough overview of the task setting, dataset, and baseline system of the MISP 2023 challenge. It also includes an in-depth analysis of the challenges participants may encounter. The experimental results highlight the demanding nature of this task, and we look forward to the innovative solutions participants will bring forward.",
        "retrieved_doc": "Author: T. Mitamura Title: Language-Agnostic Transformers and Assessing ChatGPT-Based Query Rewriting for Multilingual Document-Grounded QA Publication year: 2023 Coauthors: Srinivas Gowriraj, Soham Dinesh Tiwari, Mitali Potnis, Srijan Bansal, T. Mitamura, Eric Nyberg Abstract: The DialDoc 2023 shared task has expanded the document-grounded dialogue task to encompass multiple languages, despite having limited annotated data. This paper assesses the effectiveness of both language-agnostic and language-aware paradigms for multilingual pre-trained transformer models in a bi-encoder-based dense passage retriever (DPR), concluding that the language-agnostic approach is superior. Additionally, the study investigates the impact of query rewriting techniques using large language models, such as ChatGPT, on multilingual, document-grounded question-answering systems. The experiments conducted demonstrate that, for the examples examined, query rewriting does not enhance performance compared to the original queries. This failure is due to topic switching in final dialogue turns and irrelevant topics being considered for query rewriting.|Author: Mona T. Diab Title: Evaluating Multilingual Speech Translation under Realistic Conditions with Resegmentation and Terminology Publication year: 2023 Coauthors: Elizabeth Salesky, Kareem Darwish, Mohamed Al-Badrashiny, Mona T. Diab, J. Niehues Abstract: We present the ACL 60/60 evaluation sets for multilingual translation of ACL 2022 technical presentations into 10 target languages. This dataset enables further research into multilingual speech translation under realistic recording conditions with unsegmented audio and domain-specific terminology, applying NLP tools to text and speech in the technical domain, and evaluating and improving model robustness to diverse speaker demographics.|Author: Shinji Watanabe Title: Summary on the Multimodal Information Based Speech Processing (MISP) 2022 Challenge Publication year: 2023 Coauthors: Hang Chen, Shilong Wu, Yusheng Dai, Zhe Wang, Jun Du, Chin-Hui Lee, Jingdong Chen, Shinji Watanabe, Sabato Marco Siniscalchi, O. Scharenborg, Diyuan Liu, Baocai Yin, Jia Pan, Jianqing Gao, Cong Liu Abstract: The Multimodal Information based Speech Processing (MISP) 2022 challenge aimed to enhance speech processing performance in harsh acoustic environments by leveraging additional modalities such as video or text. The challenge included two tracks: audio-visual speaker diarization (AVSD) and audio-visual diarization and recognition (AVDR). The training material was based on previous MISP 2021 recordings, but we have accurately synchronized audio and visual data. Additionally, a new evaluation set was provided. This paper gives an overview of the challenge setup, presents the results, and summarizes the effective techniques employed by the participants. We also analyze the current technical challenges and suggest directions for future research in AVSD and AVDR.",
        "retrieved_relevant_part": false,
        "model_answer": "Shinji Watanabe",
        "actual_answer": "Shinji Watanabe",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Who is the author of the LTI paper FindAdaptNet: Find and Insert Adapters by Learned Layer Importance published in 2023?",
        "context": "Author: Shinji Watanabe Title: FindAdaptNet: Find and Insert Adapters by Learned Layer Importance Publication year: 2023 Coauthors: Junwei Huang, Karthik Ganesan, Soumi Maiti, Young Min Kim, Xuankai Chang, Paul Liang, Shinji Watanabe Abstract: Adapters are lightweight bottleneck modules introduced to assist pre-trained self-supervised learning (SSL) models to be customized to new tasks. However, searching the appropriate layers to insert adapters on large models has become difficult due to the large number of possible layers and thus a vast search space (2N possibilities for N layers). In this paper, we propose a technique that achieves automatic insertion of adapters for downstream automatic speech recognition (ASR) and spoken language understanding (SLU) tasks. Our approach is based on two-stage training. First, we train our model for a specific downstream task with additional shallow learnable layers and weight parameters to obtain the weighted summation over the output of each layer in SSL. This training method is established by the SUPERB baseline [1]. This first-stage training determines the most important layers given their respective weights. In the second stage, we proceed to insert adapters to the most important layers, retaining both performance and neural architecture search efficiency. On the CommonVoice dataset[2] we obtain 20.6% absolute improvement in Word Error Rate (WER) on the Welsh language against the conventional method, which inserts the adapter modules into the highest layers without search. In the SLURP SLU task, our method yields 4.0% intent accuracy improvement against the same conventional baseline.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Jamie Callan Title: COILcr: Efficient Semantic Matching in Contextualized Exact Match Retrieval Publication year: 2023 Coauthors: Zhen Fan, Luyu Gao, Rohan Jha, Jamie Callan |Author: Alexander I. Rudnicky Title: Tartan: an LLM Driven SocialBot Publication year: 2023 Coauthors: Liangze Li, Ziyuan Liu, Li-Wei Chen, Ta-Chung Chi, Alexander I. Rudnicky Abstract: We describe our work to date on using large language models (LLMs) in our Alexa Prize Socialbot. Our work has laid the groundwork for looking more closely at using LLMs in a conversational system. We also analyze common patterns in conversations our bot has had with users",
        "retrieved_relevant_part": false,
        "model_answer": "The author of the LTI paper FindAdaptNet: Find and Insert Adapters by Learned Layer Importance is Shinji Watanabe",
        "actual_answer": "Shinji Watanabe",
        "exact_match": false,
        "f1_max_min_avg": [
            0.2,
            0.2,
            0.2
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Who is the author of the LTI paper Understanding the Effect of Model Compression on Social Bias in Large Language Models published in 2023?",
        "context": "Author: Emma Strubell Title: Understanding the Effect of Model Compression on Social Bias in Large Language Models Publication year: 2023 Coauthors: Gustavo Gon\u00e7alves, Emma Strubell Abstract: Large Language Models (LLMs) trained with self-supervision on vast corpora of web text fit to the social biases of that text. Without intervention, these social biases persist in the model's predictions in downstream tasks, leading to representational harm. Many strategies have been proposed to mitigate the effects of inappropriate social biases learned during pretraining. Simultaneously, methods for model compression have become increasingly popular to reduce the computational burden of LLMs. Despite the popularity and need for both approaches, little work has been done to explore the interplay between these two. We perform a carefully controlled study of the impact of model compression via quantization and knowledge distillation on measures of social bias in LLMs. Longer pretraining and larger models led to higher social bias, and quantization showed a regularizer effect with its best trade-off around 20% of the original pretraining time.",
        "retrieved_doc": "Author: Emma Strubell Title: Understanding the Effect of Model Compression on Social Bias in Large Language Models Publication year: 2023 Coauthors: Gustavo Gon\u00e7alves, Emma Strubell Abstract: Large Language Models (LLMs) trained with self-supervision on vast corpora of web text fit to the social biases of that text. Without intervention, these social biases persist in the model's predictions in downstream tasks, leading to representational harm. Many strategies have been proposed to mitigate the effects of inappropriate social biases learned during pretraining. Simultaneously, methods for model compression have become increasingly popular to reduce the computational burden of LLMs. Despite the popularity and need for both approaches, little work has been done to explore the interplay between these two. We perform a carefully controlled study of the impact of model compression via quantization and knowledge distillation on measures of social bias in LLMs. Longer pretraining and larger models led to higher social bias, and quantization showed a regularizer effect with its best trade-off around 20% of the original pretraining time.|Author: Alexander I. Rudnicky Title: Tartan: an LLM Driven SocialBot Publication year: 2023 Coauthors: Liangze Li, Ziyuan Liu, Li-Wei Chen, Ta-Chung Chi, Alexander I. Rudnicky Abstract: We describe our work to date on using large language models (LLMs) in our Alexa Prize Socialbot. Our work has laid the groundwork for looking more closely at using LLMs in a conversational system. We also analyze common patterns in conversations our bot has had with users|Author: Jamie Callan Title: COILcr: Efficient Semantic Matching in Contextualized Exact Match Retrieval Publication year: 2023 Coauthors: Zhen Fan, Luyu Gao, Rohan Jha, Jamie Callan ",
        "retrieved_relevant_part": true,
        "model_answer": "Emma Strubell",
        "actual_answer": "Emma Strubell",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Who are the coauthors of the paper One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning published by LTI faculty Eric P. Xing in 2023?",
        "context": "Author: Eric P. Xing Title: One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning Publication year: 2023 Coauthors: Arnav Chavan, Zhuang Liu, D. Gupta, Eric P. Xing, Zhiqiang Shen Abstract: We present Generalized LoRA (GLoRA), an advanced approach for universal parameter-efficient fine-tuning tasks. Enhancing Low-Rank Adaptation (LoRA), GLoRA employs a generalized prompt module to optimize pre-trained model weights and adjust intermediate activations, providing more flexibility and capability across diverse tasks and datasets. Moreover, GLoRA facilitates efficient parameter adaptation by employing a scalable, modular, layer-wise structure search that learns individual adapter of each layer. Originating from a unified mathematical formulation, GLoRA exhibits strong transfer learning, few-shot learning and domain generalization abilities, as it adapts to new tasks through not only weights but also additional dimensions like activations. Comprehensive experiments demonstrate that GLoRA outperforms all previous methods in natural, specialized, and structured vision benchmarks, achieving superior accuracy with fewer parameters and computations. The proposed method on LLaMA-1 and LLaMA-2 also show considerable enhancements compared to the original LoRA in the language domain. Furthermore, our structural re-parameterization design ensures that GLoRA incurs no extra inference cost, rendering it a practical solution for resource-limited applications. Code and models are available at: https://github.com/Arnav0400/ViT-Slim/tree/master/GLoRA.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Jamie Callan Title: COILcr: Efficient Semantic Matching in Contextualized Exact Match Retrieval Publication year: 2023 Coauthors: Zhen Fan, Luyu Gao, Rohan Jha, Jamie Callan |Author: Maarten Sap Title: Don't Take This Out of Context!: On the Need for Contextual Models and Evaluations for Stylistic Rewriting Publication year: 2023 Coauthors: Akhila Yerukola, Xuhui Zhou, Elizabeth Clark, Maarten Sap ",
        "retrieved_relevant_part": false,
        "model_answer": "The coauthors of the paper are Yui Sudo, Muhammad Shakeel, Yifan Peng, and Shinji Watanabe",
        "actual_answer": "Arnav Chavan, Zhuang Liu, D. Gupta, Eric P. Xing, Zhiqiang Shen",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Which LTI faculty published the paper Structured Pruning of Self-Supervised Pre-Trained Models for Speech Recognition and Understanding in 2023?",
        "context": "Author: Shinji Watanabe Title: Structured Pruning of Self-Supervised Pre-Trained Models for Speech Recognition and Understanding Publication year: 2023 Coauthors: Yifan Peng, Kwangyoun Kim, Felix Wu, Prashant Sridhar, Shinji Watanabe Abstract: Self-supervised speech representation learning (SSL) has shown to be effective in various downstream tasks, but SSL models are usually large and slow. Model compression techniques such as pruning aim to reduce the model size and computation without degradation in accuracy. Prior studies focus on the pruning of Transformers; however, speech models not only utilize a stack of Transformer blocks, but also combine a frontend network based on multiple convolutional layers for low-level feature representation learning. This frontend has a small size but a heavy computational cost. In this work, we propose three task-specific structured pruning methods to deal with such heterogeneous networks. Experiments on LibriSpeech and SLURP show that the proposed method is more accurate than the original wav2vec2-base with 10% to 30% less computation, and is able to reduce the computation by 40% to 50% without any degradation.",
        "retrieved_doc": "Author: Shinji Watanabe Title: DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models Publication year: 2023 Coauthors: Yifan Peng, Yui Sudo, Muhammad Shakeel, Shinji Watanabe Abstract: Self-supervised learning (SSL) has achieved notable success in many speech processing tasks, but the large model size and heavy computational cost hinder the deployment. Knowledge distillation trains a small student model to mimic the behavior of a large teacher model. However, the student architecture usually needs to be manually designed and will remain fixed during training, which requires prior knowledge and can lead to suboptimal performance. Inspired by recent success of task-specific structured pruning, we propose DPHuBERT, a novel task-agnostic compression method for speech SSL based on joint distillation and pruning. Experiments on SUPERB show that DPHuBERT outperforms pure distillation methods in almost all tasks. Moreover, DPHuBERT requires little training time and performs well with limited training data, making it suitable for resource-constrained applications. Our method can also be applied to various speech SSL models. Our code and models will be publicly available.|Author: Shinji Watanabe Title: Structured Pruning of Self-Supervised Pre-Trained Models for Speech Recognition and Understanding Publication year: 2023 Coauthors: Yifan Peng, Kwangyoun Kim, Felix Wu, Prashant Sridhar, Shinji Watanabe Abstract: Self-supervised speech representation learning (SSL) has shown to be effective in various downstream tasks, but SSL models are usually large and slow. Model compression techniques such as pruning aim to reduce the model size and computation without degradation in accuracy. Prior studies focus on the pruning of Transformers; however, speech models not only utilize a stack of Transformer blocks, but also combine a frontend network based on multiple convolutional layers for low-level feature representation learning. This frontend has a small size but a heavy computational cost. In this work, we propose three task-specific structured pruning methods to deal with such heterogeneous networks. Experiments on LibriSpeech and SLURP show that the proposed method is more accurate than the original wav2vec2-base with 10% to 30% less computation, and is able to reduce the computation by 40% to 50% without any degradation.|Author: Shinji Watanabe Title: Integrating Pretrained ASR and LM to Perform Sequence Generation for Spoken Language Understanding Publication year: 2023 Coauthors: Siddhant Arora, Hayato Futami, Yosuke Kashiwagi, E. Tsunoo, Brian Yan, Shinji Watanabe Abstract: There has been an increased interest in the integration of pretrained speech recognition (ASR) and language models (LM) into the SLU framework. However, prior methods often struggle with a vocabulary mismatch between pretrained models, and LM cannot be directly utilized as they diverge from its NLU formulation. In this study, we propose a three-pass end-to-end (E2E) SLU system that effectively integrates ASR and LM subnetworks into the SLU formulation for sequence generation tasks. In the first pass, our architecture predicts ASR transcripts using the ASR subnetwork. This is followed by the LM subnetwork, which makes an initial SLU prediction. Finally, in the third pass, the deliberation subnetwork conditions on representations from the ASR and LM subnetworks to make the final prediction. Our proposed three-pass SLU system shows improved performance over cascaded and E2E SLU models on two benchmark SLU datasets, SLURP and SLUE, especially on acoustically challenging utterances.",
        "retrieved_relevant_part": true,
        "model_answer": "Shinji Watanabe",
        "actual_answer": "Shinji Watanabe",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Who is the author of the LTI paper The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features published in 2023?",
        "context": "Author: Rita Singh Title: The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features Publication year: 2023 Coauthors: Liao Qu, X. Zou, Xiang Li, Yandong Wen, Rita Singh, B. Raj Abstract: This work unveils the enigmatic link between phonemes and facial features. Traditional studies on voice-face correlations typically involve using a long period of voice input, including generating face images from voices and reconstructing 3D face meshes from voices. However, in situations like voice-based crimes, the available voice evidence may be short and limited. Additionally, from a physiological perspective, each segment of speech -- phoneme -- corresponds to different types of airflow and movements in the face. Therefore, it is advantageous to discover the hidden link between phonemes and face attributes. In this paper, we propose an analysis pipeline to help us explore the voice-face relationship in a fine-grained manner, i.e., phonemes v.s. facial anthropometric measurements (AM). We build an estimator for each phoneme-AM pair and evaluate the correlation through hypothesis testing. Our results indicate that AMs are more predictable from vowels compared to consonants, particularly with plosives. Additionally, we observe that if a specific AM exhibits more movement during phoneme pronunciation, it is more predictable. Our findings support those in physiology regarding correlation and lay the groundwork for future research on speech-face multimodal learning.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Alexander I. Rudnicky Title: Tartan: an LLM Driven SocialBot Publication year: 2023 Coauthors: Liangze Li, Ziyuan Liu, Li-Wei Chen, Ta-Chung Chi, Alexander I. Rudnicky Abstract: We describe our work to date on using large language models (LLMs) in our Alexa Prize Socialbot. Our work has laid the groundwork for looking more closely at using LLMs in a conversational system. We also analyze common patterns in conversations our bot has had with users|Author: Louis-Philippe Morency Title: Reconstructing the neurodynamics of face perception during real world vision in humans using intracranial EEG recordings Publication year: 2023 Coauthors: Arish Alreja, Michael J. Ward, J. A. Colan, Qianli Ma, R. M. Richardson, Louis-Philippe Morency, A. Ghuman ",
        "retrieved_relevant_part": false,
        "model_answer": "The author of the LTI paper is Shinji Watanabe",
        "actual_answer": "Rita Singh",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "What is the abstract of the paper Core loss analysis of soft magnetic composite under non-sinusoidal excitation based on finite element models published by LTI faculty Yiming Yang in 2023?",
        "context": "Author: Yiming Yang Title: Core loss analysis of soft magnetic composite under non-sinusoidal excitation based on finite element models Publication year: 2023 Coauthors: Lei Zhao, Chengcheng Liu, Youhua H. Wang, Yiming Yang Abstract: Due to the effect of higher harmonics on magnetic properties under actual complex operating conditions, the accurate calculation of core losses of soft magnetic composites (SMC) is complicated. First, this paper improves the existing SMC model by introducing a correction factor to correct the hysteresis loss coefficient so that the model can consider the local variation characteristics of the magnetic density waveform and then calculate the core loss under different harmonic excitation. Then, the influence of skin effect and inhomogeneous flux density within the ring sample model is analyzed. Finally, to validate the improved model, it is compared with other models in the reference based on experimental measurements, respectively. The results show that the core loss calculated by the improved model is closer to the experimental results under different harmonic excitations. In addition, the applicability of the improved SMC model under triangular and square wave excitations is also verified by the derivation of the equations.",
        "retrieved_doc": "Author: Yiming Yang Title: Core loss analysis of soft magnetic composite under non-sinusoidal excitation based on finite element models Publication year: 2023 Coauthors: Lei Zhao, Chengcheng Liu, Youhua H. Wang, Yiming Yang Abstract: Due to the effect of higher harmonics on magnetic properties under actual complex operating conditions, the accurate calculation of core losses of soft magnetic composites (SMC) is complicated. First, this paper improves the existing SMC model by introducing a correction factor to correct the hysteresis loss coefficient so that the model can consider the local variation characteristics of the magnetic density waveform and then calculate the core loss under different harmonic excitation. Then, the influence of skin effect and inhomogeneous flux density within the ring sample model is analyzed. Finally, to validate the improved model, it is compared with other models in the reference based on experimental measurements, respectively. The results show that the core loss calculated by the improved model is closer to the experimental results under different harmonic excitations. In addition, the applicability of the improved SMC model under triangular and square wave excitations is also verified by the derivation of the equations.|Author: A. Rudnicky Title: Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings Publication year: 2023 Coauthors: Ta-Chung Chi, Ting-Han Fan, Li-Wei Chen, A. Rudnicky, P. Ramadge Abstract: The use of positional embeddings in transformer language models is widely accepted. However, recent research has called into question the necessity of such embeddings. We further extend this inquiry by demonstrating that a randomly initialized and frozen transformer language model, devoid of positional embeddings, inherently encodes strong positional information through the shrinkage of self-attention variance. To quantify this variance, we derive the underlying distribution of each step within a transformer layer. Through empirical validation using a fully pretrained model, we show that the variance shrinkage effect still persists after extensive gradient updates. Our findings serve to justify the decision to discard positional embeddings and thus facilitate more efficient pretraining of transformer language models.|Author: Shinji Watanabe Title: E-Branchformer-Based E2E SLU Toward Stop on-Device Challenge Publication year: 2023 Coauthors: Yosuke Kashiwagi, Siddhant Arora, Hayato Futami, Jessica Huynh, Shih-Lun Wu, Yifan Peng, Brian Yan, E. Tsunoo, Shinji Watanabe Abstract: In this paper, we report our team\u2019s study on track 2 of the Spoken Language Understanding Grand Challenge, which is a component of the ICASSP Signal Processing Grand Challenge 2023. The task is intended for on-device processing and involves estimating semantic parse labels from speech using a model with 15 million parameters. We use E2E E-Branchformer-based spoken language understanding model, which is more parameter controllable than cascade models, and reduced the parameter size through sequential distillation and tensor decomposition techniques. On the STOP dataset, we achieved an exact match accuracy of 70.9% under the tight constraint of 15 million parameters.",
        "retrieved_relevant_part": true,
        "model_answer": "The paper investigates the accuracy of calculating core losses of soft magnetic composites under complex operating conditions using finite element models",
        "actual_answer": "Due to the effect of higher harmonics on magnetic properties under actual complex operating conditions, the accurate calculation of core losses of soft magnetic composites (SMC) is complicated. First, this paper improves the existing SMC model by introducing a correction factor to correct the hysteresis loss coefficient so that the model can consider the local variation characteristics of the magnetic density waveform and then calculate the core loss under different harmonic excitation. Then, the influence of skin effect and inhomogeneous flux density within the ring sample model is analyzed. Finally, to validate the improved model, it is compared with other models in the reference based on experimental measurements, respectively. The results show that the core loss calculated by the improved model is closer to the experimental results under different harmonic excitations. In addition, the applicability of the improved SMC model under triangular and square wave excitations is also verified by the derivation of the equations.",
        "exact_match": false,
        "f1_max_min_avg": [
            0.15,
            0.15,
            0.15
        ],
        "recall_max_min_avg": [
            0.08,
            0.08,
            0.08
        ]
    },
    {
        "question": "Who is the author of the LTI paper AutoReply: Detecting Nonsense in Dialogue with Discriminative Replies published in 2023?",
        "context": "Author: Daniel Fried Title: AutoReply: Detecting Nonsense in Dialogue with Discriminative Replies Publication year: 2023 Coauthors: Weiyan Shi, Emily Dinan, Adi Renduchintala, Daniel Fried, Athul Paul Jacob, Zhou Yu, Mike Lewis ",
        "retrieved_doc": "Author: Daniel Fried Title: AutoReply: Detecting Nonsense in Dialogue with Discriminative Replies Publication year: 2023 Coauthors: Weiyan Shi, Emily Dinan, Adi Renduchintala, Daniel Fried, Athul Paul Jacob, Zhou Yu, Mike Lewis |Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Alexander I. Rudnicky Title: Tartan: an LLM Driven SocialBot Publication year: 2023 Coauthors: Liangze Li, Ziyuan Liu, Li-Wei Chen, Ta-Chung Chi, Alexander I. Rudnicky Abstract: We describe our work to date on using large language models (LLMs) in our Alexa Prize Socialbot. Our work has laid the groundwork for looking more closely at using LLMs in a conversational system. We also analyze common patterns in conversations our bot has had with users",
        "retrieved_relevant_part": true,
        "model_answer": "Daniel Fried",
        "actual_answer": "Daniel Fried",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Who is the author of the LTI paper A Gold Standard Dataset for the Reviewer Assignment Problem published in 2023?",
        "context": "Author: Graham Neubig Title: A Gold Standard Dataset for the Reviewer Assignment Problem Publication year: 2023 Coauthors: Ivan Stelmakh, J. Wieting, Graham Neubig, Nihar B. Shah Abstract: Many peer-review venues are either using or looking to use algorithms to assign submissions to reviewers. The crux of such automated approaches is the notion of the\"similarity score\"--a numerical estimate of the expertise of a reviewer in reviewing a paper--and many algorithms have been proposed to compute these scores. However, these algorithms have not been subjected to a principled comparison, making it difficult for stakeholders to choose the algorithm in an evidence-based manner. The key challenge in comparing existing algorithms and developing better algorithms is the lack of the publicly available gold-standard data that would be needed to perform reproducible research. We address this challenge by collecting a novel dataset of similarity scores that we release to the research community. Our dataset consists of 477 self-reported expertise scores provided by 58 researchers who evaluated their expertise in reviewing papers they have read previously. We use this data to compare several popular algorithms employed in computer science conferences and come up with recommendations for stakeholders. Our main findings are as follows. First, all algorithms make a non-trivial amount of error. For the task of ordering two papers in terms of their relevance for a reviewer, the error rates range from 12%-30% in easy cases to 36%-43% in hard cases, highlighting the vital need for more research on the similarity-computation problem. Second, most existing algorithms are designed to work with titles and abstracts of papers, and in this regime the Specter+MFR algorithm performs best. Third, to improve performance, it may be important to develop modern deep-learning based algorithms that can make use of the full texts of papers: the classical TD-IDF algorithm enhanced with full texts of papers is on par with the deep-learning based Specter+MFR that cannot make use of this information.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Jamie Callan Title: COILcr: Efficient Semantic Matching in Contextualized Exact Match Retrieval Publication year: 2023 Coauthors: Zhen Fan, Luyu Gao, Rohan Jha, Jamie Callan |Author: Maarten Sap Title: Don't Take This Out of Context!: On the Need for Contextual Models and Evaluations for Stylistic Rewriting Publication year: 2023 Coauthors: Akhila Yerukola, Xuhui Zhou, Elizabeth Clark, Maarten Sap ",
        "retrieved_relevant_part": false,
        "model_answer": "The author of the LTI paper A Gold Standard Dataset for the Reviewer Assignment Problem published in 2023 is Maarten Sap",
        "actual_answer": "Graham Neubig",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Who is the author of the LTI paper Evaluating Multilingual Speech Translation under Realistic Conditions with Resegmentation and Terminology published in 2023?",
        "context": "Author: Mona T. Diab Title: Evaluating Multilingual Speech Translation under Realistic Conditions with Resegmentation and Terminology Publication year: 2023 Coauthors: Elizabeth Salesky, Kareem Darwish, Mohamed Al-Badrashiny, Mona T. Diab, J. Niehues Abstract: We present the ACL 60/60 evaluation sets for multilingual translation of ACL 2022 technical presentations into 10 target languages. This dataset enables further research into multilingual speech translation under realistic recording conditions with unsegmented audio and domain-specific terminology, applying NLP tools to text and speech in the technical domain, and evaluating and improving model robustness to diverse speaker demographics.",
        "retrieved_doc": "Author: Mona T. Diab Title: Evaluating Multilingual Speech Translation under Realistic Conditions with Resegmentation and Terminology Publication year: 2023 Coauthors: Elizabeth Salesky, Kareem Darwish, Mohamed Al-Badrashiny, Mona T. Diab, J. Niehues Abstract: We present the ACL 60/60 evaluation sets for multilingual translation of ACL 2022 technical presentations into 10 target languages. This dataset enables further research into multilingual speech translation under realistic recording conditions with unsegmented audio and domain-specific terminology, applying NLP tools to text and speech in the technical domain, and evaluating and improving model robustness to diverse speaker demographics.|Author: T. Mitamura Title: Language-Agnostic Transformers and Assessing ChatGPT-Based Query Rewriting for Multilingual Document-Grounded QA Publication year: 2023 Coauthors: Srinivas Gowriraj, Soham Dinesh Tiwari, Mitali Potnis, Srijan Bansal, T. Mitamura, Eric Nyberg Abstract: The DialDoc 2023 shared task has expanded the document-grounded dialogue task to encompass multiple languages, despite having limited annotated data. This paper assesses the effectiveness of both language-agnostic and language-aware paradigms for multilingual pre-trained transformer models in a bi-encoder-based dense passage retriever (DPR), concluding that the language-agnostic approach is superior. Additionally, the study investigates the impact of query rewriting techniques using large language models, such as ChatGPT, on multilingual, document-grounded question-answering systems. The experiments conducted demonstrate that, for the examples examined, query rewriting does not enhance performance compared to the original queries. This failure is due to topic switching in final dialogue turns and irrelevant topics being considered for query rewriting.|Author: Shinji Watanabe Title: Enhancing End-to-End Conversational Speech Translation Through Target Language Context Utilization Publication year: 2023 Coauthors: A. Hussein, Brian Yan, Antonios Anastasopoulos, Shinji Watanabe, S. Khudanpur Abstract: Incorporating longer context has been shown to benefit machine translation, but the inclusion of context in end-to-end speech translation (E2E-ST) remains under-studied. To bridge this gap, we introduce target language context in E2E-ST, enhancing coherence and overcoming memory constraints of extended audio segments. Additionally, we propose context dropout to ensure robustness to the absence of context, and further improve performance by adding speaker information. Our proposed contextual E2E-ST outperforms the isolated utterance-based E2E-ST approach. Lastly, we demonstrate that in conversational speech, contextual information primarily contributes to capturing context style, as well as resolving anaphora and named entities.",
        "retrieved_relevant_part": true,
        "model_answer": "Mona T. Diab",
        "actual_answer": "Mona T. Diab",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "What is the abstract of the paper I3D: Transformer Architectures with Input-Dependent Dynamic Depth for Speech Recognition published by LTI faculty Shinji Watanabe in 2023?",
        "context": "Author: Shinji Watanabe Title: I3D: Transformer Architectures with Input-Dependent Dynamic Depth for Speech Recognition Publication year: 2023 Coauthors: Yifan Peng, Jaesong Lee, Shinji Watanabe Abstract: Transformer-based end-to-end speech recognition has achieved great success. However, the large footprint and computational overhead make it difficult to deploy these models in some real-world applications. Model compression techniques can reduce the model size and speed up inference, but the compressed model has a fixed architecture which might be suboptimal. We propose a novel Transformer encoder with Input-Dependent Dynamic Depth (I3D) to achieve strong performance-efficiency trade-offs. With a similar number of layers at inference time, I3D-based models outperform the vanilla Transformer and the static pruned model via iterative layer pruning. We also present interesting analysis on the gate probabilities and the input-dependency, which helps us better understand deep encoders.",
        "retrieved_doc": "Author: Shinji Watanabe Title: I3D: Transformer Architectures with Input-Dependent Dynamic Depth for Speech Recognition Publication year: 2023 Coauthors: Yifan Peng, Jaesong Lee, Shinji Watanabe Abstract: Transformer-based end-to-end speech recognition has achieved great success. However, the large footprint and computational overhead make it difficult to deploy these models in some real-world applications. Model compression techniques can reduce the model size and speed up inference, but the compressed model has a fixed architecture which might be suboptimal. We propose a novel Transformer encoder with Input-Dependent Dynamic Depth (I3D) to achieve strong performance-efficiency trade-offs. With a similar number of layers at inference time, I3D-based models outperform the vanilla Transformer and the static pruned model via iterative layer pruning. We also present interesting analysis on the gate probabilities and the input-dependency, which helps us better understand deep encoders.|Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Shinji Watanabe Title: A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks Publication year: 2023 Coauthors: Yifan Peng, Kwangyoun Kim, Felix Wu, Brian Yan, Siddhant Arora, William Chen, Jiyang Tang, Suwon Shon, Prashant Sridhar, Shinji Watanabe Abstract: Conformer, a convolution-augmented Transformer variant, has become the de facto encoder architecture for speech processing due to its superior performance in various tasks, including automatic speech recognition (ASR), speech translation (ST) and spoken language understanding (SLU). Recently, a new encoder called E-Branchformer has outperformed Conformer in the LibriSpeech ASR benchmark, making it promising for more general speech applications. This work compares E-Branchformer and Conformer through extensive experiments using different types of end-to-end sequence-to-sequence models. Results demonstrate that E-Branchformer achieves comparable or better performance than Conformer in almost all evaluation sets across 15 ASR, 2 ST, and 3 SLU benchmarks, while being more stable during training. We will release our training configurations and pre-trained models for reproducibility, which can benefit the speech community.",
        "retrieved_relevant_part": true,
        "model_answer": "The paper proposes a novel Transformer encoder with Input-Dependent Dynamic Depth (I3D) to achieve strong performance-efficiency trade-offs in speech recognition",
        "actual_answer": "Transformer-based end-to-end speech recognition has achieved great success. However, the large footprint and computational overhead make it difficult to deploy these models in some real-world applications. Model compression techniques can reduce the model size and speed up inference, but the compressed model has a fixed architecture which might be suboptimal. We propose a novel Transformer encoder with Input-Dependent Dynamic Depth (I3D) to achieve strong performance-efficiency trade-offs. With a similar number of layers at inference time, I3D-based models outperform the vanilla Transformer and the static pruned model via iterative layer pruning. We also present interesting analysis on the gate probabilities and the input-dependency, which helps us better understand deep encoders.",
        "exact_match": false,
        "f1_max_min_avg": [
            0.25,
            0.25,
            0.25
        ],
        "recall_max_min_avg": [
            0.15,
            0.15,
            0.15
        ]
    },
    {
        "question": "What is the abstract of the paper ActiveRAG: Revealing the Treasures of Knowledge via Active Learning published by LTI faculty Chenyan Xiong in 2024?",
        "context": "Author: Chenyan Xiong Title: ActiveRAG: Revealing the Treasures of Knowledge via Active Learning Publication year: 2024 Coauthors: Zhipeng Xu, Zhenghao Liu, Yibin Liu, Chenyan Xiong, Yukun Yan, Shuo Wang, Shi Yu, Zhiyuan Liu, Ge Yu Abstract: Retrieval Augmented Generation (RAG) has introduced a new paradigm for Large Language Models (LLMs), aiding in the resolution of knowledge-intensive tasks. However, current RAG models position LLMs as passive knowledge receptors, thereby restricting their capacity for learning and comprehending external knowledge. In this paper, we present ActiveRAG, an innovative RAG framework that shifts from passive knowledge acquisition to an active learning mechanism. This approach utilizes the Knowledge Construction mechanism to develop a deeper understanding of external knowledge by associating it with previously acquired or memorized knowledge. Subsequently, it designs the Cognitive Nexus mechanism to incorporate the outcomes from both chains of thought and knowledge construction, thereby calibrating the intrinsic cognition of LLMs. Our experimental results demonstrate that ActiveRAG surpasses previous RAG models, achieving a 5% improvement on question-answering datasets. All data and codes are available at https://github.com/OpenMatch/ActiveRAG.",
        "retrieved_doc": "Author: Chenyan Xiong Title: CompleQA: Benchmarking the Impacts of Knowledge Graph Completion Methods on Question Answering Publication year: 2023 Coauthors: Donghan Yu, Yu Gu, Chenyan Xiong, Yiming Yang |Author: Yiming Yang Title: CompleQA: Benchmarking the Impacts of Knowledge Graph Completion Methods on Question Answering Publication year: 2023 Coauthors: Donghan Yu, Yu Gu, Chenyan Xiong, Yiming Yang |Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe ",
        "retrieved_relevant_part": false,
        "model_answer": "The paper explores the benefits of active learning in revealing the treasures of knowledge",
        "actual_answer": "Retrieval Augmented Generation (RAG) has introduced a new paradigm for Large Language Models (LLMs), aiding in the resolution of knowledge-intensive tasks. However, current RAG models position LLMs as passive knowledge receptors, thereby restricting their capacity for learning and comprehending external knowledge. In this paper, we present ActiveRAG, an innovative RAG framework that shifts from passive knowledge acquisition to an active learning mechanism. This approach utilizes the Knowledge Construction mechanism to develop a deeper understanding of external knowledge by associating it with previously acquired or memorized knowledge. Subsequently, it designs the Cognitive Nexus mechanism to incorporate the outcomes from both chains of thought and knowledge construction, thereby calibrating the intrinsic cognition of LLMs. Our experimental results demonstrate that ActiveRAG surpasses previous RAG models, achieving a 5% improvement on question-answering datasets. All data and codes are available at https://github.com/OpenMatch/ActiveRAG.",
        "exact_match": false,
        "f1_max_min_avg": [
            0.08,
            0.08,
            0.08
        ],
        "recall_max_min_avg": [
            0.04,
            0.04,
            0.04
        ]
    },
    {
        "question": "Who is the author of the LTI paper Distributionally Robust Unsupervised Dense Retrieval Training on Web Graphs published in 2023?",
        "context": "Author: Chenyan Xiong Title: Distributionally Robust Unsupervised Dense Retrieval Training on Web Graphs Publication year: 2023 Coauthors: Peixuan Han, Zhenghao Liu, Zhiyuan Liu, Chenyan Xiong Abstract: This paper introduces Web-DRO, an unsupervised dense retrieval model, which clusters documents based on web structures and reweights the groups during contrastive training. Specifically, we first leverage web graph links and contrastively train an embedding model for clustering anchor-document pairs. Then we use Group Distributional Robust Optimization to reweight different clusters of anchor-document pairs, which guides the model to assign more weights to the group with higher contrastive loss and pay more attention to the worst case during training. Our experiments on MS MARCO and BEIR show that our model, Web-DRO, significantly improves the retrieval effectiveness in unsupervised scenarios. A comparison of clustering techniques shows that training on the web graph combining URL information reaches optimal performance on clustering. Further analysis confirms that group weights are stable and valid, indicating consistent model preferences as well as effective up-weighting of valuable groups and down-weighting of uninformative ones. The code of this paper can be obtained from https://github.com/OpenMatch/Web-DRO.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Chenyan Xiong Title: Distributionally Robust Unsupervised Dense Retrieval Training on Web Graphs Publication year: 2023 Coauthors: Peixuan Han, Zhenghao Liu, Zhiyuan Liu, Chenyan Xiong Abstract: This paper introduces Web-DRO, an unsupervised dense retrieval model, which clusters documents based on web structures and reweights the groups during contrastive training. Specifically, we first leverage web graph links and contrastively train an embedding model for clustering anchor-document pairs. Then we use Group Distributional Robust Optimization to reweight different clusters of anchor-document pairs, which guides the model to assign more weights to the group with higher contrastive loss and pay more attention to the worst case during training. Our experiments on MS MARCO and BEIR show that our model, Web-DRO, significantly improves the retrieval effectiveness in unsupervised scenarios. A comparison of clustering techniques shows that training on the web graph combining URL information reaches optimal performance on clustering. Further analysis confirms that group weights are stable and valid, indicating consistent model preferences as well as effective up-weighting of valuable groups and down-weighting of uninformative ones. The code of this paper can be obtained from https://github.com/OpenMatch/Web-DRO.|Author: Jamie Callan Title: COILcr: Efficient Semantic Matching in Contextualized Exact Match Retrieval Publication year: 2023 Coauthors: Zhen Fan, Luyu Gao, Rohan Jha, Jamie Callan ",
        "retrieved_relevant_part": true,
        "model_answer": "Chenyan Xiong",
        "actual_answer": "Chenyan Xiong",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Which LTI faculty published the paper Are aligned neural networks adversarially aligned? in 2023?",
        "context": "Author: Daphne Ippolito Title: Are aligned neural networks adversarially aligned? Publication year: 2023 Coauthors: Nicholas Carlini, Milad Nasr, Christopher A. Choquette-Choo, Matthew Jagielski, Irena Gao, Anas Awadalla, Pang Wei Koh, Daphne Ippolito, Katherine Lee, Florian Tram\u00e8r, Ludwig Schmidt Abstract: Large language models are now tuned to align with the goals of their creators, namely to be\"helpful and harmless.\"These models should respond helpfully to user questions, but refuse to answer requests that could cause harm. However, adversarial users can construct inputs which circumvent attempts at alignment. In this work, we study to what extent these models remain aligned, even when interacting with an adversarial user who constructs worst-case inputs (adversarial examples). These inputs are designed to cause the model to emit harmful content that would otherwise be prohibited. We show that existing NLP-based optimization attacks are insufficiently powerful to reliably attack aligned text models: even when current NLP-based attacks fail, we can find adversarial inputs with brute force. As a result, the failure of current attacks should not be seen as proof that aligned text models remain aligned under adversarial inputs. However the recent trend in large-scale ML models is multimodal models that allow users to provide images that influence the text that is generated. We show these models can be easily attacked, i.e., induced to perform arbitrary un-aligned behavior through adversarial perturbation of the input image. We conjecture that improved NLP attacks may demonstrate this same level of adversarial control over text-only models.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Jamie Callan Title: COILcr: Efficient Semantic Matching in Contextualized Exact Match Retrieval Publication year: 2023 Coauthors: Zhen Fan, Luyu Gao, Rohan Jha, Jamie Callan |Author: Shinji Watanabe Title: Challenges of Corporate Alliance CLOMA toward Plastic Litter Publication year: 2023 Coauthors: Shinji Watanabe ",
        "retrieved_relevant_part": false,
        "model_answer": "Shinji Watanabe",
        "actual_answer": "Daphne Ippolito",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Which LTI faculty published the paper Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large Language Models in 2024?",
        "context": "Author: Eric Xing Title: Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large Language Models Publication year: 2024 Coauthors: Loka Li, Guan-Hong Chen, Yusheng Su, Zhenhao Chen, Yixuan Zhang, Eric Xing, Kun Zhang Abstract: The recent success of Large Language Models (LLMs) has catalyzed an increasing interest in their self-correction capabilities. This paper presents a comprehensive investigation into the intrinsic self-correction of LLMs, attempting to address the ongoing debate about its feasibility. Our research has identified an important latent factor - the ``confidence'' of LLMs - during the self-correction process. Overlooking this factor may cause the models to over-criticize themselves, resulting in unreliable conclusions regarding the efficacy of self-correction. We have experimentally observed that LLMs possess the capability to understand the ``confidence'' in their own responses. It motivates us to develop an ``If-or-Else'' (IoE) prompting framework, designed to guide LLMs in assessing their own ``confidence'', facilitating intrinsic self-corrections. We conduct extensive experiments and demonstrate that our IoE-based Prompt can achieve a consistent improvement regarding the accuracy of self-corrected responses over the initial answers. Our study not only sheds light on the underlying factors affecting self-correction in LLMs, but also introduces a practical framework that utilizes the IoE prompting principle to efficiently improve self-correction capabilities with ``confidence''. The code is available at \\url{https://github.com/MBZUAI-CLeaR/IoE-Prompting.git}.",
        "retrieved_doc": "Author: Eric Xing Title: Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large Language Models Publication year: 2024 Coauthors: Loka Li, Guan-Hong Chen, Yusheng Su, Zhenhao Chen, Yixuan Zhang, Eric Xing, Kun Zhang Abstract: The recent success of Large Language Models (LLMs) has catalyzed an increasing interest in their self-correction capabilities. This paper presents a comprehensive investigation into the intrinsic self-correction of LLMs, attempting to address the ongoing debate about its feasibility. Our research has identified an important latent factor - the ``confidence'' of LLMs - during the self-correction process. Overlooking this factor may cause the models to over-criticize themselves, resulting in unreliable conclusions regarding the efficacy of self-correction. We have experimentally observed that LLMs possess the capability to understand the ``confidence'' in their own responses. It motivates us to develop an ``If-or-Else'' (IoE) prompting framework, designed to guide LLMs in assessing their own ``confidence'', facilitating intrinsic self-corrections. We conduct extensive experiments and demonstrate that our IoE-based Prompt can achieve a consistent improvement regarding the accuracy of self-corrected responses over the initial answers. Our study not only sheds light on the underlying factors affecting self-correction in LLMs, but also introduces a practical framework that utilizes the IoE prompting principle to efficiently improve self-correction capabilities with ``confidence''. The code is available at \\url{https://github.com/MBZUAI-CLeaR/IoE-Prompting.git}.|Author: Eric Xing Title: Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large Language Models Publication year: 2024 Coauthors: Loka Li, Guan-Hong Chen, Yusheng Su, Zhenhao Chen, Yixuan Zhang, Eric Xing, Kun Zhang Abstract: The recent success of Large Language Models (LLMs) has catalyzed an increasing interest in their self-correction capabilities. This paper presents a comprehensive investigation into the intrinsic self-correction of LLMs, attempting to address the ongoing debate about its feasibility. Our research has identified an important latent factor - the ``confidence'' of LLMs - during the self-correction process. Overlooking this factor may cause the models to over-criticize themselves, resulting in unreliable conclusions regarding the efficacy of self-correction. We have experimentally observed that LLMs possess the capability to understand the ``confidence'' in their own responses. It motivates us to develop an ``If-or-Else'' (IoE) prompting framework, designed to guide LLMs in assessing their own ``confidence'', facilitating intrinsic self-corrections. We conduct extensive experiments and demonstrate that our IoE-based Prompt can achieve a consistent improvement regarding the accuracy of self-corrected responses over the initial answers. Our study not only sheds light on the underlying factors affecting self-correction in LLMs, but also introduces a practical framework that utilizes the IoE prompting principle to efficiently improve self-correction capabilities with ``confidence''. The code is available at \\url{https://github.com/MBZUAI-CLeaR/IoE-Prompting.git}.|Author: T. Mitamura Title: Language-Agnostic Transformers and Assessing ChatGPT-Based Query Rewriting for Multilingual Document-Grounded QA Publication year: 2023 Coauthors: Srinivas Gowriraj, Soham Dinesh Tiwari, Mitali Potnis, Srijan Bansal, T. Mitamura, Eric Nyberg Abstract: The DialDoc 2023 shared task has expanded the document-grounded dialogue task to encompass multiple languages, despite having limited annotated data. This paper assesses the effectiveness of both language-agnostic and language-aware paradigms for multilingual pre-trained transformer models in a bi-encoder-based dense passage retriever (DPR), concluding that the language-agnostic approach is superior. Additionally, the study investigates the impact of query rewriting techniques using large language models, such as ChatGPT, on multilingual, document-grounded question-answering systems. The experiments conducted demonstrate that, for the examples examined, query rewriting does not enhance performance compared to the original queries. This failure is due to topic switching in final dialogue turns and irrelevant topics being considered for query rewriting.",
        "retrieved_relevant_part": true,
        "model_answer": "Eric Xing",
        "actual_answer": "Eric Xing",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "What is the abstract of the paper Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers published by LTI faculty Yiming Yang in 2023?",
        "context": "Author: Yiming Yang Title: Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers Publication year: 2023 Coauthors: K. Choromanski, Shanda Li, Valerii Likhosherstov, Kumar Avinava Dubey, Shengjie Luo, Di He, Yiming Yang, Tam\u00e1s Sarl\u00f3s, Thomas Weingarten, Adrian Weller Abstract: We propose a new class of linear Transformers called FourierLearner-Transformers (FLTs), which incorporate a wide range of relative positional encoding mechanisms (RPEs). These include regular RPE techniques applied for nongeometric data, as well as novel RPEs operating on the sequences of tokens embedded in higher-dimensional Euclidean spaces (e.g. point clouds). FLTs construct the optimal RPE mechanism implicitly by learning its spectral representation. As opposed to other architectures combining efficient low-rank linear attention with RPEs, FLTs remain practical in terms of their memory usage and do not require additional assumptions about the structure of the RPE-mask. FLTs allow also for applying certain structural inductive bias techniques to specify masking strategies, e.g. they provide a way to learn the so-called local RPEs introduced in this paper and providing accuracy gains as compared with several other linear Transformers for language modeling. We also thoroughly tested FLTs on other data modalities and tasks, such as: image classification and 3D molecular modeling. For 3D-data FLTs are, to the best of our knowledge, the first Transformers architectures providing RPE-enhanced linear attention.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Yiming Yang Title: Functional Interpolation for Relative Positions Improves Long Context Transformers Publication year: 2023 Coauthors: Shanda Li, Chong You, Guru Guruganesh, J. Ainslie, Santiago Ontanon, M. Zaheer, Sumit K. Sanghai, Yiming Yang, Sanjiv Kumar, Srinadh Bhojanapalli Abstract: Preventing the performance decay of Transformers on inputs longer than those used for training has been an important challenge in extending the context length of these models. Though the Transformer architecture has fundamentally no limits on the input sequence lengths it can process, the choice of position encoding used during training can limit the performance of these models on longer inputs. We propose a novel functional relative position encoding with progressive interpolation, FIRE, to improve Transformer generalization to longer contexts. We theoretically prove that this can represent some of the popular relative position encodings, such as T5's RPE, Alibi, and Kerple. We next empirically show that FIRE models have better generalization to longer contexts on both zero-shot language modeling and long text benchmarks.|Author: A. Rudnicky Title: Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation Publication year: 2023 Coauthors: Ta-Chung Chi, Ting-Han Fan, A. Rudnicky, P. Ramadge Abstract: Unlike recurrent models, conventional wisdom has it that Transformers cannot perfectly model regular languages. Inspired by the notion of working memory, we propose a new Transformer variant named RegularGPT. With its novel combination of Weight-Sharing, Adaptive-Depth, and Sliding-Dilated-Attention, RegularGPT constructs working memory along the depth dimension, thereby enabling efficient and successful modeling of regular languages such as PARITY. We further test RegularGPT on the task of natural language length extrapolation and surprisingly find that it rediscovers the local windowed attention effect deemed necessary in prior work for length extrapolation.",
        "retrieved_relevant_part": false,
        "model_answer": "The paper proposes a novel functional relative position encoding with progressive interpolation called FIRE to improve Transformer generalization to longer contexts",
        "actual_answer": "We propose a new class of linear Transformers called FourierLearner-Transformers (FLTs), which incorporate a wide range of relative positional encoding mechanisms (RPEs). These include regular RPE techniques applied for nongeometric data, as well as novel RPEs operating on the sequences of tokens embedded in higher-dimensional Euclidean spaces (e.g. point clouds). FLTs construct the optimal RPE mechanism implicitly by learning its spectral representation. As opposed to other architectures combining efficient low-rank linear attention with RPEs, FLTs remain practical in terms of their memory usage and do not require additional assumptions about the structure of the RPE-mask. FLTs allow also for applying certain structural inductive bias techniques to specify masking strategies, e.g. they provide a way to learn the so-called local RPEs introduced in this paper and providing accuracy gains as compared with several other linear Transformers for language modeling. We also thoroughly tested FLTs on other data modalities and tasks, such as: image classification and 3D molecular modeling. For 3D-data FLTs are, to the best of our knowledge, the first Transformers architectures providing RPE-enhanced linear attention.",
        "exact_match": false,
        "f1_max_min_avg": [
            0.08,
            0.08,
            0.08
        ],
        "recall_max_min_avg": [
            0.05,
            0.05,
            0.05
        ]
    },
    {
        "question": "Who is the author of the LTI paper Unsupervised Dense Retrieval Training with Web Anchors published in 2023?",
        "context": "Author: Chenyan Xiong Title: Unsupervised Dense Retrieval Training with Web Anchors Publication year: 2023 Coauthors: Yiqing Xie, X. Liu, Chenyan Xiong Abstract: In this work, we present an unsupervised retrieval method with contrastive learning on web anchors. The anchor text describes the content that is referenced from the linked page. This shows similarities to search queries that aim to retrieve pertinent information from relevant documents. Based on their commonalities, we train an unsupervised dense retriever, Anchor-DR, with a contrastive learning task that matches the anchor text and the linked document. To filter out uninformative anchors (such as \"homepage\" or other functional anchors), we present a novel filtering technique to only select anchors that contain similar types of information as search queries. Experiments show that Anchor-DR outperforms state-of-the-art methods on unsupervised dense retrieval by a large margin (e.g., by 5.3% NDCG@10 on MSMARCO). The gain of our method is especially significant for search and question answering tasks. Our analysis further reveals that the pattern of anchor-document pairs is similar to that of search query-document pairs. Code available at https://github.com/Veronicium/AnchorDR.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Chenyan Xiong Title: Unsupervised Dense Retrieval Training with Web Anchors Publication year: 2023 Coauthors: Yiqing Xie, X. Liu, Chenyan Xiong Abstract: In this work, we present an unsupervised retrieval method with contrastive learning on web anchors. The anchor text describes the content that is referenced from the linked page. This shows similarities to search queries that aim to retrieve pertinent information from relevant documents. Based on their commonalities, we train an unsupervised dense retriever, Anchor-DR, with a contrastive learning task that matches the anchor text and the linked document. To filter out uninformative anchors (such as \"homepage\" or other functional anchors), we present a novel filtering technique to only select anchors that contain similar types of information as search queries. Experiments show that Anchor-DR outperforms state-of-the-art methods on unsupervised dense retrieval by a large margin (e.g., by 5.3% NDCG@10 on MSMARCO). The gain of our method is especially significant for search and question answering tasks. Our analysis further reveals that the pattern of anchor-document pairs is similar to that of search query-document pairs. Code available at https://github.com/Veronicium/AnchorDR.|Author: Jamie Callan Title: COILcr: Efficient Semantic Matching in Contextualized Exact Match Retrieval Publication year: 2023 Coauthors: Zhen Fan, Luyu Gao, Rohan Jha, Jamie Callan ",
        "retrieved_relevant_part": true,
        "model_answer": "Chenyan Xiong",
        "actual_answer": "Chenyan Xiong",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Who is the author of the LTI paper Faith and Fate: Limits of Transformers on Compositionality published in 2023?",
        "context": "Author: S. Welleck Title: Faith and Fate: Limits of Transformers on Compositionality Publication year: 2023 Coauthors: Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jian, Bill Yuchen Lin, Peter West, Chandra Bhagavatula, Ronan Le Bras, Jena D. Hwang, Soumya Sanyal, S. Welleck, Xiang Ren, Allyson Ettinger, Za\u00efd Harchaoui, Yejin Choi Abstract: Transformer large language models (LLMs) have sparked admiration for their exceptional performance on tasks that demand intricate multi-step reasoning. Yet, these models simultaneously show failures on surprisingly trivial problems. This begs the question: Are these errors incidental, or do they signal more substantial limitations? In an attempt to demystify transformer LLMs, we investigate the limits of these models across three representative compositional tasks -- multi-digit multiplication, logic grid puzzles, and a classic dynamic programming problem. These tasks require breaking problems down into sub-steps and synthesizing these steps into a precise answer. We formulate compositional tasks as computation graphs to systematically quantify the level of complexity, and break down reasoning steps into intermediate sub-procedures. Our empirical findings suggest that transformer LLMs solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, without necessarily developing systematic problem-solving skills. To round off our empirical study, we provide theoretical arguments on abstract multi-step reasoning problems that highlight how autoregressive generations' performance can rapidly decay with\\,increased\\,task\\,complexity.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Shinji Watanabe Title: Challenges of Corporate Alliance CLOMA toward Plastic Litter Publication year: 2023 Coauthors: Shinji Watanabe |Author: Jamie Callan Title: COILcr: Efficient Semantic Matching in Contextualized Exact Match Retrieval Publication year: 2023 Coauthors: Zhen Fan, Luyu Gao, Rohan Jha, Jamie Callan ",
        "retrieved_relevant_part": false,
        "model_answer": "The author of the LTI paper Faith and Fate: Limits of Transformers on Compositionality is named Shinji Watanabe",
        "actual_answer": "S. Welleck",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Which LTI faculty published the paper Active Retrieval Augmented Generation in 2023?",
        "context": "Author: Graham Neubig Title: Active Retrieval Augmented Generation Publication year: 2023 Coauthors: Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig Abstract: Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Shinji Watanabe Title: Challenges of Corporate Alliance CLOMA toward Plastic Litter Publication year: 2023 Coauthors: Shinji Watanabe |Author: Jamie Callan Title: COILcr: Efficient Semantic Matching in Contextualized Exact Match Retrieval Publication year: 2023 Coauthors: Zhen Fan, Luyu Gao, Rohan Jha, Jamie Callan ",
        "retrieved_relevant_part": false,
        "model_answer": "The paper was published by LTI faculty.<|endoftext|>",
        "actual_answer": "Graham Neubig",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Who are the coauthors of the paper The DARPA Wikidata Overlay: Wikidata as an ontology for natural language processing published by LTI faculty A. Gershman in 2023?",
        "context": "Author: A. Gershman Title: The DARPA Wikidata Overlay: Wikidata as an ontology for natural language processing Publication year: 2023 Coauthors: Elizabeth Spaulding, Kathryn Conger, A. Gershman, Rosario Uceda-Sosa, S. Brown, James Pustejovsky, Peter Anick, Martha Palmer Abstract: With 102,530,067 items currently in its crowd-sourced knowledge base, Wikidata provides NLP practitioners a unique and powerful resource for inference and reasoning over real-world entities. However, because Wikidata is very entity focused, events and actions are often labeled with eventive nouns (e.g., the process of diagnosing a person\u2019s illness is labeled \u201cdiagnosis\u201d), and the typical participants in an event are not described or linked to that event concept (e.g., the medical professional or patient). Motivated by a need for an adaptable, comprehensive, domain-flexible ontology for information extraction, including identifying the roles entities are playing in an event, we present a curated subset of Wikidata in which events have been enriched with PropBank roles. To enable richer narrative understanding between events from Wikidata concepts, we have also provided a comprehensive mapping from temporal Qnodes and Pnodes to the Allen Interval Temporal Logic relations.",
        "retrieved_doc": "Author: Maarten Sap Title: Don't Take This Out of Context!: On the Need for Contextual Models and Evaluations for Stylistic Rewriting Publication year: 2023 Coauthors: Akhila Yerukola, Xuhui Zhou, Elizabeth Clark, Maarten Sap |Author: A. Gershman Title: The DARPA Wikidata Overlay: Wikidata as an ontology for natural language processing Publication year: 2023 Coauthors: Elizabeth Spaulding, Kathryn Conger, A. Gershman, Rosario Uceda-Sosa, S. Brown, James Pustejovsky, Peter Anick, Martha Palmer Abstract: With 102,530,067 items currently in its crowd-sourced knowledge base, Wikidata provides NLP practitioners a unique and powerful resource for inference and reasoning over real-world entities. However, because Wikidata is very entity focused, events and actions are often labeled with eventive nouns (e.g., the process of diagnosing a person\u2019s illness is labeled \u201cdiagnosis\u201d), and the typical participants in an event are not described or linked to that event concept (e.g., the medical professional or patient). Motivated by a need for an adaptable, comprehensive, domain-flexible ontology for information extraction, including identifying the roles entities are playing in an event, we present a curated subset of Wikidata in which events have been enriched with PropBank roles. To enable richer narrative understanding between events from Wikidata concepts, we have also provided a comprehensive mapping from temporal Qnodes and Pnodes to the Allen Interval Temporal Logic relations.|Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe ",
        "retrieved_relevant_part": true,
        "model_answer": "The coauthors of the paper are Maarten Sap, Akhila Yerukola, Xuhui Zhou, Elizabeth Clark, Elizabeth Spaulding, Kathryn Conger, Rosario Uceda-Sosa, S. Brown, James Pustejovsky, Peter Anick, Martha Palmer, and Shinji Watanabe",
        "actual_answer": "Elizabeth Spaulding, Kathryn Conger, A. Gershman, Rosario Uceda-Sosa, S. Brown, James Pustejovsky, Peter Anick, Martha Palmer",
        "exact_match": false,
        "f1_max_min_avg": [
            0.55,
            0.55,
            0.55
        ],
        "recall_max_min_avg": [
            0.81,
            0.81,
            0.81
        ]
    },
    {
        "question": "What is the abstract of the paper Exploration of Efficient End-to-End ASR using Discretized Input from Self-Supervised Learning published by LTI faculty Shinji Watanabe in 2023?",
        "context": "Author: Shinji Watanabe Title: Exploration of Efficient End-to-End ASR using Discretized Input from Self-Supervised Learning Publication year: 2023 Coauthors: Xuankai Chang, Brian Yan, Yuya Fujita, Takashi Maekaku, Shinji Watanabe Abstract: Self-supervised learning (SSL) of speech has shown impressive results in speech-related tasks, particularly in automatic speech recognition (ASR). While most methods employ the output of intermediate layers of the SSL model as real-valued features for downstream tasks, there is potential in exploring alternative approaches that use discretized token sequences. This approach offers benefits such as lower storage requirements and the ability to apply techniques from natural language processing. In this paper, we propose a new protocol that utilizes discretized token sequences in ASR tasks, which includes de-duplication and sub-word modeling to enhance the input sequence. It reduces computational cost by decreasing the length of the sequence. Our experiments on the LibriSpeech dataset demonstrate that our proposed protocol performs competitively with conventional ASR systems using continuous input features, while reducing computational and storage costs.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Shinji Watanabe Title: E-Branchformer-Based E2E SLU Toward Stop on-Device Challenge Publication year: 2023 Coauthors: Yosuke Kashiwagi, Siddhant Arora, Hayato Futami, Jessica Huynh, Shih-Lun Wu, Yifan Peng, Brian Yan, E. Tsunoo, Shinji Watanabe Abstract: In this paper, we report our team\u2019s study on track 2 of the Spoken Language Understanding Grand Challenge, which is a component of the ICASSP Signal Processing Grand Challenge 2023. The task is intended for on-device processing and involves estimating semantic parse labels from speech using a model with 15 million parameters. We use E2E E-Branchformer-based spoken language understanding model, which is more parameter controllable than cascade models, and reduced the parameter size through sequential distillation and tensor decomposition techniques. On the STOP dataset, we achieved an exact match accuracy of 70.9% under the tight constraint of 15 million parameters.|Author: Shinji Watanabe Title: Exploration of Efficient End-to-End ASR using Discretized Input from Self-Supervised Learning Publication year: 2023 Coauthors: Xuankai Chang, Brian Yan, Yuya Fujita, Takashi Maekaku, Shinji Watanabe Abstract: Self-supervised learning (SSL) of speech has shown impressive results in speech-related tasks, particularly in automatic speech recognition (ASR). While most methods employ the output of intermediate layers of the SSL model as real-valued features for downstream tasks, there is potential in exploring alternative approaches that use discretized token sequences. This approach offers benefits such as lower storage requirements and the ability to apply techniques from natural language processing. In this paper, we propose a new protocol that utilizes discretized token sequences in ASR tasks, which includes de-duplication and sub-word modeling to enhance the input sequence. It reduces computational cost by decreasing the length of the sequence. Our experiments on the LibriSpeech dataset demonstrate that our proposed protocol performs competitively with conventional ASR systems using continuous input features, while reducing computational and storage costs.",
        "retrieved_relevant_part": true,
        "model_answer": "The paper proposes a new protocol that uses discretized token sequences in ASR tasks, which includes de-duplication and sub-word modeling to enhance the input sequence",
        "actual_answer": "Self-supervised learning (SSL) of speech has shown impressive results in speech-related tasks, particularly in automatic speech recognition (ASR). While most methods employ the output of intermediate layers of the SSL model as real-valued features for downstream tasks, there is potential in exploring alternative approaches that use discretized token sequences. This approach offers benefits such as lower storage requirements and the ability to apply techniques from natural language processing. In this paper, we propose a new protocol that utilizes discretized token sequences in ASR tasks, which includes de-duplication and sub-word modeling to enhance the input sequence. It reduces computational cost by decreasing the length of the sequence. Our experiments on the LibriSpeech dataset demonstrate that our proposed protocol performs competitively with conventional ASR systems using continuous input features, while reducing computational and storage costs.",
        "exact_match": false,
        "f1_max_min_avg": [
            0.25,
            0.25,
            0.25
        ],
        "recall_max_min_avg": [
            0.15,
            0.15,
            0.15
        ]
    },
    {
        "question": "What is the abstract of the paper Data-efficient Active Learning for Structured Prediction with Partial Annotation and Self-Training published by LTI faculty Emma Strubell in 2023?",
        "context": "Author: Emma Strubell Title: Data-efficient Active Learning for Structured Prediction with Partial Annotation and Self-Training Publication year: 2023 Coauthors: Zhisong Zhang, Emma Strubell, E. Hovy Abstract: In this work we propose a pragmatic method that reduces the annotation cost for structured label spaces using active learning. Our approach leverages partial annotation, which reduces labeling costs for structured outputs by selecting only the most informative sub-structures for annotation. We also utilize self-training to incorporate the current model's automatic predictions as pseudo-labels for un-annotated sub-structures. A key challenge in effectively combining partial annotation with self-training to reduce annotation cost is determining which sub-structures to select to label. To address this challenge, we adopt an error estimator to adaptively decide the partial selection ratio according to the current model's capability. In evaluations spanning four structured prediction tasks, we show that our combination of partial annotation and self-training using an adaptive selection ratio reduces annotation cost over strong full annotation baselines under a fair comparison scheme that takes reading time into consideration.",
        "retrieved_doc": "Author: Alexander I. Rudnicky Title: Tartan: an LLM Driven SocialBot Publication year: 2023 Coauthors: Liangze Li, Ziyuan Liu, Li-Wei Chen, Ta-Chung Chi, Alexander I. Rudnicky Abstract: We describe our work to date on using large language models (LLMs) in our Alexa Prize Socialbot. Our work has laid the groundwork for looking more closely at using LLMs in a conversational system. We also analyze common patterns in conversations our bot has had with users|Author: David R. Mortensen Title: SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing Publication year: 2023 Coauthors: Taiqi He, Lindia Tjuatja, Nathaniel R. Robinson, Shinji Watanabe, David R. Mortensen, Graham Neubig, L. Levin Abstract: In our submission to the SIGMORPHON 2023 Shared Task on interlinear glossing (IGT), we explore approaches to data augmentation and modeling across seven low-resource languages. For data augmentation, we explore two approaches: creating artificial data from the provided training data and utilizing existing IGT resources in other languages. On the modeling side, we test an enhanced version of the provided token classification baseline as well as a pretrained multilingual seq2seq model. Additionally, we apply post-correction using a dictionary for Gitksan, the language with the smallest amount of data. We find that our token classification models are the best performing, with the highest word-level accuracy for Arapaho and highest morpheme-level accuracy for Gitksan out of all submissions. We also show that data augmentation is an effective strategy, though applying artificial data pretraining has very different effects across both models tested.|Author: Shinji Watanabe Title: SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing Publication year: 2023 Coauthors: Taiqi He, Lindia Tjuatja, Nathaniel R. Robinson, Shinji Watanabe, David R. Mortensen, Graham Neubig, L. Levin Abstract: In our submission to the SIGMORPHON 2023 Shared Task on interlinear glossing (IGT), we explore approaches to data augmentation and modeling across seven low-resource languages. For data augmentation, we explore two approaches: creating artificial data from the provided training data and utilizing existing IGT resources in other languages. On the modeling side, we test an enhanced version of the provided token classification baseline as well as a pretrained multilingual seq2seq model. Additionally, we apply post-correction using a dictionary for Gitksan, the language with the smallest amount of data. We find that our token classification models are the best performing, with the highest word-level accuracy for Arapaho and highest morpheme-level accuracy for Gitksan out of all submissions. We also show that data augmentation is an effective strategy, though applying artificial data pretraining has very different effects across both models tested.",
        "retrieved_relevant_part": false,
        "model_answer": "The paper describes the use of active learning techniques for structured prediction with partial annotation and self-training",
        "actual_answer": "In this work we propose a pragmatic method that reduces the annotation cost for structured label spaces using active learning. Our approach leverages partial annotation, which reduces labeling costs for structured outputs by selecting only the most informative sub-structures for annotation. We also utilize self-training to incorporate the current model's automatic predictions as pseudo-labels for un-annotated sub-structures. A key challenge in effectively combining partial annotation with self-training to reduce annotation cost is determining which sub-structures to select to label. To address this challenge, we adopt an error estimator to adaptively decide the partial selection ratio according to the current model's capability. In evaluations spanning four structured prediction tasks, we show that our combination of partial annotation and self-training using an adaptive selection ratio reduces annotation cost over strong full annotation baselines under a fair comparison scheme that takes reading time into consideration.",
        "exact_match": false,
        "f1_max_min_avg": [
            0.14,
            0.14,
            0.14
        ],
        "recall_max_min_avg": [
            0.08,
            0.08,
            0.08
        ]
    },
    {
        "question": "Which LTI faculty published the paper Improving Massively Multilingual ASR with Auxiliary CTC Objectives in 2023?",
        "context": "Author: Shinji Watanabe Title: Improving Massively Multilingual ASR with Auxiliary CTC Objectives Publication year: 2023 Coauthors: William Chen, Brian Yan, Jiatong Shi, Yifan Peng, Soumi Maiti, Shinji Watanabe Abstract: Multilingual Automatic Speech Recognition (ASR) models have extended the usability of speech technologies to a wide variety of languages. With how many languages these models have to handle, however, a key to understanding their imbalanced performance across different languages is to examine if the model actually knows which language it should transcribe. In this paper, we introduce our work on improving performance on FLEURS, a 102-language open ASR benchmark, by conditioning the entire model on language identity (LID). We investigate techniques inspired from recent Connectionist Temporal Classification (CTC) studies to help the model handle the large number of languages, conditioning on the LID predictions of auxiliary tasks. Our experimental results demonstrate the effectiveness of our technique over standard CTC/Attention-based hybrid models. Furthermore, our state-of-the-art systems using self-supervised models with the Conformer architecture improve over the results of prior work on FLEURS by a relative 28.4% CER. Trained models are reproducible recipes are available at https://github.com/espnet/espnet/tree/master/egs2/fleurs/asr1.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Mona T. Diab Title: Evaluating Multilingual Speech Translation under Realistic Conditions with Resegmentation and Terminology Publication year: 2023 Coauthors: Elizabeth Salesky, Kareem Darwish, Mohamed Al-Badrashiny, Mona T. Diab, J. Niehues Abstract: We present the ACL 60/60 evaluation sets for multilingual translation of ACL 2022 technical presentations into 10 target languages. This dataset enables further research into multilingual speech translation under realistic recording conditions with unsegmented audio and domain-specific terminology, applying NLP tools to text and speech in the technical domain, and evaluating and improving model robustness to diverse speaker demographics.|Author: Louis-Philippe Morency Title: Reconstructing the neurodynamics of face perception during real world vision in humans using intracranial EEG recordings Publication year: 2023 Coauthors: Arish Alreja, Michael J. Ward, J. A. Colan, Qianli Ma, R. M. Richardson, Louis-Philippe Morency, A. Ghuman ",
        "retrieved_relevant_part": false,
        "model_answer": "The paper was published by the LTI faculty, including Shinji Watanabe, Yui Sudo, Muhammad Shakeel, Yifan Peng, Mona T. Diab, Elizabeth Salesky, Kareem Darwish, Mohamed Al-Badrashiny, and J. Niehues",
        "actual_answer": "Shinji Watanabe",
        "exact_match": false,
        "f1_max_min_avg": [
            0.06,
            0.06,
            0.06
        ],
        "recall_max_min_avg": [
            0.5,
            0.5,
            0.5
        ]
    },
    {
        "question": "Who are the coauthors of the paper LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning published by LTI faculty Eric P. Xing in 2023?",
        "context": "Author: Eric P. Xing Title: LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning Publication year: 2023 Coauthors: Han Guo, P. Greengard, Eric P. Xing, Yoon Kim Abstract: We propose a simple approach for memory-efficient adaptation of pretrained language models. Our approach uses an iterative algorithm to decompose each pretrained matrix into a high-precision low-rank component and a memory-efficient quantized component. During finetuning, the quantized component remains fixed and only the low-rank component is updated. We present an integer linear programming formulation of the quantization component which enables dynamic configuration of quantization parameters (e.g., bit-width, block size) for each matrix given an overall target memory budget. We further explore a data-aware version of the algorithm which uses an approximation of the Fisher information matrix to weight the reconstruction objective during matrix decomposition. Experiments on finetuning RoBERTa and LLaMA-2 (7B and 70B) demonstrate that our low-rank plus quantized matrix decomposition approach (LQ-LoRA) outperforms strong QLoRA and GPTQ-LoRA baselines and enables aggressive quantization to sub-3 bits with only minor performance degradations. When finetuned on a language modeling calibration dataset, LQ-LoRA can also be used for model compression; in this setting our 2.75-bit LLaMA-2-70B model (which has 2.85 bits on average when including the low-rank components and requires 27GB of GPU memory) performs respectably compared to the 16-bit baseline.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Jamie Callan Title: COILcr: Efficient Semantic Matching in Contextualized Exact Match Retrieval Publication year: 2023 Coauthors: Zhen Fan, Luyu Gao, Rohan Jha, Jamie Callan |Author: Alexander I. Rudnicky Title: Tartan: an LLM Driven SocialBot Publication year: 2023 Coauthors: Liangze Li, Ziyuan Liu, Li-Wei Chen, Ta-Chung Chi, Alexander I. Rudnicky Abstract: We describe our work to date on using large language models (LLMs) in our Alexa Prize Socialbot. Our work has laid the groundwork for looking more closely at using LLMs in a conversational system. We also analyze common patterns in conversations our bot has had with users",
        "retrieved_relevant_part": false,
        "model_answer": "The coauthors of the paper are Yui Sudo, Muhammad Shakeel, Yifan Peng, and Shinji Watanabe",
        "actual_answer": "Han Guo, P. Greengard, Eric P. Xing, Yoon Kim",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Who are the coauthors of the paper A Self-enhancement Approach for Domain-specific Chatbot Training via Knowledge Mining and Digest published by LTI faculty Yiming Yang in 2023?",
        "context": "Author: Yiming Yang Title: A Self-enhancement Approach for Domain-specific Chatbot Training via Knowledge Mining and Digest Publication year: 2023 Coauthors: Ruohong Zhang, Luyu Gao, Chen Zheng, Zhen Fan, Guokun Lai, Zheng Zhang, Fangzhou Ai, Yiming Yang, Hongxia Yang Abstract: Large Language Models (LLMs), despite their great power in language generation, often encounter challenges when dealing with intricate and knowledge-demanding queries in specific domains. This paper introduces a novel approach to enhance LLMs by effectively extracting the relevant knowledge from domain-specific textual sources, and the adaptive training of a chatbot with domain-specific inquiries. Our two-step approach starts from training a knowledge miner, namely LLMiner, which autonomously extracts Question-Answer pairs from relevant documents through a chain-of-thought reasoning process. Subsequently, we blend the mined QA pairs with a conversational dataset to fine-tune the LLM as a chatbot, thereby enriching its domain-specific expertise and conversational capabilities. We also developed a new evaluation benchmark which comprises four domain-specific text corpora and associated human-crafted QA pairs for testing. Our model shows remarkable performance improvement over generally aligned LLM and surpasses domain-adapted models directly fine-tuned on domain corpus. In particular, LLMiner achieves this with minimal human intervention, requiring only 600 seed instances, thereby providing a pathway towards self-improvement of LLMs through model-synthesized training data.",
        "retrieved_doc": "Author: Alexander I. Rudnicky Title: Tartan: an LLM Driven SocialBot Publication year: 2023 Coauthors: Liangze Li, Ziyuan Liu, Li-Wei Chen, Ta-Chung Chi, Alexander I. Rudnicky Abstract: We describe our work to date on using large language models (LLMs) in our Alexa Prize Socialbot. Our work has laid the groundwork for looking more closely at using LLMs in a conversational system. We also analyze common patterns in conversations our bot has had with users|Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Yiming Yang Title: A Self-enhancement Approach for Domain-specific Chatbot Training via Knowledge Mining and Digest Publication year: 2023 Coauthors: Ruohong Zhang, Luyu Gao, Chen Zheng, Zhen Fan, Guokun Lai, Zheng Zhang, Fangzhou Ai, Yiming Yang, Hongxia Yang Abstract: Large Language Models (LLMs), despite their great power in language generation, often encounter challenges when dealing with intricate and knowledge-demanding queries in specific domains. This paper introduces a novel approach to enhance LLMs by effectively extracting the relevant knowledge from domain-specific textual sources, and the adaptive training of a chatbot with domain-specific inquiries. Our two-step approach starts from training a knowledge miner, namely LLMiner, which autonomously extracts Question-Answer pairs from relevant documents through a chain-of-thought reasoning process. Subsequently, we blend the mined QA pairs with a conversational dataset to fine-tune the LLM as a chatbot, thereby enriching its domain-specific expertise and conversational capabilities. We also developed a new evaluation benchmark which comprises four domain-specific text corpora and associated human-crafted QA pairs for testing. Our model shows remarkable performance improvement over generally aligned LLM and surpasses domain-adapted models directly fine-tuned on domain corpus. In particular, LLMiner achieves this with minimal human intervention, requiring only 600 seed instances, thereby providing a pathway towards self-improvement of LLMs through model-synthesized training data.",
        "retrieved_relevant_part": true,
        "model_answer": "The coauthors of the paper are Ruohong Zhang, Luyu Gao, Chen Zheng, Zhen Fan, Guokun Lai, Zheng Zhang, Fangzhou Ai, Yiming Yang, Hongxia Yang, and Yiming Yang",
        "actual_answer": "Ruohong Zhang, Luyu Gao, Chen Zheng, Zhen Fan, Guokun Lai, Zheng Zhang, Fangzhou Ai, Yiming Yang, Hongxia Yang",
        "exact_match": false,
        "f1_max_min_avg": [
            0.76,
            0.76,
            0.76
        ],
        "recall_max_min_avg": [
            0.94,
            0.94,
            0.94
        ]
    },
    {
        "question": "Which LTI faculty published the paper Distributionally Robust Unsupervised Dense Retrieval Training on Web Graphs in 2023?",
        "context": "Author: Chenyan Xiong Title: Distributionally Robust Unsupervised Dense Retrieval Training on Web Graphs Publication year: 2023 Coauthors: Peixuan Han, Zhenghao Liu, Zhiyuan Liu, Chenyan Xiong Abstract: This paper introduces Web-DRO, an unsupervised dense retrieval model, which clusters documents based on web structures and reweights the groups during contrastive training. Specifically, we first leverage web graph links and contrastively train an embedding model for clustering anchor-document pairs. Then we use Group Distributional Robust Optimization to reweight different clusters of anchor-document pairs, which guides the model to assign more weights to the group with higher contrastive loss and pay more attention to the worst case during training. Our experiments on MS MARCO and BEIR show that our model, Web-DRO, significantly improves the retrieval effectiveness in unsupervised scenarios. A comparison of clustering techniques shows that training on the web graph combining URL information reaches optimal performance on clustering. Further analysis confirms that group weights are stable and valid, indicating consistent model preferences as well as effective up-weighting of valuable groups and down-weighting of uninformative ones. The code of this paper can be obtained from https://github.com/OpenMatch/Web-DRO.",
        "retrieved_doc": "Author: Chenyan Xiong Title: Distributionally Robust Unsupervised Dense Retrieval Training on Web Graphs Publication year: 2023 Coauthors: Peixuan Han, Zhenghao Liu, Zhiyuan Liu, Chenyan Xiong Abstract: This paper introduces Web-DRO, an unsupervised dense retrieval model, which clusters documents based on web structures and reweights the groups during contrastive training. Specifically, we first leverage web graph links and contrastively train an embedding model for clustering anchor-document pairs. Then we use Group Distributional Robust Optimization to reweight different clusters of anchor-document pairs, which guides the model to assign more weights to the group with higher contrastive loss and pay more attention to the worst case during training. Our experiments on MS MARCO and BEIR show that our model, Web-DRO, significantly improves the retrieval effectiveness in unsupervised scenarios. A comparison of clustering techniques shows that training on the web graph combining URL information reaches optimal performance on clustering. Further analysis confirms that group weights are stable and valid, indicating consistent model preferences as well as effective up-weighting of valuable groups and down-weighting of uninformative ones. The code of this paper can be obtained from https://github.com/OpenMatch/Web-DRO.|Yonatan Bisk Assistant Professor Email: ybisk@cs.cmu.edu Office: Gates & Hillman Centers Research Areas: Grounding, RoboNLP, Vision and Language, Embodiment, Unsupervised Learning|Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe ",
        "retrieved_relevant_part": true,
        "model_answer": "Chenyan Xiong",
        "actual_answer": "Chenyan Xiong",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Who is the author of the LTI paper Multilingual TTS Accent Impressions for Accented ASR published in 2023?",
        "context": "Author: David R. Mortensen Title: Multilingual TTS Accent Impressions for Accented ASR Publication year: 2023 Coauthors: G. Karakasidis, Nathaniel R. Robinson, Yaroslav Getman, Atieno Ogayo, Ragheb Al-Ghezi, Ananya Ayasi, Shinji Watanabe, David R. Mortensen, M. Kurimo ",
        "retrieved_doc": "Author: David R. Mortensen Title: Multilingual TTS Accent Impressions for Accented ASR Publication year: 2023 Coauthors: G. Karakasidis, Nathaniel R. Robinson, Yaroslav Getman, Atieno Ogayo, Ragheb Al-Ghezi, Ananya Ayasi, Shinji Watanabe, David R. Mortensen, M. Kurimo |Author: Shinji Watanabe Title: Multilingual TTS Accent Impressions for Accented ASR Publication year: 2023 Coauthors: G. Karakasidis, Nathaniel R. Robinson, Yaroslav Getman, Atieno Ogayo, Ragheb Al-Ghezi, Ananya Ayasi, Shinji Watanabe, David R. Mortensen, M. Kurimo |Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe ",
        "retrieved_relevant_part": true,
        "model_answer": "David R. Mortensen",
        "actual_answer": "David R. Mortensen",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "What is the abstract of the paper Transformed Protoform Reconstruction published by LTI faculty David R. Mortensen in 2023?",
        "context": "Author: David R. Mortensen Title: Transformed Protoform Reconstruction Publication year: 2023 Coauthors: Young Min Kim, Kalvin Chang, Chenxuan Cui, David R. Mortensen Abstract: Protoform reconstruction is the task of inferring what morphemes or words appeared like in the ancestral languages of a set of daughter languages. Meloni et al (2021) achieved the state-of-the-art on Latin protoform reconstruction with an RNN-based encoder-decoder with attention model. We update their model with the state-of-the-art seq2seq model: the Transformer. Our model outperforms their model on a suite of different metrics on two different datasets: their Romance data of 8,000 cognates spanning 5 languages and a Chinese dataset (Hou 2004) of 800+ cognates spanning 39 varieties. We also probe our model for potential phylogenetic signal contained in the model. Our code is publicly available at https://github.com/cmu-llab/acl-2023.",
        "retrieved_doc": "Author: Graham Neubig Title: DiffusER: Diffusion via Edit-based Reconstruction Publication year: 2023 Coauthors: Machel Reid, V. Hellendoorn, Graham Neubig |Author: Maarten Sap Title: Don't Take This Out of Context!: On the Need for Contextual Models and Evaluations for Stylistic Rewriting Publication year: 2023 Coauthors: Akhila Yerukola, Xuhui Zhou, Elizabeth Clark, Maarten Sap |Author: David R. Mortensen Title: Transformed Protoform Reconstruction Publication year: 2023 Coauthors: Young Min Kim, Kalvin Chang, Chenxuan Cui, David R. Mortensen Abstract: Protoform reconstruction is the task of inferring what morphemes or words appeared like in the ancestral languages of a set of daughter languages. Meloni et al (2021) achieved the state-of-the-art on Latin protoform reconstruction with an RNN-based encoder-decoder with attention model. We update their model with the state-of-the-art seq2seq model: the Transformer. Our model outperforms their model on a suite of different metrics on two different datasets: their Romance data of 8,000 cognates spanning 5 languages and a Chinese dataset (Hou 2004) of 800+ cognates spanning 39 varieties. We also probe our model for potential phylogenetic signal contained in the model. Our code is publicly available at https://github.com/cmu-llab/acl-2023.",
        "retrieved_relevant_part": true,
        "model_answer": "The paper Transformed Protoform Reconstruction discusses the use of contextual models and evaluations for stylistic rewriting",
        "actual_answer": "Protoform reconstruction is the task of inferring what morphemes or words appeared like in the ancestral languages of a set of daughter languages. Meloni et al (2021) achieved the state-of-the-art on Latin protoform reconstruction with an RNN-based encoder-decoder with attention model. We update their model with the state-of-the-art seq2seq model: the Transformer. Our model outperforms their model on a suite of different metrics on two different datasets: their Romance data of 8,000 cognates spanning 5 languages and a Chinese dataset (Hou 2004) of 800+ cognates spanning 39 varieties. We also probe our model for potential phylogenetic signal contained in the model. Our code is publicly available at https://github.com/cmu-llab/acl-2023.",
        "exact_match": false,
        "f1_max_min_avg": [
            0.08,
            0.08,
            0.08
        ],
        "recall_max_min_avg": [
            0.05,
            0.05,
            0.05
        ]
    },
    {
        "question": "Who is the author of the LTI paper Learning to Ask Questions for Zero-shot Dialogue State Tracking published in 2023?",
        "context": "Author: Alexander I. Rudnicky Title: Learning to Ask Questions for Zero-shot Dialogue State Tracking Publication year: 2023 Coauthors: Diogo Tavares, David Semedo, Alexander I. Rudnicky, Jo\u00e3o Magalh\u00e3es Abstract: We present a method for performing zero-shot Dialogue State Tracking (DST) by casting the task as a learning-to-ask-questions framework. The framework learns to pair the best question generation (QG) strategy with in-domain question answering (QA) methods to extract slot values from a dialogue without any human intervention. A novel self-supervised QA pretraining step using in-domain data is essential to learn the structure without requiring any slot-filling annotations. Moreover, we show that QG methods need to be aligned with the same grammatical person used in the dialogue. Empirical evaluation on the MultiWOZ 2.1 dataset demonstrates that our approach, when used alongside robust QA models, outperforms existing zero-shot methods in the challenging task of zero-shot cross domain adaptation-given a comparable amount of domain knowledge during data creation. Finally, we analyze the impact of the types of questions used, and demonstrate that the algorithmic approach outperforms template-based question generation.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Alexander I. Rudnicky Title: Learning to Ask Questions for Zero-shot Dialogue State Tracking Publication year: 2023 Coauthors: Diogo Tavares, David Semedo, Alexander I. Rudnicky, Jo\u00e3o Magalh\u00e3es Abstract: We present a method for performing zero-shot Dialogue State Tracking (DST) by casting the task as a learning-to-ask-questions framework. The framework learns to pair the best question generation (QG) strategy with in-domain question answering (QA) methods to extract slot values from a dialogue without any human intervention. A novel self-supervised QA pretraining step using in-domain data is essential to learn the structure without requiring any slot-filling annotations. Moreover, we show that QG methods need to be aligned with the same grammatical person used in the dialogue. Empirical evaluation on the MultiWOZ 2.1 dataset demonstrates that our approach, when used alongside robust QA models, outperforms existing zero-shot methods in the challenging task of zero-shot cross domain adaptation-given a comparable amount of domain knowledge during data creation. Finally, we analyze the impact of the types of questions used, and demonstrate that the algorithmic approach outperforms template-based question generation.|Author: Jamie Callan Title: COILcr: Efficient Semantic Matching in Contextualized Exact Match Retrieval Publication year: 2023 Coauthors: Zhen Fan, Luyu Gao, Rohan Jha, Jamie Callan ",
        "retrieved_relevant_part": true,
        "model_answer": "The author of the LTI paper is named Shinji Watanabe",
        "actual_answer": "Alexander I. Rudnicky",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Who are the coauthors of the paper The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation published by LTI faculty Graham Neubig in 2023?",
        "context": "Author: Graham Neubig Title: The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation Publication year: 2023 Coauthors: Patrick Fernandes, Daniel Deutsch, M. Finkelstein, Parker Riley, Andr\u00e9 F. T. Martins, Graham Neubig, Ankush Garg, J. Clark, Markus Freitag, Orhan Firat Abstract: Automatic evaluation of machine translation (MT) is a critical tool driving the rapid iterative development of MT systems. While considerable progress has been made on estimating a single scalar quality score, current metrics lack the informativeness of more detailed schemes that annotate individual errors, such as Multidimensional Quality Metrics (MQM). In this paper, we help fill this gap by proposing AutoMQM, a prompting technique which leverages the reasoning and in-context learning capabilities of large language models (LLMs) and asks them to identify and categorize errors in translations. We start by evaluating recent LLMs, such as PaLM and PaLM-2, through simple score prediction prompting, and we study the impact of labeled data through in-context learning and finetuning. We then evaluate AutoMQM with PaLM-2 models, and we find that it improves performance compared to just prompting for scores (with particularly large gains for larger models) while providing interpretability through error spans that align with human annotations.",
        "retrieved_doc": "Author: Maarten Sap Title: Don't Take This Out of Context!: On the Need for Contextual Models and Evaluations for Stylistic Rewriting Publication year: 2023 Coauthors: Akhila Yerukola, Xuhui Zhou, Elizabeth Clark, Maarten Sap |Author: T. Mitamura Title: Language-Agnostic Transformers and Assessing ChatGPT-Based Query Rewriting for Multilingual Document-Grounded QA Publication year: 2023 Coauthors: Srinivas Gowriraj, Soham Dinesh Tiwari, Mitali Potnis, Srijan Bansal, T. Mitamura, Eric Nyberg Abstract: The DialDoc 2023 shared task has expanded the document-grounded dialogue task to encompass multiple languages, despite having limited annotated data. This paper assesses the effectiveness of both language-agnostic and language-aware paradigms for multilingual pre-trained transformer models in a bi-encoder-based dense passage retriever (DPR), concluding that the language-agnostic approach is superior. Additionally, the study investigates the impact of query rewriting techniques using large language models, such as ChatGPT, on multilingual, document-grounded question-answering systems. The experiments conducted demonstrate that, for the examples examined, query rewriting does not enhance performance compared to the original queries. This failure is due to topic switching in final dialogue turns and irrelevant topics being considered for query rewriting.|Author: Daniel Fried Title: AutoReply: Detecting Nonsense in Dialogue with Discriminative Replies Publication year: 2023 Coauthors: Weiyan Shi, Emily Dinan, Adi Renduchintala, Daniel Fried, Athul Paul Jacob, Zhou Yu, Mike Lewis ",
        "retrieved_relevant_part": false,
        "model_answer": "Weiyan Shi, Emily Dinan, Adi Renduchintala, Daniel Fried, Athul Paul Jacob, Zhou Yu, and Mike Lewis",
        "actual_answer": "Patrick Fernandes, Daniel Deutsch, M. Finkelstein, Parker Riley, Andr\u00e9 F. T. Martins, Graham Neubig, Ankush Garg, J. Clark, Markus Freitag, Orhan Firat",
        "exact_match": false,
        "f1_max_min_avg": [
            0.05,
            0.05,
            0.05
        ],
        "recall_max_min_avg": [
            0.05,
            0.05,
            0.05
        ]
    },
    {
        "question": "What is the abstract of the paper Multi-Channel Speaker Extraction with Adversarial Training: The Wavlab Submission to The Clarity ICASSP 2023 Grand Challenge published by LTI faculty Shinji Watanabe in 2023?",
        "context": "Author: Shinji Watanabe Title: Multi-Channel Speaker Extraction with Adversarial Training: The Wavlab Submission to The Clarity ICASSP 2023 Grand Challenge Publication year: 2023 Coauthors: Samuele Cornell, Zhongqiu Wang, Yoshiki Masuyama, Shinji Watanabe, Manuel Pariente, Nobutaka Ono, S. Squartini Abstract: In this work we detail our submission to the Clarity ICASSP 2023 grand challenge, in which participants have to develop a strong target speech enhancement system for hearing-aid (HA) devices in noisy-reverberant environments. Our system builds on our previous submission at the Second Clarity Enhancement Challenge (CEC2): iNeuBe-X, which consists in an iterative neural/conventional beamforming enhancement pipeline, guided by an enrollment utterance from the target speaker. This model, which won by a large margin the CEC2, is an extension of the state-of-the-art TF-GridNet model for multi-channel, streamable target-speaker speech enhancement. Here, this approach is extended and further improved by leveraging generative adversarial training, which we show proves especially useful when the training data is limited. Using only the official 6k training scenes data, our best model achieves 0.80 hearing-aid speech perception index (HASPI) and 0.41 hearing-aid speech quality index (HASQI) scores on the synthetic evaluation set. However, our model generalized poorly on the semi-real evaluation set. This highlights the fact that our community should focus more on real-world evaluation and less on fully synthetic datasets.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Shinji Watanabe Title: A Study on the Integration of Pipeline and E2E SLU Systems for Spoken Semantic Parsing Toward Stop Quality Challenge Publication year: 2023 Coauthors: Siddhant Arora, Hayato Futami, Shih-Lun Wu, Jessica Huynh, Yifan Peng, Yosuke Kashiwagi, E. Tsunoo, Brian Yan, Shinji Watanabe Abstract: Recently there have been efforts to introduce new benchmark tasks for spoken language understanding (SLU), like semantic parsing. In this paper, we describe our proposed spoken semantic parsing system for the quality track (Track 1) in Spoken Language Understanding Grand Challenge which is part of ICASSP Signal Processing Grand Challenge 2023. We experiment with both end-to-end and pipeline systems for this task. Strong automatic speech recognition (ASR) models like Whisper and pretrained Language models (LM) like BART are utilized inside our SLU framework to boost performance. We also investigate the output level combination of various models to get an exact match accuracy of 80.8, which won the 1st place at the challenge.|Author: Shinji Watanabe Title: Multi-Channel Speaker Extraction with Adversarial Training: The Wavlab Submission to The Clarity ICASSP 2023 Grand Challenge Publication year: 2023 Coauthors: Samuele Cornell, Zhongqiu Wang, Yoshiki Masuyama, Shinji Watanabe, Manuel Pariente, Nobutaka Ono, S. Squartini Abstract: In this work we detail our submission to the Clarity ICASSP 2023 grand challenge, in which participants have to develop a strong target speech enhancement system for hearing-aid (HA) devices in noisy-reverberant environments. Our system builds on our previous submission at the Second Clarity Enhancement Challenge (CEC2): iNeuBe-X, which consists in an iterative neural/conventional beamforming enhancement pipeline, guided by an enrollment utterance from the target speaker. This model, which won by a large margin the CEC2, is an extension of the state-of-the-art TF-GridNet model for multi-channel, streamable target-speaker speech enhancement. Here, this approach is extended and further improved by leveraging generative adversarial training, which we show proves especially useful when the training data is limited. Using only the official 6k training scenes data, our best model achieves 0.80 hearing-aid speech perception index (HASPI) and 0.41 hearing-aid speech quality index (HASQI) scores on the synthetic evaluation set. However, our model generalized poorly on the semi-real evaluation set. This highlights the fact that our community should focus more on real-world evaluation and less on fully synthetic datasets.",
        "retrieved_relevant_part": true,
        "model_answer": "The paper describes their submission to the Clarity ICASSP 2023 grand challenge, which involves developing a strong target speech enhancement system for hearing-aid devices in noisy-reverberant environments",
        "actual_answer": "In this work we detail our submission to the Clarity ICASSP 2023 grand challenge, in which participants have to develop a strong target speech enhancement system for hearing-aid (HA) devices in noisy-reverberant environments. Our system builds on our previous submission at the Second Clarity Enhancement Challenge (CEC2): iNeuBe-X, which consists in an iterative neural/conventional beamforming enhancement pipeline, guided by an enrollment utterance from the target speaker. This model, which won by a large margin the CEC2, is an extension of the state-of-the-art TF-GridNet model for multi-channel, streamable target-speaker speech enhancement. Here, this approach is extended and further improved by leveraging generative adversarial training, which we show proves especially useful when the training data is limited. Using only the official 6k training scenes data, our best model achieves 0.80 hearing-aid speech perception index (HASPI) and 0.41 hearing-aid speech quality index (HASQI) scores on the synthetic evaluation set. However, our model generalized poorly on the semi-real evaluation set. This highlights the fact that our community should focus more on real-world evaluation and less on fully synthetic datasets.",
        "exact_match": false,
        "f1_max_min_avg": [
            0.2,
            0.2,
            0.2
        ],
        "recall_max_min_avg": [
            0.11,
            0.11,
            0.11
        ]
    },
    {
        "question": "Who is the author of the LTI paper Completing Visual Objects via Bridging Generation and Segmentation published in 2023?",
        "context": "Author: Rita Singh Title: Completing Visual Objects via Bridging Generation and Segmentation Publication year: 2023 Coauthors: Xiang Li, Yinpeng Chen, Chung-Ching Lin, Rita Singh, Bhiksha Raj, Zicheng Liu Abstract: This paper presents a novel approach to object completion, with the primary goal of reconstructing a complete object from its partially visible components. Our method, named MaskComp, delineates the completion process through iterative stages of generation and segmentation. In each iteration, the object mask is provided as an additional condition to boost image generation, and, in return, the generated images can lead to a more accurate mask by fusing the segmentation of images. We demonstrate that the combination of one generation and one segmentation stage effectively functions as a mask denoiser. Through alternation between the generation and segmentation stages, the partial object mask is progressively refined, providing precise shape guidance and yielding superior object completion results. Our experiments demonstrate the superiority of MaskComp over existing approaches, e.g., ControlNet and Stable Diffusion, establishing it as an effective solution for object completion.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Alexander I. Rudnicky Title: Tartan: an LLM Driven SocialBot Publication year: 2023 Coauthors: Liangze Li, Ziyuan Liu, Li-Wei Chen, Ta-Chung Chi, Alexander I. Rudnicky Abstract: We describe our work to date on using large language models (LLMs) in our Alexa Prize Socialbot. Our work has laid the groundwork for looking more closely at using LLMs in a conversational system. We also analyze common patterns in conversations our bot has had with users|Author: Maarten Sap Title: Don't Take This Out of Context!: On the Need for Contextual Models and Evaluations for Stylistic Rewriting Publication year: 2023 Coauthors: Akhila Yerukola, Xuhui Zhou, Elizabeth Clark, Maarten Sap ",
        "retrieved_relevant_part": false,
        "model_answer": "The author of the LTI paper Completing Visual Objects via Bridging Generation and Segmentation is Maarten Sap",
        "actual_answer": "Rita Singh",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Which LTI faculty published the paper An In-depth Look at Gemini's Language Abilities in 2023?",
        "context": "Author: Graham Neubig Title: An In-depth Look at Gemini's Language Abilities Publication year: 2023 Coauthors: Syeda Nahida Akter, Zichun Yu, Aashiq Muhamed, Tianyue Ou, Alex Bauerle, \u00c1ngel Alexander Cabrera, Krish Dholakia, Chenyan Xiong, Graham Neubig Abstract: The recently released Google Gemini class of models are the first to comprehensively report results that rival the OpenAI GPT series across a wide variety of tasks. In this paper, we do an in-depth exploration of Gemini's language abilities, making two contributions. First, we provide a third-party, objective comparison of the abilities of the OpenAI GPT and Google Gemini models with reproducible code and fully transparent results. Second, we take a closer look at the results, identifying areas where one of the two model classes excels. We perform this analysis over 10 datasets testing a variety of language abilities, including reasoning, answering knowledge-based questions, solving math problems, translating between languages, generating code, and acting as instruction-following agents. From this analysis, we find that Gemini Pro achieves accuracy that is close but slightly inferior to the corresponding GPT 3.5 Turbo on all tasks that we benchmarked. We further provide explanations for some of this under-performance, including failures in mathematical reasoning with many digits, sensitivity to multiple-choice answer ordering, aggressive content filtering, and others. We also identify areas where Gemini demonstrates comparably high performance, including generation into non-English languages, and handling longer and more complex reasoning chains. Code and data for reproduction can be found at https://github.com/neulab/gemini-benchmark",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Alexander I. Rudnicky Title: Tartan: an LLM Driven SocialBot Publication year: 2023 Coauthors: Liangze Li, Ziyuan Liu, Li-Wei Chen, Ta-Chung Chi, Alexander I. Rudnicky Abstract: We describe our work to date on using large language models (LLMs) in our Alexa Prize Socialbot. Our work has laid the groundwork for looking more closely at using LLMs in a conversational system. We also analyze common patterns in conversations our bot has had with users|Author: David R. Mortensen Title: Multilingual TTS Accent Impressions for Accented ASR Publication year: 2023 Coauthors: G. Karakasidis, Nathaniel R. Robinson, Yaroslav Getman, Atieno Ogayo, Ragheb Al-Ghezi, Ananya Ayasi, Shinji Watanabe, David R. Mortensen, M. Kurimo ",
        "retrieved_relevant_part": false,
        "model_answer": "The paper was authored by Shinji Watanabe, Yui Sudo, Muhammad Shakeel, Yifan Peng, and Alexander I. Rudnicky",
        "actual_answer": "Graham Neubig",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "What is the abstract of the paper ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages published by LTI faculty Graham Neubig in 2023?",
        "context": "Author: Graham Neubig Title: ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages Publication year: 2023 Coauthors: Nathaniel R. Robinson, Perez Ogayo, David R. Mortensen, Graham Neubig Abstract: Large language models (LLMs) implicitly learn to perform a range of language tasks, including machine translation (MT). Previous studies explore aspects of LLMs\u2019 MT capabilities. However, there exist a wide variety of languages for which recent LLM MT performance has never before been evaluated. Without published experimental evidence on the matter, it is difficult for speakers of the world\u2019s diverse languages to know how and whether they can use LLMs for their languages. We present the first experimental evidence for an expansive set of 204 languages, along with MT cost analysis, using the FLORES-200 benchmark. Trends reveal that GPT models approach or exceed traditional MT model performance for some high-resource languages (HRLs) but consistently lag for low-resource languages (LRLs), under-performing traditional MT for 84.1% of languages we covered. Our analysis reveals that a language\u2019s resource level is the most important feature in determining ChatGPT\u2019s relative ability to translate it, and suggests that ChatGPT is especially disadvantaged for LRLs and African languages.",
        "retrieved_doc": "Author: Graham Neubig Title: ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages Publication year: 2023 Coauthors: Nathaniel R. Robinson, Perez Ogayo, David R. Mortensen, Graham Neubig Abstract: Large language models (LLMs) implicitly learn to perform a range of language tasks, including machine translation (MT). Previous studies explore aspects of LLMs\u2019 MT capabilities. However, there exist a wide variety of languages for which recent LLM MT performance has never before been evaluated. Without published experimental evidence on the matter, it is difficult for speakers of the world\u2019s diverse languages to know how and whether they can use LLMs for their languages. We present the first experimental evidence for an expansive set of 204 languages, along with MT cost analysis, using the FLORES-200 benchmark. Trends reveal that GPT models approach or exceed traditional MT model performance for some high-resource languages (HRLs) but consistently lag for low-resource languages (LRLs), under-performing traditional MT for 84.1% of languages we covered. Our analysis reveals that a language\u2019s resource level is the most important feature in determining ChatGPT\u2019s relative ability to translate it, and suggests that ChatGPT is especially disadvantaged for LRLs and African languages.|Author: David R. Mortensen Title: ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages Publication year: 2023 Coauthors: Nathaniel R. Robinson, Perez Ogayo, David R. Mortensen, Graham Neubig Abstract: Large language models (LLMs) implicitly learn to perform a range of language tasks, including machine translation (MT). Previous studies explore aspects of LLMs\u2019 MT capabilities. However, there exist a wide variety of languages for which recent LLM MT performance has never before been evaluated. Without published experimental evidence on the matter, it is difficult for speakers of the world\u2019s diverse languages to know how and whether they can use LLMs for their languages. We present the first experimental evidence for an expansive set of 204 languages, along with MT cost analysis, using the FLORES-200 benchmark. Trends reveal that GPT models approach or exceed traditional MT model performance for some high-resource languages (HRLs) but consistently lag for low-resource languages (LRLs), under-performing traditional MT for 84.1% of languages we covered. Our analysis reveals that a language\u2019s resource level is the most important feature in determining ChatGPT\u2019s relative ability to translate it, and suggests that ChatGPT is especially disadvantaged for LRLs and African languages.|Author: Shinji Watanabe Title: Findings of the 2023 ML-Superb Challenge: Pre-Training And Evaluation Over More Languages And Beyond Publication year: 2023 Coauthors: Jiatong Shi, William Chen, Dan Berrebbi, Hsiu-Hsuan Wang, Wei-Ping Huang, En-Pei Hu, Ho-Lam Chuang, Xuankai Chang, Yuxun Tang, Shang-Wen Li, Abdelrahman Mohamed, Hung-yi Lee, Shinji Watanabe Abstract: The 2023 Multilingual Speech Universal Performance Benchmark (ML-SUPERB) Challenge expands upon the acclaimed SUPERB framework, emphasizing self-supervised models in multilingual speech recognition and language identification. The challenge comprises a research track focused on applying ML-SUPERB to specific multilingual subjects, a Challenge Track for model submissions, and a New Language Track where language resource researchers can contribute and evaluate their low-resource language data in the context of the latest progress in multilingual speech recognition. The challenge garnered 12 model submissions and 54 language corpora, resulting in a comprehensive benchmark encompassing 154 languages. The findings indicate that merely scaling models is not the definitive solution for multilingual speech tasks, and a variety of speech/voice types present significant challenges in multilingual speech processing.",
        "retrieved_relevant_part": true,
        "model_answer": "The paper presents the first experimental evidence for an expansive set of 204 languages and their relative ability to be translated by ChatGPT",
        "actual_answer": "Large language models (LLMs) implicitly learn to perform a range of language tasks, including machine translation (MT). Previous studies explore aspects of LLMs\u2019 MT capabilities. However, there exist a wide variety of languages for which recent LLM MT performance has never before been evaluated. Without published experimental evidence on the matter, it is difficult for speakers of the world\u2019s diverse languages to know how and whether they can use LLMs for their languages. We present the first experimental evidence for an expansive set of 204 languages, along with MT cost analysis, using the FLORES-200 benchmark. Trends reveal that GPT models approach or exceed traditional MT model performance for some high-resource languages (HRLs) but consistently lag for low-resource languages (LRLs), under-performing traditional MT for 84.1% of languages we covered. Our analysis reveals that a language\u2019s resource level is the most important feature in determining ChatGPT\u2019s relative ability to translate it, and suggests that ChatGPT is especially disadvantaged for LRLs and African languages.",
        "exact_match": false,
        "f1_max_min_avg": [
            0.18,
            0.18,
            0.18
        ],
        "recall_max_min_avg": [
            0.11,
            0.11,
            0.11
        ]
    },
    {
        "question": "Who is the author of the LTI paper Energy and Carbon Considerations of Fine-Tuning BERT published in 2023?",
        "context": "Author: Emma Strubell Title: Energy and Carbon Considerations of Fine-Tuning BERT Publication year: 2023 Coauthors: Xiaorong Wang, Clara Na, Emma Strubell, Sorelle A. Friedler, Sasha Luccioni Abstract: Despite the popularity of the `pre-train then fine-tune' paradigm in the NLP community, existing work quantifying energy costs and associated carbon emissions has largely focused on language model pre-training. Although a single pre-training run draws substantially more energy than fine-tuning, fine-tuning is performed more frequently by many more individual actors, and thus must be accounted for when considering the energy and carbon footprint of NLP. In order to better characterize the role of fine-tuning in the landscape of energy and carbon emissions in NLP, we perform a careful empirical study of the computational costs of fine-tuning across tasks, datasets, hardware infrastructure and measurement modalities. Our experimental results allow us to place fine-tuning energy and carbon costs into perspective with respect to pre-training and inference, and outline recommendations to NLP researchers and practitioners who wish to improve their fine-tuning energy efficiency.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Jamie Callan Title: COILcr: Efficient Semantic Matching in Contextualized Exact Match Retrieval Publication year: 2023 Coauthors: Zhen Fan, Luyu Gao, Rohan Jha, Jamie Callan |Author: Emma Strubell Title: Energy and Carbon Considerations of Fine-Tuning BERT Publication year: 2023 Coauthors: Xiaorong Wang, Clara Na, Emma Strubell, Sorelle A. Friedler, Sasha Luccioni Abstract: Despite the popularity of the `pre-train then fine-tune' paradigm in the NLP community, existing work quantifying energy costs and associated carbon emissions has largely focused on language model pre-training. Although a single pre-training run draws substantially more energy than fine-tuning, fine-tuning is performed more frequently by many more individual actors, and thus must be accounted for when considering the energy and carbon footprint of NLP. In order to better characterize the role of fine-tuning in the landscape of energy and carbon emissions in NLP, we perform a careful empirical study of the computational costs of fine-tuning across tasks, datasets, hardware infrastructure and measurement modalities. Our experimental results allow us to place fine-tuning energy and carbon costs into perspective with respect to pre-training and inference, and outline recommendations to NLP researchers and practitioners who wish to improve their fine-tuning energy efficiency.",
        "retrieved_relevant_part": true,
        "model_answer": "Emma Strubell",
        "actual_answer": "Emma Strubell",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Who are the coauthors of the paper Cross-Modal Fine-Tuning: Align then Refine published by LTI faculty Graham Neubig in 2023?",
        "context": "Author: Graham Neubig Title: Cross-Modal Fine-Tuning: Align then Refine Publication year: 2023 Coauthors: Junhong Shen, Liam Li, L. Dery, Corey Staten, M. Khodak, Graham Neubig, Ameet Talwalkar Abstract: Fine-tuning large-scale pretrained models has led to tremendous progress in well-studied modalities such as vision and NLP. However, similar gains have not been observed in many other modalities due to a lack of relevant pretrained models. In this work, we propose ORCA, a general cross-modal fine-tuning framework that extends the applicability of a single large-scale pretrained model to diverse modalities. ORCA adapts to a target task via an align-then-refine workflow: given the target input, ORCA first learns an embedding network that aligns the embedded feature distribution with the pretraining modality. The pretrained model is then fine-tuned on the embedded data to exploit the knowledge shared across modalities. Through extensive experiments, we show that ORCA obtains state-of-the-art results on 3 benchmarks containing over 60 datasets from 12 modalities, outperforming a wide range of hand-designed, AutoML, general-purpose, and task-specific methods. We highlight the importance of data alignment via a series of ablation studies and demonstrate ORCA's utility in data-limited regimes.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Maarten Sap Title: Don't Take This Out of Context!: On the Need for Contextual Models and Evaluations for Stylistic Rewriting Publication year: 2023 Coauthors: Akhila Yerukola, Xuhui Zhou, Elizabeth Clark, Maarten Sap |Author: Jamie Callan Title: COILcr: Efficient Semantic Matching in Contextualized Exact Match Retrieval Publication year: 2023 Coauthors: Zhen Fan, Luyu Gao, Rohan Jha, Jamie Callan ",
        "retrieved_relevant_part": false,
        "model_answer": "The coauthors of the paper are Maarten Sap, Akhila Yerukola, Xuhui Zhou, and Elizabeth Clark",
        "actual_answer": "Junhong Shen, Liam Li, L. Dery, Corey Staten, M. Khodak, Graham Neubig, Ameet Talwalkar",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Which LTI faculty published the paper KEEC: Embed to Control on An Equivariant Geometry in 2023?",
        "context": "Author: Yiming Yang Title: KEEC: Embed to Control on An Equivariant Geometry Publication year: 2023 Coauthors: Xiaoyuan Cheng, Yiming Yang, Wei Jiang, Yukun Hu Abstract: This paper investigates how representation learning can enable optimal control in unknown and complex dynamics, such as chaotic and non-linear systems, without relying on prior domain knowledge of the dynamics. The core idea is to establish an equivariant geometry that is diffeomorphic to the manifold defined by a dynamical system and to perform optimal control within this corresponding geometry, which is a non-trivial task. To address this challenge, Koopman Embed to Equivariant Control (KEEC) is proposed for model learning and control. Inspired by Lie theory, KEEC begins by learning a non-linear dynamical system defined on a manifold and embedding trajectories into a Lie group. Subsequently, KEEC formulates an equivariant value function equation in reinforcement learning on the equivariant geometry, ensuring an invariant effect as the value function on the original manifold. By deriving analytical-form optimal actions on the equivariant value function, KEEC theoretically achieves quadratic convergence for the optimal equivariant value function by leveraging the differential information on the equivariant geometry. The effectiveness of KEEC is demonstrated in challenging dynamical systems, including chaotic ones like Lorenz-63. Notably, our results show that isometric functions, which maintain the compactness and completeness of geometry while preserving metric and differential information, consistently outperform loss functions lacking these characteristics.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Yonatan Bisk Assistant Professor Email: ybisk@cs.cmu.edu Office: Gates & Hillman Centers Research Areas: Grounding, RoboNLP, Vision and Language, Embodiment, Unsupervised Learning|Author: Alexander Hauptmann Title: Hyperbolic vs Euclidean Embeddings in Few-Shot Learning: Two Sides of the Same Coin Publication year: 2023 Coauthors: Gabriel Moreira, Manuel Marques, J. Costeira, Alexander Hauptmann Abstract: Recent research in representation learning has shown that hierarchical data lends itself to low-dimensional and highly informative representations in hyperbolic space. However, even if hyperbolic embeddings have gathered attention in image recognition, their optimization is prone to numerical hurdles. Further, it remains unclear which applications stand to benefit the most from the implicit bias imposed by hyperbolicity, when compared to traditional Euclidean features. In this paper, we focus on prototypical hyperbolic neural networks. In particular, the tendency of hyperbolic embeddings to converge to the boundary of the Poincar\\'e ball in high dimensions and the effect this has on few-shot classification. We show that the best few-shot results are attained for hyperbolic embeddings at a common hyperbolic radius. In contrast to prior benchmark results, we demonstrate that better performance can be achieved by a fixed-radius encoder equipped with the Euclidean metric, regardless of the embedding dimension.",
        "retrieved_relevant_part": false,
        "model_answer": "Shinji Watanabe",
        "actual_answer": "Yiming Yang",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Who are the coauthors of the paper Reducing Barriers to Self-Supervised Learning: HuBERT Pre-training with Academic Compute published by LTI faculty Shinji Watanabe in 2023?",
        "context": "Author: Shinji Watanabe Title: Reducing Barriers to Self-Supervised Learning: HuBERT Pre-training with Academic Compute Publication year: 2023 Coauthors: William Chen, Xuankai Chang, Yifan Peng, Zhaoheng Ni, Soumi Maiti, Shinji Watanabe Abstract: Self-supervised learning (SSL) has led to great strides in speech processing. However, the resources needed to train these models has become prohibitively large as they continue to scale. Currently, only a few groups with substantial resources are capable of creating SSL models, which harms reproducibility. In this work, we optimize HuBERT SSL to fit in academic constraints. We reproduce HuBERT independently from the original implementation, with no performance loss. Our code and training optimizations make SSL feasible with only 8 GPUs, instead of the 32 used in the original work. We also explore a semi-supervised route, using an ASR model to skip the first pre-training iteration. Within one iteration of pre-training, our models improve over HuBERT on several tasks. Furthermore, our HuBERT Large variant requires only 8 GPUs, achieving similar performance to the original trained on 128. As our contribution to the community, all models, configurations, and code are made open-source in ESPnet.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Yonatan Bisk Assistant Professor Email: ybisk@cs.cmu.edu Office: Gates & Hillman Centers Research Areas: Grounding, RoboNLP, Vision and Language, Embodiment, Unsupervised Learning|Author: Shinji Watanabe Title: Challenges of Corporate Alliance CLOMA toward Plastic Litter Publication year: 2023 Coauthors: Shinji Watanabe ",
        "retrieved_relevant_part": false,
        "model_answer": "The coauthors of the paper are Yui Sudo, Muhammad Shakeel, Yifan Peng, and Yonatan Bisk",
        "actual_answer": "William Chen, Xuankai Chang, Yifan Peng, Zhaoheng Ni, Soumi Maiti, Shinji Watanabe",
        "exact_match": false,
        "f1_max_min_avg": [
            0.15,
            0.15,
            0.15
        ],
        "recall_max_min_avg": [
            0.17,
            0.17,
            0.17
        ]
    },
    {
        "question": "Who are the coauthors of the paper Reasoning about the Unseen for Efficient Outdoor Object Navigation published by LTI faculty Yonatan Bisk in 2023?",
        "context": "Author: Yonatan Bisk Title: Reasoning about the Unseen for Efficient Outdoor Object Navigation Publication year: 2023 Coauthors: Quanting Xie, Tianyi Zhang, Kedi Xu, M. Johnson-Roberson, Yonatan Bisk Abstract: Robots should exist anywhere humans do: indoors, outdoors, and even unmapped environments. In contrast, the focus of recent advancements in Object Goal Navigation(OGN) has targeted navigating in indoor environments by leveraging spatial and semantic cues that do not generalize outdoors. While these contributions provide valuable insights into indoor scenarios, the broader spectrum of real-world robotic applications often extends to outdoor settings. As we transition to the vast and complex terrains of outdoor environments, new challenges emerge. Unlike the structured layouts found indoors, outdoor environments lack clear spatial delineations and are riddled with inherent semantic ambiguities. Despite this, humans navigate with ease because we can reason about the unseen. We introduce a new task OUTDOOR, a new mechanism for Large Language Models (LLMs) to accurately hallucinate possible futures, and a new computationally aware success metric for pushing research forward in this more complex domain. Additionally, we show impressive results on both a simulated drone and physical quadruped in outdoor environments. Our agent has no premapping and our formalism outperforms naive LLM-based approaches",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Louis-Philippe Morency Title: Reconstructing the neurodynamics of face perception during real world vision in humans using intracranial EEG recordings Publication year: 2023 Coauthors: Arish Alreja, Michael J. Ward, J. A. Colan, Qianli Ma, R. M. Richardson, Louis-Philippe Morency, A. Ghuman |Author: Maarten Sap Title: Don't Take This Out of Context!: On the Need for Contextual Models and Evaluations for Stylistic Rewriting Publication year: 2023 Coauthors: Akhila Yerukola, Xuhui Zhou, Elizabeth Clark, Maarten Sap ",
        "retrieved_relevant_part": false,
        "model_answer": "The coauthors of the paper are Yui Sudo, Muhammad Shakeel, Yifan Peng, and Shinji Watanabe",
        "actual_answer": "Quanting Xie, Tianyi Zhang, Kedi Xu, M. Johnson-Roberson, Yonatan Bisk",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Who are the coauthors of the paper Leveraging body pose estimation for gesture recognition in human-robot interaction using synthetic data published by LTI faculty Alexander Hauptmann in 2023?",
        "context": "Author: Alexander Hauptmann Title: Leveraging body pose estimation for gesture recognition in human-robot interaction using synthetic data Publication year: 2023 Coauthors: Xiaoyu Zhu, Celso de Melo, Alexander Hauptmann Abstract: Effectively recognizing human gestures from variant viewpoints plays a fundamental role in the successful collaboration between humans and robots. Deep learning approaches have achieved promising performance in gesture recognition. However, they are usually data-hungry and require large-scale labeled data, which are not usually accessible in a practical setting. Synthetic data, on the other hand, can be easily obtained from simulators with fine-grained annotations and variant modalities. Existing state-of-the-art approaches have shown promising results using synthetic data, but there is still a large performance gap between the models trained on synthetic data and real data. To learn domain-invariant feature representations, we propose a novel approach which jointly takes RGB videos and 3D meshes as inputs to perform robust action recognition. We empirically validate our model on the RoCoG-v2 dataset, which consists of a variety of real and synthetic videos of gestures from the ground and air perspectives. We show that our model trained on synthetic data can outperform state-of-the-art models under the same training setting and models trained on real data.",
        "retrieved_doc": "Author: Louis-Philippe Morency Title: Reconstructing the neurodynamics of face perception during real world vision in humans using intracranial EEG recordings Publication year: 2023 Coauthors: Arish Alreja, Michael J. Ward, J. A. Colan, Qianli Ma, R. M. Richardson, Louis-Philippe Morency, A. Ghuman |Author: Shinji Watanabe Title: Speaker-Independent Acoustic-to-Articulatory Speech Inversion Publication year: 2023 Coauthors: Peter Wu, Li-Wei Chen, Cheol Jun Cho, Shinji Watanabe, L. Goldstein, A. Black, G. Anumanchipalli Abstract: To build speech processing methods that can handle speech as naturally as humans, researchers have explored multiple ways of building an invertible mapping from speech to an interpretable space. The articulatory space is a promising inversion target, since this space captures the mechanics of speech production. To this end, we build an acoustic-to-articulatory inversion (AAI) model that leverages autoregression, adversarial training, and self supervision to generalize to unseen speakers. Our approach obtains 0.784 correlation on an electromagnetic articulography (EMA) dataset, improving the state-of-the-art by 12.5%. Additionally, we show the interpretability of these representations through directly com-paring the behavior of estimated representations with speech production behavior. Finally, we propose a resynthesis-based AAI evaluation metric that does not rely on articulatory labels, demonstrating its efficacy with an 18-speaker dataset.|Author: Alexander Hauptmann Title: Leveraging body pose estimation for gesture recognition in human-robot interaction using synthetic data Publication year: 2023 Coauthors: Xiaoyu Zhu, Celso de Melo, Alexander Hauptmann Abstract: Effectively recognizing human gestures from variant viewpoints plays a fundamental role in the successful collaboration between humans and robots. Deep learning approaches have achieved promising performance in gesture recognition. However, they are usually data-hungry and require large-scale labeled data, which are not usually accessible in a practical setting. Synthetic data, on the other hand, can be easily obtained from simulators with fine-grained annotations and variant modalities. Existing state-of-the-art approaches have shown promising results using synthetic data, but there is still a large performance gap between the models trained on synthetic data and real data. To learn domain-invariant feature representations, we propose a novel approach which jointly takes RGB videos and 3D meshes as inputs to perform robust action recognition. We empirically validate our model on the RoCoG-v2 dataset, which consists of a variety of real and synthetic videos of gestures from the ground and air perspectives. We show that our model trained on synthetic data can outperform state-of-the-art models under the same training setting and models trained on real data.",
        "retrieved_relevant_part": true,
        "model_answer": "Xiaoyu Zhu, Celso de Melo, and Alexander Hauptmann",
        "actual_answer": "Xiaoyu Zhu, Celso de Melo, Alexander Hauptmann",
        "exact_match": false,
        "f1_max_min_avg": [
            0.93,
            0.93,
            0.93
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Which LTI faculty published the paper The DARPA Wikidata Overlay: Wikidata as an ontology for natural language processing in 2023?",
        "context": "Author: A. Gershman Title: The DARPA Wikidata Overlay: Wikidata as an ontology for natural language processing Publication year: 2023 Coauthors: Elizabeth Spaulding, Kathryn Conger, A. Gershman, Rosario Uceda-Sosa, S. Brown, James Pustejovsky, Peter Anick, Martha Palmer Abstract: With 102,530,067 items currently in its crowd-sourced knowledge base, Wikidata provides NLP practitioners a unique and powerful resource for inference and reasoning over real-world entities. However, because Wikidata is very entity focused, events and actions are often labeled with eventive nouns (e.g., the process of diagnosing a person\u2019s illness is labeled \u201cdiagnosis\u201d), and the typical participants in an event are not described or linked to that event concept (e.g., the medical professional or patient). Motivated by a need for an adaptable, comprehensive, domain-flexible ontology for information extraction, including identifying the roles entities are playing in an event, we present a curated subset of Wikidata in which events have been enriched with PropBank roles. To enable richer narrative understanding between events from Wikidata concepts, we have also provided a comprehensive mapping from temporal Qnodes and Pnodes to the Allen Interval Temporal Logic relations.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Yonatan Bisk Assistant Professor Email: ybisk@cs.cmu.edu Office: Gates & Hillman Centers Research Areas: Grounding, RoboNLP, Vision and Language, Embodiment, Unsupervised Learning|Author: A. Gershman Title: The DARPA Wikidata Overlay: Wikidata as an ontology for natural language processing Publication year: 2023 Coauthors: Elizabeth Spaulding, Kathryn Conger, A. Gershman, Rosario Uceda-Sosa, S. Brown, James Pustejovsky, Peter Anick, Martha Palmer Abstract: With 102,530,067 items currently in its crowd-sourced knowledge base, Wikidata provides NLP practitioners a unique and powerful resource for inference and reasoning over real-world entities. However, because Wikidata is very entity focused, events and actions are often labeled with eventive nouns (e.g., the process of diagnosing a person\u2019s illness is labeled \u201cdiagnosis\u201d), and the typical participants in an event are not described or linked to that event concept (e.g., the medical professional or patient). Motivated by a need for an adaptable, comprehensive, domain-flexible ontology for information extraction, including identifying the roles entities are playing in an event, we present a curated subset of Wikidata in which events have been enriched with PropBank roles. To enable richer narrative understanding between events from Wikidata concepts, we have also provided a comprehensive mapping from temporal Qnodes and Pnodes to the Allen Interval Temporal Logic relations.",
        "retrieved_relevant_part": true,
        "model_answer": "A. Gershman",
        "actual_answer": "A. Gershman",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Who is the author of the LTI paper Divergences between Language Models and Human Brains published in 2023?",
        "context": "Author: Graham Neubig Title: Divergences between Language Models and Human Brains Publication year: 2023 Coauthors: Yuchen Zhou, Emmy Liu, Graham Neubig, Leila Wehbe Abstract: Do machines and humans process language in similar ways? Recent research has hinted in the affirmative, finding that brain signals can be effectively predicted using the internal representations of language models (LMs). Although such results are thought to reflect shared computational principles between LMs and human brains, there are also clear differences in how LMs and humans represent and use language. In this work, we systematically explore the divergences between human and machine language processing by examining the differences between LM representations and human brain responses to language as measured by Magnetoencephalography (MEG) across two datasets in which subjects read and listened to narrative stories. Using a data-driven approach, we identify two domains that are not captured well by LMs: social/emotional intelligence and physical commonsense. We then validate these domains with human behavioral experiments and show that fine-tuning LMs on these domains can improve their alignment with human brain responses.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Louis-Philippe Morency Title: Reconstructing the neurodynamics of face perception during real world vision in humans using intracranial EEG recordings Publication year: 2023 Coauthors: Arish Alreja, Michael J. Ward, J. A. Colan, Qianli Ma, R. M. Richardson, Louis-Philippe Morency, A. Ghuman |Author: Graham Neubig Title: Divergences between Language Models and Human Brains Publication year: 2023 Coauthors: Yuchen Zhou, Emmy Liu, Graham Neubig, Leila Wehbe Abstract: Do machines and humans process language in similar ways? Recent research has hinted in the affirmative, finding that brain signals can be effectively predicted using the internal representations of language models (LMs). Although such results are thought to reflect shared computational principles between LMs and human brains, there are also clear differences in how LMs and humans represent and use language. In this work, we systematically explore the divergences between human and machine language processing by examining the differences between LM representations and human brain responses to language as measured by Magnetoencephalography (MEG) across two datasets in which subjects read and listened to narrative stories. Using a data-driven approach, we identify two domains that are not captured well by LMs: social/emotional intelligence and physical commonsense. We then validate these domains with human behavioral experiments and show that fine-tuning LMs on these domains can improve their alignment with human brain responses.",
        "retrieved_relevant_part": true,
        "model_answer": "The author of the LTI paper Divergences between Language Models and Human Brains published in 2023 is Graham Neubig",
        "actual_answer": "Graham Neubig",
        "exact_match": false,
        "f1_max_min_avg": [
            0.19,
            0.19,
            0.19
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "What is the abstract of the paper Active Retrieval Augmented Generation published by LTI faculty Jamie Callan in 2023?",
        "context": "Author: Jamie Callan Title: Active Retrieval Augmented Generation Publication year: 2023 Coauthors: Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig Abstract: Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.",
        "retrieved_doc": "Author: Jamie Callan Title: COILcr: Efficient Semantic Matching in Contextualized Exact Match Retrieval Publication year: 2023 Coauthors: Zhen Fan, Luyu Gao, Rohan Jha, Jamie Callan |Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Maarten Sap Title: Don't Take This Out of Context!: On the Need for Contextual Models and Evaluations for Stylistic Rewriting Publication year: 2023 Coauthors: Akhila Yerukola, Xuhui Zhou, Elizabeth Clark, Maarten Sap ",
        "retrieved_relevant_part": false,
        "model_answer": "The paper discusses the use of context in semantic matching and the need for contextual models and evaluations",
        "actual_answer": "Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.",
        "exact_match": false,
        "f1_max_min_avg": [
            0.05,
            0.05,
            0.05
        ],
        "recall_max_min_avg": [
            0.03,
            0.03,
            0.03
        ]
    },
    {
        "question": "Who is the author of the LTI paper RedCoast: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU/TPUs published in 2023?",
        "context": "Author: Eric P. Xing Title: RedCoast: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU/TPUs Publication year: 2023 Coauthors: Bowen Tan, Yun Zhu, Lijuan Liu, Hongyi Wang, Yonghao Zhuang, Jindong Chen, Eric P. Xing, Zhiting Hu Abstract: The recent progress of AI can be largely attributed to large language models (LLMs). However, their escalating memory requirements introduce challenges for machine learning (ML) researchers and engineers. Addressing this requires developers to partition a large model to distribute it across multiple GPUs or TPUs. This necessitates considerable coding and intricate configuration efforts with existing model parallel tools, such as Megatron-LM, DeepSpeed, and Alpa. These tools require users' expertise in machine learning systems (MLSys), creating a bottleneck in LLM development, particularly for developers without MLSys background. In this work, we present RedCoast(Redco), a lightweight and user-friendly tool crafted to automate distributed training and inference for LLMs, as well as to simplify ML pipeline development. The design of Redco emphasizes two key aspects. Firstly, to automate model parallism, our study identifies two straightforward rules to generate tensor parallel strategies for any given LLM. Integrating these rules into Redco facilitates effortless distributed LLM training and inference, eliminating the need of additional coding or complex configurations. We demonstrate the effectiveness by applying Redco on a set of LLM architectures, such as GPT-J, LLaMA, T5, and OPT, up to the size of 66B. Secondly, we propose a mechanism that allows for the customization of diverse ML pipelines through the definition of merely three functions, avoiding redundant and formulaic code like multi-host related processing. This mechanism proves adaptable across a spectrum of ML algorithms, from foundational language modeling to complex algorithms like meta-learning and reinforcement learning. Consequently, Redco implementations exhibit much fewer code lines compared to their official counterparts.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Alexander I. Rudnicky Title: Tartan: an LLM Driven SocialBot Publication year: 2023 Coauthors: Liangze Li, Ziyuan Liu, Li-Wei Chen, Ta-Chung Chi, Alexander I. Rudnicky Abstract: We describe our work to date on using large language models (LLMs) in our Alexa Prize Socialbot. Our work has laid the groundwork for looking more closely at using LLMs in a conversational system. We also analyze common patterns in conversations our bot has had with users|Author: Shinji Watanabe Title: DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models Publication year: 2023 Coauthors: Yifan Peng, Yui Sudo, Muhammad Shakeel, Shinji Watanabe Abstract: Self-supervised learning (SSL) has achieved notable success in many speech processing tasks, but the large model size and heavy computational cost hinder the deployment. Knowledge distillation trains a small student model to mimic the behavior of a large teacher model. However, the student architecture usually needs to be manually designed and will remain fixed during training, which requires prior knowledge and can lead to suboptimal performance. Inspired by recent success of task-specific structured pruning, we propose DPHuBERT, a novel task-agnostic compression method for speech SSL based on joint distillation and pruning. Experiments on SUPERB show that DPHuBERT outperforms pure distillation methods in almost all tasks. Moreover, DPHuBERT requires little training time and performs well with limited training data, making it suitable for resource-constrained applications. Our method can also be applied to various speech SSL models. Our code and models will be publicly available.",
        "retrieved_relevant_part": false,
        "model_answer": "The author of the LTI paper RedCoast is Shinji Watanabe",
        "actual_answer": "Eric P. Xing",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Who are the coauthors of the paper Analysis of Volatile Components in Dried Fruits and Branch Exudates of Schisandra chinensis with Different Fruit Colors Using GC-IMS Technology published by LTI faculty Yiming Yang in 2023?",
        "context": "Author: Yiming Yang Title: Analysis of Volatile Components in Dried Fruits and Branch Exudates of Schisandra chinensis with Different Fruit Colors Using GC-IMS Technology Publication year: 2023 Coauthors: Yiping Yan, Wenpeng Lu, Taiping Tian, Nan Shu, Yiming Yang, Shutian Fan, Xianyan Han, Yunhua Ge, Peilei Xu Abstract: To investigate the volatile components of Schisandra chinensis (Turcz.) Bail (commonly known as northern Schisandra) of different colors and to explore their similarities and differences, to identify the main flavor substances in the volatile components of the branch exudates of northern schisandra, and finally to establish a fingerprint map of the volatile components of the dried fruits and branch exudates of northern Schisandra of different colors, we used GC-IMS technology to analyze the volatile components of the dried fruits and branch exudates of three different colors of northern Schisandra and established a fingerprint spectra. The results showed that a total of 60 different volatile chemical components were identified in the branch exudates and dried fruits of Schisandra. The components of germplasm resources with different fruit colors were significantly different. The ion mobility spectrum and OPLS-DA results showed that white and yellow fruits were more similar compared to red fruits. The volatile components in dried fruits were significantly higher than those in branch exudates. After VIP (variable importance in projection) screening, 41 key volatile substances in dried fruits and 30 key volatile substances in branch exudates were obtained. After screening by odor activity value (OAV), there were 24 volatile components greater than 1 in both dried fruits and branch exudates. The most important contributing volatile substance was 3-methyl-butanal, and the most important contributing volatile substance in white fruit was (E)-2-hexenal.",
        "retrieved_doc": "Author: Eric P. Xing Title: Redco: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU/TPUs Publication year: 2023 Coauthors: Bowen Tan, Yun Zhu, Lijuan Liu, Hongyi Wang, Yonghao Zhuang, Jindong Chen, Eric P. Xing, Zhiting Hu Abstract: .|Author: Eric P. Xing Title: Redco: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU/TPUs Publication year: 2023 Coauthors: Bowen Tan, Yun Zhu, Lijuan Liu, Hongyi Wang, Yonghao Zhuang, Jindong Chen, Eric P. Xing, Zhiting Hu Abstract: .|Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe ",
        "retrieved_relevant_part": false,
        "model_answer": "The coauthors of the paper are Bowen Tan, Yun Zhu, Lijuan Liu, Hongyi Wang, Yonghao Zhuang, Jindong Chen, and Zhiting Hu",
        "actual_answer": "Yiping Yan, Wenpeng Lu, Taiping Tian, Nan Shu, Yiming Yang, Shutian Fan, Xianyan Han, Yunhua Ge, Peilei Xu",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Who is the author of the LTI paper RedCoast: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU/TPUs published in 2023?",
        "context": "Author: Eric P. Xing Title: RedCoast: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU/TPUs Publication year: 2023 Coauthors: Bowen Tan, Yun Zhu, Lijuan Liu, Hongyi Wang, Yonghao Zhuang, Jindong Chen, Eric P. Xing, Zhiting Hu Abstract: The recent progress of AI can be largely attributed to large language models (LLMs). However, their escalating memory requirements introduce challenges for machine learning (ML) researchers and engineers. Addressing this requires developers to partition a large model to distribute it across multiple GPUs or TPUs. This necessitates considerable coding and intricate configuration efforts with existing model parallel tools, such as Megatron-LM, DeepSpeed, and Alpa. These tools require users' expertise in machine learning systems (MLSys), creating a bottleneck in LLM development, particularly for developers without MLSys background. In this work, we present RedCoast(Redco), a lightweight and user-friendly tool crafted to automate distributed training and inference for LLMs, as well as to simplify ML pipeline development. The design of Redco emphasizes two key aspects. Firstly, to automate model parallism, our study identifies two straightforward rules to generate tensor parallel strategies for any given LLM. Integrating these rules into Redco facilitates effortless distributed LLM training and inference, eliminating the need of additional coding or complex configurations. We demonstrate the effectiveness by applying Redco on a set of LLM architectures, such as GPT-J, LLaMA, T5, and OPT, up to the size of 66B. Secondly, we propose a mechanism that allows for the customization of diverse ML pipelines through the definition of merely three functions, avoiding redundant and formulaic code like multi-host related processing. This mechanism proves adaptable across a spectrum of ML algorithms, from foundational language modeling to complex algorithms like meta-learning and reinforcement learning. Consequently, Redco implementations exhibit much fewer code lines compared to their official counterparts.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Alexander I. Rudnicky Title: Tartan: an LLM Driven SocialBot Publication year: 2023 Coauthors: Liangze Li, Ziyuan Liu, Li-Wei Chen, Ta-Chung Chi, Alexander I. Rudnicky Abstract: We describe our work to date on using large language models (LLMs) in our Alexa Prize Socialbot. Our work has laid the groundwork for looking more closely at using LLMs in a conversational system. We also analyze common patterns in conversations our bot has had with users|Author: Shinji Watanabe Title: DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models Publication year: 2023 Coauthors: Yifan Peng, Yui Sudo, Muhammad Shakeel, Shinji Watanabe Abstract: Self-supervised learning (SSL) has achieved notable success in many speech processing tasks, but the large model size and heavy computational cost hinder the deployment. Knowledge distillation trains a small student model to mimic the behavior of a large teacher model. However, the student architecture usually needs to be manually designed and will remain fixed during training, which requires prior knowledge and can lead to suboptimal performance. Inspired by recent success of task-specific structured pruning, we propose DPHuBERT, a novel task-agnostic compression method for speech SSL based on joint distillation and pruning. Experiments on SUPERB show that DPHuBERT outperforms pure distillation methods in almost all tasks. Moreover, DPHuBERT requires little training time and performs well with limited training data, making it suitable for resource-constrained applications. Our method can also be applied to various speech SSL models. Our code and models will be publicly available.",
        "retrieved_relevant_part": false,
        "model_answer": "The author of the LTI paper RedCoast is Shinji Watanabe",
        "actual_answer": "Eric P. Xing",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "What is the abstract of the paper Prompt2Model: Generating Deployable Models from Natural Language Instructions published by LTI faculty Graham Neubig in 2023?",
        "context": "Author: Graham Neubig Title: Prompt2Model: Generating Deployable Models from Natural Language Instructions Publication year: 2023 Coauthors: Vijay Viswanathan, Chenyang Zhao, Amanda Bertsch, Tongshuang Sherry Wu, Graham Neubig Abstract: Large language models (LLMs) enable system builders today to create competent NLP systems through prompting, where they only need to describe the task in natural language and provide a few examples. However, in other ways, LLMs are a step backward from traditional special-purpose NLP models; they require extensive computational resources for deployment and can be gated behind APIs. In this paper, we propose Prompt2Model, a general-purpose method that takes a natural language task description like the prompts provided to LLMs, and uses it to train a special-purpose model that is conducive to deployment. This is done through a multi-step process of retrieval of existing datasets and pretrained models, dataset generation using LLMs, and supervised fine-tuning on these retrieved and generated datasets. Over three tasks, we demonstrate that given the same few-shot prompt as input, Prompt2Model trains models that outperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20% while being up to 700 times smaller. We also show that this data can be used to obtain reliable performance estimates of model performance, enabling model developers to assess model reliability before deployment. Prompt2Model is available open-source at https://github.com/neulab/prompt2model.",
        "retrieved_doc": "Author: Maarten Sap Title: Don't Take This Out of Context!: On the Need for Contextual Models and Evaluations for Stylistic Rewriting Publication year: 2023 Coauthors: Akhila Yerukola, Xuhui Zhou, Elizabeth Clark, Maarten Sap |Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe |Author: Alexander I. Rudnicky Title: Tartan: an LLM Driven SocialBot Publication year: 2023 Coauthors: Liangze Li, Ziyuan Liu, Li-Wei Chen, Ta-Chung Chi, Alexander I. Rudnicky Abstract: We describe our work to date on using large language models (LLMs) in our Alexa Prize Socialbot. Our work has laid the groundwork for looking more closely at using LLMs in a conversational system. We also analyze common patterns in conversations our bot has had with users",
        "retrieved_relevant_part": false,
        "model_answer": "The paper describes the use of large language models (LLMs) in an Alexa Prize Socialbot and analyzes common patterns in conversations the bot has had with users.<|endoftext|>",
        "actual_answer": "Large language models (LLMs) enable system builders today to create competent NLP systems through prompting, where they only need to describe the task in natural language and provide a few examples. However, in other ways, LLMs are a step backward from traditional special-purpose NLP models",
        "exact_match": false,
        "f1_max_min_avg": [
            0.17,
            0.07,
            0.12000000000000001
        ],
        "recall_max_min_avg": [
            0.13,
            0.04,
            0.085
        ]
    },
    {
        "f1": 0.29864864864864865,
        "recall": 0.3429504504504505,
        "retrieval_acc": 0.49099099099099097
    }
]