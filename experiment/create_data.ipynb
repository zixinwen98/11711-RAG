{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LTI faculty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "urls = [r'https://lti.cs.cmu.edu/directory/all/154/1?page=0', r'https://lti.cs.cmu.edu/directory/all/154/1?page=1']\n",
    "datas = []\n",
    "for url in urls:\n",
    "    tables = pd.read_html(url) # Returns list of all tables on page\n",
    "    datas.append(tables[0]) # Select table of interest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "def parse_person_info(a):\n",
    "    try:\n",
    "        if np.isnan(a):\n",
    "            return dict()\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    data_dict = dict()\n",
    "    names = ['Email', 'Office', 'Phone', 'Research Areas']\n",
    "    idx = []\n",
    "    for name in names: \n",
    "        try:\n",
    "            idx.append(a.index(name))\n",
    "        except:\n",
    "            pass\n",
    "    idx = list(sorted(idx))\n",
    "    \n",
    "    #parse first & second name \n",
    "    name_and_title = a[:idx[0]]\n",
    "    name = ' '.join(name_and_title.split(' ')[:2])\n",
    "    title = ' '.join(name_and_title.split(' ')[2:])\n",
    "    data_dict['Name'] = name \n",
    "    data_dict['Title'] = title.strip()\n",
    "    #parse other info\n",
    "    for i in range(len(idx)):\n",
    "        if i == len(idx) - 1:\n",
    "            first, second = idx[i], len(a)\n",
    "        else:\n",
    "            first, second = idx[i], idx[i+1] \n",
    "        s = a[first:second]\n",
    "        s = s.split(':')\n",
    "        data_dict[s[0]] = s[1].strip()\n",
    "    \n",
    "    for name in names:\n",
    "        if name not in data_dict:\n",
    "            data_dict[name] = None\n",
    "    return data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dict version \n",
    "lti_faculty = []\n",
    "for data in datas:\n",
    "    for d in data.values:\n",
    "        for info in d:\n",
    "            info_dict = parse_person_info(info)\n",
    "            if info_dict:\n",
    "                lti_faculty.append(info_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "lti_faculty = []\n",
    "for data in datas:\n",
    "    for d in data.values:\n",
    "        for info in d:\n",
    "            try:\n",
    "                if np.isnan(info):\n",
    "                    pass\n",
    "            except:\n",
    "                lti_faculty.append(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "with open('/zfsauton2/home/yifuc/11711-RAG/data/cmu/lti_faculty.json', 'w') as f:\n",
    "    json.dump(lti_faculty, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LTI Staff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4101526/2916385793.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "urls = [r'https://lti.cs.cmu.edu/directory/all/154/2?page=0', r'https://lti.cs.cmu.edu/directory/all/154/2?page=1']\n",
    "datas = []\n",
    "for url in urls:\n",
    "    tables = pd.read_html(url) # Returns list of all tables on page\n",
    "    datas.append(tables[0]) # Select table of interest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lti_staff = []\n",
    "for data in datas:\n",
    "    for d in data.values:\n",
    "        for info in d:\n",
    "            try:\n",
    "                if np.isnan(info):\n",
    "                    pass\n",
    "            except:\n",
    "                lti_staff.append(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "34"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lti_staff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517',\n",
       " 'Allison Blair Associate Business Manager Email: ablair2@cs.cmu.edu Office: 5715 TCS Hall Phone: 412-268-5489',\n",
       " 'Bryan Burtner Associate Business Manager Email: bburtner@cs.cmu.edu Office: 6413 TCS Hall Phone: 412-268-2805',\n",
       " 'Zhi-Qi Cheng Hauptmann Lab/Postdoctoral Research Associate Email: zhiqic@andrew.cmu.edu',\n",
       " 'Jennifer Clark Administrative Coordinator Email: jclark2@andrew.cmu.edu',\n",
       " 'Brianna Eriksen Academic Program Manager Email: bfreema2@andrew.cmu.edu Phone: 412-268-4277',\n",
       " 'John Friday Sr. Administrative Coordinator Email: jfriday@andrew.cmu.edu Office: 5715 Gates & Hillman Centers Phone: 412-268-1810',\n",
       " 'Daniel Gongaware Sponsored Research Administrator Email: dgongawa@cs.cmu.edu Office: 355 TCS Hall Phone: 412-268-6195',\n",
       " 'Robin Hammer Levin Lab/Project Coordinator Email: robinham@andrew.cmu.edu',\n",
       " 'Annette Han Rose Lab/Data Science Course Developer Email: ahan2@andrew.cmu.edu',\n",
       " 'Susan E. Holm Mitamura Lab/Sr. Knowledge Engineer Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591',\n",
       " 'Karen Kirk Academic Program Coordinator Email: karensuk@andrew.cmu.edu Office: 6719 Gates & Hillman Centers Phone: +1 412 268 6904',\n",
       " 'Caitlin Korpus Senior Academic Program Coordinator Email: ckorpus@andrew.cmu.edu Office: 6719 Gates & Hillman Centers Phone: +1 412 268 7096',\n",
       " 'Wenhe Liu Research Scholar Email: wenhel@andrew.cmu.edu Office: 5403 Gates & Hillman Centers',\n",
       " 'Jennifer Lucas Academic Program Manager — MCDS Email: jmlucas@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-9870',\n",
       " 'Jessica Maguire Assistant to the Institute Director Email: jlm4@andrew.cmu.edu Office: 5721 Gates & Hillman Centers Phone: 412-268-5480',\n",
       " 'Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu',\n",
       " 'Vicente Malave Data Science Course Developer Email: vlm@andrew.cmu.edu',\n",
       " 'Krista McGuigan Business Director Email: klmcg@cs.cmu.edu Office: 349 TCS Hall Phone: 412-268-1635',\n",
       " 'Chas Murray Rose Lab/Systems/Software Engineer Email: rcmurray@andrew.cmu.edu',\n",
       " 'Julie Nys Employment Processes Manager Email: jnys@andrew.cmu.edu Office: 5405 TCS Hall Phone: 412-268-3515',\n",
       " 'Nicole Perrotta Administrative Coordinator Email: nperrott@andrew.cmu.edu',\n",
       " 'Hannah Rodriguez Sponsored Research Administrator Email: hmuczyns@andrew.cmu.edu Office: 6413 TCS Hall Phone: 412-268-3519',\n",
       " 'Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788',\n",
       " 'Zaid Sheikh Neubig Lab/Senior Systems/Software Engineer Email: zsheikh@andrew.cmu.edu Office: 6416 Gates & Hillman Centers Phone:',\n",
       " 'Kira Sullivan Administrative Coordinator Email: kiras@andrew.cmu.edu Phone: 412-268-8737',\n",
       " 'Emma Thomas Sponsored Research Administrator Email: emmat@andrew.cmu.edu Office: 5715 Gates & Hillman Centers Phone: 412-268-7812',\n",
       " 'Amber Vivis Sr. Academic Program Manager - MSAII Email: avivis@cs.cmu.edu Office: 357 TCS Hall Phone: 412-268-9998',\n",
       " 'Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu',\n",
       " 'Casey Walker Academic Program Coordinator Email: clwalker@andrew.cmu.edu Phone: +1 412 268 9315',\n",
       " 'Zhong-Qiu Wang Wantanabe Lab/Postdoctoral Research Associate Email: zhongqiu@andrew.cmu.edu Office: 6414 Gates & Hillman Centers Phone: 415-200-8662',\n",
       " 'Yukari Yamakawa Mitamura Lab/Associate Data Analyst Email: yukariy@andrew.cmu.edu Office: 6717 Gates & Hillman Centers Phone: 412-268-6596',\n",
       " 'Stacey Young Academic Program Manager — Ph.D. Email: staceyy@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-2623',\n",
       " nan]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lti_staff\n",
    "['name', 'title', 'email', 'phone', 'office']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "with open('/zfsauton2/home/yifuc/11711-RAG/data/cmu/lti_staff.json', 'w') as f:\n",
    "    json.dump(lti_staff, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LTI student (ignore now)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "urls = [r'https://lti.cs.cmu.edu/directory/students/current/all']\n",
    "datas = []\n",
    "for url in urls:\n",
    "    tables = pd.read_html(url) # Returns list of all tables on page\n",
    "    datas.append(tables[0]) # Select table of interest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "lti_student = []\n",
    "for data in datas:\n",
    "    for d in data.values:\n",
    "        for info in d:\n",
    "            try:\n",
    "                if np.isnan(info):\n",
    "                    pass\n",
    "            except:\n",
    "                lti_student.append(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LTI Faculty paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_cmu(faculty, threshold = 0.7):\n",
    "    total = 0\n",
    "    cs = 0\n",
    "    for p in faculty['papers']:\n",
    "        total += 1\n",
    "        if p['fieldsOfStudy'] is not None and 'Computer Science' in p['fieldsOfStudy']:\n",
    "            cs += 1\n",
    "    if total == 0:\n",
    "        return False\n",
    "    return cs/total >= threshold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_paper_info(faculty):\n",
    "    papers = faculty['papers']\n",
    "    paper_info = []\n",
    "    for p in papers:\n",
    "        if p['year'] is None or p['year'] < 2023:\n",
    "            continue\n",
    "        paper_string = ''\n",
    "        paper_string += f'Author: {faculty[\"name\"]} '\n",
    "        paper_string += f'Title: {p[\"title\"]} '\n",
    "        paper_string += f'Publication year: {p[\"year\"]} '\n",
    "        coauthor = ''\n",
    "        for a in p['authors']:\n",
    "            coauthor += a['name'] + ', '\n",
    "        coauthor = coauthor[:-2]\n",
    "        paper_string += f'Coauthors: {coauthor} '\n",
    "        if p['abstract'] is not None:\n",
    "            paper_string += f'Abstract: {p[\"abstract\"]}'\n",
    "        paper_info.append(paper_string)\n",
    "    return paper_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "faculty = json.load(open('/zfsauton2/home/yifuc/11711-RAG/data/cmu/lti_faculty.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "faculty_names = [s.split(' ')[:2] for s in faculty]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "publications = []\n",
    "for faculty_name in faculty_names:\n",
    "    if faculty_name[0] != \"Eric\":\n",
    "        name_str = faculty_name[0]+ '+' +faculty_name[1]\n",
    "        name_str = name_str.lower()\n",
    "    else:\n",
    "        name_str = 'eric+xing' #hardcode for eric ;)\n",
    "    url = f'https://api.semanticscholar.org/graph/v1/author/search?query={name_str}&fields=name,aliases,url,affiliations,papers.abstract,papers.title,papers.authors,papers.year,papers.fieldsOfStudy'\n",
    "    faculty_list = requests.get(url).json()\n",
    "    try:\n",
    "        faculty_list = faculty_list['data']\n",
    "    except:\n",
    "        continue\n",
    "    for f in faculty_list:\n",
    "        if is_cmu(f):\n",
    "            publications.extend(extract_paper_info(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Author: Yonatan Bisk Title: HomeRobot: An Open Source Software Stack for Mobile Manipulation Research Publication year: 2024 Coauthors: Chris Paxton, Austin Wang, Binit Shah, Blaine Matulevich, Dhruv Shah, Karmesh Yadav, Santhosh Ramakrishnan, Sriram Yenamandra, Yonatan Bisk Abstract: Reproducibility in robotics research requires capable, shared hardware platforms which can be used for a wide variety of research. We’ve seen the power of these sorts of shared platforms in more general machine learning research, where there is constant iteration on shared AI platforms like PyTorch. To be able to make rapid progress in robotics in the same way, we propose that we need: (1) shared real-world platforms which allow different teams to test and compare methods at low cost; (2) challenging simulations that reflect real-world environments and especially can drive perception and planning research; and (3) low-cost platforms with enough software to get started addressing all of these problems. To this end, we propose HomeRobot, a mobile manipulator software stack with associated benchmark in simulation, which is initially based on the low-cost, human-safe Hello Robot Stretch.',\n",
       " \"Author: Yonatan Bisk Title: SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs Publication year: 2023 Coauthors: Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey, Yanping Huang, David A. Ross, Irfan Essa, Yonatan Bisk, Ming Yang, K. Murphy, A. Hauptmann, Lu Jiang Abstract: In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos. SPAE converts between raw pixels and interpretable lexical tokens (or words) extracted from the LLM's vocabulary. The resulting tokens capture both the semantic meaning and the fine-grained details needed for visual reconstruction, effectively translating the visual content into a language comprehensible to the LLM, and empowering it to perform a wide array of multimodal tasks. Our approach is validated through in-context learning experiments with frozen PaLM 2 and GPT 3.5 on a diverse set of image understanding and generation tasks. Our method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%.\",\n",
       " 'Author: Yonatan Bisk Title: HomeRobot: Open-Vocabulary Mobile Manipulation Publication year: 2023 Coauthors: Sriram Yenamandra, A. Ramachandran, Karmesh Yadav, Austin S. Wang, Mukul Khanna, Théophile Gervet, Tsung-Yen Yang, Vidhi Jain, Alexander Clegg, John Turner, Z. Kira, M. Savva, Angel X. Chang, Devendra Singh Chaplot, Dhruv Batra, Roozbeh Mottaghi, Yonatan Bisk, Chris Paxton Abstract: HomeRobot (noun): An affordable compliant robot that navigates homes and manipulates a wide range of objects in order to complete everyday tasks. Open-Vocabulary Mobile Manipulation (OVMM) is the problem of picking any object in any unseen environment, and placing it in a commanded location. This is a foundational challenge for robots to be useful assistants in human environments, because it involves tackling sub-problems from across robotics: perception, language understanding, navigation, and manipulation are all essential to OVMM. In addition, integration of the solutions to these sub-problems poses its own substantial challenges. To drive research in this area, we introduce the HomeRobot OVMM benchmark, where an agent navigates household environments to grasp novel objects and place them on target receptacles. HomeRobot has two components: a simulation component, which uses a large and diverse curated object set in new, high-quality multi-room home environments; and a real-world component, providing a software stack for the low-cost Hello Robot Stretch to encourage replication of real-world experiments across labs. We implement both reinforcement learning and heuristic (model-based) baselines and show evidence of sim-to-real transfer. Our baselines achieve a 20% success rate in the real world; our experiments identify ways future research work improve performance. See videos on our website: https://ovmm.github.io/.',\n",
       " 'Author: Yonatan Bisk Title: EXCALIBUR: Encouraging and Evaluating Embodied Exploration Publication year: 2023 Coauthors: Hao Zhu, Raghav Kapoor, So Yeon Min, Winson Han, Jiatai Li, Kaiwen Geng, Graham Neubig, Yonatan Bisk, Aniruddha Kembhavi, Luca Weihs Abstract: Experience precedes understanding. Humans constantly explore and learn about their environment out of curiosity, gather information, and update their models of the world. On the other hand, machines are either trained to learn passively from static and fixed datasets, or taught to complete specific goal-conditioned tasks. To encourage the development of exploratory interactive agents, we present the EXCALIBUR benchmark. EXCALIBUR allows agents to explore their environment for long durations and then query their understanding of the physical world via inquiries like: “is the small heavy red bowl made from glass?” or “is there a silver spoon heavier than the egg?”. This design encourages agents to perform free-form home exploration without myopia induced by goal conditioning. Once the agents have answered a series of questions, they can renter the scene to refine their knowledge, update their beliefs, and improve their performance on the questions. Our experiments demonstrate the challenges posed by this dataset for the present-day state-of-the-art embodied systems and the headroom afforded to develop new innovative methods. Finally, we present a virtual reality interface that enables humans to seamlessly interact within the simulated world and use it to gather human performance measures. EXCALIBUR affords unique challenges in comparison to presentday benchmarks and represents the next frontier for embodied AI research.',\n",
       " \"Author: Yonatan Bisk Title: Plan, Eliminate, and Track - Language Models are Good Teachers for Embodied Agents Publication year: 2023 Coauthors: Yue Wu, So Yeon Min, Yonatan Bisk, R. Salakhutdinov, A. Azaria, Yuan-Fang Li, Tom M. Mitchell, Shrimai Prabhumoye Abstract: Pre-trained large language models (LLMs) capture procedural knowledge about the world. Recent work has leveraged LLM's ability to generate abstract plans to simplify challenging control tasks, either by action scoring, or action modeling (fine-tuning). However, the transformer architecture inherits several constraints that make it difficult for the LLM to directly serve as the agent: e.g. limited input lengths, fine-tuning inefficiency, bias from pre-training, and incompatibility with non-text environments. To maintain compatibility with a low-level trainable actor, we propose to instead use the knowledge in LLMs to simplify the control problem, rather than solving it. We propose the Plan, Eliminate, and Track (PET) framework. The Plan module translates a task description into a list of high-level sub-tasks. The Eliminate module masks out irrelevant objects and receptacles from the observation for the current sub-task. Finally, the Track module determines whether the agent has accomplished each sub-task. On the AlfWorld instruction following benchmark, the PET framework leads to a significant 15% improvement over SOTA for generalization to human goal specifications.\",\n",
       " 'Author: Yonatan Bisk Title: Open X-Embodiment: Robotic Learning Datasets and RT-X Models Publication year: 2023 Coauthors: A. Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, A. Irpan, Alexander Khazatsky, Anant Rai, Anika Singh, Anthony Brohan, A. Raffin, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Schölkopf, Brian Ichter, Cewu Lu, Charles Xu, Chelsea Finn, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Chuer Pan, Chuyuan Fu, Coline Devin, Danny Driess, Deepak Pathak, Dhruv Shah, Dieter Büchler, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Federico Ceola, Fei Xia, F. Stulp, Gaoyue Zhou, G. Sukhatme, G. Salhotra, Ge Yan, Giulio Schiavi, G. Kahn, Hao Su, Haoshu Fang, Haochen Shi, H. B. Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jan Peters, Jan Schneider, Jasmine Hsu, J. Bohg, Jeff Bingham, Jiajun Wu, Jialin Wu, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jitendra Malik, Jonathan Tompson, Jonathan Yang, Joseph J. Lim, João Silvério, Junhyek Han, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, K. Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Zhang, Krishan Rana, K. Srinivasan, Lawrence Yunliang Chen, Lerrel Pinto, Liam Tan, Lionel Ott, Lisa Lee, Masayoshi Tomizuka, Maximilian Du, Michael Ahn, Mingtong Zhang, Mingyu Ding, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, N. Heess, Nikhil Joshi, Niko Suenderhauf, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, P. Sanketi, Paul Wohlhart, Peng Xu, P. Sermanet, Priya Sundaresan, Q. Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan C. Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham D. Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, S. Adebola, Simon Guist, Soroush Nasiriany, S. Schaal, Stefan Welker, Stephen Tian, Sudeep Dasari, Suneel Belkhale, Takayuki Osa, Tatsuya Harada, T. Matsushima, Ted Xiao, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Zhao, Travis Armstrong, Trevor Darrell, Vidhi Jain, Vincent Vanhoucke, W. Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xuanlin Li, Yao Lu, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yueh-hua Wu, Yujin Tang, Yuke Zhu, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zhuo Xu, Zichen Jeff Cui Abstract: —Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer',\n",
       " 'Author: Yonatan Bisk Title: Toward General-Purpose Robots via Foundation Models: A Survey and Meta-Analysis Publication year: 2023 Coauthors: Yafei Hu, Quanting Xie, Vidhi Jain, Jonathan Francis, Jay Patrikar, Nikhil Varma Keetha, Seungchan Kim, Yaqi Xie, Tianyi Zhang, Shibo Zhao, Yu Quan Chong, Chen Wang, Katia P. Sycara, Matthew Johnson-Roberson, Dhruv Batra, Xiaolong Wang, Sebastian Scherer, Z. Kira, Fei Xia, Yonatan Bisk Abstract: Building general-purpose robots that can operate seamlessly, in any environment, with any object, and utilizing various skills to complete diverse tasks has been a long-standing goal in Artificial Intelligence. Unfortunately, however, most existing robotic systems have been constrained - having been designed for specific tasks, trained on specific datasets, and deployed within specific environments. These systems usually require extensively-labeled data, rely on task-specific models, have numerous generalization issues when deployed in real-world scenarios, and struggle to remain robust to distribution shifts. Motivated by the impressive open-set performance and content generation capabilities of web-scale, large-capacity pre-trained models (i.e., foundation models) in research fields such as Natural Language Processing (NLP) and Computer Vision (CV), we devote this survey to exploring (i) how these existing foundation models from NLP and CV can be applied to the field of robotics, and also exploring (ii) what a robotics-specific foundation model would look like. We begin by providing an overview of what constitutes a conventional robotic system and the fundamental barriers to making it universally applicable. Next, we establish a taxonomy to discuss current work exploring ways to leverage existing foundation models for robotics and develop ones catered to robotics. Finally, we discuss key challenges and promising future directions in using foundation models for enabling general-purpose robotic systems. We encourage readers to view our living GitHub repository of resources, including papers reviewed in this survey as well as related projects and repositories for developing foundation models for robotics.',\n",
       " \"Author: Yonatan Bisk Title: Open X-Embodiment: Robotic Learning Datasets and RT-X Models Publication year: 2023 Coauthors: Open X-Embodiment Collaboration, A. Padalkar, Acorn Pooley, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alex Herzog, A. Irpan, Alexander Khazatsky, Anant Rai, Anika Singh, Animesh Garg, Anthony Brohan, A. Raffin, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, B. Scholkopf, Brian Ichter, Cewu Lu, Charles Xu, Chelsea Finn, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Chuer Pan, Chuyuan Fu, Coline Devin, Danny Driess, Deepak Pathak, Dhruv Shah, Dieter Buchler, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Federico Ceola, Fei Xia, F. Stulp, Gaoyue Zhou, G. Sukhatme, G. Salhotra, Ge Yan, Giulio Schiavi, G. Kahn, Hao Su, Haoshu Fang, Haochen Shi, H. B. Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jan Peters, Jan Schneider, Jasmine Hsu, J. Bohg, Jeff Bingham, Jiajun Wu, Jialin Wu, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jitendra Malik, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Joseph J. Lim, Joao Silv'erio, Junhyek Han, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, K. Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Zhang, Krishan Rana, K. Srinivasan, Lawrence Yunliang Chen, Lerrel Pinto, Fei-Fei Li, Liam Tan, Lionel Ott, Lisa Lee, Masayoshi Tomizuka, Max Spero, Maximilian Du, Michael Ahn, Mingtong Zhang, Mingyu Ding, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, N. Heess, Nikhil Joshi, Niko Suenderhauf, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, P. Sanketi, Paul Wohlhart, Peng Xu, P. Sermanet, Priya Sundaresan, Q. Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Mart'in-Mart'in, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan C. Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham D. Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, S. Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Sudeep Dasari, Suneel Belkhale, Takayuki Osa, Tatsuya Harada, T. Matsushima, Ted Xiao, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Zhao, Travis Armstrong, Trevor Darrell, Vidhi Jain, Vincent Vanhoucke, W. Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xuanlin Li, Yao Lu, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yueh-hua Wu, Yujin Tang, Yuke Zhu, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zhuo Xu, Zichen Jeff Cui Abstract: Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer Vision, this has led to a consolidation of pretrained models, with general pretrained backbones serving as a starting point for many applications. Can such a consolidation happen in robotics? Conventionally, robotic learning methods train a separate model for every application, every robot, and even every environment. Can we instead train generalist X-robot policy that can be adapted efficiently to new robots, tasks, and environments? In this paper, we provide datasets in standardized data formats and models to make it possible to explore this possibility in the context of robotic manipulation, alongside experimental results that provide an example of effective X-robot policies. We assemble a dataset from 22 different robots collected through a collaboration between 21 institutions, demonstrating 527 skills (160266 tasks). We show that a high-capacity model trained on this data, which we call RT-X, exhibits positive transfer and improves the capabilities of multiple robots by leveraging experience from other platforms. More details can be found on the project website $\\\\href{https://robotics-transformer-x.github.io}{\\\\text{robotics-transformer-x.github.io}}$.\",\n",
       " \"Author: Yonatan Bisk Title: MOSAIC: Learning Unified Multi-Sensory Object Property Representations for Robot Perception Publication year: 2023 Coauthors: Gyan Tatiya, Jonathan M Francis, Ho-Hsiang Wu, Yonatan Bisk, J. Sinapov Abstract: A holistic understanding of object properties across diverse sensory modalities (e.g., visual, audio, and haptic) is essential for tasks ranging from object categorization to complex manipulation. Drawing inspiration from cognitive science studies that emphasize the significance of multi-sensory integration in human perception, we introduce MOSAIC (Multi-modal Object property learning with Self-Attention and Integrated Comprehension), a novel framework designed to facilitate the learning of unified multi-sensory object property representations. While it is undeniable that visual information plays a prominent role, we acknowledge that many fundamental object properties extend beyond the visual domain to encompass attributes like texture, mass distribution, or sounds, which significantly influence how we interact with objects. In MOSAIC, we leverage this profound insight by distilling knowledge from the extensive pre-trained Contrastive Language-Image Pre-training (CLIP) model, aligning these representations not only across vision but also haptic and auditory sensory modalities. Through extensive experiments on a dataset where a humanoid robot interacts with 100 objects across 10 exploratory behaviors, we demonstrate the versatility of MOSAIC in two task families: object categorization and object-fetching tasks. Our results underscore the efficacy of MOSAIC's unified representations, showing competitive performance in category recognition through a simple linear probe setup and excelling in the fetch object task under zero-shot transfer conditions. This work pioneers the application of CLIP-based sensory grounding in robotics, promising a significant leap in multi-sensory perception capabilities for autonomous systems. We have released the code, datasets, and additional results: https://github.com/gtatiya/MOSAIC.\",\n",
       " 'Author: Yonatan Bisk Title: Reasoning about the Unseen for Efficient Outdoor Object Navigation Publication year: 2023 Coauthors: Quanting Xie, Tianyi Zhang, Kedi Xu, M. Johnson-Roberson, Yonatan Bisk Abstract: Robots should exist anywhere humans do: indoors, outdoors, and even unmapped environments. In contrast, the focus of recent advancements in Object Goal Navigation(OGN) has targeted navigating in indoor environments by leveraging spatial and semantic cues that do not generalize outdoors. While these contributions provide valuable insights into indoor scenarios, the broader spectrum of real-world robotic applications often extends to outdoor settings. As we transition to the vast and complex terrains of outdoor environments, new challenges emerge. Unlike the structured layouts found indoors, outdoor environments lack clear spatial delineations and are riddled with inherent semantic ambiguities. Despite this, humans navigate with ease because we can reason about the unseen. We introduce a new task OUTDOOR, a new mechanism for Large Language Models (LLMs) to accurately hallucinate possible futures, and a new computationally aware success metric for pushing research forward in this more complex domain. Additionally, we show impressive results on both a simulated drone and physical quadruped in outdoor environments. Our agent has no premapping and our formalism outperforms naive LLM-based approaches',\n",
       " 'Author: Yonatan Bisk Title: Open X-Embodiment: Robotic Learning Datasets and RT-X Models Publication year: 2023 Coauthors: A. Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, A. Irpan, Alexander Khazatsky, Anant Rai, Anika Singh, Anthony Brohan, A. Raffin, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Schölkopf, Brian Ichter, Cewu Lu, Charles Xu, Chelsea Finn, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Chuer Pan, Chuyuan Fu, Coline Devin, Danny Driess, Deepak Pathak, Dhruv Shah, Dieter Büchler, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Federico Ceola, Fei Xia, F. Stulp, Gaoyue Zhou, G. Sukhatme, G. Salhotra, Ge Yan, Giulio Schiavi, G. Kahn, Hao Su, Haoshu Fang, Haochen Shi, H. B. Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jan Peters, Jan Schneider, Jasmine Hsu, J. Bohg, Jeff Bingham, Jiajun Wu, Jialin Wu, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jitendra Malik, Jonathan Tompson, Jonathan Yang, Joseph J. Lim, João Silvério, Junhyek Han, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, K. Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Zhang, Krishan Rana, K. Srinivasan, Lawrence Yunliang Chen, Lerrel Pinto, Liam Tan, Lionel Ott, Lisa Lee, Masayoshi Tomizuka, Maximilian Du, Michael Ahn, Mingtong Zhang, Mingyu Ding, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, N. Heess, Nikhil Joshi, Niko Suenderhauf, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, P. Sanketi, Paul Wohlhart, Peng Xu, P. Sermanet, Priya Sundaresan, Q. Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan C. Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham D. Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, S. Adebola, Simon Guist, Soroush Nasiriany, S. Schaal, Stefan Welker, Stephen Tian, Sudeep Dasari, Suneel Belkhale, Takayuki Osa, Tatsuya Harada, T. Matsushima, Ted Xiao, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Zhao, Travis Armstrong, Trevor Darrell, Vidhi Jain, Vincent Vanhoucke, W. Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xuanlin Li, Yao Lu, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yueh-hua Wu, Yujin Tang, Yuke Zhu, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zhuo Xu, Zichen Jeff Cui Abstract: —Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer',\n",
       " 'Author: Yonatan Bisk Title: SLAP: Spatial-Language Attention Policies Publication year: 2023 Coauthors: Priyam Parashar, Vidhi Jain, Xiaohan Zhang, Jay Vakil, Sam Powers, Yonatan Bisk, Chris Paxton Abstract: Despite great strides in language-guided manipulation, existing work has been constrained to table-top settings. Table-tops allow for perfect and consistent camera angles, properties are that do not hold in mobile manipulation. Task plans that involve moving around the environment must be robust to egocentric views and changes in the plane and angle of grasp. A further challenge is ensuring this is all true while still being able to learn skills efficiently from limited data. We propose Spatial-Language Attention Policies (SLAP) as a solution. SLAP uses three-dimensional tokens as the input representation to train a single multi-task, language-conditioned action prediction policy. Our method shows an 80% success rate in the real world across eight tasks with a single model, and a 47.5% success rate when unseen clutter and unseen object configurations are introduced, even with only a handful of examples per task. This represents an improvement of 30% over prior work (20% given unseen distractors and configurations). We see a 4x improvement over baseline in mobile manipulation setting. In addition, we show how SLAPs robustness allows us to execute Task Plans from open-vocabulary instructions using a large language model for multi-step mobile manipulation. For videos, see the website: https://robotslap.github.io',\n",
       " 'Author: Yonatan Bisk Title: The Framework Tax: Disparities Between Inference Efficiency in Research and Deployment Publication year: 2023 Coauthors: Jared Fernandez, Jacob Kahn, Clara Na, Yonatan Bisk, Emma Strubell Abstract: Increased focus on the computational efficiency of NLP systems has motivated the design of efficient model architectures and improvements to underlying hardware accelerators. However, the resulting increases in computational throughput and reductions in floating point operations have not directly translated to improvements in wall-clock inference latency. We demonstrate that these discrepancies can be largely attributed to bottlenecks introduced by deep learning frameworks. We denote this phenomenon as the \\\\textit{framework tax}, and observe that the disparity is growing as hardware speed increases over time. In this work, we examine this phenomenon through a series of case studies analyzing the effects of model design decisions, framework paradigms, and hardware platforms on total model latency. Code is available at https://github.com/JaredFern/Framework-Tax.',\n",
       " 'Author: Yonatan Bisk Title: SPRING: Studying the Paper and Reasoning to Play Games Publication year: 2023 Coauthors: Yue Wu, So Yeon Min, Shrimai Prabhumoye, Yonatan Bisk, R. Salakhutdinov, A. Azaria, Tom M. Mitchell, Yuan-Fang Li Abstract: Open-world survival games pose significant challenges for AI algorithms due to their multi-tasking, deep exploration, and goal prioritization requirements. Despite reinforcement learning (RL) being popular for solving games, its high sample complexity limits its effectiveness in complex open-world games like Crafter or Minecraft. We propose a novel approach, SPRING, to read the game\\'s original academic paper and use the knowledge learned to reason and play the game through a large language model (LLM). Prompted with the LaTeX source as game context and a description of the agent\\'s current observation, our SPRING framework employs a directed acyclic graph (DAG) with game-related questions as nodes and dependencies as edges. We identify the optimal action to take in the environment by traversing the DAG and calculating LLM responses for each node in topological order, with the LLM\\'s answer to final node directly translating to environment actions. In our experiments, we study the quality of in-context\"reasoning\"induced by different forms of prompts under the setting of the Crafter open-world environment. Our experiments suggest that LLMs, when prompted with consistent chain-of-thought, have great potential in completing sophisticated high-level trajectories. Quantitatively, SPRING with GPT-4 outperforms all state-of-the-art RL baselines, trained for 1M steps, without any training. Finally, we show the potential of games as a test bed for LLMs.',\n",
       " 'Author: Yonatan Bisk Title: WebArena: A Realistic Web Environment for Building Autonomous Agents Publication year: 2023 Coauthors: Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig Abstract: With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.',\n",
       " 'Author: Yonatan Bisk Title: Computational Language Acquisition with Theory of Mind Publication year: 2023 Coauthors: Andy T. Liu, Hao Zhu, Emmy Liu, Yonatan Bisk, Graham Neubig Abstract: Unlike current state-of-the-art language models, young children actively acquire language through interactions with their surrounding environment and caretakers. One mechanism that has been argued to be critical to language learning is the ability to infer the mental states of other agents in social environments, coined Theory of Mind (ToM) by Premack&Woodruff (1978). Drawing inspiration from the modern operationalized versions of ToM implemented in Rabinowitz et al. (2018) and Zhu et al. (2021), we build language-learning agents equipped with ToM, and measure its effects on the learning process. We model ToM by giving the speaker agent an internal listener model that is trained alongside the speaker and used to rerank potential utterances. We experiment with varying task difficulty, hypothesizing that models will acquire more complex language to adapt to stronger environmental pressures. We find that training speakers with a highly weighted ToM listener component leads to performance gains in our image referential game setting. We also find some evidence that increasing task difficulty in the training process results in more fluent and precise utterances in evaluation. This suggests the potential utility of further incorporating ToM, as well as other insights from child language acquisition, into computational models of language acquisition.',\n",
       " 'Author: Yonatan Bisk Title: Open X-Embodiment: Robotic Learning Datasets and RT-X Models Publication year: 2023 Coauthors: A. Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, A. Irpan, Alexander Khazatsky, Anant Rai, Anika Singh, Anthony Brohan, A. Raffin, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Schölkopf, Brian Ichter, Cewu Lu, Charles Xu, Chelsea Finn, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Chuer Pan, Chuyuan Fu, Coline Devin, Danny Driess, Deepak Pathak, Dhruv Shah, Dieter Büchler, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Federico Ceola, Fei Xia, F. Stulp, Gaoyue Zhou, G. Sukhatme, G. Salhotra, Ge Yan, Giulio Schiavi, Hao Su, Haoshu Fang, Haochen Shi, H. B. Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jaehyung Kim, Jan Schneider, Jasmine Hsu, J. Bohg, Jeff Bingham, Jiajun Wu, Jialin Wu, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jitendra Malik, Jonathan Tompson, Jonathan Yang, Joseph J. Lim, João Silvério, Junhyek Han, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, K. Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Zhang, K. Majd, Krishan Rana, K. Srinivasan, Lawrence Yunliang Chen, Lerrel Pinto, Liam Tan, Lionel Ott, Lisa Lee, Masayoshi Tomizuka, Maximilian Du, Michael Ahn, Mingtong Zhang, Mingyu Ding, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, N. Heess, Nikhil Joshi, Niko Suenderhauf, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, P. Sanketi, Paul Wohlhart, Peng Xu, P. Sermanet, Priya Sundaresan, Q. Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan C. Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Sherry Moore, Shikhar Bahl, Shivin Dass, Shuran Song, Sichun Xu, Siddhant Haldar, S. Adebola, Simon Guist, Soroush Nasiriany, S. Schaal, Stefan Welker, Stephen Tian, Sudeep Dasari, Suneel Belkhale, Takayuki Osa, Tatsuya Harada, T. Matsushima, Ted Xiao, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Zhao, Travis Armstrong, T. Darrell, Vidhi Jain, Vincent Vanhoucke, W. Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xuanlin Li, Yao Lu, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yueh-hua Wu, Yujin Tang, Yuke Zhu, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zhuo Xu, Zichen Jeff Cui Abstract: —Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer Vision, this has led to a consolidation of pretrained models,',\n",
       " \"Author: Yonatan Bisk Title: SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents Publication year: 2023 Coauthors: Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, Maarten Sap Abstract: Humans are social beings; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and interact under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills. These findings demonstrate SOTOPIA's promise as a general platform for research on evaluating and improving social intelligence in artificial agents.\",\n",
       " \"Author: Jamie Callan Title: Conversational Search with Random Walks over Entity Graphs Publication year: 2023 Coauthors: Gustavo Gonçalves, João Magalhães, Jamie Callan Abstract: The entities that emerge during a conversation can be used to model topics, but not all entities are equally useful for this task. Modeling the conversation with entity graphs and predicting each entity's centrality in the conversation provides additional information that improves the retrieval of answer passages for the current question. Experiments show that using random walks to estimate entity centrality on conversation entity graphs improves top precision answer passage ranking over competitive transformer-based baselines.\",\n",
       " 'Author: Jamie Callan Title: KALE: Using a K-Sparse Projector for Lexical Expansion Publication year: 2023 Coauthors: Luís Borges, Bruno Martins, Jamie Callan Abstract: Recent research has proposed retrieval approaches based on sparse representations and inverted indexes, with terms produced by neural language models and leveraging the advantages from both neural retrieval and lexical matching. This paper proposes KALE, a new lightweight method of this family that uses a small model with a k-sparse projector to convert dense representations into a sparse set of entries from a latent vocabulary. The KALE vocabulary captures semantic concepts than perform well when used in isolation, and perform better when extending the original lexical vocabulary, this way improving first-stage retrieval accuracy. Experiments with the MSMARCOv1 passage retrieval dataset, the TREC Deep Learning dataset, and BEIR datasets, examined the effectiveness of KALE under varying conditions. Results show that the KALE terms can replace the original lexical vocabulary, with gains in accuracy and efficiency. Combining KALE with the original lexical vocabulary, or with other learned terms, can further improve retrieval accuracy with only a modest increase in computational cost.',\n",
       " 'Author: Jamie Callan Title: CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms Publication year: 2023 Coauthors: Zhen Fan, Luyu Gao, Jamie Callan Abstract: Lexical exact-match systems perform text retrieval efficiently with sparse matching signals and fast retrieval through inverted lists, but naturally suffer from the mismatch between lexical surface form and implicit term semantics. This paper proposes to directly bridge the surface form space and the term semantics space in lexical exact-match retrieval via contextualized surface forms (CSF). Each CSF pairs a lexical surface form with a context source, and is represented by a lexical form weight and a contextualized semantic vector representation. This framework is able to perform sparse lexicon-based retrieval by learning to represent each query and document as a \"bag-of-CSFs\", simultaneously addressing two key factors in sparse retrieval: vocabulary expansion of surface form and semantic representation of term meaning. At retrieval time, it efficiently matches CSFs through exact-match of learned surface forms, and effectively scores each CSF pair via contextual semantic representations, leading to joint improvement in both term match and term scoring. Multiple experiments show that this approach successfully resolves the main mismatch issues in lexical exact-match retrieval and outperforms state-of-the-art lexical exact-match systems, reaching comparable accuracy as lexical all-to-all soft match systems as an efficient exact-match-based system.',\n",
       " 'Author: Jamie Callan Title: COILcr: Efficient Semantic Matching in Contextualized Exact Match Retrieval Publication year: 2023 Coauthors: Zhen Fan, Luyu Gao, Rohan Jha, Jamie Callan ',\n",
       " 'Author: Jamie Callan Title: Active Retrieval Augmented Generation Publication year: 2023 Coauthors: Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig Abstract: Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.',\n",
       " 'Author: Jamie Callan Title: Multi-Objective Improvement of Android Applications Publication year: 2023 Coauthors: Jamie Callan, J. Petke Abstract: Non-functional properties, such as runtime or memory use, are important to mobile app users and developers, as they affect user experience. Previous work on automated improvement of non-functional properties in mobile apps failed to address the inherent trade-offs between such properties. We propose a practical approach and the first open-source tool, GIDroid (2023), for multi-objective automated improvement of Android apps. In particular, we use Genetic improvement, a search-based technique that navigates the space of software variants to find improved software. We use a simulation-based testing framework to greatly improve the speed of search. GIDroid contains three state-of-the-art multi-objective algorithms, and two new mutation operators, which cache the results of method calls. Genetic improvement relies on testing to validate patches. Previous work showed that tests in open-source Android applications are scarce. We thus wrote tests for 21 versions of 7 Android apps, creating a new benchmark for performance improvements. We used GIDroid to improve versions of mobile apps where developers had previously found improvements to runtime, memory, and bandwidth use. Our technique automatically re-discovers 64% of existing improvements. We then applied our approach to current versions of software in which there were no known improvements. We were able to improve execution time by up to 35%, and memory use by up to 33% in these apps.',\n",
       " 'Author: Jamie Callan Title: Building Retrieval Systems for the ClueWeb22-B Corpus Publication year: 2024 Coauthors: Harshit Mehrotra, Jamie Callan, Zhen Fan Abstract: The ClueWeb22 dataset containing nearly 10 billion documents was released in 2022 to support academic and industry research. The goal of this project was to build retrieval baselines for the English section of the\"super head\"part (category B) of this dataset. These baselines can then be used by the research community to compare their systems and also to generate data to train/evaluate new retrieval and ranking algorithms. The report covers sparse and dense first stage retrievals as well as neural rerankers that were implemented for this dataset. These systems are available as a service on a Carnegie Mellon University cluster.',\n",
       " \"Author: Mona T. Diab Title: Can Large Language Models Infer Causation from Correlation? Publication year: 2023 Coauthors: Zhijing Jin, Jiarui Liu, Zhiheng Lyu, Spencer Poff, Mrinmaya Sachan, Rada Mihalcea, Mona T. Diab, B. Scholkopf Abstract: Causal inference is one of the hallmarks of human intelligence. While the field of CausalNLP has attracted much interest in the recent years, existing causal inference datasets in NLP primarily rely on discovering causality from empirical knowledge (e.g., commonsense knowledge). In this work, we propose the first benchmark dataset to test the pure causal inference skills of large language models (LLMs). Specifically, we formulate a novel task Corr2Cause, which takes a set of correlational statements and determines the causal relationship between the variables. We curate a large-scale dataset of more than 200K samples, on which we evaluate seventeen existing LLMs. Through our experiments, we identify a key shortcoming of LLMs in terms of their causal inference skills, and show that these models achieve almost close to random performance on the task. This shortcoming is somewhat mitigated when we try to re-purpose LLMs for this skill via finetuning, but we find that these models still fail to generalize -- they can only perform causal inference in in-distribution settings when variable names and textual expressions used in the queries are similar to those in the training set, but fail in out-of-distribution settings generated by perturbing these queries. Corr2Cause is a challenging task for LLMs, and would be helpful in guiding future research on improving LLMs' pure reasoning skills and generalizability. Our data is at https://huggingface.co/datasets/causalnlp/corr2cause. Our code is at https://github.com/causalNLP/corr2cause.\",\n",
       " 'Author: Mona T. Diab Title: ALERT: Adapt Language Models to Reasoning Tasks Publication year: 2023 Coauthors: Ping Yu, Tianlu Wang, O. Yu. Golovneva, Badr AlKhamissi, Siddharth Verma, Zhijing Jin, Gargi Ghosh, Mona T. Diab, Asli Celikyilmaz ',\n",
       " 'Author: Mona T. Diab Title: Care4Lang at MEDIQA-Chat 2023: Fine-tuning Language Models for Classifying and Summarizing Clinical Dialogues Publication year: 2023 Coauthors: Amal AlQahtani, R. Salama, Mona T. Diab, Abdou Youssef Abstract: Summarizing medical conversations is one of the tasks proposed by MEDIQA-Chat to promote research on automatic clinical note generation from doctor-patient conversations. In this paper, we present our submission to this task using fine-tuned language models, including T5, BART and BioGPT models. The fine-tuned models are evaluated using ensemble metrics including ROUGE, BERTScore andBLEURT. Among the fine-tuned models, Flan-T5 achieved the highest aggregated score for dialogue summarization.',\n",
       " 'Author: Mona T. Diab Title: The Troubling Emergence of Hallucination in Large Language Models - An Extensive Definition, Quantification, and Prescriptive Remediations Publication year: 2023 Coauthors: Vipula Rawte, Swagata Chakraborty, Agnibh Pathak, Anubhav Sarkar, S.M. Towhidul Islam Tonmoy, Islam Tonmoy, Aman Chadha, Amit P. Sheth, Amitava Das, Paris, A. Sridhar, Erik Visser, Improved, Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, Yunfeng Liu, Roformer, Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori Hashimoto, Stanford, Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-icz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, Susan Zhang, Stephen Roller, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher De-wan, Mona T. Diab, Xi Xian Li, Todor Victoria Lin, Myle Ott, Kurt Shuster, Punit Daniel Simig, Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer. 2022, Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom Brown, Alec Radford, Dario Amodei, Paul F. Chris-tiano Abstract: The recent advancements in Large Language Models (LLMs) have garnered widespread acclaim for their remarkable emerging capabilities. However, the issue of hallucination has parallelly emerged as a by-product, posing significant concerns. While some recent endeavors have been made to identify and mitigate different types of hallucination, there has been a limited emphasis on the nuanced categorization of hallucination and associated mitigation methods. To address this gap, we offer a fine-grained discourse on profiling hallucination based on its degree, orientation, and category, along with offering strategies for alleviation. As such, we define two overarching orientations of hallucination: (i) factual mirage (FM) and (ii) silver lining (SL). To provide a more comprehensive understanding, both orientations are further sub-categorized into intrinsic and extrinsic, with three degrees of severity - (i) mild, (ii) moderate, and (iii) alarming. We also meticulously categorize hallucination into six types: (i) acronym ambiguity, (ii) numeric nuisance, (iii) generated golem, (iv) virtual voice, (v) geographic erratum, and (vi) time wrap. Furthermore, we curate HallucInation eLiciTation (HILT), a publicly available dataset comprising of 75,000 samples generated using 15 contemporary LLMs along with human annotations for the aforementioned categories. Finally, to establish a method for quantifying and to offer a comparative spectrum that allows us to evaluate and rank LLMs based on their vulnerability to producing hallucinations, we propose Hallucination Vulnerability Index (HVI). We firmly believe that HVI holds significant value as a tool for the wider NLP community, with the potential to serve as a rubric in AI-related policy-making. In conclusion, we propose two solution strategies for mitigating hallucinations.',\n",
       " 'Author: Mona T. Diab Title: OPT-R: Exploring the Role of Explanations in Finetuning and Prompting for Reasoning Skills of Large Language Models Publication year: 2023 Coauthors: Badr AlKhamissi, Siddharth Verma, Ping Yu, Zhijing Jin, Asli Celikyilmaz, Mona T. Diab Abstract: We conduct a thorough investigation into the reasoning capabilities of Large Language Models (LLMs), focusing specifically on the Open Pretrained Transformers (OPT) models as a representative of such models. Our study entails finetuning three different sizes of OPT on a carefully curated reasoning corpus, resulting in two sets of finetuned models: OPT-R, finetuned without explanations, and OPT-RE, finetuned with explanations. We then evaluate all models on 57 out-of-domain tasks drawn from the Super-NaturalInstructions benchmark, covering 26 distinct reasoning skills, utilizing three prompting techniques. Through a comprehensive grid of 27 configurations and 6,156 test evaluations, we investigate the dimensions of finetuning, prompting, and scale to understand the role of explanations on different reasoning skills. Our findings reveal that having explanations in the fewshot exemplar has no significant impact on the model’s performance when the model is finetuned, while positively affecting the non-finetuned counterpart. Moreover, we observe a slight yet consistent increase in classification accuracy as we incorporate explanations during prompting and finetuning, respectively. Finally, we offer insights on which reasoning skills benefit the most from incorporating explanations during finetuning and prompting, such as Numerical (+20.4%) and Analogical (+13.9%) reasoning, as well as skills that exhibit negligible or negative effects.',\n",
       " 'Author: Mona T. Diab Title: Methods for Measuring, Updating, and Visualizing Factual Beliefs in Language Models Publication year: 2023 Coauthors: Peter Hase, Mona T. Diab, Asli Celikyilmaz, Xian Li, Zornitsa Kozareva, Veselin Stoyanov, Mohit Bansal, Srini Iyer Abstract: Language models can memorize a considerable amount of factual information during pretraining that can be elicited through prompting or finetuning models on tasks like question answering. In this paper, we discuss approaches to measuring model factual beliefs, updating incorrect factual beliefs in models, and visualizing graphical relationships between factual beliefs. Our main contributions include: (1) new metrics for evaluating belief-updating methods focusing on the logical consistency of beliefs, (2) a training objective for Sequential, Local, and Generalizing updates (SLAG) that improves the performance of existing hypernetwork approaches, and (3) the introduction of the belief graph, a new form of visualization for language models that shows relationships between stored model beliefs. Our experiments suggest that models show only limited consistency between factual beliefs, but update methods can both fix incorrect model beliefs and greatly improve their consistency. Although off-the-shelf optimizers are surprisingly strong belief-updating baselines, our learned optimizers can outperform them in more difficult settings than have been considered in past work.',\n",
       " 'Author: Mona T. Diab Title: Author Correction: Arabic natural language processing for Qur’anic research: a systematic review Publication year: 2023 Coauthors: M. Bashir, Aqil M. Azmi, H. Nawaz, W. Zaghouani, Mona T. Diab, Ala I. Al-Fuqaha, Junaid Qadir ',\n",
       " 'Author: Mona T. Diab Title: Evaluating Multilingual Speech Translation under Realistic Conditions with Resegmentation and Terminology Publication year: 2023 Coauthors: Elizabeth Salesky, Kareem Darwish, Mohamed Al-Badrashiny, Mona T. Diab, J. Niehues Abstract: We present the ACL 60/60 evaluation sets for multilingual translation of ACL 2022 technical presentations into 10 target languages. This dataset enables further research into multilingual speech translation under realistic recording conditions with unsegmented audio and domain-specific terminology, applying NLP tools to text and speech in the technical domain, and evaluating and improving model robustness to diverse speaker demographics.',\n",
       " 'Author: Fernando Diaz Title: Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision Publication year: 2023 Coauthors: Fernando Diaz Abstract: Across a variety of ranking tasks, researchers use reciprocal rank to measure the effectiveness for users interested in exactly one relevant item. Despite its widespread use, evidence suggests that reciprocal rank is brittle when discriminating between systems. This brittleness, in turn, is compounded in modern evaluation settings where current, high-precision systems may be difficult to distinguish. We address the lack of sensitivity of reciprocal rank by introducing and connecting it to the concept of best-case retrieval, an evaluation method focusing on assessing the quality of a ranking for the most satisfied possible user across possible recall requirements. This perspective allows us to generalize reciprocal rank and define a new preference-based evaluation we call lexicographic precision or lexiprecision. By mathematical construction, we ensure that lexiprecision preserves differences detected by reciprocal rank, while empirically improving sensitivity and robustness across a broad set of retrieval and recommendation tasks.',\n",
       " 'Author: Fernando Diaz Title: Fairness Through Domain Awareness: Mitigating Popularity Bias For Music Discovery Publication year: 2023 Coauthors: Rebecca Salganik, Fernando Diaz, G. Farnadi Abstract: As online music platforms grow, music recommender systems play a vital role in helping users navigate and discover content within their vast musical databases. At odds with this larger goal, is the presence of popularity bias, which causes algorithmic systems to favor mainstream content over, potentially more relevant, but niche items. In this work we explore the intrinsic relationship between music discovery and popularity bias. To mitigate this issue we propose a domain-aware, individual fairness-based approach which addresses popularity bias in graph neural network (GNNs) based recommender systems. Our approach uses individual fairness to reflect a ground truth listening experience, i.e., if two songs sound similar, this similarity should be reflected in their representations. In doing so, we facilitate meaningful music discovery that is robust to popularity bias and grounded in the music domain. We apply our BOOST methodology to two discovery based tasks, performing recommendations at both the playlist level and user level. Then, we ground our evaluation in the cold start setting, showing that our approach outperforms existing fairness benchmarks in both performance and recommendation of lesser-known content. Finally, our analysis explains why our proposed methodology is a novel and promising approach to mitigating popularity bias and improving the discovery of new and niche content in music recommender systems.',\n",
       " \"Author: Fernando Diaz Title: Overview of the TREC 2021 Fair Ranking Track Publication year: 2023 Coauthors: Asia J. Biega, Fernando Diaz, Michael D. Ekstrand, Sebastian Kohlmeier Abstract: The TREC Fair Ranking Track aims to provide a platform for participants to develop and evaluate novel retrieval algorithms that can provide a fair exposure to a mixture of demographics or attributes, such as ethnicity, that are represented by relevant documents in response to a search query. For example, particular demographics or attributes can be represented by the documents' topical content or authors. The 2021 Fair Ranking Track adopted a resource allocation task. The task focused on supporting Wikipedia editors who are looking to improve the encyclopedia's coverage of topics under the purview of a WikiProject. WikiProject coordinators and/or Wikipedia editors search for Wikipedia documents that are in need of editing to improve the quality of the article. The 2021 Fair Ranking track aimed to ensure that documents that are about, or somehow represent, certain protected characteristics receive a fair exposure to the Wikipedia editors, so that the documents have an fair opportunity of being improved and, therefore, be well-represented in Wikipedia. The under-representation of particular protected characteristics in Wikipedia can result in systematic biases that can have a negative human, social, and economic impact, particularly for disadvantaged or protected societal groups.\",\n",
       " \"Author: Fernando Diaz Title: Recall, Robustness, and Lexicographic Evaluation Publication year: 2023 Coauthors: Fernando Diaz, Bhaskar Mitra Abstract: Researchers use recall to evaluate rankings across a variety of retrieval, recommendation, and machine learning tasks. While there is a colloquial interpretation of recall in set-based evaluation, the research community is far from a principled understanding of recall metrics for rankings. The lack of principled understanding of or motivation for recall has resulted in criticism amongst the retrieval community that recall is useful as a measure at all. In this light, we reflect on the measurement of recall in rankings from a formal perspective. Our analysis is composed of three tenets: recall, robustness, and lexicographic evaluation. First, we formally define `recall-orientation' as sensitivity to movement of the bottom-ranked relevant item. Second, we analyze our concept of recall orientation from the perspective of robustness with respect to possible searchers and content providers. Finally, we extend this conceptual and theoretical treatment of recall by developing a practical preference-based evaluation method based on lexicographic comparison. Through extensive empirical analysis across 17 TREC tracks, we establish that our new evaluation method, lexirecall, is correlated with existing recall metrics and exhibits substantially higher discriminative power and stability in the presence of missing labels. Our conceptual, theoretical, and empirical analysis substantially deepens our understanding of recall and motivates its adoption through connections to robustness and fairness.\",\n",
       " 'Author: S. Fahlman Title: Score: A Rule Engine for the Scone Knowledge Base System Publication year: 2023 Coauthors: Jeffrey Chen, S. Fahlman Abstract: We present Score, a rule engine designed and implemented for the Scone knowledge base system. Scone is a knowledge base system designed for storing and manipulating rich representations of general knowledge in symbolic form. It represents knowledge in the form of nodes and links in a network structure, and it can perform basic inference about the relationships between different elements efficiently. On its own, Scone acts as a sort of\"smart memory\"that can interface with other software systems. One area of improvement for Scone is how useful it can be in supplying knowledge to an intelligent agent that can use the knowledge to perform actions and update the knowledge base with its observations. We augment the Scone system with a production rule engine that automatically performs simple inference based on existing and newly-added structures in Scone\\'s knowledge base, potentially improving the capabilities of any planning systems built on top of Scone. Production rule systems consist of\"if-then\"production rules that try to match their predicates to existing knowledge and fire their actions when their predicates are satisfied. We propose two kinds of production rules, if-added and if-needed rules, that differ in how they are checked and fired to cover multiple use cases. We then implement methods to efficiently check and fire these rules in a large knowledge base. The new rule engine is not meant to be a complex stand-alone planner, so we discuss how it fits into the context of Scone and future work on planning systems.',\n",
       " 'Author: Daniel Fried Title: Pragmatic Inference with a CLIP Listener for Contrastive Captioning Publication year: 2023 Coauthors: Jiefu Ou, Benno Krojer, Daniel Fried Abstract: We propose a simple yet effective and robust method for contrastive captioning: generating discriminative captions that distinguish target images from very similar alternative distractor images. Our approach is built on a pragmatic inference procedure that formulates captioning as a reference game between a speaker, which produces possible captions describing the target, and a listener, which selects the target given the caption. Unlike previous methods that derive both speaker and listener distributions from a single captioning model, we leverage an off-the-shelf CLIP model to parameterize the listener. Compared with captioner-only pragmatic models, our method benefits from rich vision language alignment representations from CLIP when reasoning over distractors. Like previous methods for discriminative captioning, our method uses a hyperparameter to control the tradeoff between the informativity (how likely captions are to allow a human listener to discriminate the target image) and the fluency of the captions. However, we find that our method is substantially more robust to the value of this hyperparameter than past methods, which allows us to automatically optimize the captions for informativity - outperforming past methods for discriminative captioning by 11% to 15% accuracy in human evaluations',\n",
       " \"Author: Daniel Fried Title: SantaCoder: don't reach for the stars! Publication year: 2023 Coauthors: Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Muñoz Ferrandis, Niklas Muennighoff, Mayank Mishra, A. Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi, J. Poirier, Hailey Schoelkopf, S. Troshin, Dmitry Abulkhanov, M. Romero, M. Lappert, F. Toni, Bernardo Garc'ia del R'io, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, I. Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, D. Lansky, Huu Nguyen, Danish Contractor, Luisa Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, S. Hughes, Daniel Fried, Arjun Guha, Harm de Vries, Leandro von Werra Abstract: The BigCode project is an open-scientific collaboration working on the responsible development of large language models for code. This tech report describes the progress of the collaboration until December 2022, outlining the current state of the Personally Identifiable Information (PII) redaction pipeline, the experiments conducted to de-risk the model architecture, and the experiments investigating better preprocessing methods for the training data. We train 1.1B parameter models on the Java, JavaScript, and Python subsets of The Stack and evaluate them on the MultiPL-E text-to-code benchmark. We find that more aggressive filtering of near-duplicates can further boost performance and, surprisingly, that selecting files from repositories with 5+ GitHub stars deteriorates performance significantly. Our best model outperforms previous open-source multilingual code generation models (InCoder-6.7B and CodeGen-Multi-2.7B) in both left-to-right generation and infilling on the Java, JavaScript, and Python portions of MultiPL-E, despite being a substantially smaller model. All models are released under an OpenRAIL license at https://hf.co/bigcode.\",\n",
       " 'Author: Daniel Fried Title: Grounding Language Models to Images for Multimodal Generation Publication year: 2023 Coauthors: Jing Yu Koh, R. Salakhutdinov, Daniel Fried Abstract: We propose an efﬁcient method to ground pre-trained text-only language models to the visual domain, enabling them to process and generate arbitrarily interleaved image-and-text data. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and ﬁnetune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text interleaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contextual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an ef-fective, general solution for leveraging pretrained language models in visually grounded settings.',\n",
       " 'Author: Daniel Fried Title: StarCoder: may the source be with you! Publication year: 2023 Coauthors: Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, J. Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, J. Stillerman, S. Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, N. Fahmy, Urvashi Bhattacharyya, W. Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, M. Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jana Ebert, Tri Dao, Mayank Mishra, A. Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, Sean M. Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, Harm de Vries Abstract: The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process. We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python, can be prompted to achieve 40\\\\% pass@1 on HumanEval, and still retains its performance on other programming languages. We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license.',\n",
       " 'Author: Daniel Fried Title: Grounding Language Models to Images for Multimodal Inputs and Outputs Publication year: 2023 Coauthors: Jing Yu Koh, R. Salakhutdinov, Daniel Fried Abstract: We propose an efficient method to ground pretrained text-only language models to the visual domain, enabling them to process arbitrarily interleaved image-and-text data, and generate text interleaved with retrieved images. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and finetune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text interleaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contextual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained language models in visually grounded settings.',\n",
       " 'Author: Daniel Fried Title: Generating Images with Multimodal Language Models Publication year: 2023 Coauthors: Jing Yu Koh, Daniel Fried, R. Salakhutdinov Abstract: We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespecified dataset, and decides whether to retrieve or generate at inference time. This is done with a learnt decision module which conditions on the hidden representations of the LLM. Our model exhibits a wider range of capabilities compared to prior multimodal language models. It can process image-and-text inputs, and produce retrieved images, generated images, and generated text -- outperforming non-LLM based generation models across several text-to-image tasks that measure context dependence.',\n",
       " 'Author: Daniel Fried Title: WebArena: A Realistic Web Environment for Building Autonomous Agents Publication year: 2023 Coauthors: Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig Abstract: With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.',\n",
       " \"Author: Daniel Fried Title: Amortizing Pragmatic Program Synthesis with Rankings Publication year: 2023 Coauthors: Yewen Pu, Saujas Vaduguru, Priyan Vaithilingam, Elena L. Glassman, Daniel Fried Abstract: In program synthesis, an intelligent system takes in a set of user-generated examples and returns a program that is logically consistent with these examples. The usage of Rational Speech Acts (RSA) framework has been successful in building \\\\emph{pragmatic} program synthesizers that return programs which -- in addition to being logically consistent -- account for the fact that a user chooses their examples informatively. However, the computational burden of running the RSA algorithm has restricted the application of pragmatic program synthesis to domains with a small number of possible programs. This work presents a novel method of amortizing the RSA algorithm by leveraging a \\\\emph{global pragmatic ranking} -- a single, total ordering of all the hypotheses. We prove that for a pragmatic synthesizer that uses a single demonstration, our global ranking method exactly replicates RSA's ranked responses. We further empirically show that global rankings effectively approximate the full pragmatic synthesizer in an online, multi-demonstration setting. Experiments on two program synthesis domains using our pragmatic ranking method resulted in orders of magnitudes of speed ups compared to the RSA synthesizer, while outperforming the standard, non-pragmatic synthesizer.\",\n",
       " 'Author: Daniel Fried Title: Symbolic Planning and Code Generation for Grounded Dialogue Publication year: 2023 Coauthors: Justin T Chiu, Wenting Zhao, Derek Chen, Saujas Vaduguru, Alexander M. Rush, Daniel Fried Abstract: Large language models (LLMs) excel at processing and generating both text and code. However, LLMs have had limited applicability in grounded task-oriented dialogue as they are difficult to steer toward task objectives and fail to handle novel grounding. We present a modular and interpretable grounded dialogue system that addresses these shortcomings by composing LLMs with a symbolic planner and grounded code execution. Our system consists of a reader and planner: the reader leverages an LLM to convert partner utterances into executable code, calling functions that perform grounding. The translated code’s output is stored to track dialogue state, while a symbolic planner determines the next appropriate response. We evaluate our system’s performance on the demanding OneCommon dialogue task, involving collaborative reference resolution on abstract images of scattered dots. Our system substantially outperforms the previous state-of-the-art, including improving task success in human evaluations from 56% to 69% in the most challenging setting.',\n",
       " \"Author: Daniel Fried Title: Generating Pragmatic Examples to Train Neural Program Synthesizers Publication year: 2023 Coauthors: Saujas Vaduguru, Daniel Fried, Yewen Pu Abstract: Programming-by-example is the task of synthesizing a program that is consistent with a set of user-provided input-output examples. As examples are often an under-specification of one's intent, a good synthesizer must choose the intended program from the many that are consistent with the given set of examples. Prior work frames program synthesis as a cooperative game between a listener (that synthesizes programs) and a speaker (a user choosing examples), and shows that models of computational pragmatic inference are effective in choosing the user intended programs. However, these models require counterfactual reasoning over a large set of programs and examples, which is infeasible in realistic program spaces. In this paper, we propose a novel way to amortize this search with neural networks. We sample pairs of programs and examples via self-play between listener and speaker models, and use pragmatic inference to choose informative training examples from this sample.We then use the informative dataset to train models to improve the synthesizer's ability to disambiguate user-provided examples without human supervision. We validate our method on the challenging task of synthesizing regular expressions from example strings, and find that our method (1) outperforms models trained without choosing pragmatic examples by 23% (a 51% relative increase) (2) matches the performance of supervised learning on a dataset of pragmatic examples provided by humans, despite using no human data in training.\",\n",
       " 'Author: Daniel Fried Title: Asking More Informative Questions for Grounded Retrieval Publication year: 2023 Coauthors: Sedrick Scott Keh, Justin T. Chiu, Daniel Fried Abstract: When a model is trying to gather information in an interactive setting, it benefits from asking informative questions. However, in the case of a grounded multi-turn image identification task, previous studies have been constrained to polar yes/no questions, limiting how much information the model can gain in a single turn. We present an approach that formulates more informative, open-ended questions. In doing so, we discover that off-the-shelf visual question answering (VQA) models often make presupposition errors, which standard information gain question selection methods fail to account for. To address this issue, we propose a method that can incorporate presupposition handling into both question selection and belief updates. Specifically, we use a two-stage process, where the model first filters out images which are irrelevant to a given question, then updates its beliefs about which image the user intends. Through self-play and human evaluations, we show that our method is successful in asking informative open-ended questions, increasing accuracy over the past state-of-the-art by 14%, while resulting in 48% more efficient games in human evaluations.',\n",
       " 'Author: Daniel Fried Title: VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks Publication year: 2024 Coauthors: Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, Daniel Fried Abstract: Autonomous agents capable of planning, reasoning, and executing actions on the web offer a promising avenue for automating computer tasks. However, the majority of existing benchmarks primarily focus on text-based agents, neglecting many natural tasks that require visual information to effectively solve. Given that most computer interfaces cater to human perception, visual information often augments textual data in ways that text-only models struggle to harness effectively. To bridge this gap, we introduce VisualWebArena, a benchmark designed to assess the performance of multimodal web agents on realistic \\\\textit{visually grounded tasks}. VisualWebArena comprises of a set of diverse and complex web-based tasks that evaluate various capabilities of autonomous multimodal agents. To perform on this benchmark, agents need to accurately process image-text inputs, interpret natural language instructions, and execute actions on websites to accomplish user-defined objectives. We conduct an extensive evaluation of state-of-the-art LLM-based autonomous agents, including several multimodal models. Through extensive quantitative and qualitative analysis, we identify several limitations of text-only LLM agents, and reveal gaps in the capabilities of state-of-the-art multimodal language agents. VisualWebArena provides a framework for evaluating multimodal autonomous language agents, and offers insights towards building stronger autonomous agents for the web. Our code, baseline models, and data is publicly available at https://jykoh.com/vwa.',\n",
       " \"Author: Daniel Fried Title: Comparative Knowledge Distillation Publication year: 2023 Coauthors: Alex Wilf, Alex Tianyi Xu, P. Liang, A. Obolenskiy, Daniel Fried, Louis-Philippe Morency Abstract: In the era of large scale pretrained models, Knowledge Distillation (KD) serves an important role in transferring the wisdom of computationally heavy teacher models to lightweight, efficient student models while preserving performance. Traditional KD paradigms, however, assume readily available access to teacher models for frequent inference -- a notion increasingly at odds with the realities of costly, often proprietary, large scale models. Addressing this gap, our paper considers how to minimize the dependency on teacher model inferences in KD in a setting we term Few Teacher Inference Knowledge Distillation (FTI KD). We observe that prevalent KD techniques and state of the art data augmentation strategies fall short in this constrained setting. Drawing inspiration from educational principles that emphasize learning through comparison, we propose Comparative Knowledge Distillation (CKD), which encourages student models to understand the nuanced differences in a teacher model's interpretations of samples. Critically, CKD provides additional learning signals to the student without making additional teacher calls. We also extend the principle of CKD to groups of samples, enabling even more efficient learning from limited teacher calls. Empirical evaluation across varied experimental settings indicates that CKD consistently outperforms state of the art data augmentation and KD techniques.\",\n",
       " \"Author: Daniel Fried Title: SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents Publication year: 2023 Coauthors: Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, Maarten Sap Abstract: Humans are social beings; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and interact under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills. These findings demonstrate SOTOPIA's promise as a general platform for research on evaluating and improving social intelligence in artificial agents.\",\n",
       " 'Author: Daniel Fried Title: TroVE: Inducing Verifiable and Efficient Toolboxes for Solving Programmatic Tasks Publication year: 2024 Coauthors: Zhiruo Wang, Daniel Fried, Graham Neubig Abstract: Language models (LMs) can solve tasks such as answering questions about tables or images by writing programs. However, using primitive functions often leads to verbose and error-prone programs, and higher-level functions require expert design. To enable better solutions without human labor, we ask code LMs to curate reusable high-level functions, and use them to write solutions. We present TROVE, a training-free method of inducing a verifiable and efficient toolbox of functions, by generating via using, growing, and periodically trimming the toolbox. On 11 datasets from math, table question answering, and image reasoning tasks, TROVE consistently yields simpler solutions with higher accuracy than baselines using CODELLAMA and previous methods using GPT, while using 79-98% smaller toolboxes. TROVE further enables 31% faster and 13% more accurate human verification than baselines. With the same pipeline, it creates diverse functions for varied tasks and datasets, providing insights into their individual characteristics.',\n",
       " 'Author: Daniel Fried Title: API-Assisted Code Generation for Question Answering on Varied Table Structures Publication year: 2023 Coauthors: Yihan Cao, Shuyi Chen, Ryan Liu, Zhiruo Wang, Daniel Fried Abstract: A persistent challenge to table question answering (TableQA) by generating executable programs has been adapting to varied table structures, typically requiring domain-specific logical forms. In response, this paper introduces a unified TableQA framework that: (1) provides a unified representation for structured tables as multi-index Pandas data frames, (2) uses Python as a powerful querying language, and (3) uses few-shot prompting to translate NL questions into Python programs, which are executable on Pandas data frames. Furthermore, to answer complex relational questions with extended program functionality and external knowledge, our framework allows customized APIs that Python programs can call. We experiment with four TableQA datasets that involve tables of different structures -- relational, multi-table, and hierarchical matrix shapes -- and achieve prominent improvements over past state-of-the-art systems. In ablation studies, we (1) show benefits from our multi-index representation and APIs over baselines that use only an LLM, and (2) demonstrate that our approach is modular and can incorporate additional APIs.',\n",
       " 'Author: Daniel Fried Title: AutoReply: Detecting Nonsense in Dialogue with Discriminative Replies Publication year: 2023 Coauthors: Weiyan Shi, Emily Dinan, Adi Renduchintala, Daniel Fried, Athul Paul Jacob, Zhou Yu, Mike Lewis ',\n",
       " 'Author: Daniel Fried Title: Data Augmentation for Code Translation with Comparable Corpora and Multiple References Publication year: 2023 Coauthors: Yiqing Xie, Atharva Naik, Daniel Fried, Carolyn Rose Abstract: One major challenge of translating code between programming languages is that parallel training data is often limited. To overcome this challenge, we present two data augmentation techniques, one that builds comparable corpora (i.e., code pairs with similar functionality), and another that augments existing parallel data with multiple reference translations. Specifically, we build and analyze multiple types of comparable corpora, including programs generated from natural language documentation using a code generation model. Furthermore, to reduce overfitting to a single reference translation, we automatically generate additional translation references for available parallel data and filter the translations by unit tests, which increases variation in target translations. Experiments show that our data augmentation techniques significantly improve CodeT5 for translation between Java, Python, and C++ by an average of 7.5% Computational Accuracy (CA@1), which verifies the correctness of translations by execution. The code is available at https://github.com/Veronicium/CMTrans.',\n",
       " 'Author: A. Gershman Title: The DARPA Wikidata Overlay: Wikidata as an ontology for natural language processing Publication year: 2023 Coauthors: Elizabeth Spaulding, Kathryn Conger, A. Gershman, Rosario Uceda-Sosa, S. Brown, James Pustejovsky, Peter Anick, Martha Palmer Abstract: With 102,530,067 items currently in its crowd-sourced knowledge base, Wikidata provides NLP practitioners a unique and powerful resource for inference and reasoning over real-world entities. However, because Wikidata is very entity focused, events and actions are often labeled with eventive nouns (e.g., the process of diagnosing a person’s illness is labeled “diagnosis”), and the typical participants in an event are not described or linked to that event concept (e.g., the medical professional or patient). Motivated by a need for an adaptable, comprehensive, domain-flexible ontology for information extraction, including identifying the roles entities are playing in an event, we present a curated subset of Wikidata in which events have been enriched with PropBank roles. To enable richer narrative understanding between events from Wikidata concepts, we have also provided a comprehensive mapping from temporal Qnodes and Pnodes to the Allen Interval Temporal Logic relations.',\n",
       " 'Author: Alexander Hauptmann Title: Leveraging body pose estimation for gesture recognition in human-robot interaction using synthetic data Publication year: 2023 Coauthors: Xiaoyu Zhu, Celso de Melo, Alexander Hauptmann Abstract: Effectively recognizing human gestures from variant viewpoints plays a fundamental role in the successful collaboration between humans and robots. Deep learning approaches have achieved promising performance in gesture recognition. However, they are usually data-hungry and require large-scale labeled data, which are not usually accessible in a practical setting. Synthetic data, on the other hand, can be easily obtained from simulators with fine-grained annotations and variant modalities. Existing state-of-the-art approaches have shown promising results using synthetic data, but there is still a large performance gap between the models trained on synthetic data and real data. To learn domain-invariant feature representations, we propose a novel approach which jointly takes RGB videos and 3D meshes as inputs to perform robust action recognition. We empirically validate our model on the RoCoG-v2 dataset, which consists of a variety of real and synthetic videos of gestures from the ground and air perspectives. We show that our model trained on synthetic data can outperform state-of-the-art models under the same training setting and models trained on real data.',\n",
       " 'Author: Alexander Hauptmann Title: Towards Open-Domain Twitter User Profile Inference Publication year: 2023 Coauthors: Haoyang Wen, Zhenxin Xiao, E. Hovy, Alexander Hauptmann Abstract: ,',\n",
       " 'Author: A. Hauptmann Title: Zero-Shot and Few-Shot Stance Detection on Varied Topics via Conditional Generation Publication year: 2023 Coauthors: Haoyang Wen, A. Hauptmann Abstract: Zero-shot and few-shot stance detection identify the polarity of text with regard to a certain target when we have only limited or no training resources for the target. Previous work generally formulates the problem into a classification setting, ignoring the potential use of label text. In this paper, we instead utilize a conditional generation framework and formulate the problem as denoising from partially-filled templates, which can better utilize the semantics among input, label, and target texts. We further propose to jointly train an auxiliary task, target prediction, and to incorporate manually constructed incorrect samples with unlikelihood training to improve the representations for both target and label texts. We also verify the effectiveness of target-related Wikipedia knowledge with the generation framework. Experiments show that our proposed method significantly outperforms several strong baselines on VAST, and achieves new state-of-the-art performance.',\n",
       " \"Author: A. Hauptmann Title: SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs Publication year: 2023 Coauthors: Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey, Yanping Huang, David A. Ross, Irfan Essa, Yonatan Bisk, Ming Yang, K. Murphy, A. Hauptmann, Lu Jiang Abstract: In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos. SPAE converts between raw pixels and interpretable lexical tokens (or words) extracted from the LLM's vocabulary. The resulting tokens capture both the semantic meaning and the fine-grained details needed for visual reconstruction, effectively translating the visual content into a language comprehensible to the LLM, and empowering it to perform a wide array of multimodal tasks. Our approach is validated through in-context learning experiments with frozen PaLM 2 and GPT 3.5 on a diverse set of image understanding and generation tasks. Our method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%.\",\n",
       " 'Author: A. Hauptmann Title: STMT: A Spatial-Temporal Mesh Transformer for MoCap-Based Action Recognition Publication year: 2023 Coauthors: Xiaoyu Zhu, Po-Yao (Bernie) Huang, Junwei Liang, Celso M. de Melo, A. Hauptmann Abstract: We study the problem of human action recognition using motion capture (MoCap) sequences. Unlike existing techniques that take multiple manual steps to derive standard-ized skeleton representations as model input, we propose a novel Spatial-Temporal Mesh Transformer (STMT) to directly model the mesh sequences. The model uses a hierarchical transformer with intra-frame off-set attention and inter-frame self-attention. The attention mechanism allows the model to freely attend between any two vertex patches to learn nonlocal relationships in the spatial-temporal domain. Masked vertex modeling and future frame prediction are used as two self-supervised tasks to fully activate the bi-directional and auto-regressive attention in our hierarchical transformer. The proposed method achieves state-of-the-art performance compared to skeleton-based and point-cloud-based models on common MoCap benchmarks. Code is available at https://github.com/zgzxy001/STMT.',\n",
       " 'Author: A. Hauptmann Title: ChartReader: A Unified Framework for Chart Derendering and Comprehension without Heuristic Rules Publication year: 2023 Coauthors: Zhi-Qi Cheng, Qianwen Dai, Siyao Li, Jingdong Sun, T. Mitamura, A. Hauptmann Abstract: Charts are a powerful tool for visually conveying complex data, but their comprehension poses a challenge due to the diverse chart types and intricate components. Existing chart comprehension methods suffer from either heuristic rules or an over-reliance on OCR systems, resulting in suboptimal performance. To address these issues, we present ChartReader, a unified framework that seamlessly integrates chart derendering and comprehension tasks. Our approach includes a transformer-based chart component detection module and an extended pre-trained vision-language model for chart-to-X tasks. By learning the rules of charts automatically from annotated datasets, our approach eliminates the need for manual rule-making, reducing effort and enhancing accuracy. We also introduce a data variable replacement technique and extend the input and position embeddings of the pre-trained model for cross-task training. We evaluate ChartReader on Chart-to-Table, ChartQA, and Chart-to-Text tasks, demonstrating its superiority over existing methods. Our proposed framework can significantly reduce the manual effort involved in chart analysis, providing a step towards a universal chart understanding model. Moreover, our approach offers opportunities for plug-and-play integration with mainstream LLMs such as T5 and TaPas, extending their capability to chart comprehension tasks.1',\n",
       " 'Author: A. Hauptmann Title: DocumentNet: Bridging the Data Gap in Document Pre-training Publication year: 2023 Coauthors: Lijun Yu, Jin Miao, Xiaoyu Sun, Jiayi Chen, A. Hauptmann, H. Dai, Wei Wei Abstract: Document understanding tasks, in particular, Visually-rich Document Entity Retrieval (VDER), have gained significant attention in recent years thanks to their broad applications in enterprise AI. However, publicly available data have been scarce for these tasks due to strict privacy constraints and high annotation costs. To make things worse, the non-overlapping entity spaces from different datasets hinder the knowledge transfer between document types. In this paper, we propose a method to collect massive-scale and weakly labeled data from the web to benefit the training of VDER models. The collected dataset, named DocumentNet, does not depend on specific document types or entity sets, making it universally applicable to all VDER tasks. The current DocumentNet consists of 30M documents spanning nearly 400 document types organized in a four-level ontology. Experiments on a set of broadly adopted VDER tasks show significant improvements when DocumentNet is incorporated into the pre-training for both classic and few-shot learning settings. With the recent emergence of large language models (LLMs), DocumentNet provides a large data source to extend their multi-modal capabilities for VDER.',\n",
       " \"Author: Alexander Hauptmann Title: Hyperbolic vs Euclidean Embeddings in Few-Shot Learning: Two Sides of the Same Coin Publication year: 2023 Coauthors: Gabriel Moreira, Manuel Marques, J. Costeira, Alexander Hauptmann Abstract: Recent research in representation learning has shown that hierarchical data lends itself to low-dimensional and highly informative representations in hyperbolic space. However, even if hyperbolic embeddings have gathered attention in image recognition, their optimization is prone to numerical hurdles. Further, it remains unclear which applications stand to benefit the most from the implicit bias imposed by hyperbolicity, when compared to traditional Euclidean features. In this paper, we focus on prototypical hyperbolic neural networks. In particular, the tendency of hyperbolic embeddings to converge to the boundary of the Poincar\\\\'e ball in high dimensions and the effect this has on few-shot classification. We show that the best few-shot results are attained for hyperbolic embeddings at a common hyperbolic radius. In contrast to prior benchmark results, we demonstrate that better performance can be achieved by a fixed-radius encoder equipped with the Euclidean metric, regardless of the embedding dimension.\",\n",
       " 'Author: Alexander G. Hauptmann Title: Breaking The Limits of Text-conditioned 3D Motion Synthesis with Elaborative Descriptions Publication year: 2023 Coauthors: Yijun Qian, Jack Urbanek, Alexander G. Hauptmann, Jungdam Won Abstract: Given its wide applications, there is increasing focus on generating 3D human motions from textual descriptions. Differing from the majority of previous works, which regard actions as single entities and can only generate short sequences for simple motions, we propose EMS, an elaborative motion synthesis model conditioned on detailed natural language descriptions. It generates natural and smooth motion sequences for long and complicated actions by factorizing them into groups of atomic actions. Meanwhile, it understands atomic-action level attributes (e.g., motion direction, speed, and body parts) and enables users to generate sequences of unseen complex actions from unique sequences of known atomic actions with independent attribute settings and timings applied. We evaluate our method on the KIT Motion-Language and BABEL benchmarks, where it outperforms all previous state-of-the-art with noticeable margins.',\n",
       " 'Author: Daphne Ippolito Title: Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System Publication year: 2023 Coauthors: Daphne Ippolito, Nicholas Carlini, Katherine Lee, Milad Nasr, Yun William Yu Abstract: Neural language models are increasingly deployed into APIs and websites that allow a user to pass in a prompt and receive generated text. Many of these systems do not reveal generation parameters. In this paper, we present methods to reverse-engineer the decoding method used to generate text (i.e., top-_k_ or nucleus sampling). Our ability to discover which decoding strategy was used has implications for detecting generated text. Additionally, the process of discovering the decoding strategy can reveal biases caused by selecting decoding settings which severely truncate a model’s predicted distributions. We perform our attack on several families of open-source language models, as well as on production systems (e.g., ChatGPT).',\n",
       " \"Author: Daphne Ippolito Title: A Pretrainer's Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity Publication year: 2023 Coauthors: S. Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph, Denny Zhou, Jason Wei, Kevin Robinson, David M. Mimno, Daphne Ippolito Abstract: Pretraining is the preliminary and fundamental step in developing capable language models (LM). Despite this, pretraining data design is critically under-documented and often guided by empirically unsupported intuitions. To address this, we pretrain 28 1.5B parameter decoder-only models, training on data curated (1) at different times, (2) with varying toxicity and quality filters, and (3) with different domain compositions. First, we quantify the effect of pretraining data age. A temporal shift between evaluation data and pretraining data leads to performance degradation, which is not overcome by finetuning. Second, we explore the effect of quality and toxicity filters, showing a trade-off between performance on standard benchmarks and risk of toxic generations. Our findings indicate there does not exist a one-size-fits-all solution to filtering training data. We also find that the effects of different types of filtering are not predictable from text domain characteristics. Lastly, we empirically validate that the inclusion of heterogeneous data sources, like books and web, is broadly beneficial and warrants greater prioritization. These findings constitute the largest set of experiments to validate, quantify, and expose many undocumented intuitions about text pretraining, which we hope will help support more informed data-centric decisions in LM development.\",\n",
       " 'Author: Daphne Ippolito Title: Extracting Training Data from Diffusion Models Publication year: 2023 Coauthors: Nicholas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramèr, B. Balle, Daphne Ippolito, Eric Wallace Abstract: Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training.',\n",
       " 'Author: Daphne Ippolito Title: Report of the 1st Workshop on Generative AI and Law Publication year: 2023 Coauthors: A. F. Cooper, Katherine Lee, James Grimmelmann, Daphne Ippolito, Christopher Callison-Burch, Christopher A. Choquette-Choo, Niloofar Mireshghallah, Miles Brundage, David Mimno, M. Choksi, J. Balkin, Nicholas Carlini, Christopher De Sa, Jonathan Frankle, Deep Ganguli, Bryant Gipson, A. Guadamuz, Swee Leng Harris, Abigail Z. Jacobs, Elizabeth Joh, Gautam Kamath, M. Lemley, Cass Matthews, C. McLeavey, Corynne Mcsherry, Milad Nasr, Paul Ohm, Adam Roberts, Tom Rubin, Pamela Samuelson, Ludwig Schubert, Kristen Vaccaro, Luis Villa, Felix Wu, Elana Zeide Abstract: This report presents the takeaways of the inaugural Workshop on Generative AI and Law (GenLaw), held in July 2023. A cross-disciplinary group of practitioners and scholars from computer science and law convened to discuss the technical, doctrinal, and policy challenges presented by law for Generative AI, and by Generative AI for law, with an emphasis on U.S. law in particular. We begin the report with a high-level statement about why Generative AI is both immensely significant and immensely challenging for law. To meet these challenges, we conclude that there is an essential need for 1) a shared knowledge base that provides a common conceptual language for experts across disciplines; 2) clarification of the distinctive technical capabilities of generative-AI systems, as compared and contrasted to other computer and AI systems; 3) a logical taxonomy of the legal issues these systems raise; and, 4) a concrete research agenda to promote collaboration and knowledge-sharing on emerging issues at the intersection of Generative AI and law. In this report, we synthesize the key takeaways from the GenLaw workshop that begin to address these needs. All of the listed authors contributed to the workshop upon which this report is based, but they and their organizations do not necessarily endorse all of the specific claims in this report.',\n",
       " 'Author: Daphne Ippolito Title: Are aligned neural networks adversarially aligned? Publication year: 2023 Coauthors: Nicholas Carlini, Milad Nasr, Christopher A. Choquette-Choo, Matthew Jagielski, Irena Gao, Anas Awadalla, Pang Wei Koh, Daphne Ippolito, Katherine Lee, Florian Tramèr, Ludwig Schmidt Abstract: Large language models are now tuned to align with the goals of their creators, namely to be\"helpful and harmless.\"These models should respond helpfully to user questions, but refuse to answer requests that could cause harm. However, adversarial users can construct inputs which circumvent attempts at alignment. In this work, we study to what extent these models remain aligned, even when interacting with an adversarial user who constructs worst-case inputs (adversarial examples). These inputs are designed to cause the model to emit harmful content that would otherwise be prohibited. We show that existing NLP-based optimization attacks are insufficiently powerful to reliably attack aligned text models: even when current NLP-based attacks fail, we can find adversarial inputs with brute force. As a result, the failure of current attacks should not be seen as proof that aligned text models remain aligned under adversarial inputs. However the recent trend in large-scale ML models is multimodal models that allow users to provide images that influence the text that is generated. We show these models can be easily attacked, i.e., induced to perform arbitrary un-aligned behavior through adversarial perturbation of the input image. We conjecture that improved NLP attacks may demonstrate this same level of adversarial control over text-only models.',\n",
       " \"Author: Daphne Ippolito Title: Effective Prompt Extraction from Language Models Publication year: 2023 Coauthors: Yiming Zhang, Daphne Ippolito Abstract: The text generated by large language models is commonly controlled by prompting, where a prompt prepended to a user's query guides the model's output. The prompts used by companies to guide their models are often treated as secrets, to be hidden from the user making the query. They have even been treated as commodities to be bought and sold. However, anecdotal reports have shown adversarial users employing prompt extraction attacks to recover these prompts. In this paper, we present a framework for systematically measuring the effectiveness of these attacks. In experiments with 3 different sources of prompts and 11 underlying large language models, we find that simple text-based attacks can in fact reveal prompts with high probability. Our framework determines with high precision whether an extracted prompt is the actual secret prompt, rather than a model hallucination. Prompt extraction experiments on real systems such as Bing Chat and ChatGPT suggest that system prompts can be revealed by an adversary despite existing defenses in place.\",\n",
       " 'Author: Daphne Ippolito Title: Scalable Extraction of Training Data from (Production) Language Models Publication year: 2023 Coauthors: Milad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A. F. Cooper, Daphne Ippolito, Christopher A. Choquette-Choo, Eric Wallace, Florian Tramèr, Katherine Lee Abstract: This paper studies extractable memorization: training data that an adversary can efficiently extract by querying a machine learning model without prior knowledge of the training dataset. We show an adversary can extract gigabytes of training data from open-source language models like Pythia or GPT-Neo, semi-open models like LLaMA or Falcon, and closed models like ChatGPT. Existing techniques from the literature suffice to attack unaligned models; in order to attack the aligned ChatGPT, we develop a new divergence attack that causes the model to diverge from its chatbot-style generations and emit training data at a rate 150x higher than when behaving properly. Our methods show practical attacks can recover far more data than previously thought, and reveal that current alignment techniques do not eliminate memorization.',\n",
       " 'Author: Li Lei Title: Long-tailed Extreme Multi-label Text Classification by the Retrieval of Generated Pseudo Label Descriptions Publication year: 2023 Coauthors: Ruohong Zhang, Yau-Shian Wang, Yiming Yang, Donghan Yu, Tom Vu, Li Lei Abstract: Extreme Multi-label Text Classification (XMTC) has been a tough challenge in machine learning research and applications due to the sheer sizes of the label spaces and the severe data scarcity problem associated with the long tail of rare labels in highly skewed distributions. This paper addresses the challenge of tail label prediction by leveraging the power of dense neural retrieval model in mapping input documents (as queries) to relevant label descriptions. To further enhance the quality of label descriptions, we propose to generate pseudo label descriptions from a trained bag-of-words (BoW) classifier, which demonstrates better classification performance under severe scarce data conditions.The proposed approach achieves the state-of-the-art (SOTA) performance of overall label prediction on XMTC benchmark datasets and especially outperforms the SOTA models in the tail label prediction. We also provide a theoretical analysis for relating the BoW and neural models w.r.t. performance lower bound.',\n",
       " 'Author: T. Mitamura Title: Language-Agnostic Transformers and Assessing ChatGPT-Based Query Rewriting for Multilingual Document-Grounded QA Publication year: 2023 Coauthors: Srinivas Gowriraj, Soham Dinesh Tiwari, Mitali Potnis, Srijan Bansal, T. Mitamura, Eric Nyberg Abstract: The DialDoc 2023 shared task has expanded the document-grounded dialogue task to encompass multiple languages, despite having limited annotated data. This paper assesses the effectiveness of both language-agnostic and language-aware paradigms for multilingual pre-trained transformer models in a bi-encoder-based dense passage retriever (DPR), concluding that the language-agnostic approach is superior. Additionally, the study investigates the impact of query rewriting techniques using large language models, such as ChatGPT, on multilingual, document-grounded question-answering systems. The experiments conducted demonstrate that, for the examples examined, query rewriting does not enhance performance compared to the original queries. This failure is due to topic switching in final dialogue turns and irrelevant topics being considered for query rewriting.',\n",
       " 'Author: T. Mitamura Title: ChartReader: A Unified Framework for Chart Derendering and Comprehension without Heuristic Rules Publication year: 2023 Coauthors: Zhi-Qi Cheng, Qianwen Dai, Siyao Li, Jingdong Sun, T. Mitamura, A. Hauptmann Abstract: Charts are a powerful tool for visually conveying complex data, but their comprehension poses a challenge due to the diverse chart types and intricate components. Existing chart comprehension methods suffer from either heuristic rules or an over-reliance on OCR systems, resulting in suboptimal performance. To address these issues, we present ChartReader, a unified framework that seamlessly integrates chart derendering and comprehension tasks. Our approach includes a transformer-based chart component detection module and an extended pre-trained vision-language model for chart-to-X tasks. By learning the rules of charts automatically from annotated datasets, our approach eliminates the need for manual rule-making, reducing effort and enhancing accuracy. We also introduce a data variable replacement technique and extend the input and position embeddings of the pre-trained model for cross-task training. We evaluate ChartReader on Chart-to-Table, ChartQA, and Chart-to-Text tasks, demonstrating its superiority over existing methods. Our proposed framework can significantly reduce the manual effort involved in chart analysis, providing a step towards a universal chart understanding model. Moreover, our approach offers opportunities for plug-and-play integration with mainstream LLMs such as T5 and TaPas, extending their capability to chart comprehension tasks.1',\n",
       " \"Author: T. Mitamura Title: Hierarchical Event Grounding Publication year: 2023 Coauthors: Jiefu Ou, Adithya Pratapa, Rishubh Gupta, T. Mitamura Abstract: Event grounding aims at linking mention references in text corpora to events from a knowledge base (KB). Previous work on this task focused primarily on linking to a single KB event, thereby overlooking the hierarchical aspects of events. Events in documents are typically described at various levels of spatio-temporal granularity. These hierarchical relations are utilized in downstream tasks of narrative understanding and schema construction. In this work, we present an extension to the event grounding task that requires tackling hierarchical event structures from the KB. Our proposed task involves linking a mention reference to a set of event labels from a subevent hierarchy in the KB. We propose a retrieval methodology that leverages event hierarchy through an auxiliary hierarchical loss. On an automatically created multilingual dataset from Wikipedia and Wikidata, our experiments demonstrate the effectiveness of the hierarchical loss against retrieve and re-rank baselines. Furthermore, we demonstrate the systems' ability to aid hierarchical discovery among unseen events. Code is available at https://github.com/JefferyO/Hierarchical-Event-Grounding\",\n",
       " 'Author: Louis-Philippe Morency Title: Quantifying&Modeling Multimodal Interactions: An Information Decomposition Framework Publication year: 2023 Coauthors: P. Liang, Yun Cheng, Xiang Fan, Chun Kai Ling, Suzanne Nie, Richard J. Chen, Zihao Deng, Nicholas Allen, R. Auerbach, Faisal Mahmood, R. Salakhutdinov, Louis-Philippe Morency Abstract: The recent explosion of interest in multimodal applications has resulted in a wide selection of datasets and methods for representing and integrating information from different modalities. Despite these empirical advances, there remain fundamental research questions: How can we quantify the interactions that are necessary to solve a multimodal task? Subsequently, what are the most suitable multimodal models to capture these interactions? To answer these questions, we propose an information-theoretic approach to quantify the degree of redundancy, uniqueness, and synergy relating input modalities with an output task. We term these three measures as the PID statistics of a multimodal distribution (or PID for short), and introduce two new estimators for these PID statistics that scale to high-dimensional distributions. To validate PID estimation, we conduct extensive experiments on both synthetic datasets where the PID is known and on large-scale multimodal benchmarks where PID estimations are compared with human annotations. Finally, we demonstrate their usefulness in (1) quantifying interactions within multimodal datasets, (2) quantifying interactions captured by multimodal models, (3) principled approaches for model selection, and (4) three real-world case studies engaging with domain experts in pathology, mood prediction, and robotic perception where our framework helps to recommend strong multimodal models for each application.',\n",
       " 'Author: Louis-Philippe Morency Title: Quantifying & Modeling Feature Interactions: An Information Decomposition Framework Publication year: 2023 Coauthors: P. Liang, Yun Cheng, Xiang Fan, Chun Kai Ling, Suzanne Nie, Richard J. Chen, Zihao Deng, Faisal Mahmood, R. Salakhutdinov, Louis-Philippe Morency Abstract: The recent explosion of interest in multimodal applications has resulted in a wide selection of datasets and methods for representing and inte-grating information from different signals. Despite these empirical advances, there remain fundamental research questions: how can we quantify the nature of interactions that exist among input features? Subsequently, how can we capture these interactions using suitable data-driven methods? To answer this question, we propose an information-theoretic approach to quantify the degree of redundancy , uniqueness , and synergy across input features, which we term the PID statistics of a multimodal distribution. Using 2 newly proposed estimators that scale to high-dimensional distributions, we demonstrate their usefulness in quantifying the interactions within multimodal datasets, the nature of interactions captured by multimodal models, and principled approaches for model selection. We conduct extensive experiments on both synthetic datasets where the PID statistics are known and on large-scale multimodal benchmarks where PID estimation was previously impossible. Finally, to demonstrate the real-world applicability of our approach, we present three case studies in pathology, mood prediction, and robotic perception where our framework accurately recommends strong multimodal models for each application.',\n",
       " 'Author: Louis-Philippe Morency Title: Think Twice: Perspective-Taking Improves Large Language Models\\' Theory-of-Mind Capabilities Publication year: 2023 Coauthors: Alex Wilf, Sihyun Shawn Lee, P. Liang, Louis-Philippe Morency Abstract: Human interactions are deeply rooted in the interplay of thoughts, beliefs, and desires made possible by Theory of Mind (ToM): our cognitive ability to understand the mental states of ourselves and others. Although ToM may come naturally to us, emulating it presents a challenge to even the most advanced Large Language Models (LLMs). Recent improvements to LLMs\\' reasoning capabilities from simple yet effective prompting techniques such as Chain-of-Thought have seen limited applicability to ToM. In this paper, we turn to the prominent cognitive science theory\"Simulation Theory\"to bridge this gap. We introduce SimToM, a novel two-stage prompting framework inspired by Simulation Theory\\'s notion of perspective-taking. To implement this idea on current ToM benchmarks, SimToM first filters context based on what the character in question knows before answering a question about their mental state. Our approach, which requires no additional training and minimal prompt-tuning, shows substantial improvement over existing methods, and our analysis reveals the importance of perspective-taking to Theory-of-Mind capabilities. Our findings suggest perspective-taking as a promising direction for future research into improving LLMs\\' ToM capabilities.',\n",
       " 'Author: Louis-Philippe Morency Title: Reconstructing the neurodynamics of face perception during real world vision in humans using intracranial EEG recordings Publication year: 2023 Coauthors: Arish Alreja, Michael J. Ward, J. A. Colan, Qianli Ma, R. M. Richardson, Louis-Philippe Morency, A. Ghuman ',\n",
       " 'Author: Louis-Philippe Morency Title: Tutorial on Multimodal Machine Learning: Principles, Challenges, and Open Questions Publication year: 2023 Coauthors: P. Liang, Louis-Philippe Morency Abstract: Multimodal machine learning is a vibrant multi-disciplinary research field that aims to design computer agents capable of understanding, reasoning, and learning through integrating multiple communicative modalities, including linguistic, acoustic, visual, tactile, and physiological messages. With the recent interest in video understanding, embodied autonomous agents, text-to-image generation, and multisensor fusion in healthcare and robotics, multimodality has brought unique computational and theoretical challenges to the machine learning community given the heterogeneity of data sources and the interconnections often found between modalities. However, the breadth of progress in multimodal research has made it difficult to identify the common themes and open questions in the field. By synthesizing a broad range of application domains and theoretical frameworks from both historical and recent perspectives, this tutorial is designed to provide an overview of the computational and theoretical foundations of multimodal machine learning. Building upon a new edition of our survey paper on multimodal ML and academic courses at CMU, this tutorial will cover three topics: (1) what is multimodal: the principles in learning from heterogeneous, connected, and interacting data, (2) why is it hard: a taxonomy of six core technical challenges faced in multimodal ML but understudied in unimodal ML, and (3) what is next: major directions for future research as identified by our taxonomy.',\n",
       " 'Author: Louis-Philippe Morency Title: Continual Learning for Personalized Co-Speech Gesture Generation Publication year: 2023 Coauthors: Chaitanya Ahuja, Pratik Joshi, Ryo Ishii, Louis-Philippe Morency Abstract: Co-speech gestures are a key channel of human communication, making them important for personalized chat agents to generate. In the past, gesture generation models assumed that data for each speaker is available all at once, and in large amounts. However in practical scenarios, speaker data comes sequentially and in small amounts as the agent personalizes with more speakers, akin to a continual learning paradigm. While more recent works have shown progress in adapting to low-resource data, they catastrophically forget the gesture styles of initial speakers they were trained on. Also, prior generative continual learning works are not multimodal, making this space less studied. In this paper, we explore this new paradigm and propose C-DiffGAN: an approach that continually learns new speaker gesture styles with only a few minutes of per-speaker data, while retaining previously learnt styles. Inspired by prior continual learning works, C-DiffGAN encourages knowledge retention by 1) generating reminiscences of previous low-resource speaker data, then 2) crossmodally aligning to them to mitigate catastrophic forgetting. We quantitatively demonstrate improved performance and reduced forgetting over strong baselines through standard continual learning measures, reinforced by a qualitative user study that shows that our method produces more natural, style-preserving gestures. Code and videos can be found at https://chahuja.com/cdiffgan',\n",
       " 'Author: Louis-Philippe Morency Title: MultiZoo & MultiBench: A Standardized Toolkit for Multimodal Deep Learning Publication year: 2023 Coauthors: P. Liang, Yiwei Lyu, Xiang Fan, Arav Agarwal, Yun Cheng, Louis-Philippe Morency, R. Salakhutdinov Abstract: Learning multimodal representations involves integrating information from multiple heterogeneous sources of data. In order to accelerate progress towards understudied modalities and tasks while ensuring real-world robustness, we release MultiZoo, a public toolkit consisting of standardized implementations of>20 core multimodal algorithms and MultiBench, a large-scale benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas. Together, these provide an automated end-to-end machine learning pipeline that simplifies and standardizes data loading, experimental setup, and model evaluation. To enable holistic evaluation, we offer a comprehensive methodology to assess (1) generalization, (2) time and space complexity, and (3) modality robustness. MultiBench paves the way towards a better understanding of the capabilities and limitations of multimodal models, while ensuring ease of use, accessibility, and reproducibility. Our toolkits are publicly available, will be regularly updated, and welcome inputs from the community.',\n",
       " 'Author: Louis-Philippe Morency Title: MMOE: Mixture of Multimodal Interaction Experts Publication year: 2023 Coauthors: Haofei Yu, P. Liang, R. Salakhutdinov, Louis-Philippe Morency Abstract: Multimodal machine learning, which studies the information and interactions across various input modalities, has made significant advancements in understanding the relationship between images and descriptive text. However, this is just a portion of the potential multimodal interactions seen in the real world and does not include new interactions between conflicting utterances and gestures in predicting sarcasm, for example. Notably, the current methods for capturing shared information often do not extend well to these more nuanced interactions, sometimes performing as low as 50% in binary classification. In this paper, we address this problem via a new approach called MMOE, which stands for a mixture of multimodal interaction experts. Our method automatically classifies data points from unlabeled multimodal datasets by their interaction type and employs specialized models for each specific interaction. Based on our experiments, this approach improves performance on these challenging interactions by more than 10%, leading to an overall increase of 2% for tasks like sarcasm prediction. As a result, interaction quantification provides new insights for dataset analysis and yields simple approaches that obtain state-of-the-art performance.',\n",
       " \"Author: Louis-Philippe Morency Title: SenteCon: Leveraging Lexicons to Learn Human-Interpretable Language Representations Publication year: 2023 Coauthors: Victoria Lin, Louis-Philippe Morency Abstract: Although deep language representations have become the dominant form of language featurization in recent years, in many settings it is important to understand a model's decision-making process. This necessitates not only an interpretable model but also interpretable features. In particular, language must be featurized in a way that is interpretable while still characterizing the original text well. We present SenteCon, a method for introducing human interpretability in deep language representations. Given a passage of text, SenteCon encodes the text as a layer of interpretable categories in which each dimension corresponds to the relevance of a specific category. Our empirical evaluations indicate that encoding language with SenteCon provides high-level interpretability at little to no cost to predictive performance on downstream tasks. Moreover, we find that SenteCon outperforms existing interpretable language representations with respect to both its downstream performance and its agreement with human characterizations of the text.\",\n",
       " 'Author: Louis-Philippe Morency Title: Randomized trial of brief interpersonal psychotherapy and cognitive behavioral therapy for depression delivered both in-person and by telehealth. Publication year: 2023 Coauthors: H. Swartz, Lauren M. Bylsma, Jay Fournier, J. Girard, C. Spotts, J. Cohn, Louis-Philippe Morency ',\n",
       " 'Author: Louis-Philippe Morency Title: MultiIoT: Towards Large-scale Multisensory Learning for the Internet of Things Publication year: 2023 Coauthors: Shentong Mo, P. Liang, Russ Salakhutdinov, Louis-Philippe Morency Abstract: The Internet of Things (IoT), the network integrating billions of smart physical devices embedded with sensors, software, and communication technologies for the purpose of connecting and exchanging data with other devices and systems, is a critical and rapidly expanding component of our modern world. The IoT ecosystem provides a rich source of real-world modalities such as motion, thermal, geolocation, imaging, depth, sensors, video, and audio for prediction tasks involving the pose, gaze, activities, and gestures of humans as well as the touch, contact, pose, 3D of physical objects. Machine learning presents a rich opportunity to automatically process IoT data at scale, enabling efficient inference for impact in understanding human wellbeing, controlling physical devices, and interconnecting smart cities. To develop machine learning technologies for IoT, this paper proposes MultiIoT, the most expansive IoT benchmark to date, encompassing over 1.15 million samples from 12 modalities and 8 tasks. MultiIoT introduces unique challenges involving (1) learning from many sensory modalities, (2) fine-grained interactions across long temporal ranges, and (3) extreme heterogeneity due to unique structure and noise topologies in real-world sensors. We also release a set of strong modeling baselines, spanning modality and task-specific methods to multisensory and multitask models to encourage future research in multisensory representation learning for IoT.',\n",
       " 'Author: Louis-Philippe Morency Title: Neural Mixed Effects for Nonlinear Personalized Predictions Publication year: 2023 Coauthors: T. Wörtwein, Nicholas Allen, Lisa B. Sheeber, R. Auerbach, J. Cohn, Louis-Philippe Morency Abstract: Personalized prediction is a machine learning approach that predicts a person’s future observations based on their past labeled observations and is typically used for sequential tasks, e.g., to predict daily mood ratings. When making personalized predictions, a model can combine two types of trends: (a) trends shared across people, i.e., person-generic trends, such as being happier on weekends, and (b) unique trends for each person, i.e., person-specific trends, such as a stressful weekly meeting. Mixed effect models are popular statistical models to study both trends by combining person-generic and person-specific parameters. Though linear mixed effect models are gaining popularity in machine learning by integrating them with neural networks, these integrations are currently limited to linear person-specific parameters: ruling out nonlinear person-specific trends. In this paper, we propose Neural Mixed Effect (NME) models to optimize nonlinear person-specific parameters anywhere in a neural network in a scalable manner1. NME combines the efficiency of neural network optimization with nonlinear mixed effects modeling. Empirically, we observe that NME improves performance across six unimodal and multimodal datasets, including a smartphone dataset to predict daily mood and a mother-adolescent dataset to predict affective state sequences where half the mothers experience symptoms of depression. Furthermore, we evaluate NME for two model architectures, including for neural conditional random fields (CRF) to predict affective state sequences where the CRF learns nonlinear person-specific temporal transitions between affective states. Analysis of these person-specific transitions on the mother-adolescent dataset shows interpretable trends related to the mother’s depression symptoms.',\n",
       " 'Author: Louis-Philippe Morency Title: MultiZoo and MultiBench: A Standardized Toolkit for Multimodal Deep Learning Publication year: 2023 Coauthors: P. Liang, Yiwei Lyu, Xiang Fan, Arav Agarwal, Yun Cheng, Louis-Philippe Morency, R. Salakhutdinov ',\n",
       " \"Author: Louis-Philippe Morency Title: SHAP-based Prediction of Mother's History of Depression to Understand the Influence on Child Behavior Publication year: 2023 Coauthors: Maneesh Bilalpur, Saurabh Hinduja, Laura Cariola, Lisa B. Sheeber, Nicholas B Allen, Louis-Philippe Morency, Jeffrey F. Cohn Abstract: Depression strongly impacts parents’ behavior. Does parents’ depression strongly affect the behavior of their children as well? To investigate this question, we compared dyadic interactions between 73 depressed and 75 non-depressed mothers and their adolescent child. Families were of low income and 84% were white. Child behavior was measured from audio-video recordings using manual annotation of verbal and nonverbal behavior by expert coders and by multimodal computational measures of facial expression, face and head dynamics, prosody, speech behavior, and linguistics. For both sets of measures, we used Support Vector Machines. For computational measures, we investigated the relative contribution of single versus multiple modalities using a novel approach to SHapley Additive exPlanations (SHAP). Computational measures outperformed manual ratings by human experts. Among individual computational measures, prosody was the most informative. SHAP reduction resulted in a four-fold decrease in the number of features and highest performance (77% accuracy; positive and negative agreements at 75% and 76%, respectively). These findings suggest that maternal depression strongly impacts the behavior of adolescent children; differences are most revealed in prosody; multimodal features together with SHAP reduction are most powerful.\",\n",
       " \"Author: Louis-Philippe Morency Title: Text-Transport: Toward Learning Causal Effects of Natural Language Publication year: 2023 Coauthors: Victoria Lin, Louis-Philippe Morency, Eli Ben-Michael Abstract: As language technologies gain prominence in real-world settings, it is important to understand how changes to language affect reader perceptions. This can be formalized as the causal effect of varying a linguistic attribute (e.g., sentiment) on a reader's response to the text. In this paper, we introduce Text-Transport, a method for estimation of causal effects from natural language under any text distribution. Current approaches for valid causal effect estimation require strong assumptions about the data, meaning the data from which one can estimate valid causal effects often is not representative of the actual target domain of interest. To address this issue, we leverage the notion of distribution shift to describe an estimator that transports causal effects between domains, bypassing the need for strong assumptions in the target domain. We derive statistical guarantees on the uncertainty of this estimator, and we report empirical results and analyses that support the validity of Text-Transport across data settings. Finally, we use Text-Transport to study a realistic setting--hate speech on social media--in which causal effects do shift significantly between text domains, demonstrating the necessity of transport when conducting causal inference on natural language.\",\n",
       " 'Author: Louis-Philippe Morency Title: Lecture Presentations Multimodal Dataset: Towards Understanding Multimodality in Educational Videos Publication year: 2023 Coauthors: Dong Won Lee, Chaitanya Ahuja, P. Liang, Sanika Natu, Louis-Philippe Morency Abstract: Many educational videos use slide presentations, a sequence of visual pages that contain text and figures accompanied by spoken language, which are constructed and presented carefully in order to optimally transfer knowledge to students. Previous studies in multimedia and psychology attribute the effectiveness of lecture presentations to their multimodal nature. As a step toward developing vision-language models to aid in student learning as intelligent teacher assistants, we introduce the Lecture Presentations Multimodal (LPM) Dataset as a large-scale benchmark testing the capabilities of vision-and-language models in multimodal understanding of educational videos. Our dataset contains aligned slides and spoken language, for 180+ hours of video and 9000+ slides, with 10 lecturers from various subjects (e.g., computer science, dentistry, biology). We introduce three research tasks, (1) figure-to-text retrieval, (2) text-to-figure retrieval, and (3) generation of slide explanations, which are grounded in multimedia learning and psychology principles to test a vision-language model’s understanding of multimodal content. We provide manual annotations to help implement these tasks and establish baselines on them. Comparing baselines and human student performances, we find that state-of-the-art vision-language models (zero-shot and fine-tuned) struggle in (1) weak crossmodal alignment between slides and spoken text, (2) learning novel visual mediums, (3) technical language, and (4) long-range sequences. We introduce PolyViLT, a novel multimodal transformer trained with a multi-instance learning loss that is more effective than current approaches for retrieval. We conclude by shedding light on the challenges and opportunities in multimodal understanding of educational presentation videos.',\n",
       " 'Author: Louis-Philippe Morency Title: Counterfactual Augmentation for Multimodal Learning Under Presentation Bias Publication year: 2023 Coauthors: Victoria Lin, Louis-Philippe Morency, D. Dimitriadis, Srinagesh Sharma Abstract: In real-world machine learning systems, labels are often derived from user behaviors that the system wishes to encourage. Over time, new models must be trained as new training examples and features become available. However, feedback loops between users and models can bias future user behavior, inducing a presentation bias in the labels that compromises the ability to train new models. In this paper, we propose counterfactual augmentation, a novel causal method for correcting presentation bias using generated counterfactual labels. Our empirical evaluations demonstrate that counterfactual augmentation yields better downstream performance compared to both uncorrected models and existing bias-correction methods. Model analyses further indicate that the generated counterfactuals align closely with true counterfactuals in an oracle setting.',\n",
       " \"Author: Louis-Philippe Morency Title: Multimodal Feature Selection for Detecting Mothers' Depression in Dyadic Interactions with their Adolescent Offspring Publication year: 2023 Coauthors: Maneesh Bilalpur, Saurabh Hinduja, Laura A. Cariola, Lisa B. Sheeber, Nick Alien, László A. Jeni, Louis-Philippe Morency, J. Cohn Abstract: Depression is the most common psychological disorder, a leading cause of disability world-wide, and a major contributor to inter-generational transmission of psychopathol-ogy within families. To contribute to our understanding of depression within families and to inform modality selection and feature reduction, it is critical to identify interpretable features in developmentally appropriate contexts. Mothers with and without depression were studied. Depression was defined as history of treatment for depression and elevations in current or recent symptoms. We explored two multimodal feature selection strategies in dyadic interaction tasks of mothers with their adolescent children for depression detection. Modalities included face and head dynamics, facial action units, speech-related behavior, and verbal features. The initial feature space was vast and inter-correlated (collinear). To reduce dimension-ality and gain insight into the relative contribution of each modality and feature, we explored feature selection strategies using Variance Inflation Factor (VIF) and Shapley values. On an average collinearity correction through VIF resulted in about 4 times feature reduction across unimodal and multimodal features. Collinearity correction was also found to be an optimal intermediate step prior to Shapley analysis. Shapley feature selection following VIF yielded best performance. The top 15 features obtained through Shapley achieved 78 % accuracy. The most informative features came from all four modalities sampled, which supports the importance of multimodal feature selection.\",\n",
       " 'Author: Louis-Philippe Morency Title: Difference-Masking: Choosing What to Mask in Continued Pretraining Publication year: 2023 Coauthors: Alex Wilf, Syeda Nahida Akter, Leena Mathur, P. Liang, Sheryl Mathew, Mengrou Shou, Eric Nyberg, Louis-Philippe Morency Abstract: The self-supervised objective of masking-and-predicting has led to promising performance gains on a variety of downstream tasks. However, while most approaches randomly mask tokens, there is strong intuition that deciding what to mask can substantially improve learning outcomes. We investigate this in continued pretraining setting in which pretrained models continue to pretrain on domain-specific data before performing some downstream task. We introduce Difference-Masking, a masking strategy that automatically chooses what to mask during continued pretraining by considering what makes a task domain different from the pretraining domain. Empirically, we find that Difference-Masking outperforms baselines on continued pretraining settings across four diverse language-only and multimodal video tasks.',\n",
       " \"Author: Louis-Philippe Morency Title: Expanding the Role of Affective Phenomena in Multimodal Interaction Research Publication year: 2023 Coauthors: Leena Mathur, Maja J Matari'c, Louis-Philippe Morency Abstract: In recent decades, the field of affective computing has made substantial progress in advancing the ability of AI systems to recognize and express affective phenomena, such as affect and emotions, during human-human and human-machine interactions. This paper describes our examination of research at the intersection of multimodal interaction and affective computing, with the objective of observing trends and identifying understudied areas. We examined over 16,000 papers from selected conferences in multimodal interaction, affective computing, and natural language processing: ACM International Conference on Multimodal Interaction, AAAC International Conference on Affective Computing and Intelligent Interaction, Annual Meeting of the Association for Computational Linguistics, and Conference on Empirical Methods in Natural Language Processing. We identified 910 affect-related papers and present our analysis of the role of affective phenomena in these papers. We find that this body of research has primarily focused on enabling machines to recognize or express affect and emotion; there has been limited research on how affect and emotion predictions might, in turn, be used by AI systems to enhance machine understanding of human social behaviors and cognitive states. Based on our analysis, we discuss directions to expand the role of affective phenomena in multimodal interaction research.\",\n",
       " 'Author: Louis-Philippe Morency Title: Multimodal Fusion Interactions: A Study of Human and Automatic Quantification Publication year: 2023 Coauthors: P. Liang, Yun Cheng, R. Salakhutdinov, Louis-Philippe Morency Abstract: In order to perform multimodal fusion of heterogeneous signals, we need to understand their interactions: how each modality individually provides information useful for a task and how this information changes in the presence of other modalities. In this paper, we perform a comparative study of how humans annotate two categorizations of multimodal interactions: (1) partial labels, where different annotators annotate the label given the first, second, and both modalities, and (2) counterfactual labels, where the same annotator annotates the label given the first modality before asking them to explicitly reason about how their answer changes when given the second. We further propose an alternative taxonomy based on (3) information decomposition, where annotators annotate the degrees of redundancy: the extent to which modalities individually and together give the same predictions, uniqueness: the extent to which one modality enables a prediction that the other does not, and synergy: the extent to which both modalities enable one to make a prediction that one would not otherwise make using individual modalities. Through experiments and annotations, we highlight several opportunities and limitations of each approach and propose a method to automatically convert annotations of partial and counterfactual labels to information decomposition, yielding an accurate and efficient method for quantifying multimodal interactions.',\n",
       " 'Author: Louis-Philippe Morency Title: Intensive Longitudinal Assessment of Adolescents to Predict Suicidal Thoughts and Behaviors. Publication year: 2023 Coauthors: R. Auerbach, Ranqing Lan, H. Galfalvy, Kira L. Alqueza, J. Cohn, Ryann Crowley, Katherine Durham, Karla Joyce, Lauren E. Kahn, Rahil A. Kamath, Louis-Philippe Morency, G. Porta, A. Srinivasan, Jamie Zelazny, D. Brent, Nicholas Allen ',\n",
       " 'Author: Louis-Philippe Morency Title: Multimodal Learning Without Labeled Multimodal Data: Guarantees and Applications Publication year: 2023 Coauthors: P. Liang, Chun Kai Ling, Yun Cheng, A. Obolenskiy, Yudong Liu, Rohan Pandey, Alex Wilf, Louis-Philippe Morency, R. Salakhutdinov Abstract: In many machine learning systems that jointly learn from multiple modalities, a core research question is to understand the nature of multimodal interactions: the emergence of new task-relevant information during learning from both modalities that was not present in either alone. We study this challenge of interaction quantification in a semi-supervised setting with only labeled unimodal data and naturally co-occurring multimodal data (e.g., unlabeled images and captions, video and corresponding audio) but when labeling them is time-consuming. Using a precise information-theoretic definition of interactions, our key contributions are the derivations of lower and upper bounds to quantify the amount of multimodal interactions in this semi-supervised setting. We propose two lower bounds based on the amount of shared information between modalities and the disagreement between separately trained unimodal classifiers, and derive an upper bound through connections to approximate algorithms for min-entropy couplings. We validate these estimated bounds and show how they accurately track true interactions. Finally, two semi-supervised multimodal applications are explored based on these theoretical results: (1) analyzing the relationship between multimodal performance and estimated interactions, and (2) self-supervised learning that embraces disagreement between modalities beyond agreement as is typically done.',\n",
       " 'Author: Louis-Philippe Morency Title: Representation Learning for Interpersonal and Multimodal Behavior Dynamics: A Multiview Extension of Latent Change Score Models Publication year: 2023 Coauthors: A. Vail, J. Girard, Lauren M. Bylsma, Jay Fournier, Holly A. Swartz, Jeffrey F. Cohn, Louis-Philippe Morency Abstract: Characterizing the dynamics of behavior across multiple modalities and individuals is a vital component of computational behavior analysis. This is especially important in certain applications, such as psychotherapy, where individualized tracking of behavior patterns can provide valuable information about the patient’s mental state. Conventional methods that rely on aggregate statistics and correlational metrics may not always suffice, as they are often unable to capture causal relationships or evaluate the true probability of identified patterns. To address these challenges, we present a novel approach to learning multimodal and interpersonal representations of behavior dynamics during one-on-one interaction. Our approach is enabled by the introduction of a multiview extension of latent change score models, which facilitates the concurrent capture of both inter-modal and interpersonal behavior dynamics and the identification of directional relationships between them. A core advantage of our approach is its high level of interpretability while simultaneously achieving strong predictive performance. We evaluate our approach within the domain of therapist-client interactions, with the objective of gaining a deeper understanding about the collaborative relationship between the two, a crucial element of the therapeutic process. Our results demonstrate improved performance over conventional approaches that rely upon summary statistics or correlational metrics. Furthermore, since our multiview approach includes the explicit modeling of uncertainty, it naturally lends itself to integration with probabilistic classifiers, such as Gaussian process models. We demonstrate that this integration leads to even further improved performance, all the while maintaining highly interpretable qualities. Our analysis provides compelling motivation for further exploration of stochastic systems within computational models of behavior.',\n",
       " \"Author: Louis-Philippe Morency Title: Comparative Knowledge Distillation Publication year: 2023 Coauthors: Alex Wilf, Alex Tianyi Xu, P. Liang, A. Obolenskiy, Daniel Fried, Louis-Philippe Morency Abstract: In the era of large scale pretrained models, Knowledge Distillation (KD) serves an important role in transferring the wisdom of computationally heavy teacher models to lightweight, efficient student models while preserving performance. Traditional KD paradigms, however, assume readily available access to teacher models for frequent inference -- a notion increasingly at odds with the realities of costly, often proprietary, large scale models. Addressing this gap, our paper considers how to minimize the dependency on teacher model inferences in KD in a setting we term Few Teacher Inference Knowledge Distillation (FTI KD). We observe that prevalent KD techniques and state of the art data augmentation strategies fall short in this constrained setting. Drawing inspiration from educational principles that emphasize learning through comparison, we propose Comparative Knowledge Distillation (CKD), which encourages student models to understand the nuanced differences in a teacher model's interpretations of samples. Critically, CKD provides additional learning signals to the student without making additional teacher calls. We also extend the principle of CKD to groups of samples, enabling even more efficient learning from limited teacher calls. Empirical evaluation across varied experimental settings indicates that CKD consistently outperforms state of the art data augmentation and KD techniques.\",\n",
       " 'Author: Louis-Philippe Morency Title: MultiViz: Towards User-Centric Visualizations and Interpretations of Multimodal Models Publication year: 2023 Coauthors: P. Liang, Yiwei Lyu, Gunjan Chhablani, Nihal Jain, Zihao Deng, Xingbo Wang, Louis-Philippe Morency, R. Salakhutdinov Abstract: The nature of human and computer interactions are inherently multimodal, which has led to substantial interest in building interpretable, interactive, and reliable multimodal interfaces. However, modern multimodal models and interfaces are typically black-box neural networks, which makes it challenging to understand their internal mechanics. How can we visualize their internal workings in order to empower stakeholders to visualize model behavior, perform model debugging, and promote trust in these models? Our paper proposes MultiViz, a method for analyzing the behavior of multimodal models via 4 stages: (1) unimodal importance, (2) cross-modal interactions, (3) multimodal representations and (4) multimodal prediction. MultiViz includes modular visualization tools for each stage before combining outputs from all stages through an interactive and human-in-the-loop API. Through user studies with 21 participants on 8 trained models across 6 real-world tasks, we show that the complementary stages in MultiViz together enable users to (1) simulate model predictions, (2) assign interpretable concepts to features, (3) perform error analysis on model misclassifications, and (4) use insights from error analysis to debug models. MultiViz is publicly available at https://github.com/pliang279/MultiViz, will be regularly updated with new visualization tools and metrics, and welcomes input from the community1.',\n",
       " 'Author: Louis-Philippe Morency Title: Understanding Masked Autoencoders via Hierarchical Latent Variable Models Publication year: 2023 Coauthors: Lingjing Kong, Martin Q. Ma, Guan-Hong Chen, E. Xing, Yuejie Chi, Louis-Philippe Morency, Kun Zhang Abstract: Masked autoencoder (MAE), a simple and effective self-supervised learning framework based on the reconstruction of masked image regions, has recently achieved prominent success in a variety of vision tasks. Despite the emergence of intriguing empirical observations on MAE, a theoretically principled understanding is still lacking. In this work, we formally characterize and justify existing empirical in-sights and provide theoretical guarantees of MAE. We formulate the underlying data-generating process as a hierarchical latent variable model, and show that under reasonable assumptions, MAE provably identifies a set of latent variables in the hierarchical model, explaining why MAE can extract high-level information from pixels. Further, we show how key hyperparameters in MAE (the masking ratio and the patch size) determine which true latent variables to be recovered, therefore influencing the level of semantic information in the representation. Specifically, extremely large or small masking ratios inevitably lead to low-level representations. Our theory offers coherent explanations of existing empirical observations and provides insights for potential empirical improvements and fundamental limitations of the masked-reconstruction paradigm. We conduct extensive experiments to validate our theoretical insights.',\n",
       " 'Author: Louis-Philippe Morency Title: Factorized Contrastive Learning: Going Beyond Multi-view Redundancy Publication year: 2023 Coauthors: P. Liang, Zihao Deng, Martin Q. Ma, James Y. Zou, Louis-Philippe Morency, R. Salakhutdinov Abstract: In a wide range of multimodal tasks, contrastive learning has become a particularly appealing approach since it can successfully learn representations from abundant unlabeled data with only pairing information (e.g., image-caption or video-audio pairs). Underpinning these approaches is the assumption of multi-view redundancy - that shared information between modalities is necessary and sufficient for downstream tasks. However, in many real-world settings, task-relevant information is also contained in modality-unique regions: information that is only present in one modality but still relevant to the task. How can we learn self-supervised multimodal representations to capture both shared and unique information relevant to downstream tasks? This paper proposes FactorCL, a new multimodal representation learning method to go beyond multi-view redundancy. FactorCL is built from three new contributions: (1) factorizing task-relevant information into shared and unique representations, (2) capturing task-relevant information via maximizing MI lower bounds and removing task-irrelevant information via minimizing MI upper bounds, and (3) multimodal data augmentations to approximate task relevance without labels. On large-scale real-world datasets, FactorCL captures both shared and unique information and achieves state-of-the-art results on six benchmarks',\n",
       " \"Author: Louis-Philippe Morency Title: SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents Publication year: 2023 Coauthors: Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, Maarten Sap Abstract: Humans are social beings; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and interact under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills. These findings demonstrate SOTOPIA's promise as a general platform for research on evaluating and improving social intelligence in artificial agents.\",\n",
       " 'Author: Louis-Philippe Morency Title: Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions Publication year: 2023 Coauthors: Himanshu Thakur, Atishay Jain, Praneetha Vaddamanu, P. Liang, Louis-Philippe Morency Abstract: Societal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people. Since large-scale retraining of these models from scratch is both time and compute-expensive, a variety of approaches have been previously proposed that de-bias a pre-trained model. While the majority of current state-of-the-art debiasing methods focus on changes to the training regime, in this paper, we propose data intervention strategies as a powerful yet simple technique to reduce gender bias in pre-trained models. Specifically, we empirically show that by fine-tuning a pre-trained model on only 10 debiased (intervened) training examples, the tendency to favor any gender is significantly reduced. Since our proposed method only needs a few training examples, we argue that our few-shot de-biasing approach is highly feasible and practical. Through extensive experimentation, we show that our de-biasing technique performs better than competitive state-of-the-art baselines with minimal loss in language modeling ability.',\n",
       " 'Author: David R. Mortensen Title: ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages Publication year: 2023 Coauthors: Nathaniel R. Robinson, Perez Ogayo, David R. Mortensen, Graham Neubig Abstract: Large language models (LLMs) implicitly learn to perform a range of language tasks, including machine translation (MT). Previous studies explore aspects of LLMs’ MT capabilities. However, there exist a wide variety of languages for which recent LLM MT performance has never before been evaluated. Without published experimental evidence on the matter, it is difficult for speakers of the world’s diverse languages to know how and whether they can use LLMs for their languages. We present the first experimental evidence for an expansive set of 204 languages, along with MT cost analysis, using the FLORES-200 benchmark. Trends reveal that GPT models approach or exceed traditional MT model performance for some high-resource languages (HRLs) but consistently lag for low-resource languages (LRLs), under-performing traditional MT for 84.1% of languages we covered. Our analysis reveals that a language’s resource level is the most important feature in determining ChatGPT’s relative ability to translate it, and suggests that ChatGPT is especially disadvantaged for LRLs and African languages.',\n",
       " \"Author: David R. Mortensen Title: Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models Publication year: 2023 Coauthors: Orevaoghene Ahia, Sachin Kumar, Hila Gonen, Jungo Kasai, David R. Mortensen, Noah A. Smith, Yulia Tsvetkov Abstract: Language models have graduated from being research prototypes to commercialized products offered as web APIs, and recent works have highlighted the multilingual capabilities of these products. The API vendors charge their users based on usage, more specifically on the number of ``tokens'' processed or generated by the underlying language models. What constitutes a token, however, is training data and model dependent with a large variance in the number of tokens required to convey the same information in different languages. In this work, we analyze the effect of this non-uniformity on the fairness of an API's pricing policy across languages. We conduct a systematic analysis of the cost and utility of OpenAI's language model API on multilingual benchmarks in 22 typologically diverse languages. We show evidence that speakers of a large number of the supported languages are overcharged while obtaining poorer results. These speakers tend to also come from regions where the APIs are less affordable to begin with. Through these analyses, we aim to increase transparency around language model APIs' pricing policies and encourage the vendors to make them more equitable.\",\n",
       " 'Author: David R. Mortensen Title: African Substrates Rather Than European Lexifiers to Augment African-diaspora Creole Translation Publication year: 2023 Coauthors: David R. Mortensen Abstract: Machine translation (MT) model training is difficult for low-resource languages. This is especially true for African-diaspora Creole languages because of data scarcity. Cross-lingual data augmentation methods with knowledge transfer from related high-resource languages are a common technique to overcome this disadvantage. For instance, practitioners may transfer knowledge from a language in the same language family as the low-resource language of interest. Africandiaspora Creole languages are low-resource and simultaneously have relationships with multiple language groups. These languages, such as Haitian Creole and Jamaican Patois, are typically lexified by colonial European languages, but they are structurally similar to African languages. We explore the advantages of transferring knowledge from the European lexifier language versus the phylogenetic and typological relatives of the African substrate languages. We analysed Haitian and Jamaican MT: both controlling tightly for data properties across compared transfer languages and later allowing use of all data we collected. Our inquiry demonstrates a significant advantage in using African transfer languages in some settings.',\n",
       " 'Author: David R. Mortensen Title: SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing Publication year: 2023 Coauthors: Taiqi He, Lindia Tjuatja, Nathaniel R. Robinson, Shinji Watanabe, David R. Mortensen, Graham Neubig, L. Levin Abstract: In our submission to the SIGMORPHON 2023 Shared Task on interlinear glossing (IGT), we explore approaches to data augmentation and modeling across seven low-resource languages. For data augmentation, we explore two approaches: creating artificial data from the provided training data and utilizing existing IGT resources in other languages. On the modeling side, we test an enhanced version of the provided token classification baseline as well as a pretrained multilingual seq2seq model. Additionally, we apply post-correction using a dictionary for Gitksan, the language with the smallest amount of data. We find that our token classification models are the best performing, with the highest word-level accuracy for Arapaho and highest morpheme-level accuracy for Gitksan out of all submissions. We also show that data augmentation is an effective strategy, though applying artificial data pretraining has very different effects across both models tested.',\n",
       " 'Author: David R. Mortensen Title: Multilingual TTS Accent Impressions for Accented ASR Publication year: 2023 Coauthors: G. Karakasidis, Nathaniel R. Robinson, Yaroslav Getman, Atieno Ogayo, Ragheb Al-Ghezi, Ananya Ayasi, Shinji Watanabe, David R. Mortensen, M. Kurimo ',\n",
       " 'Author: David R. Mortensen Title: Construction Grammar Provides Unique Insight into Neural Language Models Publication year: 2023 Coauthors: Leonie Weissweiler, Taiqi He, Naoki Otani, David R. Mortensen, L. Levin, Hinrich Schütze Abstract: Construction Grammar (CxG) has recently been used as the basis for probing studies that have investigated the performance of large pretrained language models (PLMs) with respect to the structure and meaning of constructions. In this position paper, we make suggestions for the continuation and augmentation of this line of research. We look at probing methodology that was not designed with CxG in mind, as well as probing methodology that was designed for specific constructions. We analyse selected previous work in detail, and provide our view of the most important challenges and research questions that this promising new field faces.',\n",
       " 'Author: David R. Mortensen Title: Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation Publication year: 2023 Coauthors: David R. Mortensen, Ela Gulsen, Taiqi He, Nathaniel R. Robinson, Jonathan D. Amith, Lindia Tjuatja, L. Levin Abstract: Interlinear glossing provides a vital type of morphosyntactic annotation, both for linguists and language revitalists, and numerous conventions exist for representing it formally and computationally. Some of these formats are human readable; others are machine readable. Some are easy to edit with general-purpose tools. Few represent non-concatentative processes like infixation, reduplication, mutation, truncation, and tonal overwriting in a consistent and formally rigorous way (on par with affixation). We propose an annotation conventionâ€”Generalized Glossing Guidelines (GGG) that combines all of these positive properties using an Item-and-Process (IP) framework. We describe the format, demonstrate its linguistic adequacy, and compare it with two other interlinear glossed text annotation schemes.',\n",
       " 'Author: David R. Mortensen Title: Transformed Protoform Reconstruction Publication year: 2023 Coauthors: Young Min Kim, Kalvin Chang, Chenxuan Cui, David R. Mortensen Abstract: Protoform reconstruction is the task of inferring what morphemes or words appeared like in the ancestral languages of a set of daughter languages. Meloni et al (2021) achieved the state-of-the-art on Latin protoform reconstruction with an RNN-based encoder-decoder with attention model. We update their model with the state-of-the-art seq2seq model: the Transformer. Our model outperforms their model on a suite of different metrics on two different datasets: their Romance data of 8,000 cognates spanning 5 languages and a Chinese dataset (Hou 2004) of 800+ cognates spanning 39 varieties. We also probe our model for potential phylogenetic signal contained in the model. Our code is publicly available at https://github.com/cmu-llab/acl-2023.',\n",
       " 'Author: David R. Mortensen Title: PWESuite: Phonetic Word Embeddings and Tasks They Facilitate Publication year: 2023 Coauthors: Vilém Zouhar, Kalvin Chang, Chenxuan Cui, Nathaniel Carlson, Nathaniel R. Robinson, Mrinmaya Sachan, David R. Mortensen Abstract: Mapping words into a fixed-dimensional vector space is the backbone of modern NLP. While most word embedding methods successfully encode semantic information, they overlook phonetic information that is crucial for many tasks. We develop three methods that use articulatory features to build phonetically informed word embeddings. To address the inconsistent evaluation of existing phonetic word embedding methods, we also contribute a task suite to fairly evaluate past, current, and future methods. We evaluate both (1) intrinsic aspects of phonetic word embeddings, such as word retrieval and correlation with sound similarity, and (2) extrinsic performance on tasks such as rhyme and cognate detection and sound analogies. We hope our task suite will promote reproducibility and inspire future phonetic embedding research.',\n",
       " 'Author: David R. Mortensen Title: Phonotactic Complexity across Dialects Publication year: 2024 Coauthors: Ryan Soh-Eun Shim, Kalvin Chang, David R. Mortensen Abstract: Received wisdom in linguistic typology holds that if the structure of a language becomes more complex in one dimension, it will simplify in another, building on the assumption that all languages are equally complex (Joseph and Newmeyer, 2012). We study this claim on a micro-level, using a tightly-controlled sample of Dutch dialects (across 366 collection sites) and Min dialects (across 60 sites), which enables a more fair comparison across varieties. Even at the dialect level, we find empirical evidence for a tradeoff between word length and a computational measure of phonotactic complexity from a LSTM-based phone-level language model-a result previously documented only at the language level. A generalized additive model (GAM) shows that dialects with low phonotactic complexity concentrate around the capital regions, which we hypothesize to correspond to prior hypotheses that language varieties of greater or more diverse populations show reduced phonotactic complexity. We also experiment with incorporating the auxiliary task of predicting syllable constituency, but do not find an increase in the negative correlation observed.',\n",
       " 'Author: David R. Mortensen Title: Automating Sound Change Prediction for Phylogenetic Inference: A Tukanoan Case Study Publication year: 2024 Coauthors: Kalvin Chang, Nathaniel Robinson, Anna Cai, Ting Chen, Annie Zhang, David R. Mortensen Abstract: We describe a set of new methods to partially automate linguistic phylogenetic inference given (1) cognate sets with their respective protoforms and sound laws, (2) a mapping from phones to their articulatory features and (3) a typological database of sound changes.We train a neural network on these sound change data to weight articulatory distances between phones and predict intermediate sound change steps between historical protoforms and their modern descendants, replacing a linguistic expert in part of a parsimony-based phylogenetic inference algorithm. In our best experiments on Tukanoan languages, this method produces trees with a Generalized Quartet Distance of 0.12 from a tree that used expert annotations, a significant improvement over other semi-automated baselines. We discuss potential benefits and drawbacks to our neural approach and parsimony-based tree prediction. We also experiment with a minimal generalization learner for automatic sound law induction, finding it less effective than sound laws from expert annotation. Our code is publicly available.',\n",
       " 'Author: David R Mortensen Title: Calibrated Seq2seq Models for Efficient and Generalizable Ultra-fine Entity Typing Publication year: 2023 Coauthors: Yanlin Feng, Adithya Pratapa, David R Mortensen Abstract: Ultra-fine entity typing plays a crucial role in information extraction by predicting fine-grained semantic types for entity mentions in text. However, this task poses significant challenges due to the massive number of entity types in the output space. The current state-of-the-art approaches, based on standard multi-label classifiers or cross-encoder models, suffer from poor generalization performance or inefficient inference. In this paper, we present CASENT, a seq2seq model designed for ultra-fine entity typing that predicts ultra-fine types with calibrated confidence scores. Our model takes an entity mention as input and employs constrained beam search to generate multiple types autoregressively. The raw sequence probabilities associated with the predicted types are then transformed into confidence scores using a novel calibration method. We conduct extensive experiments on the UFET dataset which contains over 10k types. Our method outperforms the previous state-of-the-art in terms of F1 score and calibration error, while achieving an inference speedup of over 50 times. Additionally, we demonstrate the generalization capabilities of our model by evaluating it in zero-shot and few-shot settings on five specialized domain entity typing datasets that are unseen during training. Remarkably, our model outperforms large language models with 10 times more parameters in the zero-shot setting, and when fine-tuned on 50 examples, it significantly outperforms ChatGPT on all datasets. Our code, models and demo are available at https://github.com/yanlinf/CASENT.',\n",
       " \"Author: David R. Mortensen Title: Counting the Bugs in ChatGPT's Wugs: A Multilingual Investigation into the Morphological Capabilities of a Large Language Model Publication year: 2023 Coauthors: Leonie Weissweiler, Valentin Hofmann, Anjali Kantharuban, Anna Cai, Ritam Dutt, Amey Hengle, Anubha Kabra, Atharva Kulkarni, Abhishek Vijayakumar, Haofei Yu, Hinrich Schütze, Kemal Oflazer, David R. Mortensen Abstract: Large language models (LLMs) have recently reached an impressive level of linguistic capability, prompting comparisons with human language skills. However, there have been relatively few systematic inquiries into the linguistic capabilities of the latest generation of LLMs, and those studies that do exist (i) ignore the remarkable ability of humans to generalize, (ii) focus only on English, and (iii) investigate syntax or semantics and overlook other capabilities that lie at the heart of human language, like morphology. Here, we close these gaps by conducting the first rigorous analysis of the morphological capabilities of ChatGPT in four typologically varied languages (specifically, English, German, Tamil, and Turkish). We apply a version of Berko's (1958) wug test to ChatGPT, using novel, uncontaminated datasets for the four examined languages. We find that ChatGPT massively underperforms purpose-built systems, particularly in English. Overall, our results -- through the lens of morphology -- cast a new light on the linguistic capabilities of ChatGPT, suggesting that claims of human-like language skills are premature and misleading.\",\n",
       " 'Author: Graham Neubig Title: Fine-grained Hallucination Detection and Editing for Language Models Publication year: 2024 Coauthors: Abhika Mishra, Akari Asai, Vidhisha Balachandran, Yizhong Wang, Graham Neubig, Yulia Tsvetkov, Hannaneh Hajishirzi Abstract: Large language models (LMs) are prone to generate factual errors, which are often called hallucinations. In this paper, we introduce a comprehensive taxonomy of hallucinations and argue that hallucinations manifest in diverse forms, each requiring varying degrees of careful assessments to verify factuality. We propose a novel task of automatic fine-grained hallucination detection and construct a new evaluation benchmark, FavaBench, that includes about one thousand fine-grained human judgments on three LM outputs across various domains. Our analysis reveals that ChatGPT and Llama2-Chat (70B, 7B) exhibit diverse types of hallucinations in the majority of their outputs in information-seeking scenarios. We train FAVA, a retrieval-augmented LM by carefully creating synthetic data to detect and correct fine-grained hallucinations. On our benchmark, our automatic and human evaluations show that FAVA significantly outperforms ChatGPT and GPT-4 on fine-grained hallucination detection, and edits suggested by FAVA improve the factuality of LM-generated text.',\n",
       " 'Author: Graham Neubig Title: Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate Publication year: 2024 Coauthors: Steffi Chern, Ethan Chern, Graham Neubig, Pengfei Liu Abstract: Despite the utility of Large Language Models (LLMs) across a wide range of tasks and scenarios, developing a method for reliably evaluating LLMs across varied contexts continues to be challenging. Modern evaluation approaches often use LLMs to assess responses generated by LLMs. However, the meta-evaluation conducted to assess the effectiveness of these LLMs as evaluators is typically constrained by the coverage of existing benchmarks or requires extensive human annotation. This underscores the urgency of methods for scalable meta-evaluation that can effectively, reliably, and efficiently evaluate the performance of LLMs as evaluators across diverse tasks and scenarios, particularly in potentially new, user-defined scenarios. To fill this gap, we propose ScaleEval, an agent-debate-assisted meta-evaluation framework that leverages the capabilities of multiple communicative LLM agents. This framework supports multi-round discussions to assist human annotators in discerning the most capable LLMs as evaluators, which significantly eases their workload in cases that used to require large-scale annotations during meta-evaluation. We release the code for our framework, which is publicly available at: \\\\url{https://github.com/GAIR-NLP/scaleeval}.',\n",
       " 'Author: Graham Neubig Title: TroVE: Inducing Verifiable and Efficient Toolboxes for Solving Programmatic Tasks Publication year: 2024 Coauthors: Zhiruo Wang, Daniel Fried, Graham Neubig Abstract: Language models (LMs) can solve tasks such as answering questions about tables or images by writing programs. However, using primitive functions often leads to verbose and error-prone programs, and higher-level functions require expert design. To enable better solutions without human labor, we ask code LMs to curate reusable high-level functions, and use them to write solutions. We present TROVE, a training-free method of inducing a verifiable and efficient toolbox of functions, by generating via using, growing, and periodically trimming the toolbox. On 11 datasets from math, table question answering, and image reasoning tasks, TROVE consistently yields simpler solutions with higher accuracy than baselines using CODELLAMA and previous methods using GPT, while using 79-98% smaller toolboxes. TROVE further enables 31% faster and 13% more accurate human verification than baselines. With the same pipeline, it creates diverse functions for varied tasks and datasets, providing insights into their individual characteristics.',\n",
       " 'Author: Graham Neubig Title: VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks Publication year: 2024 Coauthors: Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, Daniel Fried Abstract: Autonomous agents capable of planning, reasoning, and executing actions on the web offer a promising avenue for automating computer tasks. However, the majority of existing benchmarks primarily focus on text-based agents, neglecting many natural tasks that require visual information to effectively solve. Given that most computer interfaces cater to human perception, visual information often augments textual data in ways that text-only models struggle to harness effectively. To bridge this gap, we introduce VisualWebArena, a benchmark designed to assess the performance of multimodal web agents on realistic \\\\textit{visually grounded tasks}. VisualWebArena comprises of a set of diverse and complex web-based tasks that evaluate various capabilities of autonomous multimodal agents. To perform on this benchmark, agents need to accurately process image-text inputs, interpret natural language instructions, and execute actions on websites to accomplish user-defined objectives. We conduct an extensive evaluation of state-of-the-art LLM-based autonomous agents, including several multimodal models. Through extensive quantitative and qualitative analysis, we identify several limitations of text-only LLM agents, and reveal gaps in the capabilities of state-of-the-art multimodal language agents. VisualWebArena provides a framework for evaluating multimodal autonomous language agents, and offers insights towards building stronger autonomous agents for the web. Our code, baseline models, and data is publicly available at https://jykoh.com/vwa.',\n",
       " 'Author: Graham Neubig Title: DiffusER: Diffusion via Edit-based Reconstruction Publication year: 2023 Coauthors: Machel Reid, V. Hellendoorn, Graham Neubig ',\n",
       " \"Author: Graham Neubig Title: Cross-Modal Fine-Tuning: Align then Refine Publication year: 2023 Coauthors: Junhong Shen, Liam Li, L. Dery, Corey Staten, M. Khodak, Graham Neubig, Ameet Talwalkar Abstract: Fine-tuning large-scale pretrained models has led to tremendous progress in well-studied modalities such as vision and NLP. However, similar gains have not been observed in many other modalities due to a lack of relevant pretrained models. In this work, we propose ORCA, a general cross-modal fine-tuning framework that extends the applicability of a single large-scale pretrained model to diverse modalities. ORCA adapts to a target task via an align-then-refine workflow: given the target input, ORCA first learns an embedding network that aligns the embedded feature distribution with the pretraining modality. The pretrained model is then fine-tuned on the embedded data to exploit the knowledge shared across modalities. Through extensive experiments, we show that ORCA obtains state-of-the-art results on 3 benchmarks containing over 60 datasets from 12 modalities, outperforming a wide range of hand-designed, AutoML, general-purpose, and task-specific methods. We highlight the importance of data alignment via a series of ablation studies and demonstrate ORCA's utility in data-limited regimes.\",\n",
       " 'Author: Graham Neubig Title: ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages Publication year: 2023 Coauthors: Nathaniel R. Robinson, Perez Ogayo, David R. Mortensen, Graham Neubig Abstract: Large language models (LLMs) implicitly learn to perform a range of language tasks, including machine translation (MT). Previous studies explore aspects of LLMs’ MT capabilities. However, there exist a wide variety of languages for which recent LLM MT performance has never before been evaluated. Without published experimental evidence on the matter, it is difficult for speakers of the world’s diverse languages to know how and whether they can use LLMs for their languages. We present the first experimental evidence for an expansive set of 204 languages, along with MT cost analysis, using the FLORES-200 benchmark. Trends reveal that GPT models approach or exceed traditional MT model performance for some high-resource languages (HRLs) but consistently lag for low-resource languages (LRLs), under-performing traditional MT for 84.1% of languages we covered. Our analysis reveals that a language’s resource level is the most important feature in determining ChatGPT’s relative ability to translate it, and suggests that ChatGPT is especially disadvantaged for LRLs and African languages.',\n",
       " 'Author: Graham Neubig Title: GlobalBench: A Benchmark for Global Progress in Natural Language Processing Publication year: 2023 Coauthors: Yueqi Song, Catherine Cui, Simran Khanuja, Pengfei Liu, FAHIM FAISAL, Alissa Ostapenko, Genta Indra Winata, Alham Fikri Aji, Samuel Cahyawijaya, Yulia Tsvetkov, Antonios Anastasopoulos, Graham Neubig Abstract: Despite the major advances in NLP, significant disparities in NLP system performance across languages still exist. Arguably, these are due to uneven resource allocation and sub-optimal incentives to work on less resourced languages. To track and further incentivize the global development of equitable language technology, we introduce GlobalBench. Prior multilingual benchmarks are static and have focused on a limited number of tasks and languages. In contrast, GlobalBench is an ever-expanding collection that aims to dynamically track progress on all NLP datasets in all languages. Rather than solely measuring accuracy, GlobalBench also tracks the estimated per-speaker utility and equity of technology across all languages, providing a multi-faceted view of how language technology is serving people of the world. Furthermore, GlobalBench is designed to identify the most under-served languages, and rewards research efforts directed towards those languages. At present, the most under-served languages are the ones with a relatively high population, but nonetheless overlooked by composite multilingual benchmarks (like Punjabi, Portuguese, and Wu Chinese). Currently, GlobalBench covers 966 datasets in 190 languages, and has 1,128 system submissions spanning 62 languages.',\n",
       " \"Author: Graham Neubig Title: Learning Performance-Improving Code Edits Publication year: 2023 Coauthors: Aman Madaan, Alex Shypula, Uri Alon, Milad Hashemi, Parthasarathy Ranganathan, Yiming Yang, Graham Neubig, A. Yazdanbakhsh Abstract: The waning of Moore's Law has shifted the focus of the tech industry towards alternative methods for continued performance gains. While optimizing compilers are a standard tool to help increase program efficiency, programmers continue to shoulder much responsibility in crafting and refactoring code with better performance characteristics. In this paper, we investigate the ability of large language models (LLMs) to suggest functionally correct, performance improving code edits. We hypothesize that language models can suggest such edits in ways that would be impractical for static analysis alone. We investigate these questions by curating a large-scale dataset of Performance-Improving Edits, PIE. PIE contains trajectories of programs, where a programmer begins with an initial, slower version and iteratively makes changes to improve the program's performance. We use PIE to evaluate and improve the capacity of large language models. Specifically, use examples from PIE to fine-tune multiple variants of CODEGEN, a billion-scale Transformer-decoder model. Additionally, we use examples from PIE to prompt OpenAI's CODEX using a few-shot prompting. By leveraging PIE, we find that both CODEX and CODEGEN can generate performance-improving edits, with speedups of more than 2.5x for over 25% of the programs, for C++ and Python, even after the C++ programs were compiled using the O3 optimization level. Crucially, we show that PIE allows CODEGEN, an open-sourced and 10x smaller model than CODEX, to match the performance of CODEX on this challenging task. Overall, this work opens new doors for creating systems and methods that can help programmers write efficient code.\",\n",
       " 'Author: Graham Neubig Title: CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code Publication year: 2023 Coauthors: Shuyan Zhou, Uri Alon, Sumit Agarwal, Graham Neubig Abstract: Since the rise of neural natural-language-to-code models (NL->Code) that can generate long expressions and statements rather than a single next-token, one of the major problems has been reliably evaluating their generated output. In this paper, we propose CodeBERTScore: an evaluation metric for code generation, which builds on BERTScore (Zhang et al., 2020). Instead of encoding only the generated tokens as in BERTScore, CodeBERTScore also encodes the natural language input preceding the generated code, thus modeling the consistency between the generated code and its given natural language context as well. We perform an extensive evaluation of CodeBERTScore across four programming languages. We find that CodeBERTScore achieves a higher correlation with human preference and with functional correctness than all existing metrics. That is, generated code that receives a higher score by CodeBERTScore is more likely to be preferred by humans, as well as to function correctly when executed. We release five language-specific pretrained models to use with our publicly available code. Our language-specific models have been downloaded more than 1,000,000 times from the Huggingface Hub. Our code and data are available at https://github.com/neulab/code-bert-score',\n",
       " 'Author: Graham Neubig Title: Divergences between Language Models and Human Brains Publication year: 2023 Coauthors: Yuchen Zhou, Emmy Liu, Graham Neubig, Leila Wehbe Abstract: Do machines and humans process language in similar ways? Recent research has hinted in the affirmative, finding that brain signals can be effectively predicted using the internal representations of language models (LMs). Although such results are thought to reflect shared computational principles between LMs and human brains, there are also clear differences in how LMs and humans represent and use language. In this work, we systematically explore the divergences between human and machine language processing by examining the differences between LM representations and human brain responses to language as measured by Magnetoencephalography (MEG) across two datasets in which subjects read and listened to narrative stories. Using a data-driven approach, we identify two domains that are not captured well by LMs: social/emotional intelligence and physical commonsense. We then validate these domains with human behavioral experiments and show that fine-tuning LMs on these domains can improve their alignment with human brain responses.',\n",
       " 'Author: Graham Neubig Title: Neural Machine Translation for the Indigenous Languages of the Americas: An Introduction Publication year: 2023 Coauthors: Manuel Mager, R. Bhatnagar, Graham Neubig, Ngoc Thang Vu, Katharina Kann Abstract: Neural models have drastically advanced state of the art for machine translation (MT) between high-resource languages. Traditionally, these models rely on large amounts of training data, but many language pairs lack these resources. However, an important part of the languages in the world do not have this amount of data. Most languages from the Americas are among them, having a limited amount of parallel and monolingual data, if any. Here, we present an introduction to the interested reader to the basic challenges, concepts, and techniques that involve the creation of MT systems for these languages. Finally, we discuss the recent advances and findings and open questions, product of an increased interest of the NLP community in these languages.',\n",
       " \"Author: Graham Neubig Title: User-Centric Evaluation of OCR Systems for Kwak’wala Publication year: 2023 Coauthors: Shruti Rijhwani, Daisy Rosenblum, Michayla King, Antonios Anastasopoulos, Graham Neubig Abstract: There has been recent interest in improving optical character recognition (OCR) for endangered languages, particularly because a large number of documents and books in these languages are not in machine-readable formats. The performance of OCR systems is typically evaluated using automatic metrics such as character and word error rates. While error rates are useful for the comparison of different models and systems, they do not measure whether and how the transcriptions produced from OCR tools are useful to downstream users. In this paper, we present a human-centric evaluation of OCR systems, focusing on the Kwak'wala language as a case study. With a user study, we show that utilizing OCR reduces the time spent in the manual transcription of culturally valuable documents -- a task that is often undertaken by endangered language community members and researchers -- by over 50%. Our results demonstrate the potential benefits that OCR tools can have on downstream language documentation and revitalization efforts.\",\n",
       " 'Author: Graham Neubig Title: EXCALIBUR: Encouraging and Evaluating Embodied Exploration Publication year: 2023 Coauthors: Hao Zhu, Raghav Kapoor, So Yeon Min, Winson Han, Jiatai Li, Kaiwen Geng, Graham Neubig, Yonatan Bisk, Aniruddha Kembhavi, Luca Weihs Abstract: Experience precedes understanding. Humans constantly explore and learn about their environment out of curiosity, gather information, and update their models of the world. On the other hand, machines are either trained to learn passively from static and fixed datasets, or taught to complete specific goal-conditioned tasks. To encourage the development of exploratory interactive agents, we present the EXCALIBUR benchmark. EXCALIBUR allows agents to explore their environment for long durations and then query their understanding of the physical world via inquiries like: “is the small heavy red bowl made from glass?” or “is there a silver spoon heavier than the egg?”. This design encourages agents to perform free-form home exploration without myopia induced by goal conditioning. Once the agents have answered a series of questions, they can renter the scene to refine their knowledge, update their beliefs, and improve their performance on the questions. Our experiments demonstrate the challenges posed by this dataset for the present-day state-of-the-art embodied systems and the headroom afforded to develop new innovative methods. Finally, we present a virtual reality interface that enables humans to seamlessly interact within the simulated world and use it to gather human performance measures. EXCALIBUR affords unique challenges in comparison to presentday benchmarks and represents the next frontier for embodied AI research.',\n",
       " 'Author: Graham Neubig Title: Multi-Dimensional Evaluation of Text Summarization with In-Context Learning Publication year: 2023 Coauthors: Sameer Jain, Vaishakh Keshava, Swarnashree Mysore Sathyendra, Patrick Fernandes, Pengfei Liu, Graham Neubig, Chunting Zhou Abstract: Evaluation of natural language generation (NLG) is complex and multi-dimensional. Generated text can be evaluated for fluency, coherence, factuality, or any other dimensions of interest. Most frameworks that perform such multi-dimensional evaluation require training on large manually or synthetically generated datasets. In this paper, we study the efficacy of large language models as multi-dimensional evaluators using in-context learning, obviating the need for large training datasets. Our experiments show that in-context learning-based evaluators are competitive with learned evaluation frameworks for the task of text summarization, establishing state-of-the-art on dimensions such as relevance and factual consistency. We then analyze the effects of factors such as the selection and number of in-context examples on performance. Finally, we study the efficacy of in-context learning based evaluators in evaluating zero-shot summaries written by large language models such as GPT-3.',\n",
       " 'Author: Graham Neubig Title: A Gold Standard Dataset for the Reviewer Assignment Problem Publication year: 2023 Coauthors: Ivan Stelmakh, J. Wieting, Graham Neubig, Nihar B. Shah Abstract: Many peer-review venues are either using or looking to use algorithms to assign submissions to reviewers. The crux of such automated approaches is the notion of the\"similarity score\"--a numerical estimate of the expertise of a reviewer in reviewing a paper--and many algorithms have been proposed to compute these scores. However, these algorithms have not been subjected to a principled comparison, making it difficult for stakeholders to choose the algorithm in an evidence-based manner. The key challenge in comparing existing algorithms and developing better algorithms is the lack of the publicly available gold-standard data that would be needed to perform reproducible research. We address this challenge by collecting a novel dataset of similarity scores that we release to the research community. Our dataset consists of 477 self-reported expertise scores provided by 58 researchers who evaluated their expertise in reviewing papers they have read previously. We use this data to compare several popular algorithms employed in computer science conferences and come up with recommendations for stakeholders. Our main findings are as follows. First, all algorithms make a non-trivial amount of error. For the task of ordering two papers in terms of their relevance for a reviewer, the error rates range from 12%-30% in easy cases to 36%-43% in hard cases, highlighting the vital need for more research on the similarity-computation problem. Second, most existing algorithms are designed to work with titles and abstracts of papers, and in this regime the Specter+MFR algorithm performs best. Third, to improve performance, it may be important to develop modern deep-learning based algorithms that can make use of the full texts of papers: the classical TD-IDF algorithm enhanced with full texts of papers is on par with the deep-learning based Specter+MFR that cannot make use of this information.',\n",
       " 'Author: Graham Neubig Title: SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing Publication year: 2023 Coauthors: Taiqi He, Lindia Tjuatja, Nathaniel R. Robinson, Shinji Watanabe, David R. Mortensen, Graham Neubig, L. Levin Abstract: In our submission to the SIGMORPHON 2023 Shared Task on interlinear glossing (IGT), we explore approaches to data augmentation and modeling across seven low-resource languages. For data augmentation, we explore two approaches: creating artificial data from the provided training data and utilizing existing IGT resources in other languages. On the modeling side, we test an enhanced version of the provided token classification baseline as well as a pretrained multilingual seq2seq model. Additionally, we apply post-correction using a dictionary for Gitksan, the language with the smallest amount of data. We find that our token classification models are the best performing, with the highest word-level accuracy for Arapaho and highest morpheme-level accuracy for Gitksan out of all submissions. We also show that data augmentation is an effective strategy, though applying artificial data pretraining has very different effects across both models tested.',\n",
       " \"Author: Graham Neubig Title: Alignment for Honesty Publication year: 2023 Coauthors: Yuqing Yang, Ethan Chern, Xipeng Qiu, Graham Neubig, Pengfei Liu Abstract: Recent research has made significant strides in applying alignment techniques to enhance the helpfulness and harmlessness of large language models (LLMs) in accordance with human intentions. In this paper, we argue for the importance of alignment for honesty, ensuring that LLMs proactively refuse to answer questions when they lack knowledge, while still not being overly conservative. However, a pivotal aspect of alignment for honesty involves discerning the limits of an LLM's knowledge, which is far from straightforward. This challenge demands comprehensive solutions in terms of metric development, benchmark creation, and training methodologies. In this paper, we address these challenges by first establishing a precise problem definition and defining ``honesty'' inspired by the Analects of Confucius. This serves as a cornerstone for developing metrics that effectively measure an LLM's honesty by quantifying its progress post-alignment. Furthermore, we introduce a flexible training framework which is further instantiated by several efficient fine-tuning techniques that emphasize honesty without sacrificing performance on other tasks. Our extensive experiments reveal that these aligned models show a marked increase in honesty, as indicated by our proposed metrics. We open-source a wealth of resources to facilitate future research at https://github.com/GAIR-NLP/alignment-for-honesty, including honesty-aligned models, training and evaluation datasets for honesty alignment, concept glossary, as well as all relevant source code.\",\n",
       " 'Author: Graham Neubig Title: Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation Publication year: 2023 Coauthors: Patrick Fernandes, Aman Madaan, Emmy Liu, António Farinhas, Pedro Henrique Martins, Amanda Bertsch, José G. C. de Souza, Shuyan Zhou, Tongshuang Sherry Wu, Graham Neubig, André F. T. Martins Abstract: Many recent advances in natural language generation have been fueled by training large language models on internet-scale data. However, this paradigm can lead to models that generate toxic, inaccurate, and unhelpful content, and automatic evaluation metrics often fail to identify these behaviors. As models become more capable, human feedback is an invaluable signal for evaluating and improving models. This survey aims to provide an overview of the recent research that has leveraged human feedback to improve natural language generation. First, we introduce an encompassing formalization of feedback, and identify and organize existing research into a taxonomy following this formalization. Next, we discuss how feedback can be described by its format and objective, and cover the two approaches proposed to use feedback (either for training or decoding): directly using the feedback or training feedback models. We also discuss existing datasets for human-feedback data collection, and concerns surrounding feedback collection. Finally, we provide an overview of the nascent field of AI feedback, which exploits large language models to make judgments based on a set of principles and minimize the need for human intervention.',\n",
       " 'Author: Graham Neubig Title: Program-Aided Reasoners (better) Know What They Know Publication year: 2023 Coauthors: Anubha Kabra, Sanketh Rangreji, Yash Mathur, Aman Madaan, Emmy Liu, Graham Neubig Abstract: Prior work shows that program-aided reasoning, in which large language models (LLMs) are combined with programs written in programming languages such as Python, can significantly improve accuracy on various reasoning tasks. However, while accuracy is essential, it is also important for such reasoners to\"know what they know\", which can be quantified through the calibration of the model. In this paper, we compare the calibration of Program Aided Language Models (PAL) and text-based Chain-of-thought (COT) prompting techniques over 5 datasets and 2 model types: LLaMA models and OpenAI models. Our results indicate that PAL leads to improved calibration in 75% of the instances. Our analysis uncovers that prompting styles that produce lesser diversity in generations also have more calibrated results, and thus we also experiment with inducing lower generation diversity using temperature scaling and find that for certain temperatures, PAL is not only more accurate but is also more calibrated than COT. Overall, we demonstrate that, in the majority of cases, program-aided reasoners better know what they know than text-based counterparts.',\n",
       " 'Author: Graham Neubig Title: Learning to Filter Context for Retrieval-Augmented Generation Publication year: 2023 Coauthors: Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md. Rizwan Parvez, Graham Neubig Abstract: On-the-fly retrieval of relevant knowledge has proven an essential element of reliable systems for tasks such as open-domain question answering and fact verification. However, because retrieval systems are not perfect, generation models are required to generate outputs given partially or entirely irrelevant passages. This can cause over- or under-reliance on context, and result in problems in the generated output such as hallucinations. To alleviate these problems, we propose FILCO, a method that improves the quality of the context provided to the generator by (1) identifying useful context based on lexical and information-theoretic approaches, and (2) training context filtering models that can filter retrieved contexts at test time. We experiment on six knowledge-intensive tasks with FLAN-T5 and LLaMa2, and demonstrate that our method outperforms existing approaches on extractive question answering (QA), complex multi-hop and long-form QA, fact verification, and dialog generation tasks. FILCO effectively improves the quality of context, whether or not it supports the canonical output.',\n",
       " 'Author: Graham Neubig Title: FacTool: Factuality Detection in Generative AI - A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios Publication year: 2023 Coauthors: Ethan Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou, Junxian He, Graham Neubig, Pengfei Liu Abstract: The emergence of generative pre-trained models has facilitated the synthesis of high-quality text, but it has also posed challenges in identifying factual errors in the generated text. In particular: (1) A wider range of tasks now face an increasing risk of containing factual errors when handled by generative models. (2) Generated texts tend to be lengthy and lack a clearly defined granularity for individual facts. (3) There is a scarcity of explicit evidence available during the process of fact checking. With the above challenges in mind, in this paper, we propose FacTool, a task and domain agnostic framework for detecting factual errors of texts generated by large language models (e.g., ChatGPT). Experiments on four different tasks (knowledge-based QA, code generation, mathematical reasoning, and scientific literature review) show the efficacy of the proposed method. We release the code of FacTool associated with ChatGPT plugin interface at https://github.com/GAIR-NLP/factool .',\n",
       " 'Author: Graham Neubig Title: Active Retrieval Augmented Generation Publication year: 2023 Coauthors: Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig Abstract: Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.',\n",
       " \"Author: Graham Neubig Title: Large Language Models Enable Few-Shot Clustering Publication year: 2023 Coauthors: Vijay Viswanathan, Kiril Gashteovski, Carolin (Haas) Lawrence, Tongshuang Sherry Wu, Graham Neubig Abstract: Unlike traditional unsupervised clustering, semi-supervised clustering allows users to provide meaningful structure to the data, which helps the clustering algorithm to match the user's intent. Existing approaches to semi-supervised clustering require a significant amount of feedback from an expert to improve the clusters. In this paper, we ask whether a large language model can amplify an expert's guidance to enable query-efficient, few-shot semi-supervised text clustering. We show that LLMs are surprisingly effective at improving clustering. We explore three stages where LLMs can be incorporated into clustering: before clustering (improving input features), during clustering (by providing constraints to the clusterer), and after clustering (using LLMs post-correction). We find incorporating LLMs in the first two stages can routinely provide significant improvements in cluster quality, and that LLMs enable a user to make trade-offs between cost and accuracy to produce desired clusters. We release our code and LLM prompts for the public to use.\",\n",
       " \"Author: Graham Neubig Title: Solving NLP Problems through Human-System Collaboration: A Discussion-based Approach Publication year: 2023 Coauthors: Masahiro Kaneko, Graham Neubig, Naoaki Okazaki Abstract: Humans work together to solve common problems by having discussions, explaining, and agreeing or disagreeing with each other. Similarly, if a system can have discussions with humans when solving tasks, it can improve the system's performance and reliability. In previous research on explainability, it has only been possible for the system to make predictions and for humans to ask questions about them rather than having a mutual exchange of opinions. This research aims to create a dataset and computational framework for systems that discuss and refine their predictions through dialogue. Through experiments, we show that the proposed system can have beneficial discussions with humans improving the accuracy by up to 25 points in the natural language inference task.\",\n",
       " 'Author: Graham Neubig Title: Crossing the Threshold: Idiomatic Machine Translation through Retrieval Augmentation and Loss Weighting Publication year: 2023 Coauthors: Emmy Liu, Aditi Chaudhary, Graham Neubig Abstract: Idioms are common in everyday language, but often pose a challenge to translators because their meanings do not follow from the meanings of their parts. Despite significant advances, machine translation systems still struggle to translate idiomatic expressions. We provide a simple characterization of idiomatic translation and related issues. This allows us to conduct a synthetic experiment revealing a tipping point at which transformer-based machine translation models correctly default to idiomatic translations. To expand multilingual resources, we compile a dataset of ~4k natural sentences containing idiomatic expressions in French, Finnish, and Japanese. To improve translation of natural idioms, we introduce two straightforward yet effective techniques: the strategic upweighting of training loss on potentially idiomatic sentences, and using retrieval-augmented models. This not only improves the accuracy of a strong pretrained MT model on idiomatic sentences by up to 13% in absolute accuracy, but also holds potential benefits for non-idiomatic sentences.',\n",
       " 'Author: Graham Neubig Title: Why do Nearest Neighbor Language Models Work? Publication year: 2023 Coauthors: Frank F. Xu, Uri Alon, Graham Neubig Abstract: Language models (LMs) compute the probability of a text by sequentially computing a representation of an already-seen context and using this representation to predict the next word. Currently, most LMs calculate these representations through a neural network consuming the immediate previous context. However recently, retrieval-augmented LMs have shown to improve over standard neural LMs, by accessing information retrieved from a large datastore, in addition to their standard, parametric, next-word prediction. In this paper, we set out to understand why retrieval-augmented language models, and specifically why k-nearest neighbor language models (kNN-LMs) perform better than standard parametric LMs, even when the k-nearest neighbor component retrieves examples from the same training set that the LM was originally trained on. To this end, we perform a careful analysis of the various dimensions over which kNN-LM diverges from standard LMs, and investigate these dimensions one by one. Empirically, we identify three main reasons why kNN-LM performs better than standard LMs: using a different input representation for predicting the next tokens, approximate kNN search, and the importance of softmax temperature for the kNN distribution. Further, we incorporate these insights into the model architecture or the training procedure of the standard parametric LM, improving its results without the need for an explicit retrieval component. The code is available at https://github.com/frankxu2004/knnlm-why.',\n",
       " 'Author: Graham Neubig Title: Syntax and Semantics Meet in the “Middle”: Probing the Syntax-Semantics Interface of LMs Through Agentivity Publication year: 2023 Coauthors: Lindia Tjuatja, Emmy Liu, L. Levin, Graham Neubig Abstract: Recent advances in large language models have prompted researchers to examine their abilities across a variety of linguistic tasks, but little has been done to investigate how models handle the interactions in meaning across words and larger syntactic forms—i.e. phenomena at the intersection of syntax and semantics. We present the semantic notion of agentivity as a case study for probing such interactions. We created a novel evaluation dataset by utilitizing the unique linguistic properties of a subset of optionally transitive English verbs. This dataset was used to prompt varying sizes of three model classes to see if they are sensitive to agentivity at the lexical level, and if they can appropriately employ these word-level priors given a specific syntactic context. Overall, GPT-3 text-davinci-003 performs extremely well across all experiments, outperforming all other models tested by far. In fact, the results are even better correlated with human judgements than both syntactic and semantic corpus statistics. This suggests that LMs may potentially serve as more useful tools for linguistic annotation, theory testing, and discovery than select corpora for certain tasks.',\n",
       " 'Author: Graham Neubig Title: DataFinder: Scientific Dataset Recommendation from Natural Language Descriptions Publication year: 2023 Coauthors: Vijay Viswanathan, Luyu Gao, Tongshuang Sherry Wu, Pengfei Liu, Graham Neubig Abstract: Modern machine learning relies on datasets to develop and validate research ideas. Given the growth of publicly available data, finding the right dataset to use is increasingly difficult. Any research question imposes explicit and implicit constraints on how well a given dataset will enable researchers to answer this question, such as dataset size, modality, and domain. We operationalize the task of recommending datasets given a short natural language description of a research idea, to help people find relevant datasets for their needs. Dataset recommendation poses unique challenges as an information retrieval problem; datasets are hard to directly index for search and there are no corpora readily available for this task. To facilitate this task, we build the DataFinder Dataset which consists of a larger automatically-constructed training set (17.5K queries) and a smaller expert-annotated evaluation set (392 queries). Using this data, we compare various information retrieval algorithms on our test set and present a superior bi-encoder retriever for text-based dataset recommendation. This system, trained on the DataFinder Dataset, finds more relevant search results than existing third-party dataset search engines. To encourage progress on dataset recommendation, we release our dataset and models to the public.',\n",
       " \"Author: Graham Neubig Title: Multi-lingual and Multi-cultural Figurative Language Understanding Publication year: 2023 Coauthors: Anubha Kabra, Emmy Liu, Simran Khanuja, Alham Fikri Aji, Genta Indra Winata, Samuel Cahyawijaya, Anuoluwapo Aremu, Perez Ogayo, Graham Neubig Abstract: Figurative language permeates human communication, but at the same time is relatively understudied in NLP. Datasets have been created in English to accelerate progress towards measuring and improving figurative language processing in language models (LMs). However, the use of figurative language is an expression of our cultural and societal experiences, making it difficult for these phrases to be universally applicable. In this work, we create a figurative language inference dataset, \\\\datasetname, for seven diverse languages associated with a variety of cultures: Hindi, Indonesian, Javanese, Kannada, Sundanese, Swahili and Yoruba. Our dataset reveals that each language relies on cultural and regional concepts for figurative expressions, with the highest overlap between languages originating from the same region. We assess multilingual LMs' abilities to interpret figurative language in zero-shot and few-shot settings. All languages exhibit a significant deficiency compared to English, with variations in performance reflecting the availability of pre-training and fine-tuning data, emphasizing the need for LMs to be exposed to a broader range of linguistic and cultural variation during training.\",\n",
       " 'Author: Graham Neubig Title: It’s MBR All the Way Down: Modern Generation Techniques Through the Lens of Minimum Bayes Risk Publication year: 2023 Coauthors: Amanda Bertsch, Alex Xie, Graham Neubig, Matthew R. Gormley Abstract: Minimum Bayes Risk (MBR) decoding is a method for choosing the outputs of a machine learning system based not on the output with the highest probability, but the output with the lowest risk (expected error) among multiple candidates. It is a simple but powerful method: for an additional cost at inference time, MBR provides reliable several-point improvements across metrics for a wide variety of tasks without any additional data or training. Despite this, MBR is not frequently applied in NLP works, and knowledge of the method itself is limited. We first provide an introduction to the method and the recent literature. We show that several recent methods that do not reference MBR can be written as special cases of MBR; this reformulation provides additional theoretical justification for the performance of these methods, explaining some results that were previously only empirical. We provide theoretical and empirical results about the effectiveness of various MBR variants and make concrete recommendations for the application of MBR in NLP models, including future directions in this area.',\n",
       " 'Author: Graham Neubig Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input Publication year: 2023 Coauthors: Amanda Bertsch, Uri Alon, Graham Neubig, Matthew R. Gormley Abstract: Since the proposal of transformers, these models have been limited to bounded input lengths, because of their need to attend to every token in the input. In this work, we propose Unlimiformer: a general approach that wraps any existing pretrained encoder-decoder transformer, and offloads the cross-attention computation to a single k-nearest-neighbor (kNN) index, while the returned kNN distances are the attention dot-product scores. This kNN index can be kept on either the GPU or CPU memory and queried in sub-linear time; this way, we can index practically unlimited input sequences, while every attention head in every decoder layer retrieves its top-k keys, instead of attending to every key. We evaluate Unlimiformer on several long-document and book-summarization benchmarks, showing that it can process even 500k token-long inputs from the BookSum dataset, without any input truncation at test time. We demonstrate that Unlimiformer improves pretrained models such as BART and Longformer by extending them to unlimited inputs without additional learned weights and without modifying their code. We make our code and models publicly available at https://github.com/abertsch72/unlimiformer .',\n",
       " 'Author: Graham Neubig Title: WebArena: A Realistic Web Environment for Building Autonomous Agents Publication year: 2023 Coauthors: Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig Abstract: With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.',\n",
       " 'Author: Graham Neubig Title: Prompt2Model: Generating Deployable Models from Natural Language Instructions Publication year: 2023 Coauthors: Vijay Viswanathan, Chenyang Zhao, Amanda Bertsch, Tongshuang Sherry Wu, Graham Neubig Abstract: Large language models (LLMs) enable system builders today to create competent NLP systems through prompting, where they only need to describe the task in natural language and provide a few examples. However, in other ways, LLMs are a step backward from traditional special-purpose NLP models; they require extensive computational resources for deployment and can be gated behind APIs. In this paper, we propose Prompt2Model, a general-purpose method that takes a natural language task description like the prompts provided to LLMs, and uses it to train a special-purpose model that is conducive to deployment. This is done through a multi-step process of retrieval of existing datasets and pretrained models, dataset generation using LLMs, and supervised fine-tuning on these retrieved and generated datasets. Over three tasks, we demonstrate that given the same few-shot prompt as input, Prompt2Model trains models that outperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20% while being up to 700 times smaller. We also show that this data can be used to obtain reliable performance estimates of model performance, enabling model developers to assess model reliability before deployment. Prompt2Model is available open-source at https://github.com/neulab/prompt2model.',\n",
       " 'Author: Graham Neubig Title: Computational Language Acquisition with Theory of Mind Publication year: 2023 Coauthors: Andy T. Liu, Hao Zhu, Emmy Liu, Yonatan Bisk, Graham Neubig Abstract: Unlike current state-of-the-art language models, young children actively acquire language through interactions with their surrounding environment and caretakers. One mechanism that has been argued to be critical to language learning is the ability to infer the mental states of other agents in social environments, coined Theory of Mind (ToM) by Premack&Woodruff (1978). Drawing inspiration from the modern operationalized versions of ToM implemented in Rabinowitz et al. (2018) and Zhu et al. (2021), we build language-learning agents equipped with ToM, and measure its effects on the learning process. We model ToM by giving the speaker agent an internal listener model that is trained alongside the speaker and used to rerank potential utterances. We experiment with varying task difficulty, hypothesizing that models will acquire more complex language to adapt to stronger environmental pressures. We find that training speakers with a highly weighted ToM listener component leads to performance gains in our image referential game setting. We also find some evidence that increasing task difficulty in the training process results in more fluent and precise utterances in evaluation. This suggests the potential utility of further incorporating ToM, as well as other insights from child language acquisition, into computational models of language acquisition.',\n",
       " 'Author: Graham Neubig Title: DeMuX: Data-efficient Multilingual Learning Publication year: 2023 Coauthors: Simran Khanuja, Srinivas Gowriraj, L. Dery, Graham Neubig Abstract: We consider the task of optimally fine-tuning pre-trained multilingual models, given small amounts of unlabelled target data and an annotation budget. In this paper, we introduce DEMUX, a framework that prescribes the exact data-points to label from vast amounts of unlabelled multilingual data, having unknown degrees of overlap with the target set. Unlike most prior works, our end-to-end framework is language-agnostic, accounts for model representations, and supports multilingual target configurations. Our active learning strategies rely upon distance and uncertainty measures to select task-specific neighbors that are most informative to label, given a model. DeMuX outperforms strong baselines in 84% of the test cases, in the zero-shot setting of disjoint source and target language sets (including multilingual target pools), across three models and four tasks. Notably, in low-budget settings (5-100 examples), we observe gains of up to 8-11 F1 points for token-level tasks, and 2-5 F1 for complex tasks. Our code is released here: https://github.com/simran-khanuja/demux.',\n",
       " 'Author: Graham Neubig Title: Improving Factuality of Abstractive Summarization via Contrastive Reward Learning Publication year: 2023 Coauthors: Ethan Chern, Zhiruo Wang, Sanjan Das, Bhavuk Sharma, Pengfei Liu, Graham Neubig Abstract: Modern abstractive summarization models often generate summaries that contain hallucinated or contradictory information. In this paper, we propose a simple but effective contrastive learning framework that incorporates recent developments in reward learning and factuality metrics. Empirical studies demonstrate that the proposed framework enables summarization models to learn from feedback of factuality metrics using contrastive reward learning, leading to more factual summaries by human evaluations. This suggests that further advances in learning and evaluation algorithms can feed directly into providing more factual summaries. Code and human evaluation results will be publicly available at \\\\url{https://github.com/EthanC111/factuality_summarization}.',\n",
       " \"Author: Graham Neubig Title: SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents Publication year: 2023 Coauthors: Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, Maarten Sap Abstract: Humans are social beings; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and interact under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills. These findings demonstrate SOTOPIA's promise as a general platform for research on evaluating and improving social intelligence in artificial agents.\",\n",
       " 'Author: Graham Neubig Title: The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation Publication year: 2023 Coauthors: Patrick Fernandes, Daniel Deutsch, M. Finkelstein, Parker Riley, André F. T. Martins, Graham Neubig, Ankush Garg, J. Clark, Markus Freitag, Orhan Firat Abstract: Automatic evaluation of machine translation (MT) is a critical tool driving the rapid iterative development of MT systems. While considerable progress has been made on estimating a single scalar quality score, current metrics lack the informativeness of more detailed schemes that annotate individual errors, such as Multidimensional Quality Metrics (MQM). In this paper, we help fill this gap by proposing AutoMQM, a prompting technique which leverages the reasoning and in-context learning capabilities of large language models (LLMs) and asks them to identify and categorize errors in translations. We start by evaluating recent LLMs, such as PaLM and PaLM-2, through simple score prediction prompting, and we study the impact of labeled data through in-context learning and finetuning. We then evaluate AutoMQM with PaLM-2 models, and we find that it improves performance compared to just prompting for scores (with particularly large gains for larger models) while providing interpretability through error spans that align with human annotations.',\n",
       " 'Author: Graham Neubig Title: Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes Publication year: 2024 Coauthors: L. Dery, Steven Kolawole, Jean-Francois Kagey, Virginia Smith, Graham Neubig, Ameet Talwalkar Abstract: Given the generational gap in available hardware between lay practitioners and the most endowed institutions, LLMs are becoming increasingly inaccessible as they grow in size. Whilst many approaches have been proposed to compress LLMs to make their resource consumption manageable, these methods themselves tend to be resource intensive, putting them out of the reach of the very user groups they target. In this work, we explore the problem of structured pruning of LLMs using only forward passes. We seek to empower practitioners to prune models so large that their available hardware has just enough memory to run inference. We develop Bonsai, a gradient-free, perturbative pruning method capable of delivering small, fast, and accurate pruned models. We observe that Bonsai outputs pruned models that (i) outperform those generated by more expensive gradient-based structured pruning methods, and (ii) are twice as fast (with comparable accuracy) as those generated by semi-structured pruning methods requiring comparable resources as Bonsai. We also leverage Bonsai to produce a new sub-2B model using a single A6000 that yields state-of-the-art performance on 4/6 tasks on the Huggingface Open LLM leaderboard.',\n",
       " \"Author: Graham Neubig Title: Multitask Learning Can Improve Worst-Group Outcomes Publication year: 2023 Coauthors: Atharva Kulkarni, L. Dery, Amrith Rajagopal Setlur, Aditi Raghunathan, Ameet Talwalkar, Graham Neubig Abstract: In order to create machine learning systems that serve a variety of users well, it is vital to not only achieve high average performance but also ensure equitable outcomes across diverse groups. However, most machine learning methods are designed to improve a model's average performance on a chosen end task without consideration for their impact on worst group error. Multitask learning (MTL) is one such widely used technique. In this paper, we seek not only to understand the impact of MTL on worst-group accuracy but also to explore its potential as a tool to address the challenge of group-wise fairness. We primarily consider the common setting of fine-tuning a pre-trained model, where, following recent work (Gururangan et al., 2020; Dery et al., 2023), we multitask the end task with the pre-training objective constructed from the end task data itself. In settings with few or no group annotations, we find that multitasking often, but not always, achieves better worst-group accuracy than Just-Train-Twice (JTT; Liu et al. (2021)) -- a representative distributionally robust optimization (DRO) method. Leveraging insights from synthetic data experiments, we propose to modify standard MTL by regularizing the joint multitask representation space. We run a large number of fine-tuning experiments across computer vision and natural language and find that our regularized MTL approach consistently outperforms JTT on both worst and average group outcomes. Our official code can be found here: https://github.com/atharvajk98/MTL-group-robustness.\",\n",
       " 'Author: Graham Neubig Title: Do LLMs exhibit human-like response biases? A case study in survey design Publication year: 2023 Coauthors: Lindia Tjuatja, Valerie Chen, Sherry Tongshuang Wu, Ameet Talwalkar, Graham Neubig Abstract: As large language models (LLMs) become more capable, there is growing excitement about the possibility of using LLMs as proxies for humans in real-world tasks where subjective labels are desired, such as in surveys and opinion polling. One widely-cited barrier to the adoption of LLMs as proxies for humans in subjective tasks is their sensitivity to prompt wording - but interestingly, humans also display sensitivities to instruction changes in the form of response biases. We investigate the extent to which LLMs reflect human response biases, if at all. We look to survey design, where human response biases caused by changes in the wordings of\"prompts\"have been extensively explored in social psychology literature. Drawing from these works, we design a dataset and framework to evaluate whether LLMs exhibit human-like response biases in survey questionnaires. Our comprehensive evaluation of nine models shows that popular open and commercial LLMs generally fail to reflect human-like behavior, particularly in models that have undergone RLHF. Furthermore, even if a model shows a significant change in the same direction as humans, we find that they are sensitive to perturbations that do not elicit significant changes in humans. These results highlight the pitfalls of using LLMs as human proxies, and underscore the need for finer-grained characterizations of model behavior. Our code, dataset, and collected samples are available at https://github.com/lindiatjuatja/BiasMonkey',\n",
       " \"Author: Graham Neubig Title: An In-depth Look at Gemini's Language Abilities Publication year: 2023 Coauthors: Syeda Nahida Akter, Zichun Yu, Aashiq Muhamed, Tianyue Ou, Alex Bauerle, Ángel Alexander Cabrera, Krish Dholakia, Chenyan Xiong, Graham Neubig Abstract: The recently released Google Gemini class of models are the first to comprehensively report results that rival the OpenAI GPT series across a wide variety of tasks. In this paper, we do an in-depth exploration of Gemini's language abilities, making two contributions. First, we provide a third-party, objective comparison of the abilities of the OpenAI GPT and Google Gemini models with reproducible code and fully transparent results. Second, we take a closer look at the results, identifying areas where one of the two model classes excels. We perform this analysis over 10 datasets testing a variety of language abilities, including reasoning, answering knowledge-based questions, solving math problems, translating between languages, generating code, and acting as instruction-following agents. From this analysis, we find that Gemini Pro achieves accuracy that is close but slightly inferior to the corresponding GPT 3.5 Turbo on all tasks that we benchmarked. We further provide explanations for some of this under-performance, including failures in mathematical reasoning with many digits, sensitivity to multiple-choice answer ordering, aggressive content filtering, and others. We also identify areas where Gemini demonstrates comparably high performance, including generation into non-English languages, and handling longer and more complex reasoning chains. Code and data for reproduction can be found at https://github.com/neulab/gemini-benchmark\",\n",
       " 'Author: Eric P. Xing Title: Learning to Prompt Segment Anything Models Publication year: 2024 Coauthors: Jiaxing Huang, Kai Jiang, Jingyi Zhang, Han Qiu, Lewei Lu, Shijian Lu, Eric P. Xing Abstract: Segment Anything Models (SAMs) like SEEM and SAM have demonstrated great potential in learning to segment anything. The core design of SAMs lies with Promptable Segmentation, which takes a handcrafted prompt as input and returns the expected segmentation mask. SAMs work with two types of prompts including spatial prompts (e.g., points) and semantic prompts (e.g., texts), which work together to prompt SAMs to segment anything on downstream datasets. Despite the important role of prompts, how to acquire suitable prompts for SAMs is largely under-explored. In this work, we examine the architecture of SAMs and identify two challenges for learning effective prompts for SAMs. To this end, we propose spatial-semantic prompt learning (SSPrompt) that learns effective semantic and spatial prompts for better SAMs. Specifically, SSPrompt introduces spatial prompt learning and semantic prompt learning, which optimize spatial prompts and semantic prompts directly over the embedding space and selectively leverage the knowledge encoded in pre-trained prompt encoders. Extensive experiments show that SSPrompt achieves superior image segmentation performance consistently across multiple widely adopted datasets.',\n",
       " 'Author: Eric P. Xing Title: 3D Open-vocabulary Segmentation with Foundation Models Publication year: 2023 Coauthors: Kunhao Liu, Fangneng Zhan, Jiahui Zhang, Muyu Xu, Yingchen Yu, Abdulmotaleb El-Saddik, Christian Theobalt, Eric P. Xing, Shijian Lu Abstract: Open-vocabulary segmentation of 3D scenes is a fundamental function of human perception and thus a crucial objective in computer vision research. However, this task is heavily impeded by the lack of large-scale and diverse 3D open-vocabulary segmentation datasets for training robust and generalizable models. Distilling knowledge from pre-trained 2D open-vocabulary segmentation models helps but it compromises the open-vocabulary feature significantly as the 2D models are mostly finetuned with close-vocabulary datasets. We tackle the challenges in 3D open-vocabulary segmentation by exploiting the open-vocabulary multimodal knowledge and object reasoning capability of pre-trained foundation models CLIP and DINO, without necessitating any fine-tuning. Specifically, we distill open-vocabulary visual and textual knowledge from CLIP into a neural radiance field (NeRF) which effectively lifts 2D features into view-consistent 3D segmentation. Furthermore, we introduce the Relevancy-Distribution Alignment loss and Feature-Distribution Alignment loss to respectively mitigate the ambiguities of CLIP features and distill precise object boundaries from DINO features, eliminating the need for segmentation annotations during training. Extensive experiments show that our method even outperforms fully supervised models trained with segmentation annotations, suggesting that 3D open-vocabulary segmentation can be effectively learned from 2D images and text-image pairs.',\n",
       " 'Author: Eric P. Xing Title: TrustLLM: Trustworthiness in Large Language Models Publication year: 2024 Coauthors: Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, Zheng Liu, Yixin Liu, Yijue Wang, Zhikun Zhang, B. Kailkhura, Caiming Xiong, Chaowei Xiao, Chun-Yan Li, Eric P. Xing, Furong Huang, Haodong Liu, Heng Ji, Hongyi Wang, Huan Zhang, Huaxiu Yao, M. Kellis, M. Zitnik, Meng Jiang, Mohit Bansal, James Zou, Jian Pei, Jian Liu, Jianfeng Gao, Jiawei Han, Jieyu Zhao, Jiliang Tang, Jindong Wang, John Mitchell, Kai Shu, Kaidi Xu, Kai-Wei Chang, Lifang He, Lifu Huang, M. Backes, Neil Zhenqiang Gong, Philip S. Yu, Pin-Yu Chen, Quanquan Gu, Ran Xu, Rex Ying, Shuiwang Ji, S. Jana, Tian-Xiang Chen, Tianming Liu, Tianying Zhou, William Wang, Xiang Li, Xiang-Yu Zhang, Xiao Wang, Xingyao Xie, Xun Chen, Xuyu Wang, Yan Liu, Yanfang Ye, Yinzhi Cao, Yue Zhao Abstract: Large language models (LLMs), exemplified by ChatGPT, have gained considerable attention for their excellent natural language processing capabilities. Nonetheless, these LLMs present many challenges, particularly in the realm of trustworthiness. Therefore, ensuring the trustworthiness of LLMs emerges as an important topic. This paper introduces TrustLLM, a comprehensive study of trustworthiness in LLMs, including principles for different dimensions of trustworthiness, established benchmark, evaluation, and analysis of trustworthiness for mainstream LLMs, and discussion of open challenges and future directions. Specifically, we first propose a set of principles for trustworthy LLMs that span eight different dimensions. Based on these principles, we further establish a benchmark across six dimensions including truthfulness, safety, fairness, robustness, privacy, and machine ethics. We then present a study evaluating 16 mainstream LLMs in TrustLLM, consisting of over 30 datasets. Our findings firstly show that in general trustworthiness and utility (i.e., functional effectiveness) are positively related. Secondly, our observations reveal that proprietary LLMs generally outperform most open-source counterparts in terms of trustworthiness, raising concerns about the potential risks of widely accessible open-source LLMs. However, a few open-source LLMs come very close to proprietary ones. Thirdly, it is important to note that some LLMs may be overly calibrated towards exhibiting trustworthiness, to the extent that they compromise their utility by mistakenly treating benign prompts as harmful and consequently not responding. Finally, we emphasize the importance of ensuring transparency not only in the models themselves but also in the technologies that underpin trustworthiness. Knowing the specific trustworthy technologies that have been employed is crucial for analyzing their effectiveness.',\n",
       " \"Author: Eric P. Xing Title: SlimPajama-DC: Understanding Data Combinations for LLM Training Publication year: 2023 Coauthors: Zhiqiang Shen, Tianhua Tao, Liqun Ma, W. Neiswanger, Zhengzhong Liu, Hongyi Wang, Bowen Tan, Joel Hestness, Natalia Vassilieva, Daria Soboleva, Eric P. Xing Abstract: This paper aims to understand the impacts of various data combinations (e.g., web text, wikipedia, github, books) on the training of large language models using SlimPajama. SlimPajama is a rigorously deduplicated, multi-source dataset, which has been refined and further deduplicated to 627B tokens from the extensive 1.2T tokens RedPajama dataset contributed by Together. We've termed our research as SlimPajama-DC, an empirical analysis designed to uncover fundamental characteristics and best practices associated with employing SlimPajama in the training of large language models. During our research with SlimPajama, two pivotal observations emerged: (1) Global deduplication vs. local deduplication. We analyze and discuss how global (across different sources of datasets) and local (within the single source of dataset) deduplications affect the performance of trained models. (2) Proportions of high-quality/highly-deduplicated multi-source datasets in the combination. To study this, we construct six configurations of SlimPajama dataset and train individual ones using 1.3B Cerebras-GPT model with Alibi and SwiGLU. Our best configuration outperforms the 1.3B model trained on RedPajama using the same number of training tokens by a significant margin. All our 1.3B models are trained on Cerebras 16$\\\\times$ CS-2 cluster with a total of 80 PFLOP/s in bf16 mixed precision. We further extend our discoveries (such as increasing data diversity is crucial after global deduplication) on a 7B model with large batch-size training. Our models and the separate SlimPajama-DC datasets are available at: https://huggingface.co/MBZUAI-LLM and https://huggingface.co/datasets/cerebras/SlimPajama-627B.\",\n",
       " 'Author: Eric P. Xing Title: LLM360: Towards Fully Transparent Open-Source LLMs Publication year: 2023 Coauthors: Zhengzhong Liu, Aurick Qiao, W. Neiswanger, Hongyi Wang, Bowen Tan, Tianhua Tao, Junbo Li, Yuqi Wang, Suqi Sun, Omkar Pangarkar, Richard Fan, Yi Gu, Victor Miller, Yonghao Zhuang, Guowei He, Haonan Li, Fajri Koto, Liping Tang, Nikhil Ranjan, Zhiqiang Shen, Xuguang Ren, Roberto Iriondo, Cun Mu, Zhiting Hu, Mark Schulze, Preslav Nakov, Timothy Baldwin, Eric P. Xing Abstract: The recent surge in open-source Large Language Models (LLMs), such as LLaMA, Falcon, and Mistral, provides diverse options for AI practitioners and researchers. However, most LLMs have only released partial artifacts, such as the final model weights or inference code, and technical reports increasingly limit their scope to high-level design choices and surface statistics. These choices hinder progress in the field by degrading transparency into the training of LLMs and forcing teams to rediscover many details in the training process. We present LLM360, an initiative to fully open-source LLMs, which advocates for all training code and data, model checkpoints, and intermediate results to be made available to the community. The goal of LLM360 is to support open and collaborative AI research by making the end-to-end LLM training process transparent and reproducible by everyone. As a first step of LLM360, we release two 7B parameter LLMs pre-trained from scratch, Amber and CrystalCoder, including their training code, data, intermediate checkpoints, and analyses (at https://www.llm360.ai). We are committed to continually pushing the boundaries of LLMs through this open-source effort. More large-scale and stronger models are underway and will be released in the future.',\n",
       " 'Author: Eric P. Xing Title: Fusing Models with Complementary Expertise Publication year: 2023 Coauthors: Hongyi Wang, Felipe Maia Polo, Yuekai Sun, Souvik Kundu, Eric P. Xing, M. Yurochkin Abstract: Training AI models that generalize across tasks and domains has long been among the open problems driving AI research. The emergence of Foundation Models made it easier to obtain expert models for a given task, but the heterogeneity of data that may be encountered at test time often means that any single expert is insufficient. We consider the Fusion of Experts (FoE) problem of fusing outputs of expert models with complementary knowledge of the data distribution and formulate it as an instance of supervised learning. Our method is applicable to both discriminative and generative tasks and leads to significant performance improvements in image and text classification, text summarization, multiple-choice QA, and automatic evaluation of generated text. We also extend our method to the\"frugal\"setting where it is desired to reduce the number of expert model evaluations at test time.',\n",
       " 'Author: Eric P. Xing Title: Making Scalable Meta Learning Practical Publication year: 2023 Coauthors: Sang Keun Choe, Sanket Vaibhav Mehta, Hwijeen Ahn, W. Neiswanger, Pengtao Xie, Emma Strubell, Eric P. Xing Abstract: Despite its flexibility to learn diverse inductive biases in machine learning programs, meta learning (i.e., learning to learn) has long been recognized to suffer from poor scalability due to its tremendous compute/memory costs, training instability, and a lack of efficient distributed training support. In this work, we focus on making scalable meta learning practical by introducing SAMA, which combines advances in both implicit differentiation algorithms and systems. Specifically, SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients. Evaluated on multiple large-scale meta learning benchmarks, SAMA showcases up to 1.7/4.8x increase in throughput and 2.0/3.8x decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms. Furthermore, we show that SAMA-based data optimization leads to consistent improvements in text classification accuracy with BERT and RoBERTa large language models, and achieves state-of-the-art results in both small- and large-scale data pruning on image classification tasks, demonstrating the practical applicability of scalable meta learning across language and vision domains.',\n",
       " 'Author: Eric P. Xing Title: PromptAgent: Strategic Planning with Language Models Enables Expert-level Prompt Optimization Publication year: 2023 Coauthors: Xinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Haotian Luo, Jiayou Zhang, Nebojsa Jojic, Eric P. Xing, Zhiting Hu Abstract: Highly effective, task-specific prompts are often heavily engineered by experts to integrate detailed instructions and domain insights based on a deep understanding of both instincts of large language models (LLMs) and the intricacies of the target task. However, automating the generation of such expert-level prompts remains elusive. Existing prompt optimization methods tend to overlook the depth of domain knowledge and struggle to efficiently explore the vast space of expert-level prompts. Addressing this, we present PromptAgent, an optimization method that autonomously crafts prompts equivalent in quality to those handcrafted by experts. At its core, PromptAgent views prompt optimization as a strategic planning problem and employs a principled planning algorithm, rooted in Monte Carlo tree search, to strategically navigate the expert-level prompt space. Inspired by human-like trial-and-error exploration, PromptAgent induces precise expert-level insights and in-depth instructions by reflecting on model errors and generating constructive error feedback. Such a novel framework allows the agent to iteratively examine intermediate prompts (states), refine them based on error feedbacks (actions), simulate future rewards, and search for high-reward paths leading to expert prompts. We apply PromptAgent to 12 tasks spanning three practical domains: BIG-Bench Hard (BBH), as well as domain-specific and general NLP tasks, showing it significantly outperforms strong Chain-of-Thought and recent prompt optimization baselines. Extensive analyses emphasize its capability to craft expert-level, detailed, and domain-insightful prompts with great efficiency and generalizability.',\n",
       " 'Author: Eric P. Xing Title: Cappy: Outperforming and Boosting Large Multi-Task LMs with a Small Scorer Publication year: 2023 Coauthors: Bowen Tan, Yun Zhu, Lijuan Liu, Eric P. Xing, Zhiting Hu, Jindong Chen Abstract: Large language models (LLMs) such as T0, FLAN, and OPT-IML, excel in multi-tasking under a unified instruction-following paradigm, where they also exhibit remarkable generalization abilities to unseen tasks. Despite their impressive performance, these LLMs, with sizes ranging from several billion to hundreds of billions of parameters, demand substantial computational resources, making their training and inference expensive and inefficient. Furthermore, adapting these models to downstream applications, particularly complex tasks, is often unfeasible due to the extensive hardware requirements for finetuning, even when utilizing parameter-efficient approaches such as prompt tuning. Additionally, the most powerful multi-task LLMs, such as OPT-IML-175B and FLAN-PaLM-540B, are not publicly accessible, severely limiting their customization potential. To address these challenges, we introduce a pretrained small scorer, Cappy, designed to enhance the performance and efficiency of multi-task LLMs. With merely 360 million parameters, Cappy functions either independently on classification tasks or serve as an auxiliary component for LLMs, boosting their performance. Moreover, Cappy enables efficiently integrating downstream supervision without requiring LLM finetuning nor the access to their parameters. Our experiments demonstrate that, when working independently on 11 language understanding tasks from PromptSource, Cappy outperforms LLMs that are several orders of magnitude larger. Besides, on 45 complex tasks from BIG-Bench, Cappy boosts the performance of the advanced multi-task LLM, FLAN-T5, by a large margin. Furthermore, Cappy is flexible to cooperate with other LLM adaptations, including finetuning and in-context learning, offering additional performance enhancement.',\n",
       " \"Author: Eric P. Xing Title: RedCoast: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU/TPUs Publication year: 2023 Coauthors: Bowen Tan, Yun Zhu, Lijuan Liu, Hongyi Wang, Yonghao Zhuang, Jindong Chen, Eric P. Xing, Zhiting Hu Abstract: The recent progress of AI can be largely attributed to large language models (LLMs). However, their escalating memory requirements introduce challenges for machine learning (ML) researchers and engineers. Addressing this requires developers to partition a large model to distribute it across multiple GPUs or TPUs. This necessitates considerable coding and intricate configuration efforts with existing model parallel tools, such as Megatron-LM, DeepSpeed, and Alpa. These tools require users' expertise in machine learning systems (MLSys), creating a bottleneck in LLM development, particularly for developers without MLSys background. In this work, we present RedCoast(Redco), a lightweight and user-friendly tool crafted to automate distributed training and inference for LLMs, as well as to simplify ML pipeline development. The design of Redco emphasizes two key aspects. Firstly, to automate model parallism, our study identifies two straightforward rules to generate tensor parallel strategies for any given LLM. Integrating these rules into Redco facilitates effortless distributed LLM training and inference, eliminating the need of additional coding or complex configurations. We demonstrate the effectiveness by applying Redco on a set of LLM architectures, such as GPT-J, LLaMA, T5, and OPT, up to the size of 66B. Secondly, we propose a mechanism that allows for the customization of diverse ML pipelines through the definition of merely three functions, avoiding redundant and formulaic code like multi-host related processing. This mechanism proves adaptable across a spectrum of ML algorithms, from foundational language modeling to complex algorithms like meta-learning and reinforcement learning. Consequently, Redco implementations exhibit much fewer code lines compared to their official counterparts.\",\n",
       " 'Author: Eric P. Xing Title: Neural-Symbolic Interaction and Co-Evolving Publication year: 2023 Coauthors: Bowen Tan, Shibo Hao, Eric P. Xing, Zhiting Hu ',\n",
       " 'Author: Eric P. Xing Title: Redco: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU/TPUs Publication year: 2023 Coauthors: Bowen Tan, Yun Zhu, Lijuan Liu, Hongyi Wang, Yonghao Zhuang, Jindong Chen, Eric P. Xing, Zhiting Hu Abstract: .',\n",
       " 'Author: Eric P. Xing Title: Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale From A New Perspective Publication year: 2023 Coauthors: Zeyuan Yin, Eric P. Xing, Zhiqiang Shen Abstract: We present a new dataset condensation framework termed Squeeze, Recover and Relabel (SRe$^2$L) that decouples the bilevel optimization of model and synthetic data during training, to handle varying scales of datasets, model architectures and image resolutions for efficient dataset condensation. The proposed method demonstrates flexibility across diverse dataset scales and exhibits multiple advantages in terms of arbitrary resolutions of synthesized images, low training cost and memory consumption with high-resolution synthesis, and the ability to scale up to arbitrary evaluation network architectures. Extensive experiments are conducted on Tiny-ImageNet and full ImageNet-1K datasets. Under 50 IPC, our approach achieves the highest 42.5% and 60.8% validation accuracy on Tiny-ImageNet and ImageNet-1K, outperforming all previous state-of-the-art methods by margins of 14.5% and 32.9%, respectively. Our approach also surpasses MTT in terms of speed by approximately 52$\\\\times$ (ConvNet-4) and 16$\\\\times$ (ResNet-18) faster with less memory consumption of 11.6$\\\\times$ and 6.4$\\\\times$ during data synthesis. Our code and condensed datasets of 50, 200 IPC with 4K recovery budget are available at https://github.com/VILA-Lab/SRe2L.',\n",
       " 'Author: Eric P. Xing Title: One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning Publication year: 2023 Coauthors: Arnav Chavan, Zhuang Liu, D. Gupta, Eric P. Xing, Zhiqiang Shen Abstract: We present Generalized LoRA (GLoRA), an advanced approach for universal parameter-efficient fine-tuning tasks. Enhancing Low-Rank Adaptation (LoRA), GLoRA employs a generalized prompt module to optimize pre-trained model weights and adjust intermediate activations, providing more flexibility and capability across diverse tasks and datasets. Moreover, GLoRA facilitates efficient parameter adaptation by employing a scalable, modular, layer-wise structure search that learns individual adapter of each layer. Originating from a unified mathematical formulation, GLoRA exhibits strong transfer learning, few-shot learning and domain generalization abilities, as it adapts to new tasks through not only weights but also additional dimensions like activations. Comprehensive experiments demonstrate that GLoRA outperforms all previous methods in natural, specialized, and structured vision benchmarks, achieving superior accuracy with fewer parameters and computations. The proposed method on LLaMA-1 and LLaMA-2 also show considerable enhancements compared to the original LoRA in the language domain. Furthermore, our structural re-parameterization design ensures that GLoRA incurs no extra inference cost, rendering it a practical solution for resource-limited applications. Code and models are available at: https://github.com/Arnav0400/ViT-Slim/tree/master/GLoRA.',\n",
       " 'Author: Eric P. Xing Title: Defending Against Malicious Behaviors in Federated Learning with Blockchain Publication year: 2023 Coauthors: Nanqing Dong, Zhipeng Wang, Jiahao Sun, Michael C. Kampffmeyer, Yizhe Wen, Shuoying Zhang, W. Knottenbelt, Eric P. Xing Abstract: In the era of deep learning, federated learning (FL) presents a promising approach that allows multi-institutional data owners, or clients, to collaboratively train machine learning models without compromising data privacy. However, most existing FL approaches rely on a centralized server for global model aggregation, leading to a single point of failure. This makes the system vulnerable to malicious attacks when dealing with dishonest clients. In this work, we address this problem by proposing a secure and reliable FL system based on blockchain and distributed ledger technology. Our system incorporates a peer-to-peer voting mechanism and a reward-and-slash mechanism, which are powered by on-chain smart contracts, to detect and deter malicious behaviors. Both theoretical and empirical analyses are presented to demonstrate the effectiveness of the proposed approach, showing that our framework is robust against malicious client-side behaviors.',\n",
       " 'Author: Eric P. Xing Title: Weakly Supervised 3D Open-vocabulary Segmentation Publication year: 2023 Coauthors: Kunhao Liu, Fangneng Zhan, Jiahui Zhang, Muyu Xu, Yingchen Yu, A. E. Saddik, C. Theobalt, Eric P. Xing, Shijian Lu Abstract: Open-vocabulary segmentation of 3D scenes is a fundamental function of human perception and thus a crucial objective in computer vision research. However, this task is heavily impeded by the lack of large-scale and diverse 3D open-vocabulary segmentation datasets for training robust and generalizable models. Distilling knowledge from pre-trained 2D open-vocabulary segmentation models helps but it compromises the open-vocabulary feature as the 2D models are mostly finetuned with close-vocabulary datasets. We tackle the challenges in 3D open-vocabulary segmentation by exploiting pre-trained foundation models CLIP and DINO in a weakly supervised manner. Specifically, given only the open-vocabulary text descriptions of the objects in a scene, we distill the open-vocabulary multimodal knowledge and object reasoning capability of CLIP and DINO into a neural radiance field (NeRF), which effectively lifts 2D features into view-consistent 3D segmentation. A notable aspect of our approach is that it does not require any manual segmentation annotations for either the foundation models or the distillation process. Extensive experiments show that our method even outperforms fully supervised models trained with segmentation annotations in certain scenes, suggesting that 3D open-vocabulary segmentation can be effectively learned from 2D images and text-image pairs. Code is available at \\\\url{https://github.com/Kunhao-Liu/3D-OVS}.',\n",
       " 'Author: Eric P. Xing Title: Supplementary Material for KD-DLGAN: Data Limited Image Generation via Knowledge Distillation Publication year: 2023 Coauthors: Kaiwen Cui, Yingchen Yu, Fangneng Zhan, Shengcai Liao, Shijian Lu, Eric P. Xing Abstract: We conduct experiments over multiple widely adopted datasets including: 100-shot, AFHQ, CIFAR-10, CIFAR100 and ImageNet. 100-shot: 100-shot contains three datasets each of which has 100 samples of resolution 256 × 256. The three datasets are 100-shot Obama, 100-shot Grumpy Cat and 100-shot Panda. AFHQ: AFHQ consists of face images of three types of animals including Cat, Dog and Wildlife, each of which has 5k training images. We follow DA [9] and use 160 AFHQCat images and 389 AFHQ-Dog images (at a resolution of 256 × 256) for training. CIFAR-10: CIFAR-10 contains 50k training images and 10k validation images with 10 classes. The image resolution is 32 × 32. In our experiments, three networks are trained with 100%, 20% or 10% training images, respectively, and the trained models are valuated over all the validation images. CIFAR-100: CIFAR-100 contains 50k training images and 10k validation images of 100 classes. The image resolution is 32 × 32. In our experiments, three networks are trained with 100%, 20% or 10% training images, respectively, and the trained models are evaluated over all the validation data.',\n",
       " 'Author: Eric P. Xing Title: Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming Publication year: 2023 Coauthors: Hanlin Zhang, Jiani Huang, Ziyang Li, M. Naik, Eric P. Xing Abstract: Pre-trained large language models (LMs) struggle to perform logical reasoning reliably despite advances in scale and compositionality. In this work, we tackle this challenge through the lens of symbolic programming. We propose DSR-LM, a Differentiable Symbolic Reasoning framework where pre-trained LMs govern the perception of factual knowledge, and a symbolic module performs deductive reasoning. In contrast to works that rely on hand-crafted logic rules, our differentiable symbolic reasoning framework efficiently learns weighted rules and applies semantic loss to further improve LMs. DSR-LM is scalable, interpretable, and allows easy integration of prior knowledge, thereby supporting extensive symbolic programming to robustly derive a logical conclusion. The results of our experiments suggest that DSR-LM improves the logical reasoning abilities of pre-trained language models, resulting in a significant increase in accuracy of over 20% on deductive reasoning benchmarks. Furthermore, DSR-LM outperforms a variety of competitive baselines when faced with systematic changes in sequence length.',\n",
       " 'Author: Eric P. Xing Title: Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models Publication year: 2023 Coauthors: Neha Sengupta, Sunil Kumar Sahu, Bokang Jia, Satheesh Katipomu, Haonan Li, Fajri Koto, Osama Mohammed Afzal, Samta Kamboj, O. Pandit, Rahul Pal, Lalit Pradhan, Zainul Mujahid, Massa Baali, Xudong Han, Alham Fikri Aji, Zhengzhong Liu, Andy Hock, Andrew Feldman, Jonathan Lee, A. Jackson, Preslav Nakov, Timothy Baldwin, Eric P. Xing Abstract: We introduce Jais and Jais-chat, new state-of-the-art Arabic-centric foundation and instruction-tuned open generative large language models (LLMs). The models are based on the GPT-3 decoder-only architecture and are pretrained on a mixture of Arabic and English texts, including source code in various programming languages. With 13 billion parameters, they demonstrate better knowledge and reasoning capabilities in Arabic than any existing open Arabic and multilingual models by a sizable margin, based on extensive evaluation. Moreover, the models are competitive in English compared to English-centric open models of similar size, despite being trained on much less English data. We provide a detailed description of the training, the tuning, the safety alignment, and the evaluation of the models. We release two open versions of the model -- the foundation Jais model, and an instruction-tuned Jais-chat variant -- with the aim of promoting research on Arabic LLMs. Available at https://huggingface.co/inception-mbzuai/jais-13b-chat',\n",
       " 'Author: Eric P. Xing Title: KD-DLGAN: Data Limited Image Generation via Knowledge Distillation Publication year: 2023 Coauthors: Kaiwen Cui, Yingchen Yu, Fangneng Zhan, Shengcai Liao, Shijian Lu1, Eric P. Xing Abstract: Generative Adversarial Networks (GANs) rely heavily on large-scale training data for training high-quality image generation models. With limited training data, the GAN discriminator often suffers from severe overfitting which directly leads to degraded generation especially in generation diversity. Inspired by the recent advances in knowledge distillation (KD), we propose KD-DLGAN, a knowledge-distillation based generation framework that introduces pre-trained vision-language models for training effective data-limited generation models. KD-DLGAN consists of two innovative designs. The first is aggregated generative KD that mitigates the discriminator overfitting by challenging the discriminator with harder learning tasks and distilling more generalizable knowledge from the pre-trained models. The second is correlated generative KD that improves the generation diversity by distilling and preserving the diverse image-text correlation within the pre-trained models. Extensive experiments over multiple benchmarks show that KD-DLGAN achieves superior image generation with limited training data. In addition, KD-DLGAN complements the state-of-the-art with consistent and substantial performance gains. Note that codes will be released.',\n",
       " 'Author: Eric P. Xing Title: 3D Semantic Segmentation in the Wild: Learning Generalized Models for Adverse-Condition Point Clouds Publication year: 2023 Coauthors: Aoran Xiao, Jiaxing Huang, Weihao Xuan, Ruijie Ren, Kangcheng Liu, Dayan Guan, A. E. Saddik, Shijian Lu, Eric P. Xing Abstract: Robust point cloud parsing under all-weather conditions is crucial to level-5 autonomy in autonomous driving. However, how to learn a universal 3D semantic segmentation (3DSS) model is largely neglected as most existing benchmarks are dominated by point clouds captured under normal weather. We introduce SemanticSTF, an adverse-weather point cloud dataset that provides dense point-level annotations and allows to study 3DSS under various adverse weather conditions. We study all-weather 3DSS modeling under two setups: 1) domain adaptive 3DSS that adapts from normal-weather data to adverse-weather data; 2) domain generalizable 3DSS that learns all-weather 3DSS models from normal-weather data. Our studies reveal the challenge while existing 3DSS methods encounter adverse-weather data, showing the great value of SemanticSTF in steering the future endeavor along this very meaningful research direction. In addition, we design a domain randomization technique that alternatively randomizes the geometry styles of point clouds and aggregates their embeddings, ultimately leading to a generalizable model that can improve 3DSS under various adverse weather effectively. The SemanticSTF and related codes are available at https://github.com/xiaoaoran/SemanticSTF.',\n",
       " 'Author: Eric P. Xing Title: Cuttlefish: Low-Rank Model Training without All the Tuning Publication year: 2023 Coauthors: Hongyi Wang, Saurabh Agarwal, Pongsakorn U-chupala, Yoshiki Tanaka, Eric P. Xing, Dimitris Papailiopoulos Abstract: Recent research has shown that training low-rank neural networks can effectively reduce the total number of trainable parameters without sacrificing predictive accuracy, resulting in end-to-end speedups. However, low-rank model training necessitates adjusting several additional factorization hyperparameters, such as the rank of the factorization at each layer. In this paper, we tackle this challenge by introducing Cuttlefish, an automated low-rank training approach that eliminates the need for tuning factorization hyperparameters. Cuttlefish leverages the observation that after a few epochs of full-rank training, the stable rank (i.e., an approximation of the true rank) of each layer stabilizes at a constant value. Cuttlefish switches from full-rank to low-rank training once the stable ranks of all layers have converged, setting the dimension of each factorization to its corresponding stable rank. Our results show that Cuttlefish generates models up to 5.6 times smaller than full-rank models, and attains up to a 1.2 times faster end-to-end training process while preserving comparable accuracy. Moreover, Cuttlefish outperforms state-of-the-art low-rank model training methods and other prominent baselines. The source code for our implementation can be found at: https://github.com/hwang595/Cuttlefish.',\n",
       " 'Author: Eric P. Xing Title: Does compressing activations help model parallel training? Publication year: 2023 Coauthors: S. Bian, Dacheng Li, Hongyi Wang, Eric P. Xing, S. Venkataraman Abstract: Large-scale Transformer models are known for their exceptional performance in a range of tasks, but training them can be difficult due to the requirement for communication-intensive model parallelism. One way to improve training speed is to compress the message size in communication. Previous approaches have primarily focused on compressing gradients in a data parallelism setting, but compression in a model-parallel setting is an understudied area. We have discovered that model parallelism has fundamentally different characteristics than data parallelism. In this work, we present the first empirical study on the effectiveness of compression methods for model parallelism. We implement and evaluate three common classes of compression algorithms - pruning-based, learning-based, and quantization-based - using a popular Transformer training framework. We evaluate these methods across more than 160 settings and 8 popular datasets, taking into account different hyperparameters, hardware, and both fine-tuning and pre-training stages. We also provide analysis when the model is scaled up. Finally, we provide insights for future development of model parallelism compression algorithms.',\n",
       " 'Author: Eric P. Xing Title: Federated Learning as Variational Inference: A Scalable Expectation Propagation Approach Publication year: 2023 Coauthors: Han Guo, P. Greengard, Hongyi Wang, A. Gelman, Yoon Kim, Eric P. Xing Abstract: The canonical formulation of federated learning treats it as a distributed optimization problem where the model parameters are optimized against a global loss function that decomposes across client loss functions. A recent alternative formulation instead treats federated learning as a distributed inference problem, where the goal is to infer a global posterior from partitioned client data (Al-Shedivat et al., 2021). This paper extends the inference view and describes a variational inference formulation of federated learning where the goal is to find a global variational posterior that well-approximates the true posterior. This naturally motivates an expectation propagation approach to federated learning (FedEP), where approximations to the global posterior are iteratively refined through probabilistic message-passing between the central server and the clients. We conduct an extensive empirical study across various algorithmic considerations and describe practical strategies for scaling up expectation propagation to the modern federated setting. We apply FedEP on standard federated learning benchmarks and find that it outperforms strong baselines in terms of both convergence speed and accuracy.',\n",
       " \"Author: Eric P. Xing Title: Memory-adaptive Depth-wise Heterogenous Federated Learning Publication year: 2023 Coauthors: Kai Zhang, Yutong Dai, Hongyi Wang, Eric P. Xing, Xun Chen, Lichao Sun Abstract: Federated learning is a promising paradigm that allows multiple clients to collaboratively train a model without sharing the local data. However, the presence of heterogeneous devices in federated learning, such as mobile phones and IoT devices with varying memory capabilities, would limit the scale and hence the performance of the model could be trained. The mainstream approaches to address memory limitations focus on width-slimming techniques, where different clients train subnetworks with reduced widths locally and then the server aggregates the subnetworks. The global model produced from these methods suffers from performance degradation due to the negative impact of the actions taken to handle the varying subnetwork widths in the aggregation phase. In this paper, we introduce a memory-adaptive depth-wise learning solution in FL called FeDepth, which adaptively decomposes the full model into blocks according to the memory budgets of each client and trains blocks sequentially to obtain a full inference model. Our method outperforms state-of-the-art approaches, achieving 5% and more than 10% improvements in top-1 accuracy on CIFAR-10 and CIFAR-100, respectively. We also demonstrate the effectiveness of depth-wise fine-tuning on ViT. Our findings highlight the importance of memory-aware techniques for federated learning with heterogeneous devices and the success of depth-wise training strategy in improving the global model's performance.\",\n",
       " \"Author: E. Xing Title: Identification of Nonlinear Latent Hierarchical Models Publication year: 2023 Coauthors: Lingjing Kong, Biwei Huang, Feng Xie, E. Xing, Yuejie Chi, Kun Zhang Abstract: Identifying latent variables and causal structures from observational data is essential to many real-world applications involving biological data, medical data, and unstructured data such as images and languages. However, this task can be highly challenging, especially when observed variables are generated by causally related latent variables and the relationships are nonlinear. In this work, we investigate the identification problem for nonlinear latent hierarchical causal models in which observed variables are generated by a set of causally related latent variables, and some latent variables may not have observed children. We show that the identifiability of causal structures and latent variables (up to invertible transformations) can be achieved under mild assumptions: on causal structures, we allow for multiple paths between any pair of variables in the graph, which relaxes latent tree assumptions in prior work; on structural functions, we permit general nonlinearity and multi-dimensional continuous variables, alleviating existing work's parametric assumptions. Specifically, we first develop an identification criterion in the form of novel identifiability guarantees for an elementary latent variable model. Leveraging this criterion, we show that both causal structures and latent variables of the hierarchical model can be identified asymptotically by explicitly constructing an estimation procedure. To the best of our knowledge, our work is the first to establish identifiability guarantees for both causal structures and latent variables in nonlinear latent hierarchical models.\",\n",
       " 'Author: E. Xing Title: StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields Publication year: 2023 Coauthors: Kunhao Liu, Fangneng Zhan, Yiwen Chen, Jiahui Zhang, Yingchen Yu, Abdulmotaleb El Saddik, Shijian Lu, E. Xing Abstract: 3D style transfer aims to render stylized novel views of a 3D scene with multiview consistency. However, most existing work suffers from a three-way dilemma over accurate geometry reconstruction, high-quality stylization, and being generalizable to arbitrary new styles. We propose StyleRF (Style Radiance Fields), an innovative 3D style transfer technique that resolves the three-way dilemma by performing style transformation within the feature space of a radiance field. StyleRF employs an explicit grid of high-level features to represent 3D scenes, with which highfidelity geometry can be reliably restored via volume rendering. In addition, it transforms the grid features according to the reference style which directly leads to high-quality zero-shot style transfer. StyleRF consists of two innovative designs. The first is sampling-invariant content transformation that makes the transformation invariant to the holistic statistics of the sampled 3D points and accordingly ensures multi-view consistency. The second is deferred style transformation of 2D feature maps which is equivalent to the transformation of 3D points but greatly reduces memory footprint without degrading multi-view consistency. Extensive experiments show that StyleRF achieves superior 3D stylization quality with precise geometry reconstruction and it can generalize to various new styles in a zero-shot manner. Project website: https://kunhao-liu.github.io/StyleRF/',\n",
       " 'Author: E. Xing Title: Judging LLM-as-a-judge with MT-Bench and Chatbot Arena Publication year: 2023 Coauthors: Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, E. Xing, Haotong Zhang, Joseph Gonzalez, I. Stoica Abstract: Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.',\n",
       " 'Author: E. Xing Title: Understanding Masked Autoencoders via Hierarchical Latent Variable Models Publication year: 2023 Coauthors: Lingjing Kong, Martin Q. Ma, Guan-Hong Chen, E. Xing, Yuejie Chi, Louis-Philippe Morency, Kun Zhang Abstract: Masked autoencoder (MAE), a simple and effective self-supervised learning framework based on the reconstruction of masked image regions, has recently achieved prominent success in a variety of vision tasks. Despite the emergence of intriguing empirical observations on MAE, a theoretically principled understanding is still lacking. In this work, we formally characterize and justify existing empirical in-sights and provide theoretical guarantees of MAE. We formulate the underlying data-generating process as a hierarchical latent variable model, and show that under reasonable assumptions, MAE provably identifies a set of latent variables in the hierarchical model, explaining why MAE can extract high-level information from pixels. Further, we show how key hyperparameters in MAE (the masking ratio and the patch size) determine which true latent variables to be recovered, therefore influencing the level of semantic information in the representation. Specifically, extremely large or small masking ratios inevitably lead to low-level representations. Our theory offers coherent explanations of existing empirical observations and provides insights for potential empirical improvements and fundamental limitations of the masked-reconstruction paradigm. We conduct extensive experiments to validate our theoretical insights.',\n",
       " 'Author: Eric P. Xing Title: LightSeq: Sequence Level Parallelism for Distributed Training of Long Context Transformers Publication year: 2023 Coauthors: Dacheng Li, Rulin Shao, Anze Xie, Eric P. Xing, Joseph E. Gonzalez, I. Stoica, Xuezhe Ma, Hao Zhang Abstract: Increasing the context length of large language models (LLMs) unlocks fundamentally new capabilities, but also significantly increases the memory footprints of training. Previous model-parallel systems such as Megatron-LM partition and compute different attention heads in parallel, resulting in large communication volumes, so they cannot scale beyond the number of attention heads, thereby hindering its adoption. In this paper, we introduce a new approach, LightSeq, for long-context LLMs training. LightSeq has many notable advantages. First, LightSeq partitions over the sequence dimension, hence is agnostic to model architectures and readily applicable for models with varying numbers of attention heads, such as Multi-Head, Multi-Query and Grouped-Query attention. Second, LightSeq not only requires up to 4.7x less communication than Megatron-LM on popular LLMs but also overlaps the communication with computation. To further reduce the training time, LightSeq features a novel gradient checkpointing scheme to bypass an forward computation for memory-efficient attention. We evaluate LightSeq on Llama-7B and its variants with sequence lengths from 32K to 512K. Through comprehensive experiments on single and cross-node training, we show that LightSeq achieves up to 1.24-2.01x end-to-end speedup, and a 2-8x longer sequence length on models with fewer heads, compared to Megatron-LM. Codes will be available at https://github.com/RulinShao/LightSeq.',\n",
       " \"Author: Eric P. Xing Title: LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset Publication year: 2023 Coauthors: Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric P. Xing, Joseph E. Gonzalez, I. Stoica, Haotong Zhang Abstract: Studying how people interact with large language models (LLMs) in real-world scenarios is increasingly important due to their widespread use in various applications. In this paper, we introduce LMSYS-Chat-1M, a large-scale dataset containing one million real-world conversations with 25 state-of-the-art LLMs. This dataset is collected from 210K unique IP addresses in the wild on our Vicuna demo and Chatbot Arena website. We offer an overview of the dataset's content, including its curation process, basic statistics, and topic distribution, highlighting its diversity, originality, and scale. We demonstrate its versatility through four use cases: developing content moderation models that perform similarly to GPT-4, building a safety benchmark, training instruction-following models that perform similarly to Vicuna, and creating challenging benchmark questions. We believe that this dataset will serve as a valuable resource for understanding and advancing LLM capabilities. The dataset is publicly available at https://huggingface.co/datasets/lmsys/lmsys-chat-1m.\",\n",
       " \"Author: Eric Xing Title: ALISON: Fast and Effective Stylometric Authorship Obfuscation Publication year: 2024 Coauthors: Eric Xing, Saranya Venkatraman, Thai Le, Dongwon Lee Abstract: Authorship Attribution (AA) and Authorship Obfuscation (AO) are two competing tasks of increasing importance in privacy research. Modern AA leverages an author's consistent writing style to match a text to its author using an AA classifier. AO is the corresponding adversarial task, aiming to modify a text in such a way that its semantics are preserved, yet an AA model cannot correctly infer its authorship. To address privacy concerns raised by state-of-the-art (SOTA) AA methods, new AO methods have been proposed but remain largely impractical to use due to their prohibitively slow training and obfuscation speed, often taking hours. To this challenge, we propose a practical AO method, ALISON, that (1) dramatically reduces training/obfuscation time, demonstrating more than 10x faster obfuscation than SOTA AO methods, (2) achieves better obfuscation success through attacking three transformer-based AA methods on two benchmark datasets, typically performing 15% better than competing methods, (3) does not require direct signals from a target AA classifier during obfuscation, and (4) utilizes unique stylometric features, allowing sound model interpretation for explainable obfuscation. We also demonstrate that ALISON can effectively prevent four SOTA AA methods from accurately determining the authorship of ChatGPT-generated texts, all while minimally changing the original text semantics. To ensure the reproducibility of our findings, our code and data are available at: https://github.com/EricX003/ALISON.\",\n",
       " 'Author: Eric Xing Title: A Study on the Calibration of In-context Learning Publication year: 2023 Coauthors: Hanlin Zhang, Yi-Fan Zhang, Yaodong Yu, Dhruv Madeka, Dean Foster, Eric Xing, Hima Lakkaraju, S. Kakade Abstract: Accurate uncertainty quantification is crucial for the safe deployment of language models (LMs), and prior research has demonstrated improvements in the calibration of modern LMs. Our study focuses on in-context learning (ICL), a prevalent method for adapting static LMs through tailored prompts, and examines the balance between performance and calibration across a broad spectrum of natural language understanding and reasoning tasks. Through comprehensive experiments, we observe that, with an increasing number of ICL examples, models initially exhibit increased miscalibration before achieving better calibration and miscalibration tends to arise in low-shot settings. Moreover, we find that methods aimed at improving usability, such as fine-tuning and chain-of-thought (CoT) prompting, can lead to miscalibration and unreliable natural language explanations, suggesting that new methods may be required for scenarios where models are expected to be reliable.',\n",
       " \"Author: Eric Xing Title: SegMix: A Simple Structure-Aware Data Augmentation Method Publication year: 2023 Coauthors: Yuxin Pei, Pushkar Bhuse, Zhengzhong Liu, Eric Xing Abstract: Interpolation-based Data Augmentation (DA) methods (Mixup) linearly interpolate the inputs and labels of two or more training examples. Mixup has more recently been adapted to the field of Natural Language Processing (NLP), mainly for sequence labeling tasks. However, such a simple adoption yields mixed or unstable improvements over the baseline models. We argue that the direct-adoption methods do not account for structures in NLP tasks. To this end, we propose SegMix, a collection of interpolation-based DA algorithms that can adapt to task-specific structures. SegMix poses fewer constraints on data structures, is robust to various hyperparameter settings, applies to more task settings, and adds little computational overhead. In the algorithm's core, we apply interpolation methods on task-specific meaningful segments, in contrast to applying them on sequences as in prior work. We find SegMix to be a flexible framework that combines rule-based DA methods with interpolation-based methods, creating interesting mixtures of DA techniques. We show that SegMix consistently improves performance over strong baseline models in Named Entity Recognition (NER) and Relation Extraction (RE) tasks, especially under data-scarce settings. Furthermore, this method is easy to implement and adds negligible training overhead.\",\n",
       " \"Author: Eric Xing Title: Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large Language Models Publication year: 2024 Coauthors: Loka Li, Guan-Hong Chen, Yusheng Su, Zhenhao Chen, Yixuan Zhang, Eric Xing, Kun Zhang Abstract: The recent success of Large Language Models (LLMs) has catalyzed an increasing interest in their self-correction capabilities. This paper presents a comprehensive investigation into the intrinsic self-correction of LLMs, attempting to address the ongoing debate about its feasibility. Our research has identified an important latent factor - the ``confidence'' of LLMs - during the self-correction process. Overlooking this factor may cause the models to over-criticize themselves, resulting in unreliable conclusions regarding the efficacy of self-correction. We have experimentally observed that LLMs possess the capability to understand the ``confidence'' in their own responses. It motivates us to develop an ``If-or-Else'' (IoE) prompting framework, designed to guide LLMs in assessing their own ``confidence'', facilitating intrinsic self-corrections. We conduct extensive experiments and demonstrate that our IoE-based Prompt can achieve a consistent improvement regarding the accuracy of self-corrected responses over the initial answers. Our study not only sheds light on the underlying factors affecting self-correction in LLMs, but also introduces a practical framework that utilizes the IoE prompting principle to efficiently improve self-correction capabilities with ``confidence''. The code is available at \\\\url{https://github.com/MBZUAI-CLeaR/IoE-Prompting.git}.\",\n",
       " 'Author: Eric Xing Title: Temporally Disentangled Representation Learning under Unknown Nonstationarity Publication year: 2023 Coauthors: Xiangchen Song, Weiran Yao, Yewen Fan, Xinshuai Dong, Guan-Hong Chen, Juan Carlos Niebles, Eric Xing, Kun Zhang Abstract: In unsupervised causal representation learning for sequential data with time-delayed latent causal influences, strong identifiability results for the disentanglement of causally-related latent variables have been established in stationary settings by leveraging temporal structure. However, in nonstationary setting, existing work only partially addressed the problem by either utilizing observed auxiliary variables (e.g., class labels and/or domain indexes) as side information or assuming simplified latent causal dynamics. Both constrain the method to a limited range of scenarios. In this study, we further explored the Markov Assumption under time-delayed causally related process in nonstationary setting and showed that under mild conditions, the independent latent components can be recovered from their nonlinear mixture up to a permutation and a component-wise transformation, without the observation of auxiliary variables. We then introduce NCTRL, a principled estimation framework, to reconstruct time-delayed latent causal variables and identify their relations from measured sequential data only. Empirical evaluations demonstrated the reliable identification of time-delayed latent causal influences, with our methodology substantially outperforming existing baselines that fail to exploit the nonstationarity adequately and then, consequently, cannot distinguish distribution shifts.',\n",
       " 'Author: Eric P. Xing Title: LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning Publication year: 2023 Coauthors: Han Guo, P. Greengard, Eric P. Xing, Yoon Kim Abstract: We propose a simple approach for memory-efficient adaptation of pretrained language models. Our approach uses an iterative algorithm to decompose each pretrained matrix into a high-precision low-rank component and a memory-efficient quantized component. During finetuning, the quantized component remains fixed and only the low-rank component is updated. We present an integer linear programming formulation of the quantization component which enables dynamic configuration of quantization parameters (e.g., bit-width, block size) for each matrix given an overall target memory budget. We further explore a data-aware version of the algorithm which uses an approximation of the Fisher information matrix to weight the reconstruction objective during matrix decomposition. Experiments on finetuning RoBERTa and LLaMA-2 (7B and 70B) demonstrate that our low-rank plus quantized matrix decomposition approach (LQ-LoRA) outperforms strong QLoRA and GPTQ-LoRA baselines and enables aggressive quantization to sub-3 bits with only minor performance degradations. When finetuned on a language modeling calibration dataset, LQ-LoRA can also be used for model compression; in this setting our 2.75-bit LLaMA-2-70B model (which has 2.85 bits on average when including the low-rank components and requires 27GB of GPU memory) performs respectably compared to the 16-bit baseline.',\n",
       " 'Author: Bhiksha Ramakrishnan Title: Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive Instruction-Tuning Benchmark for Speech Publication year: 2023 Coauthors: Chien-yu Huang, Ke-Han Lu, Shi Wang, Chi-Yuan Hsiao, Chun-Yi Kuan, Haibin Wu, Siddhant Arora, Kai-Wei Chang, Jiatong Shi, Yifan Peng, Roshan Sharma, Shinji Watanabe, Bhiksha Ramakrishnan, Shady Shehata, Hung-yi Lee Abstract: Text language models have shown remarkable zero-shot capability in generalizing to unseen tasks when provided with well-formulated instructions. However, existing studies in speech processing primarily focus on limited or specific tasks. Moreover, the lack of standardized benchmarks hinders a fair comparison across different approaches. Thus, we present Dynamic-SUPERB, a benchmark designed for building universal speech models capable of leveraging instruction tuning to perform multiple tasks in a zero-shot fashion. To achieve comprehensive coverage of diverse speech tasks and harness instruction tuning, we invite the community to collaborate and contribute, facilitating the dynamic growth of the benchmark. To initiate, Dynamic-SUPERB features 55 evaluation instances by combining 33 tasks and 22 datasets. This spans a broad spectrum of dimensions, providing a comprehensive platform for evaluation. Additionally, we propose several approaches to establish benchmark baselines. These include the utilization of speech models, text language models, and the multimodal encoder. Evaluation results indicate that while these baselines perform reasonably on seen tasks, they struggle with unseen ones. We also conducted an ablation study to assess the robustness and seek improvements in the performance. We release all materials to the public and welcome researchers to collaborate on the project, advancing technologies in the field together.',\n",
       " 'Author: C. Rosé Title: High school students’ data modeling practices and processes: from modeling unstructured data to evaluating automated decisions Publication year: 2023 Coauthors: Shiyan Jiang, Hengtao Tang, Can Tatar, C. Rosé, J. Chao Abstract: ABSTRACT It’s critical to foster artificial intelligence (AI) literacy for high school students, the first generation to grow up surrounded by AI, to understand working mechanism of data-driven AI technologies and critically evaluate automated decisions from predictive models. While efforts have been made to engage youth in understanding AI through developing machine learning models, few provided in-depth insights into the nuanced learning processes. In this study, we examined high school students’ data modeling practices and processes. Twenty-eight students developed machine learning models with text data for classifying negative and positive reviews of ice cream stores. We identified nine data modeling practices that describe students’ processes of model exploration, development, and testing and two themes about evaluating automated decisions from data technologies. The results provide implications for designing accessible data modeling experiences for students to understand data justice as well as the role and responsibility of data modelers in creating AI technologies.',\n",
       " 'Author: C. Rosé Title: Linguistic representations for fewer-shot relation extraction across domains Publication year: 2023 Coauthors: Sireesh Gururaja, Ritam Dutt, Ting-gen Liao, C. Rosé Abstract: Recent work has demonstrated the positive impact of incorporating linguistic representations as additional context and scaffolds on the in-domain performance of several NLP tasks. We extend this work by exploring the impact of linguistic representations on cross-domain performance in a few-shot transfer setting. An important question is whether linguistic representations enhance generalizability by providing features that function as cross-domain pivots. We focus on the task of relation extraction on three datasets of procedural text in two domains, cooking and materials science. Our approach augments a popular transformer-based architecture by alternately incorporating syntactic and semantic graphs constructed by freely available off-the-shelf tools. We examine their utility for enhancing generalization, and investigate whether earlier findings, e.g. that semantic representations can be more helpful than syntactic ones, extend to relation extraction in multiple domains. We find that while the inclusion of these graphs results in significantly higher performance in few-shot transfer, both types of graph exhibit roughly equivalent utility.',\n",
       " 'Author: C. Rosé Title: Using counterfactual contrast to improve compositional generalization for multi-step quantitative reasoning Publication year: 2023 Coauthors: Armineh Nourbakhsh, Sameena Shah, C. Rosé Abstract: In quantitative question answering, compositional generalization is one of the main challenges of state of the art models, especially when longer sequences of reasoning steps are required. In this paper we propose CounterComp, a method that uses counterfactual scenarios to generate samples with compositional contrast. Instead of a data augmentation approach, CounterComp is based on metric learning, which allows for direct sampling from the training set and circumvents the need for additional human labels. Our proposed auxiliary metric learning loss improves the performance of three state of the art models on four recently released datasets. We also show how the approach can improve OOD performance on unseen domains, as well as unseen compositions. Lastly, we demonstrate how the method can lead to better compositional attention patterns during training.',\n",
       " 'Author: C. Rosé Title: Towards Extracting and Understanding the Implicit Rubrics of Transformer Based Automatic Essay Scoring Models Publication year: 2023 Coauthors: James Fiacco, David Adamson, C. Rosé Abstract: By aligning the functional components derived from the activations of transformer models trained for AES with external knowledge such as human-understandable feature groups, the proposed method improves the interpretability of a Longformer Automatic Essay Scoring (AES) system and provides tools for performing such analyses on further neural AES systems. The analysis focuses on models trained to score essays based on organization, main idea, support, and language. The findings provide insights into the models’ decision-making processes, biases, and limitations, contributing to the development of more transparent and reliable AES systems.',\n",
       " 'Author: C. Rosé Title: Exploring Artificial Intelligence in English Language Arts with StoryQ Publication year: 2023 Coauthors: J. Chao, Rebecca Ellis, Shiyan Jiang, C. Rosé, W. Finzer, Can Tatar, James Fiacco, Kenia Wiedemann Abstract: Exploring Artificial Intelligence (AI) in English Language Arts (ELA) with StoryQ is a 10-hour curriculum module designed for high school ELA classes. The module introduces students to fundamental AI concepts and essential machine learning workflow using StoryQ, a web-based GUI environment for Grades 6-12 learners. In this module, students work with unstructured text data and learn to train, test, and improve text classification models such as intent recognition, clickbait filter, and sentiment analysis. As they interact with machine-learning language models deeply, students also gain a nuanced understanding of language and how to wield it, not just as a data structure, but as a tool in our human-human encounters as well. The current version contains eight lessons, all delivered through a full-featured online learning and teaching platform. Computers and Internet access are required to implement the module. The module was piloted in an ELA class in the Spring of 2022, and the student learning outcomes were positive. The module is currently undergoing revision and will be further tested and improved in Fall 2022.',\n",
       " 'Author: Carolyn Rosé Title: SPEERLoom: An Open-Source Loom Kit for Interdisciplinary Engagement in Math, Engineering, and Textiles Publication year: 2023 Coauthors: Samantha Speer, Ana P Garcia-Alonzo, Joey Huang, N. Yankova, Carolyn Rosé, Kylie A Peppler, James Mccann, Melisa Orta Martinez Abstract: Weaving is a fabrication process that is grounded in mathematics and engineering: from the binary, matrix-like nature of the pattern drafts weavers have used for centuries, to the punch card programming of the first Jacquard looms. This intersection of disciplines provides an opportunity to ground abstract mathematical concepts in a concrete and embodied art, viewing this textile art through the lens of engineering. Currently, available looms are not optimized to take advantage of this opportunity to increase mathematics learning by providing hands-on interdisciplinary learning in collegiate classrooms. In this work, we present SPEERLoom: an open-source, robotic Jacquard loom kit designed to be a tool for interweaving cloth fabrication, mathematics, and engineering to support interdisciplinary learning in the classroom. We discuss the design requirements and subsequent design of SPEERLoom. We also present the results of a pilot study in a post-secondary class finding that SPEERLoom supports hands-on, interdisciplinary learning of math, engineering, and textiles.',\n",
       " 'Author: Alexander I. Rudnicky Title: Overview of the Tenth Dialog System Technology Challenge: DSTC10 Publication year: 2024 Coauthors: Koichiro Yoshino, Yun-Nung (Vivian) Chen, Paul A. Crook, Satwik Kottur, Jinchao Li, Behnam Hedayatnia, Seungwhan Moon, Zhengcong Fei, Zekang Li, Jinchao Zhang, Yang Feng, Jie Zhou, Seokhwan Kim, Yang Liu, Di Jin, A. Papangelis, Karthik Gopalakrishnan, Dilek Z. Hakkani-Tür, B. Damavandi, A. Geramifard, Chiori Hori, Ankit Shah, Chen Zhang, Haizhou Li, João Sedoc, L. F. D’Haro, Rafael E. Banchs, Alexander I. Rudnicky Abstract: This article introduces the Tenth Dialog System Technology Challenge (DSTC-10). This edition of the DSTC focuses on applying end-to-end dialog technologies for five distinct tasks in dialog systems, namely 1. Incorporation of Meme images into open domain dialogs, 2. Knowledge-grounded Task-oriented Dialogue Modeling on Spoken Conversations, 3. Situated Interactive Multimodal dialogs, 4. Reasoning for Audio Visual Scene-Aware Dialog, and 5. Automatic Evaluation and Moderation of Open-domainDialogue Systems. This article describes the task definition, provided datasets, baselines, and evaluation setup for each track. We also summarize the results of the submitted systems to highlight the general trends of the state-of-the-art technologies for the tasks.',\n",
       " 'Author: Alexander I. Rudnicky Title: Advancing Regular Language Reasoning in Linear Recurrent Neural Networks Publication year: 2023 Coauthors: Ting-Han Fan, Ta-Chung Chi, Alexander I. Rudnicky Abstract: In recent studies, linear recurrent neural networks (LRNNs) have achieved Transformer-level performance in natural language modeling and long-range modeling while offering rapid parallel training and constant inference costs. With the resurged interest in LRNNs, we study whether they can learn the hidden rules in training sequences, such as the grammatical structures of regular language. We theoretically analyze some existing LRNNs and discover their limitations on regular language. Motivated by the analysis, we propose a new LRNN equipped with a block-diagonal and input-dependent transition matrix. Experiments suggest that the proposed model is the only LRNN that can perform length extrapolation on regular language tasks such as Sum, Even Pair, and Modular Arithmetic.',\n",
       " 'Author: Alexander I. Rudnicky Title: Structured Dialogue Discourse Parsing Publication year: 2023 Coauthors: Ta-Chung Chi, Alexander I. Rudnicky Abstract: Dialogue discourse parsing aims to uncover the internal structure of a multi-participant conversation by finding all the discourse links and corresponding relations. Previous work either treats this task as a series of independent multiple-choice problems, in which the link existence and relations are decoded separately, or the encoding is restricted to only local interaction, ignoring the holistic structural information. In contrast, we propose a principled method that improves upon previous work from two perspectives: encoding and decoding. From the encoding side, we perform structured encoding on the adjacency matrix followed by the matrix-tree learning algorithm, where all discourse links and relations in the dialogue are jointly optimized based on latent tree-level distribution. From the decoding side, we perform structured inference using the modified Chiu-Liu-Edmonds algorithm, which explicitly generates the labeled multi-root non-projective spanning tree that best captures the discourse structure. In addition, unlike in previous work, we do not rely on hand-crafted features; this improves the model’s robustness. Experiments show that our method achieves new state-of-the-art, surpassing the previous model by 2.3 on STAC and 1.5 on Molweni (F1 scores).',\n",
       " 'Author: Alexander I. Rudnicky Title: A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech Publication year: 2023 Coauthors: Li-Wei Chen, Shinji Watanabe, Alexander I. Rudnicky Abstract: Recent Text-to-Speech (TTS) systems trained on reading or acted corpora have achieved near human-level naturalness. The diversity of human speech, however, often goes beyond the coverage of these corpora. We believe the ability to handle such diversity is crucial for AI systems to achieve human-level communication. Our work explores the use of more abundant real-world data for building speech synthesizers. We train TTS systems using real-world speech from YouTube and podcasts. We observe the mismatch between training and inference alignments in mel-spectrogram based autoregressive models, leading to unintelligible synthesis, and demonstrate that learned discrete codes within multiple code groups effectively resolves this issue. We introduce our MQTTS system whose architecture is designed for multiple code generation and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality. We conduct ablation analyses to identify the efficacy of our methods. We show that MQTTS outperforms existing TTS systems in several objective and subjective measures.',\n",
       " 'Author: Alexander I. Rudnicky Title: Tartan: an LLM Driven SocialBot Publication year: 2023 Coauthors: Liangze Li, Ziyuan Liu, Li-Wei Chen, Ta-Chung Chi, Alexander I. Rudnicky Abstract: We describe our work to date on using large language models (LLMs) in our Alexa Prize Socialbot. Our work has laid the groundwork for looking more closely at using LLMs in a conversational system. We also analyze common patterns in conversations our bot has had with users',\n",
       " \"Author: Alexander I. Rudnicky Title: Overview of Robust and Multilingual Automatic Evaluation Metrics\\n\\nfor Open-Domain Dialogue Systems at DSTC 11 Track 4 Publication year: 2023 Coauthors: Mario Rodr'iguez-Cantelar, Chen Zhang, Chengguang Tang, Ke Shi, Sarik Ghazarian, João Sedoc, L. F. D’Haro, Alexander I. Rudnicky Abstract: The advent and fast development of neural networks have revolutionized the research on dialogue systems and subsequently have triggered various challenges regarding their automatic evaluation. Automatic evaluation of open-domain dialogue systems as an open challenge has been the center of the attention of many researchers. Despite the consistent efforts to improve automatic metrics’ correlations with human evaluation, there have been very few attempts to assess their robustness over multiple domains and dimensions. Also, their focus is mainly on the English language. All of these challenges prompt the development of automatic evaluation metrics that are reliable in various domains, dimensions, and languages. This track in the 11th Dialogue System Technology Challenge (DSTC11) is part of the ongoing effort to promote robust and multilingual automatic evaluation metrics. This article describes the datasets and baselines provided to participants and discusses the submission and result details of the two proposed subtasks.\",\n",
       " 'Author: Alexander I. Rudnicky Title: Learning to Ask Questions for Zero-shot Dialogue State Tracking Publication year: 2023 Coauthors: Diogo Tavares, David Semedo, Alexander I. Rudnicky, João Magalhães Abstract: We present a method for performing zero-shot Dialogue State Tracking (DST) by casting the task as a learning-to-ask-questions framework. The framework learns to pair the best question generation (QG) strategy with in-domain question answering (QA) methods to extract slot values from a dialogue without any human intervention. A novel self-supervised QA pretraining step using in-domain data is essential to learn the structure without requiring any slot-filling annotations. Moreover, we show that QG methods need to be aligned with the same grammatical person used in the dialogue. Empirical evaluation on the MultiWOZ 2.1 dataset demonstrates that our approach, when used alongside robust QA models, outperforms existing zero-shot methods in the challenging task of zero-shot cross domain adaptation-given a comparable amount of domain knowledge during data creation. Finally, we analyze the impact of the types of questions used, and demonstrate that the algorithmic approach outperforms template-based question generation.',\n",
       " 'Author: A. Rudnicky Title: Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings Publication year: 2023 Coauthors: Ta-Chung Chi, Ting-Han Fan, Li-Wei Chen, A. Rudnicky, P. Ramadge Abstract: The use of positional embeddings in transformer language models is widely accepted. However, recent research has called into question the necessity of such embeddings. We further extend this inquiry by demonstrating that a randomly initialized and frozen transformer language model, devoid of positional embeddings, inherently encodes strong positional information through the shrinkage of self-attention variance. To quantify this variance, we derive the underlying distribution of each step within a transformer layer. Through empirical validation using a fully pretrained model, we show that the variance shrinkage effect still persists after extensive gradient updates. Our findings serve to justify the decision to discard positional embeddings and thus facilitate more efficient pretraining of transformer language models.',\n",
       " 'Author: A. Rudnicky Title: Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation Publication year: 2023 Coauthors: Ta-Chung Chi, Ting-Han Fan, A. Rudnicky, P. Ramadge Abstract: Unlike recurrent models, conventional wisdom has it that Transformers cannot perfectly model regular languages. Inspired by the notion of working memory, we propose a new Transformer variant named RegularGPT. With its novel combination of Weight-Sharing, Adaptive-Depth, and Sliding-Dilated-Attention, RegularGPT constructs working memory along the depth dimension, thereby enabling efficient and successful modeling of regular languages such as PARITY. We further test RegularGPT on the task of natural language length extrapolation and surprisingly find that it rediscovers the local windowed attention effect deemed necessary in prior work for length extrapolation.',\n",
       " \"Author: Maarten Sap Title: Relying on the Unreliable: The Impact of Language Models' Reluctance to Express Uncertainty Publication year: 2024 Coauthors: Kaitlyn Zhou, Jena D. Hwang, Xiang Ren, Maarten Sap Abstract: As natural language becomes the default interface for human-AI interaction, there is a critical need for LMs to appropriately communicate uncertainties in downstream applications. In this work, we investigate how LMs incorporate confidence about their responses via natural language and how downstream users behave in response to LM-articulated uncertainties. We examine publicly deployed models and find that LMs are unable to express uncertainties when answering questions even when they produce incorrect responses. LMs can be explicitly prompted to express confidences, but tend to be overconfident, resulting in high error rates (on average 47%) among confident responses. We test the risks of LM overconfidence by running human experiments and show that users rely heavily on LM generations, whether or not they are marked by certainty. Lastly, we investigate the preference-annotated datasets used in RLHF alignment and find that humans have a bias against texts with uncertainty. Our work highlights a new set of safety harms facing human-LM interactions and proposes design recommendations and mitigating strategies moving forward.\",\n",
       " 'Author: Maarten Sap Title: FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions Publication year: 2023 Coauthors: Hyunwoo Kim, Melanie Sclar, Xuhui Zhou, R. L. Bras, Gunhee Kim, Yejin Choi, Maarten Sap Abstract: Theory of mind (ToM) evaluations currently focus on testing models using passive narratives that inherently lack interactivity. We introduce FANToM, a new benchmark designed to stress-test ToM within information-asymmetric conversational contexts via question answering. Our benchmark draws upon important theoretical requisites from psychology and necessary empirical considerations when evaluating large language models (LLMs). In particular, we formulate multiple types of questions that demand the same underlying reasoning to identify illusory or false sense of ToM capabilities in LLMs. We show that FANToM is challenging for state-of-the-art LLMs, which perform significantly worse than humans even with chain-of-thought reasoning or fine-tuning.',\n",
       " \"Author: Maarten Sap Title: Modeling Empathic Similarity in Personal Narratives Publication year: 2023 Coauthors: Jocelyn Shen, Maarten Sap, Pedro Colon-Hernandez, Hae Won Park, C. Breazeal Abstract: The most meaningful connections between people are often fostered through expression of shared vulnerability and emotional experiences in personal narratives. We introduce a new task of identifying similarity in personal stories based on empathic resonance, i.e., the extent to which two people empathize with each others' experiences, as opposed to raw semantic or lexical similarity, as has predominantly been studied in NLP. Using insights from social psychology, we craft a framework that operationalizes empathic similarity in terms of three key features of stories: main events, emotional trajectories, and overall morals or takeaways. We create EmpathicStories, a dataset of 1,500 personal stories annotated with our empathic similarity features, and 2,000 pairs of stories annotated with empathic similarity scores. Using our dataset, we fine-tune a model to compute empathic similarity of story pairs, and show that this outperforms semantic similarity models on automated correlation and retrieval metrics. Through a user study with 150 participants, we also assess the effect our model has on retrieving stories that users empathize with, compared to naive semantic similarity-based retrieval, and find that participants empathized significantly more with stories retrieved by our model. Our work has strong implications for the use of empathy-aware models to foster human connection and empathy between people.\",\n",
       " 'Author: Maarten Sap Title: COBRA Frames: Contextual Reasoning about Effects and Harms of Offensive Statements Publication year: 2023 Coauthors: Xuhui Zhou, Haojie Zhu, Akhila Yerukola, Thomas Davidson, Jena D. Hwang, Swabha Swayamdipta, Maarten Sap Abstract: Warning: This paper contains content that may be offensive or upsetting. Understanding the harms and offensiveness of statements requires reasoning about the social and situational context in which statements are made. For example, the utterance\"your English is very good\"may implicitly signal an insult when uttered by a white man to a non-white colleague, but uttered by an ESL teacher to their student would be interpreted as a genuine compliment. Such contextual factors have been largely ignored by previous approaches to toxic language detection. We introduce COBRA frames, the first context-aware formalism for explaining the intents, reactions, and harms of offensive or biased statements grounded in their social and situational context. We create COBRACORPUS, a dataset of 33k potentially offensive statements paired with machine-generated contexts and free-text explanations of offensiveness, implied biases, speaker intents, and listener reactions. To study the contextual dynamics of offensiveness, we train models to generate COBRA explanations, with and without access to the context. We find that explanations by context-agnostic models are significantly worse than by context-aware ones, especially in situations where the context inverts the statement\\'s offensiveness (29% accuracy drop). Our work highlights the importance and feasibility of contextualized NLP by modeling social factors.',\n",
       " 'Author: Maarten Sap Title: Riveter: Measuring Power and Social Dynamics Between Entities Publication year: 2023 Coauthors: Maria Antoniak, Anjalie Field, Jimin Mun, Melanie Walsh, Lauren F. Klein, Maarten Sap Abstract: Riveter provides a complete easy-to-use pipeline for analyzing verb connotations associated with entities in text corpora. We prepopulate the package with connotation frames of sentiment, power, and agency, which have demonstrated usefulness for capturing social phenomena, such as gender bias, in a broad range of corpora. For decades, lexical frameworks have been foundational tools in computational social science, digital humanities, and natural language processing, facilitating multifaceted analysis of text corpora. But working with verb-centric lexica specifically requires natural language processing skills, reducing their accessibility to other researchers. By organizing the language processing pipeline, providing complete lexicon scores and visualizations for all entities in a corpus, and providing functionality for users to target specific research questions, Riveter greatly improves the accessibility of verb lexica and can facilitate a broad range of future research.',\n",
       " \"Author: Maarten Sap Title: Don't Take This Out of Context!: On the Need for Contextual Models and Evaluations for Stylistic Rewriting Publication year: 2023 Coauthors: Akhila Yerukola, Xuhui Zhou, Elizabeth Clark, Maarten Sap \",\n",
       " \"Author: Maarten Sap Title: Beyond Denouncing Hate: Strategies for Countering Implied Biases and Stereotypes in Language Publication year: 2023 Coauthors: Jimin Mun, Emily Allaway, Akhila Yerukola, Laura Vianna, Sarah-Jane Leslie, Maarten Sap Abstract: Counterspeech, i.e., responses to counteract potential harms of hateful speech, has become an increasingly popular solution to address online hate speech without censorship. However, properly countering hateful language requires countering and dispelling the underlying inaccurate stereotypes implied by such language. In this work, we draw from psychology and philosophy literature to craft six psychologically inspired strategies to challenge the underlying stereotypical implications of hateful language. We first examine the convincingness of each of these strategies through a user study, and then compare their usages in both human- and machine-generated counterspeech datasets. Our results show that human-written counterspeech uses countering strategies that are more specific to the implied stereotype (e.g., counter examples to the stereotype, external factors about the stereotype's origins), whereas machine-generated counterspeech uses less specific strategies (e.g., generally denouncing the hatefulness of speech). Furthermore, machine-generated counterspeech often employs strategies that humans deem less convincing compared to human-produced counterspeech. Our findings point to the importance of accounting for the underlying stereotypical implications of speech when generating counterspeech and for better machine reasoning about anti-stereotypical examples.\",\n",
       " 'Author: Maarten Sap Title: BiasX: \"Thinking Slow\" in Toxic Content Moderation with Explanations of Implied Social Biases Publication year: 2023 Coauthors: Yiming Zhang, Sravani Nanduri, Liwei Jiang, Tongshuang Wu, Maarten Sap Abstract: Toxicity annotators and content moderators often default to mental shortcuts when making decisions. This can lead to subtle toxicity being missed, and seemingly toxic but harmless content being over-detected. We introduce BiasX, a framework that enhances content moderation setups with free-text explanations of statements\\' implied social biases, and explore its effectiveness through a large-scale crowdsourced user study. We show that indeed, participants substantially benefit from explanations for correctly identifying subtly (non-)toxic content. The quality of explanations is critical: imperfect machine-generated explanations (+2.4% on hard toxic examples) help less compared to expert-written human explanations (+7.2%). Our results showcase the promise of using free-text explanations to encourage more thoughtful toxicity moderation.',\n",
       " 'Author: Maarten Sap Title: Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory Publication year: 2023 Coauthors: Niloofar Mireshghallah, Hyunwoo Kim, Xuhui Zhou, Yulia Tsvetkov, Maarten Sap, Reza Shokri, Yejin Choi Abstract: The interactive use of large language models (LLMs) in AI assistants (at work, home, etc.) introduces a new set of inference-time privacy risks: LLMs are fed different types of information from multiple sources in their inputs and are expected to reason about what to share in their outputs, for what purpose and with whom, within a given context. In this work, we draw attention to the highly critical yet overlooked notion of contextual privacy by proposing ConfAIde, a benchmark designed to identify critical weaknesses in the privacy reasoning capabilities of instruction-tuned LLMs. Our experiments show that even the most capable models such as GPT-4 and ChatGPT reveal private information in contexts that humans would not, 39% and 57% of the time, respectively. This leakage persists even when we employ privacy-inducing prompts or chain-of-thought reasoning. Our work underscores the immediate need to explore novel inference-time privacy-preserving approaches, based on reasoning and theory of mind.',\n",
       " \"Author: Maarten Sap Title: Improving Language Models with Advantage-based Offline Policy Gradients Publication year: 2023 Coauthors: Ashutosh Baheti, Ximing Lu, Faeze Brahman, Ronan Le Bras, Maarten Sap, Mark O. Riedl Abstract: Language Models (LMs) achieve substantial language capabilities when finetuned using Reinforcement Learning with Human Feedback (RLHF). However, RLHF is an unstable and data-hungry process that continually requires new high-quality LM-generated data for finetuning. We introduce Advantage-Leftover Lunch RL (A-LoL), a new class of offline policy gradient algorithms that enable RL training on any pre-existing data. By assuming the entire LM output sequence as a single action, A-LoL allows incorporating sequence-level classifiers or human-designed scoring functions as rewards. Subsequently, by using LM's internal sequence-level value estimate, A-LoL filters negative advantage (low-quality) data points during training, making it resilient to noise. Overall, A-LoL is an easy-to-implement LM training recipe that is sample-efficient and stable. We demonstrate the effectiveness of A-LoL and its variants with a set of four different language generation tasks. We compare against both online RL (PPO) and recent preference-based (DPO, PRO) and reward-based (GOLD) offline RL baselines. On the commonly-used RLHF benchmark, Helpful and Harmless Assistant (HHA), LMs trained with A-LoL methods achieve the highest diversity while also being rated more safe and helpful than baselines according to humans. Additionally, in the remaining three tasks, A-LoL could optimize multiple distinct reward functions even when using noisy or suboptimal training data. We also release our experimental code. https://github.com/abaheti95/LoL-RL\",\n",
       " 'Author: Maarten Sap Title: From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models Publication year: 2023 Coauthors: Julia Mendelsohn, Ronan Le Bras, Yejin Choi, Maarten Sap Abstract: Dogwhistles are coded expressions that simultaneously convey one meaning to a broad audience and a second, often hateful or provocative, meaning to a narrow in-group; they are deployed to evade both political repercussions and algorithmic content moderation. For example, the word “cosmopolitan” in a sentence such as “we need to end the cosmopolitan experiment” can mean “worldly” to many but also secretly mean “Jewish” to a select few. We present the first large-scale computational investigation of dogwhistles. We develop a typology of dogwhistles, curate the largest-to-date glossary of over 300 dogwhistles with rich contextual information and examples, and analyze their usage in historical U.S. politicians’ speeches. We then assess whether a large language model (GPT-3) can identify dogwhistles and their meanings, and find that GPT-3’s performance varies widely across types of dogwhistles and targeted groups. Finally, we show that harmful content containing dogwhistles avoids toxicity detection, highlighting online risks presented by such coded language. This work sheds light on the theoretical and applied importance of dogwhistles in both NLP and computational social science, and provides resources to facilitate future research in modeling dogwhistles and mitigating their online harms.',\n",
       " 'Author: Maarten Sap Title: NLPositionality: Characterizing Design Biases of Datasets and Models Publication year: 2023 Coauthors: Sebastin Santy, Jenny T Liang, Ronan Le Bras, Katharina Reinecke, Maarten Sap Abstract: Design biases in NLP systems, such as performance differences for different populations, often stem from their creator’s positionality, i.e., views and lived experiences shaped by identity and background. Despite the prevalence and risks of design biases, they are hard to quantify because researcher, system, and dataset positionality is often unobserved. We introduce NLPositionality, a framework for characterizing design biases and quantifying the positionality of NLP datasets and models. Our framework continuously collects annotations from a diverse pool of volunteer participants on LabintheWild, and statistically quantifies alignment with dataset labels and model predictions. We apply NLPositionality to existing datasets and models for two tasks—social acceptability and hate speech detection. To date, we have collected 16,299 annotations in over a year for 600 instances from 1,096 annotators across 87 countries.We find that datasets and models align predominantly with Western, White, college-educated, and younger populations. Additionally, certain groups, such as non-binary people and non-native English speakers, are further marginalized by datasets and models as they rank least in alignment across all tasks. Finally, we draw from prior literature to discuss how researchers can examine their own positionality and that of their datasets and models, opening the door for more inclusive NLP systems.',\n",
       " \"Author: Maarten Sap Title: Don't Take This Out of Context! On the Need for Contextual Models and Evaluations for Stylistic Rewriting Publication year: 2023 Coauthors: Akhila Yerukola, Xuhui Zhou, Maarten Sap Abstract: Most existing stylistic text rewriting methods and evaluation metrics operate on a sentence level, but ignoring the broader context of the text can lead to preferring generic, ambiguous, and incoherent rewrites. In this paper, we investigate integrating the preceding textual context into both the $\\\\textit{rewriting}$ and $\\\\textit{evaluation}$ stages of stylistic text rewriting, and introduce a new composite contextual evaluation metric $\\\\texttt{CtxSimFit}$ that combines similarity to the original sentence with contextual cohesiveness. We comparatively evaluate non-contextual and contextual rewrites in formality, toxicity, and sentiment transfer tasks. Our experiments show that humans significantly prefer contextual rewrites as more fitting and natural over non-contextual ones, yet existing sentence-level automatic metrics (e.g., ROUGE, SBERT) correlate poorly with human preferences ($\\\\rho$=0--0.3). In contrast, human preferences are much better reflected by both our novel $\\\\texttt{CtxSimFit}$ ($\\\\rho$=0.7--0.9) as well as proposed context-infused versions of common metrics ($\\\\rho$=0.4--0.7). Overall, our findings highlight the importance of integrating context into the generation and especially the evaluation stages of stylistic text rewriting.\",\n",
       " 'Author: Maarten Sap Title: Where Do People Tell Stories Online? Story Detection Across Online Communities Publication year: 2023 Coauthors: Maria Antoniak, Joel Mire, Maarten Sap, Elliott Ash, Andrew Piper Abstract: People share stories online for a myriad of purposes, whether as a means of self-disclosure, processing difficult personal experiences, providing needed information or entertainment, or persuading others to share their beliefs. Better understanding of online storytelling can illuminate the dynamics of social movements, sensemaking practices, persuasion strategies, and more. However, unlike other media such as books and visual content where the narrative nature of the content is often overtly signaled at the document level, studying storytelling in online communities is challenging due to the mixture of storytelling and non-storytelling behavior, which can be interspersed within documents and across diverse topics and settings. We introduce a codebook and create the Storytelling in Online Communities Corpus, an expert-annotated dataset of 502 English-language posts and comments with labeled story and event spans. Using our corpus, we train and evaluate an online story detection model, which we use to investigate the role storytelling of in different social contexts. We identify distinctive features of online storytelling, the prevalence of storytelling among different communities, and the conversational patterns of storytelling.',\n",
       " 'Author: Maarten Sap Title: Queer In AI: A Case Study in Community-Led Participatory AI Publication year: 2023 Coauthors: AI OrganizersOfQueerin, Anaelia Ovalle, Arjun Subramonian, Ashwin Singh, C. Voelcker, Danica J. Sutherland, Davide Locatelli, Eva Breznik, Filip Klubicka, Hang Yuan, J. Hetvi, Huan Zhang, Jaidev Shriram, Kruno Lehman, Luca Soldaini, Maarten Sap, M. Deisenroth, Maria Leonor Pacheco, Maria Ryskina, Martin Mundt, M. Agarwal, Nyx McLean, Pan Xu, Pranav A, Raj Korpan, Ruchira Ray, Sarah Mathew, Sarthak Arora, S. T. John, Tanvi Anand, Vishakha Agrawal, William Agnew, Yanan Long, Zijie J. Wang, Zeerak Talat, Avijit Ghosh, N. Dennler, Michael Noseworthy, Sharvani Jha, Emi Baylor, Aditya Joshi, Natalia Y. Bilenko, Andrew McNamara, Raphael Gontijo-Lopes, Alex Markham, Evyn Dǒng, J. Kay, Manu Saraswat, Nikhil Vytla, Luke Stark Abstract: Queerness and queer people face an uncertain future in the face of ever more widely deployed and invasive artificial intelligence (AI). These technologies have caused numerous harms to queer people, including privacy violations, censoring and downranking queer content, exposing queer people and spaces to harassment by making them hypervisible, deadnaming and outing queer people. More broadly, they have violated core tenets of queerness by classifying and controlling queer identities. In response to this, the queer community in AI has organized Queer in AI, a global, decentralized, volunteer-run grassroots organization that employs intersectional and community-led participatory design to build an inclusive and equitable AI future. In this paper, we present Queer in AI as a case study for community-led participatory design in AI. We examine how participatory design and intersectional tenets started and shaped this community’s programs over the years. We discuss different challenges that emerged in the process, look at ways this organization has fallen short of operationalizing participatory and intersectional principles, and then assess the organization’s impact. Queer in AI provides important lessons and insights for practitioners and theorists of participatory methods broadly through its rejection of hierarchy in favor of decentralization, success at building aid and programs by and for the queer community, and effort to change actors and institutions outside of the queer community. Finally, we theorize how communities like Queer in AI contribute to the participatory design in AI more broadly by fostering cultures of participation in AI, welcoming and empowering marginalized participants, critiquing poor or exploitative participatory practices, and bringing participation to institutions outside of individual research projects. Queer in AI’s work serves as a case study of grassroots activism and participatory methods within AI, demonstrating the potential of community-led participatory methods and intersectional praxis, while also providing challenges, case studies, and nuanced insights to researchers developing and using participatory methods.',\n",
       " \"Author: Maarten Sap Title: Value Kaleidoscope: Engaging AI with Pluralistic Human Values, Rights, and Duties Publication year: 2023 Coauthors: Taylor Sorensen, Liwei Jiang, Jena D. Hwang, Sydney Levine, Valentina Pyatkin, Peter West, Nouha Dziri, Ximing Lu, Kavel Rao, Chandra Bhagavatula, Maarten Sap, J. Tasioulas, Yejin Choi Abstract: Human values are crucial to human decision-making. Value pluralism is the view that multiple correct values may be held in tension with one another (e.g., when considering lying to a friend to protect their feelings, how does one balance honesty with friendship?). As statistical learners, AI systems fit to averages by default, washing out these potentially irreducible value conflicts. To improve AI systems to better reflect value pluralism, the first-order challenge is to explore the extent to which AI systems can model pluralistic human values, rights, and duties as well as their interaction. We introduce ValuePrism, a large-scale dataset of 218k values, rights, and duties connected to 31k human-written situations. ValuePrism's contextualized values are generated by GPT-4 and deemed high-quality by human annotators 91% of the time. We conduct a large-scale study with annotators across diverse social and demographic backgrounds to try to understand whose values are represented. With ValuePrism, we build Kaleido, an open, light-weight, and structured language-based multi-task model that generates, explains, and assesses the relevance and valence (i.e., support or oppose) of human values, rights, and duties within a specific context. Humans prefer the sets of values output by our system over the teacher GPT-4, finding them more accurate and with broader coverage. In addition, we demonstrate that Kaleido can help explain variability in human decision-making by outputting contrasting values. Finally, we show that Kaleido's representations transfer to other philosophical frameworks and datasets, confirming the benefit of an explicit, modular, and interpretable approach to value pluralism. We hope that our work will serve as a step to making more explicit the implicit values behind human decision-making and to steering AI systems to make decisions that are more in accordance with them.\",\n",
       " 'Author: Maarten Sap Title: Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models Publication year: 2023 Coauthors: Natalie Shapira, Mosh Levy, S. Alavi, Xuhui Zhou, Yejin Choi, Yoav Goldberg, Maarten Sap, Vered Shwartz Abstract: The escalating debate on AI\\'s capabilities warrants developing reliable metrics to assess machine\"intelligence\". Recently, many anecdotal examples were used to suggest that newer large language models (LLMs) like ChatGPT and GPT-4 exhibit Neural Theory-of-Mind (N-ToM); however, prior work reached conflicting conclusions regarding those abilities. We investigate the extent of LLMs\\' N-ToM through an extensive evaluation on 6 tasks and find that while LLMs exhibit certain N-ToM abilities, this behavior is far from being robust. We further examine the factors impacting performance on N-ToM tasks and discover that LLMs struggle with adversarial examples, indicating reliance on shallow heuristics rather than robust ToM abilities. We caution against drawing conclusions from anecdotal examples, limited benchmark testing, and using human-designed psychological tests to evaluate models.',\n",
       " \"Author: Maarten Sap Title: Towards Countering Essentialism through Social Bias Reasoning Publication year: 2023 Coauthors: Emily Allaway, Nina Taneja, S. Leslie, Maarten Sap Abstract: Essentialist beliefs (i.e., believing that members of the same group are fundamentally alike) play a central role in social stereotypes and can lead to harm when left unchallenged. In our work, we conduct exploratory studies into the task of countering essentialist beliefs (e.g., ``liberals are stupid''). Drawing on prior work from psychology and NLP, we construct five types of counterstatements and conduct human studies on the effectiveness of these different strategies. Our studies also investigate the role in choosing a counterstatement of the level of explicitness with which an essentialist belief is conveyed. We find that statements that broaden the scope of a stereotype (e.g., to other groups, as in ``conservatives can also be stupid'') are the most popular countering strategy. We conclude with a discussion of challenges and open questions for future work in this area (e.g., improving factuality, studying community-specific variation) and we emphasize the importance of work at the intersection of NLP and psychology.\",\n",
       " \"Author: Maarten Sap Title: SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents Publication year: 2023 Coauthors: Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, Maarten Sap Abstract: Humans are social beings; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and interact under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills. These findings demonstrate SOTOPIA's promise as a general platform for research on evaluating and improving social intelligence in artificial agents.\",\n",
       " 'Author: Rita Singh Title: BASS: Block-wise Adaptation for Speech Summarization Publication year: 2023 Coauthors: Roshan Sharma, Kenneth Zheng, Siddhant Arora, Shinji Watanabe, Rita Singh, B. Raj Abstract: End-to-end speech summarization has been shown to improve performance over cascade baselines. However, such models are difficult to train on very large inputs (dozens of minutes or hours) owing to compute restrictions and are hence trained with truncated model inputs. Truncation leads to poorer models, and a solution to this problem rests in block-wise modeling, i.e., processing a portion of the input frames at a time. In this paper, we develop a method that allows one to train summarization models on very long sequences in an incremental manner. Speech summarization is realized as a streaming process, where hypothesis summaries are updated every block based on new acoustic information. We devise and test strategies to pass semantic context across the blocks. Experiments on the How2 dataset demonstrate that the proposed block-wise training method improves by 3 points absolute on ROUGE-L over a truncated input baseline.',\n",
       " 'Author: Rita Singh Title: Rethinking Voice-Face Correlation: A Geometry View Publication year: 2023 Coauthors: Xiang Li, Yandong Wen, Muqiao Yang, Jinglu Wang, Rita Singh, B. Raj Abstract: Previous works on voice-face matching and voice-guided face synthesis demonstrate strong correlations between voice and face, but mainly rely on coarse semantic cues such as gender, age, and emotion. In this paper, we aim to investigate the capability of reconstructing the 3D facial shape from voice from a geometry perspective without any semantic information. We propose a voice-anthropometric measurement (AM)-face paradigm, which identifies predictable facial AMs from the voice and uses them to guide 3D face reconstruction. By leveraging AMs as a proxy to link the voice and face geometry, we can eliminate the influence of unpredictable AMs and make the face geometry tractable. Our approach is evaluated on our proposed dataset with ground-truth 3D face scans and corresponding voice recordings, and we find significant correlations between voice and specific parts of the face geometry, such as the nasal cavity and cranium. Our work offers a new perspective on voice-face correlation and can serve as a good empirical study for anthropometry science.',\n",
       " 'Author: Rita Singh Title: A Gene-Based Algorithm for Identifying Factors That May Affect a Speaker’s Voice Publication year: 2023 Coauthors: Rita Singh Abstract: Over the past decades, many machine-learning- and artificial-intelligence-based technologies have been created to deduce biometric or bio-relevant parameters of speakers from their voice. These voice profiling technologies have targeted a wide range of parameters, from diseases to environmental factors, based largely on the fact that they are known to influence voice. Recently, some have also explored the prediction of parameters whose influence on voice is not easily observable through data-opportunistic biomarker discovery techniques. However, given the enormous range of factors that can possibly influence voice, more informed methods for selecting those that may be potentially deducible from voice are needed. To this end, this paper proposes a simple path-finding algorithm that attempts to find links between vocal characteristics and perturbing factors using cytogenetic and genomic data. The links represent reasonable selection criteria for use by computational by profiling technologies only, and are not intended to establish any unknown biological facts. The proposed algorithm is validated using a simple example from medical literature—that of the clinically observed effects of specific chromosomal microdeletion syndromes on the vocal characteristics of affected people. In this example, the algorithm attempts to link the genes involved in these syndromes to a single example gene (FOXP2) that is known to play a broad role in voice production. We show that in cases where strong links are exposed, vocal characteristics of the patients are indeed reported to be correspondingly affected. Validation experiments and subsequent analyses confirm that the methodology could be potentially useful in predicting the existence of vocal signatures in naïve cases where their existence has not been otherwise observed.',\n",
       " 'Author: Rita Singh Title: Deriving Vocal Fold Oscillation Information from Recorded Voice Signals Using Models of Phonation Publication year: 2023 Coauthors: Wayne Zhao, Rita Singh Abstract: During phonation, the vocal folds exhibit a self-sustained oscillatory motion, which is influenced by the physical properties of the speaker’s vocal folds and driven by the balance of bio-mechanical and aerodynamic forces across the glottis. Subtle changes in the speaker’s physical state can affect voice production and alter these oscillatory patterns. Measuring these can be valuable in developing computational tools that analyze voice to infer the speaker’s state. Traditionally, vocal fold oscillations (VFOs) are measured directly using physical devices in clinical settings. In this paper, we propose a novel analysis-by-synthesis approach that allows us to infer the VFOs directly from recorded speech signals on an individualized, speaker-by-speaker basis. The approach, called the ADLES-VFT algorithm, is proposed in the context of a joint model that combines a phonation model (with a glottal flow waveform as the output) and a vocal tract acoustic wave propagation model such that the output of the joint model is an estimated waveform. The ADLES-VFT algorithm is a forward-backward algorithm which minimizes the error between the recorded waveform and the output of this joint model to estimate its parameters. Once estimated, these parameter values are used in conjunction with a phonation model to obtain its solutions. Since the parameters correlate with the physical properties of the vocal folds of the speaker, model solutions obtained using them represent the individualized VFOs for each speaker. The approach is flexible and can be applied to various phonation models. In addition to presenting the methodology, we show how the VFOs can be quantified from a dynamical systems perspective for classification purposes. Mathematical derivations are provided in an appendix for better readability.',\n",
       " 'Author: Rita Singh Title: Imprecise Label Learning: A Unified Framework for Learning with Various Imprecise Label Configurations Publication year: 2023 Coauthors: Hao Chen, Ankit Shah, Jindong Wang, R. Tao, Yidong Wang, Xingxu Xie, Masashi Sugiyama, Rita Singh, B. Raj Abstract: Learning with reduced labeling standards, such as noisy label, partial label, and multiple label candidates, which we generically refer to as \\\\textit{imprecise} labels, is a commonplace challenge in machine learning tasks. Previous methods tend to propose specific designs for every emerging imprecise label configuration, which is usually unsustainable when multiple configurations of imprecision coexist. In this paper, we introduce imprecise label learning (ILL), a framework for the unification of learning with various imprecise label configurations. ILL leverages expectation-maximization (EM) for modeling the imprecise label information, treating the precise labels as latent variables.Instead of approximating the correct labels for training, it considers the entire distribution of all possible labeling entailed by the imprecise information. We demonstrate that ILL can seamlessly adapt to partial label learning, semi-supervised learning, noisy label learning, and, more importantly, a mixture of these settings. Notably, ILL surpasses the existing specified techniques for handling imprecise labels, marking the first unified framework with robust and effective performance across various challenging settings. We hope our work will inspire further research on this topic, unleashing the full potential of ILL in wider scenarios where precise labels are expensive and complicated to obtain.',\n",
       " 'Author: Rita Singh Title: The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features Publication year: 2023 Coauthors: Liao Qu, X. Zou, Xiang Li, Yandong Wen, Rita Singh, B. Raj Abstract: This work unveils the enigmatic link between phonemes and facial features. Traditional studies on voice-face correlations typically involve using a long period of voice input, including generating face images from voices and reconstructing 3D face meshes from voices. However, in situations like voice-based crimes, the available voice evidence may be short and limited. Additionally, from a physiological perspective, each segment of speech -- phoneme -- corresponds to different types of airflow and movements in the face. Therefore, it is advantageous to discover the hidden link between phonemes and face attributes. In this paper, we propose an analysis pipeline to help us explore the voice-face relationship in a fine-grained manner, i.e., phonemes v.s. facial anthropometric measurements (AM). We build an estimator for each phoneme-AM pair and evaluate the correlation through hypothesis testing. Our results indicate that AMs are more predictable from vowels compared to consonants, particularly with plosives. Additionally, we observe that if a specific AM exhibits more movement during phoneme pronunciation, it is more predictable. Our findings support those in physiology regarding correlation and lay the groundwork for future research on speech-face multimodal learning.',\n",
       " 'Author: Rita Singh Title: Pengi: An Audio Language Model for Audio Tasks Publication year: 2023 Coauthors: Soham Deshmukh, Benjamin Elizalde, Rita Singh, Huaming Wang Abstract: In the domain of audio processing, Transfer Learning has facilitated the rise of Self-Supervised Learning and Zero-Shot Learning techniques. These approaches have led to the development of versatile models capable of tackling a wide array of tasks, while delivering state-of-the-art performance. However, current models inherently lack the capacity to produce the requisite language for open-ended tasks, such as Audio Captioning or Audio Question&Answering. We introduce Pengi, a novel Audio Language Model that leverages Transfer Learning by framing all audio tasks as text-generation tasks. It takes as input, an audio recording, and text, and generates free-form text as output. The input audio is represented as a sequence of continuous embeddings by an audio encoder. A text encoder does the same for the corresponding text input. Both sequences are combined as a prefix to prompt a pre-trained frozen language model. The unified architecture of Pengi enables open-ended tasks and close-ended tasks without any additional fine-tuning or task-specific extensions. When evaluated on 22 downstream tasks, our approach yields state-of-the-art performance in several of them. Our results show that connecting language models with audio models is a major step towards general-purpose audio understanding',\n",
       " 'Author: Rita Singh Title: Domain Adaptation for Contrastive Audio-Language Models Publication year: 2024 Coauthors: Soham Deshmukh, Rita Singh, Bhiksha Raj Abstract: Audio-Language Models (ALM) aim to be general-purpose audio models by providing zero-shot capabilities at test time. The zero-shot performance of ALM improves by using suitable text prompts for each domain. The text prompts are usually hand-crafted through an ad-hoc process and lead to a drop in ALM generalization and out-of-distribution performance. Existing approaches to improve domain performance, like few-shot learning or fine-tuning, require access to annotated data and iterations of training. Therefore, we propose a test-time domain adaptation method for ALMs that does not require access to annotations. Our method learns a domain vector by enforcing consistency across augmented views of the testing audio. We extensively evaluate our approach on 12 downstream tasks across domains. With just one example, our domain adaptation method leads to 3.2% (max 8.4%) average zero-shot performance improvement. After adaptation, the model still retains the generalization property of ALMs.',\n",
       " 'Author: Rita Singh Title: PAM: Prompting Audio-Language Models for Audio Quality Assessment Publication year: 2024 Coauthors: Soham Deshmukh, Dareen Alharthi, Benjamin Elizalde, Hannes Gamper, Mahmoud Al Ismail, Rita Singh, Bhiksha Raj, Huaming Wang Abstract: While audio quality is a key performance metric for various audio processing tasks, including generative modeling, its objective measurement remains a challenge. Audio-Language Models (ALMs) are pre-trained on audio-text pairs that may contain information about audio quality, the presence of artifacts, or noise. Given an audio input and a text prompt related to quality, an ALM can be used to calculate a similarity score between the two. Here, we exploit this capability and introduce PAM, a no-reference metric for assessing audio quality for different audio processing tasks. Contrary to other\"reference-free\"metrics, PAM does not require computing embeddings on a reference dataset nor training a task-specific model on a costly set of human listening scores. We extensively evaluate the reliability of PAM against established metrics and human listening scores on four tasks: text-to-audio (TTA), text-to-music generation (TTM), text-to-speech (TTS), and deep noise suppression (DNS). We perform multiple ablation studies with controlled distortions, in-the-wild setups, and prompt choices. Our evaluation shows that PAM correlates well with existing metrics and human listening scores. These results demonstrate the potential of ALMs for computing a general-purpose audio quality metric.',\n",
       " 'Author: Rita Singh Title: A General Framework for Learning from Weak Supervision Publication year: 2024 Coauthors: Hao Chen, Jindong Wang, Lei Feng, Xiang Li, Yidong Wang, Xing Xie, Masashi Sugiyama, Rita Singh, Bhiksha Raj Abstract: Weakly supervised learning generally faces challenges in applicability to various scenarios with diverse weak supervision and in scalability due to the complexity of existing algorithms, thereby hindering the practical deployment. This paper introduces a general framework for learning from weak supervision (GLWS) with a novel algorithm. Central to GLWS is an Expectation-Maximization (EM) formulation, adeptly accommodating various weak supervision sources, including instance partial labels, aggregate statistics, pairwise observations, and unlabeled data. We further present an advanced algorithm that significantly simplifies the EM computational demands using a Non-deterministic Finite Automaton (NFA) along with a forward-backward algorithm, which effectively reduces time complexity from quadratic or factorial often required in existing solutions to linear scale. The problem of learning from arbitrary weak supervision is therefore converted to the NFA modeling of them. GLWS not only enhances the scalability of machine learning models but also demonstrates superior performance and versatility across 11 weak supervision scenarios. We hope our work paves the way for further advancements and practical deployment in this field.',\n",
       " 'Author: Rita Singh Title: Towards Robust Audiovisual Segmentation in Complex Environments with Quantization-based Semantic Decomposition Publication year: 2023 Coauthors: Xiang Li, Jinglu Wang, Xiaohao Xu, Xiulian Peng, Rita Singh, Yan Lu, Bhiksha Raj Abstract: Audiovisual segmentation (AVS) is a challenging task that aims to segment visual objects in videos according to their associated acoustic cues. With multiple sound sources and background disturbances involved, establishing robust correspondences between audio and visual contents poses unique challenges due to (1) complex entanglement across sound sources and (2) frequent changes in the occurrence of distinct sound events. Assuming sound events occur independently, the multi-source semantic space can be represented as the Cartesian product of single-source sub-spaces. We are motivated to decompose the multi-source audio semantics into single-source semantics for more effective interactions with visual content. We propose a semantic decomposition method based on product quantization, where the multi-source semantics can be decomposed and represented by several disentangled and noise-suppressed single-source semantics. Furthermore, we introduce a global-to-local quantization mechanism, which distills knowledge from stable global (clip-level) features into local (frame-level) ones, to handle frequent changes in audio semantics. Extensive experiments demonstrate that our semantically decomposed audio representation significantly improves AVS performance, e.g., +21.2% mIoU on the challenging AVS-Semantic benchmark with ResNet50 backbone. https://github.com/lxa9867/QSD.',\n",
       " \"Author: Rita Singh Title: LoFT: Local Proxy Fine-tuning For Improving Transferability Of Adversarial Attacks Against Large Language Model Publication year: 2023 Coauthors: Muhammad Ahmed Shah, Roshan Sharma, Hira Dhamyal, R. Olivier, Ankit Shah, Dareen Alharthi, Hazim T Bukhari, Massa Baali, Soham Deshmukh, Michael Kuhlmann, Bhiksha Raj, Rita Singh Abstract: It has been shown that Large Language Model (LLM) alignments can be circumvented by appending specially crafted attack suffixes with harmful queries to elicit harmful responses. To conduct attacks against private target models whose characterization is unknown, public models can be used as proxies to fashion the attack, with successful attacks being transferred from public proxies to private target models. The success rate of attack depends on how closely the proxy model approximates the private model. We hypothesize that for attacks to be transferrable, it is sufficient if the proxy can approximate the target model in the neighborhood of the harmful query. Therefore, in this paper, we propose \\\\emph{Local Fine-Tuning (LoFT)}, \\\\textit{i.e.}, fine-tuning proxy models on similar queries that lie in the lexico-semantic neighborhood of harmful queries to decrease the divergence between the proxy and target models. First, we demonstrate three approaches to prompt private target models to obtain similar queries given harmful queries. Next, we obtain data for local fine-tuning by eliciting responses from target models for the generated similar queries. Then, we optimize attack suffixes to generate attack prompts and evaluate the impact of our local fine-tuning on the attack's success rate. Experiments show that local fine-tuning of proxy models improves attack transferability and increases attack success rate by $39\\\\%$, $7\\\\%$, and $0.5\\\\%$ (absolute) on target models ChatGPT, GPT-4, and Claude respectively.\",\n",
       " 'Author: Rita Singh Title: Evaluating Speech Synthesis by Training Recognizers on Synthetic Speech Publication year: 2023 Coauthors: Dareen Alharthi, Roshan Sharma, Hira Dhamyal, Soumi Maiti, Bhiksha Raj, Rita Singh Abstract: Modern speech synthesis systems have improved significantly, with synthetic speech being indistinguishable from real speech. However, efficient and holistic evaluation of synthetic speech still remains a significant challenge. Human evaluation using Mean Opinion Score (MOS) is ideal, but inefficient due to high costs. Therefore, researchers have developed auxiliary automatic metrics like Word Error Rate (WER) to measure intelligibility. Prior works focus on evaluating synthetic speech based on pre-trained speech recognition models, however, this can be limiting since this approach primarily measures speech intelligibility. In this paper, we propose an evaluation technique involving the training of an ASR model on synthetic speech and assessing its performance on real speech. Our main assumption is that by training the ASR model on the synthetic speech, the WER on real speech reflects the similarity between distributions, a broader assessment of synthetic speech quality beyond intelligibility. Our proposed metric demonstrates a strong correlation with both MOS naturalness and MOS intelligibility when compared to SpeechLMScore and MOSNet on three recent Text-to-Speech (TTS) systems: MQTTS, StyleTTS, and YourTTS.',\n",
       " 'Author: Rita Singh Title: Rethinking Audiovisual Segmentation with Semantic Quantization and Decomposition Publication year: 2023 Coauthors: Xiang Li, Jinglu Wang, Xiaohao Xu, Xiulian Peng, Rita Singh, Yan Lu, Bhiksha Raj Abstract: Audiovisual segmentation (AVS) is a challenging task that aims to segment visual objects in videos based on their associated acoustic cues. With multiple sound sources involved, establishing robust correspondences between audio and visual contents poses unique challenges due to its (1) intricate entanglement across sound sources and (2) frequent shift among sound events. Assuming sound events occur independently, the multi-source semantic space (which encompasses all possible semantic categories) can be represented as the Cartesian product of single-source sub-spaces. This motivates us to decompose the multi-source audio semantics into single-source semantics, enabling more effective interaction with visual content. Specifically, we propose a semantic decomposition method based on product quantization, where the multi-source semantics can be decomposed and represented by several quantized single-source semantics. Furthermore, we introduce a global-to-local quantization mechanism, which distills knowledge from stable global (clip-level) features into local (frame-level) ones, to handle the constant shift of audio semantics. Extensive experiments demonstrate that semantically quantized and decomposed audio representation significantly improves AVS performance, e.g., +21.2% mIoU on the most challenging AVS-Semantic benchmark.',\n",
       " 'Author: Rita Singh Title: Towards Noise-Tolerant Speech-Referring Video Object Segmentation: Bridging Speech and Text Publication year: 2023 Coauthors: Xiang Li, Jinglu Wang, Xiaohao Xu, Muqiao Yang, Fan Yang, Yizhou Zhao, Rita Singh, Bhiksha Raj ',\n",
       " 'Author: Rita Singh Title: Espnet-Summ: Introducing a Novel Large Dataset, Toolkit, and a Cross-Corpora Evaluation of Speech Summarization Systems Publication year: 2023 Coauthors: Roshan Sharma, William Chen, Takatomo Kano, Ruchira Sharma, Siddhant Arora, Shinji Watanabe, A. Ogawa, Marc Delcroix, Rita Singh, Bhiksha Raj Abstract: Speech summarization has garnered significant interest and progressed rapidly over the past few years. In particular, end-to-end models have recently emerged as a competitive alternative to cascade systems for abstractive video summarization. This paper aims to establish progress in this rapidly evolving research field, by introducing ESPNet-SUMM, a new open-source toolkit that facilitates a comprehensive comparison of end-to-end and cascade speech summarization models on 4 different speech summarization tasks spanning diverse applications. Experiments demonstrate that end-to-end models perform better for larger corpora with shorter inputs. This work also introduces Interview, the largest public open-domain multiparty interview corpus with $4400 \\\\mathrm{~h}$ of conversations between radio hosts and guests. Finally, this work explores the use of multiple datasets to improve end-to-end summarization, and experiments demonstrate the benefit of multi-style training over fine-tuning. 1',\n",
       " 'Author: Rita Singh Title: Importance of negative sampling in weak label learning Publication year: 2023 Coauthors: Ankit Shah, Fuyu Tang, Zelin Ye, Rita Singh, Bhiksha Raj Abstract: Weak-label learning is a challenging task that requires learning from data\"bags\"containing positive and negative instances, but only the bag labels are known. The pool of negative instances is usually larger than positive instances, thus making selecting the most informative negative instance critical for performance. Such a selection strategy for negative instances from each bag is an open problem that has not been well studied for weak-label learning. In this paper, we study several sampling strategies that can measure the usefulness of negative instances for weak-label learning and select them accordingly. We test our method on CIFAR-10 and AudioSet datasets and show that it improves the weak-label classification performance and reduces the computational cost compared to random sampling methods. Our work reveals that negative instances are not all equally irrelevant, and selecting them wisely can benefit weak-label learning.',\n",
       " 'Author: Rita Singh Title: Completing Visual Objects via Bridging Generation and Segmentation Publication year: 2023 Coauthors: Xiang Li, Yinpeng Chen, Chung-Ching Lin, Rita Singh, Bhiksha Raj, Zicheng Liu Abstract: This paper presents a novel approach to object completion, with the primary goal of reconstructing a complete object from its partially visible components. Our method, named MaskComp, delineates the completion process through iterative stages of generation and segmentation. In each iteration, the object mask is provided as an additional condition to boost image generation, and, in return, the generated images can lead to a more accurate mask by fusing the segmentation of images. We demonstrate that the combination of one generation and one segmentation stage effectively functions as a mask denoiser. Through alternation between the generation and segmentation stages, the partial object mask is progressively refined, providing precise shape guidance and yielding superior object completion results. Our experiments demonstrate the superiority of MaskComp over existing approaches, e.g., ControlNet and Stable Diffusion, establishing it as an effective solution for object completion.',\n",
       " 'Author: Rita Singh Title: Training Audio Captioning Models without Audio Publication year: 2023 Coauthors: Soham Deshmukh, Benjamin Elizalde, Dimitra Emmanouilidou, Bhiksha Raj, Rita Singh, Huaming Wang Abstract: Automated Audio Captioning (AAC) is the task of generating natural language descriptions given an audio stream. A typical AAC system requires manually curated training data of audio segments and corresponding text caption annotations. The creation of these audio-caption pairs is costly, resulting in general data scarcity for the task. In this work, we address this major limitation and propose an approach to train AAC systems using only text. Our approach leverages the multimodal space of contrastively trained audio-text models, such as CLAP. During training, a decoder generates captions conditioned on the pretrained CLAP text encoder. During inference, the text encoder is replaced with the pretrained CLAP audio encoder. To bridge the modality gap between text and audio embeddings, we propose the use of noise injection or a learnable adapter, during training. We find that the proposed text-only framework performs competitively with state-of-the-art models trained with paired audio, showing that efficient text-to-audio transfer is possible. Finally, we showcase both stylized audio captioning and caption enrichment while training without audio or human-created text captions.',\n",
       " \"Author: Rita Singh Title: Prompting Audios Using Acoustic Properties For Emotion Representation Publication year: 2023 Coauthors: Hira Dhamyal, Benjamin Elizalde, Soham Deshmukh, Huaming Wang, Bhiksha Raj, Rita Singh Abstract: Emotions lie on a continuum, but current models treat emotions as a finite valued discrete variable. This representation does not capture the diversity in the expression of emotion. To better represent emotions we propose the use of natural language descriptions (or prompts). In this work, we address the challenge of automatically generating these prompts and training a model to better learn emotion representations from audio and prompt pairs. We use acoustic properties that are correlated to emotion like pitch, intensity, speech rate, and articulation rate to automatically generate prompts i.e. 'acoustic prompts'. We use a contrastive learning objective to map speech to their respective acoustic prompts. We evaluate our model on Emotion Audio Retrieval and Speech Emotion Recognition. Our results show that the acoustic prompts significantly improve the model's performance in EAR, in various Precision@K metrics. In SER, we observe a 3.8% relative accuracy improvement on the Ravdess dataset.\",\n",
       " 'Author: Rita Singh Title: Pairwise Similarity Learning is SimPLE Publication year: 2023 Coauthors: Yandong Wen, Weiyang Liu, Yao Feng, Bhiksha Raj, Rita Singh, Adrian Weller, Michael J. Black, Bernhard Schölkopf Abstract: In this paper, we focus on a general yet important learning problem, pairwise similarity learning (PSL). PSL subsumes a wide range of important applications, such as open-set face recognition, speaker verification, image retrieval and person re-identification. The goal of PSL is to learn a pairwise similarity function assigning a higher similarity score to positive pairs (i.e., a pair of samples with the same label) than to negative pairs (i.e., a pair of samples with different label). We start by identifying a key desideratum for PSL, and then discuss how existing methods can achieve this desideratum. We then propose a surprisingly simple proxy-free method, called SimPLE, which requires neither feature/proxy normalization nor angular margin and yet is able to generalize well in open-set recognition. We apply the proposed method to three challenging PSL tasks: open-set face recognition, image retrieval and speaker verification. Comprehensive experimental results on large-scale benchmarks show that our method performs significantly better than current state-of-the-art methods. Our project page is available at simple.is.tue.mpg.de.',\n",
       " 'Author: Emma Strubell Title: AboutMe: Using Self-Descriptions in Webpages to Document the Effects of English Pretraining Data Filters Publication year: 2024 Coauthors: Li Lucy, Suchin Gururangan, Luca Soldaini, Emma Strubell, David Bamman, Lauren Klein, Jesse Dodge Abstract: Large language models\\' (LLMs) abilities are drawn from their pretraining data, and model development begins with data curation. However, decisions around what data is retained or removed during this initial stage is under-scrutinized. In our work, we ground web text, which is a popular pretraining data source, to its social and geographic contexts. We create a new dataset of 10.3 million self-descriptions of website creators, and extract information about who they are and where they are from: their topical interests, social roles, and geographic affiliations. Then, we conduct the first study investigating how ten\"quality\"and English language identification (langID) filters affect webpages that vary along these social dimensions. Our experiments illuminate a range of implicit preferences in data curation: we show that some quality classifiers act like topical domain filters, and langID can overlook English content from some regions of the world. Overall, we hope that our work will encourage a new line of research on pretraining data curation practices and its social implications.',\n",
       " 'Author: Emma Strubell Title: OLMo: Accelerating the Science of Language Models Publication year: 2024 Coauthors: Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, A. Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Daniel Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, Hanna Hajishirzi Abstract: Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, this technical report details the first release of OLMo, a state-of-the-art, truly Open Language Model and its framework to build and study the science of language modeling. Unlike most prior efforts that have only released model weights and inference code, we release OLMo and the whole framework, including training data and training and evaluation code. We hope this release will empower and strengthen the open research community and inspire a new wave of innovation.',\n",
       " 'Author: Emma Strubell Title: Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research Publication year: 2024 Coauthors: Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Raghavi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, A. Jha, Sachin Kumar, L. Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Daniel Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah A. Smith, Hanna Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, Kyle Lo Abstract: Language models have become a critical technology to tackling a wide range of natural language processing tasks, yet many details about how the best-performing language models were developed are not reported. In particular, information about their pretraining corpora is seldom discussed: commercial language models rarely provide any information about their data; even open models rarely release datasets they are trained on, or an exact recipe to reproduce them. As a result, it is challenging to conduct certain threads of language modeling research, such as understanding how training data impacts model capabilities and shapes their limitations. To facilitate open research on language model pretraining, we release Dolma, a three trillion tokens English corpus, built from a diverse mixture of web content, scientific papers, code, public-domain books, social media, and encyclopedic materials. In addition, we open source our data curation toolkit to enable further experimentation and reproduction of our work. In this report, we document Dolma, including its design principles, details about its construction, and a summary of its contents. We interleave this report with analyses and experimental results from training language models on intermediate states of Dolma to share what we have learned about important data curation practices, including the role of content or quality filters, deduplication, and multi-source mixing. Dolma has been used to train OLMo, a state-of-the-art, open language model and framework designed to build and study the science of language modeling.',\n",
       " \"Author: Emma Strubell Title: To Build Our Future, We Must Know Our Past: Contextualizing Paradigm Shifts in Natural Language Processing Publication year: 2023 Coauthors: Sireesh Gururaja, Amanda Bertsch, Clara Na, D. Widder, Emma Strubell Abstract: NLP is in a period of disruptive change that is impacting our methodologies, funding sources, and public perception. In this work, we seek to understand how to shape our future by better understanding our past. We study factors that shape NLP as a field, including culture, incentives, and infrastructure by conducting long-form interviews with 26 NLP researchers of varying seniority, research area, institution, and social identity. Our interviewees identify cyclical patterns in the field, as well as new shifts without historical parallel, including changes in benchmark culture and software infrastructure. We complement this discussion with quantitative analysis of citation, authorship, and language use in the ACL Anthology over time. We conclude by discussing shared visions, concerns, and hopes for the future of NLP. We hope that this study of our field's past and present can prompt informed discussion of our community's implicit norms and more deliberate action to consciously shape the future.\",\n",
       " \"Author: Emma Strubell Title: Energy and Carbon Considerations of Fine-Tuning BERT Publication year: 2023 Coauthors: Xiaorong Wang, Clara Na, Emma Strubell, Sorelle A. Friedler, Sasha Luccioni Abstract: Despite the popularity of the `pre-train then fine-tune' paradigm in the NLP community, existing work quantifying energy costs and associated carbon emissions has largely focused on language model pre-training. Although a single pre-training run draws substantially more energy than fine-tuning, fine-tuning is performed more frequently by many more individual actors, and thus must be accounted for when considering the energy and carbon footprint of NLP. In order to better characterize the role of fine-tuning in the landscape of energy and carbon emissions in NLP, we perform a careful empirical study of the computational costs of fine-tuning across tasks, datasets, hardware infrastructure and measurement modalities. Our experimental results allow us to place fine-tuning energy and carbon costs into perspective with respect to pre-training and inference, and outline recommendations to NLP researchers and practitioners who wish to improve their fine-tuning energy efficiency.\",\n",
       " \"Author: Emma Strubell Title: Understanding the Effect of Model Compression on Social Bias in Large Language Models Publication year: 2023 Coauthors: Gustavo Gonçalves, Emma Strubell Abstract: Large Language Models (LLMs) trained with self-supervision on vast corpora of web text fit to the social biases of that text. Without intervention, these social biases persist in the model's predictions in downstream tasks, leading to representational harm. Many strategies have been proposed to mitigate the effects of inappropriate social biases learned during pretraining. Simultaneously, methods for model compression have become increasingly popular to reduce the computational burden of LLMs. Despite the popularity and need for both approaches, little work has been done to explore the interplay between these two. We perform a carefully controlled study of the impact of model compression via quantization and knowledge distillation on measures of social bias in LLMs. Longer pretraining and larger models led to higher social bias, and quantization showed a regularizer effect with its best trade-off around 20% of the original pretraining time.\",\n",
       " \"Author: Emma Strubell Title: Surveying (Dis)Parities and Concerns of Compute Hungry NLP Research Publication year: 2023 Coauthors: Ji-Ung Lee, Haritz Puerto, Betty van Aken, Yuki Arase, J. Forde, Leon Derczynski, Andreas Ruckl'e, Iryna Gurevych, Roy Schwartz, Emma Strubell, Jesse Dodge Abstract: Many recent improvements in NLP stem from the development and use of large pre-trained language models (PLMs) with billions of parameters. Large model sizes makes computational cost one of the main limiting factors for training and evaluating such models; and has raised severe concerns about the sustainability, reproducibility, and inclusiveness for researching PLMs. These concerns are often based on personal experiences and observations. However, there had not been any large-scale surveys that investigate them. In this work, we provide a first attempt to quantify these concerns regarding three topics, namely, environmental impact, equity, and impact on peer reviewing. By conducting a survey with 312 participants from the NLP community, we capture existing (dis)parities between different and within groups with respect to seniority, academia, and industry; and their impact on the peer reviewing process. For each topic, we provide an analysis and devise recommendations to mitigate found disparities, some of which already successfully implemented. Finally, we discuss additional concerns raised by many participants in free-text responses.\",\n",
       " 'Author: Emma Strubell Title: On the Interactions of Structural Constraints and Data Resources for Structured Prediction Publication year: 2023 Coauthors: Zhisong Zhang, Emma Strubell, E. Hovy Abstract: In this work, we provide an analysis on the interactions of the effectiveness of decoding with structural constraints and the amount of available training data for structured prediction tasks in NLP. Our exploration adopts a simple protocol that enforces constraints upon constraint-agnostic local models at testing time. With evaluations on three typical structured prediction tasks (named entity recognition, dependency parsing, and event argument extraction), we find that models trained with less data predict outputs with more structural violations in greedy decoding mode. Incorporating constraints provides consistent performance improvements and such benefits are larger in lower resource scenarios. Moreover, there are similar patterns with regard to the model sizes and more efficient models tend to enjoy more benefits. Finally, we also investigate settings with genre transfer and discover patterns that are related to domain discrepancies.',\n",
       " \"Author: Emma Strubell Title: Efficiency Pentathlon: A Standardized Arena for Efficiency Evaluation Publication year: 2023 Coauthors: Hao Peng, Qingqing Cao, Jesse Dodge, Matthew E. Peters, Jared Fernandez, Tom Sherborne, Kyle Lo, Sam Skjonsberg, Emma Strubell, Darrell Plessas, Iz Beltagy, Pete Walsh, Noah A. Smith, Hannaneh Hajishirzi Abstract: Rising computational demands of modern natural language processing (NLP) systems have increased the barrier to entry for cutting-edge research while posing serious environmental concerns. Yet, progress on model efficiency has been impeded by practical challenges in model evaluation and comparison. For example, hardware is challenging to control due to disparate levels of accessibility across different institutions. Moreover, improvements in metrics such as FLOPs often fail to translate to progress in real-world applications. In response, we introduce Pentathlon, a benchmark for holistic and realistic evaluation of model efficiency. Pentathlon focuses on inference, which accounts for a majority of the compute in a model's lifecycle. It offers a strictly-controlled hardware platform, and is designed to mirror real-world applications scenarios. It incorporates a suite of metrics that target different aspects of efficiency, including latency, throughput, memory overhead, and energy consumption. Pentathlon also comes with a software library that can be seamlessly integrated into any codebase and enable evaluation. As a standardized and centralized evaluation platform, Pentathlon can drastically reduce the workload to make fair and reproducible efficiency comparisons. While initially focused on natural language processing (NLP) models, Pentathlon is designed to allow flexible extension to other fields. We envision Pentathlon will stimulate algorithmic innovations in building efficient models, and foster an increased awareness of the social and environmental implications in the development of future-generation NLP models.\",\n",
       " 'Author: Emma Strubell Title: Queer People are People First: Deconstructing Sexual Identity Stereotypes in Large Language Models Publication year: 2023 Coauthors: Harnoor Dhingra, Preetiha Jayashanker, Sayali S. Moghe, Emma Strubell Abstract: Large Language Models (LLMs) are trained primarily on minimally processed web text, which exhibits the same wide range of social biases held by the humans who created that content. Consequently, text generated by LLMs can inadvertently perpetuate stereotypes towards marginalized groups, like the LGBTQIA+ community. In this paper, we perform a comparative study of how LLMs generate text describing people with different sexual identities. Analyzing bias in the text generated by an LLM using regard score shows measurable bias against queer people. We then show that a post-hoc method based on chain-of-thought prompting using SHAP analysis can increase the regard of the sentence, representing a promising approach towards debiasing the output of LLMs in this setting.',\n",
       " 'Author: Emma Strubell Title: Power Hungry Processing: Watts Driving the Cost of AI Deployment? Publication year: 2023 Coauthors: A. Luccioni, Yacine Jernite, Emma Strubell Abstract: Recent years have seen a surge in the popularity of commercial AI products based on generative, multi-purpose AI systems promising a unified approach to building machine learning (ML) models into technology. However, this ambition of\"generality\"comes at a steep cost to the environment, given the amount of energy these systems require and the amount of carbon that they emit. In this work, we propose the first systematic comparison of the ongoing inference cost of various categories of ML systems, covering both task-specific (i.e. finetuned models that carry out a single task) and `general-purpose\\' models, (i.e. those trained for multiple tasks). We measure deployment cost as the amount of energy and carbon required to perform 1,000 inferences on representative benchmark dataset using these models. We find that multi-purpose, generative architectures are orders of magnitude more expensive than task-specific systems for a variety of tasks, even when controlling for the number of model parameters. We conclude with a discussion around the current trend of deploying multi-purpose generative ML systems, and caution that their utility should be more intentionally weighed against increased costs in terms of energy and emissions. All the data from our study can be accessed via an interactive demo to carry out further exploration and analysis.',\n",
       " 'Author: Emma Strubell Title: Making Scalable Meta Learning Practical Publication year: 2023 Coauthors: Sang Keun Choe, Sanket Vaibhav Mehta, Hwijeen Ahn, W. Neiswanger, Pengtao Xie, Emma Strubell, Eric P. Xing Abstract: Despite its flexibility to learn diverse inductive biases in machine learning programs, meta learning (i.e., learning to learn) has long been recognized to suffer from poor scalability due to its tremendous compute/memory costs, training instability, and a lack of efficient distributed training support. In this work, we focus on making scalable meta learning practical by introducing SAMA, which combines advances in both implicit differentiation algorithms and systems. Specifically, SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients. Evaluated on multiple large-scale meta learning benchmarks, SAMA showcases up to 1.7/4.8x increase in throughput and 2.0/3.8x decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms. Furthermore, we show that SAMA-based data optimization leads to consistent improvements in text classification accuracy with BERT and RoBERTa large language models, and achieves state-of-the-art results in both small- and large-scale data pruning on image classification tasks, demonstrating the practical applicability of scalable meta learning across language and vision domains.',\n",
       " 'Author: Emma Strubell Title: Regularizing Self-training for Unsupervised Domain Adaptation via Structural Constraints Publication year: 2023 Coauthors: Rajshekhar Das, Jonathan M Francis, Sanket Vaibhav Mehta, Jean Oh, Emma Strubell, Jose Moura Abstract: Self-training based on pseudo-labels has emerged as a dominant approach for addressing conditional distribution shifts in unsupervised domain adaptation (UDA) for semantic segmentation problems. A notable drawback, however, is that this family of approaches is susceptible to erroneous pseudo labels that arise from confirmation biases in the source domain and that manifest as nuisance factors in the target domain. A possible source for this mismatch is the reliance on only photometric cues provided by RGB image inputs, which may ultimately lead to sub-optimal adaptation. To mitigate the effect of mismatched pseudo-labels, we propose to incorporate structural cues from auxiliary modalities, such as depth, to regularise conventional self-training objectives. Specifically, we introduce a contrastive pixel-level objectness constraint that pulls the pixel representations within a region of an object instance closer, while pushing those from different object categories apart. To obtain object regions consistent with the true underlying object, we extract information from both depth maps and RGB-images in the form of multimodal clustering. Crucially, the objectness constraint is agnostic to the ground-truth semantic labels and, hence, appropriate for unsupervised domain adaptation. In this work, we show that our regularizer significantly improves top performing self-training methods (by up to $2$ points) in various UDA benchmarks for semantic segmentation. We include all code in the supplementary.',\n",
       " 'Author: Emma Strubell Title: The Framework Tax: Disparities Between Inference Efficiency in Research and Deployment Publication year: 2023 Coauthors: Jared Fernandez, Jacob Kahn, Clara Na, Yonatan Bisk, Emma Strubell Abstract: Increased focus on the computational efficiency of NLP systems has motivated the design of efficient model architectures and improvements to underlying hardware accelerators. However, the resulting increases in computational throughput and reductions in floating point operations have not directly translated to improvements in wall-clock inference latency. We demonstrate that these discrepancies can be largely attributed to bottlenecks introduced by deep learning frameworks. We denote this phenomenon as the \\\\textit{framework tax}, and observe that the disparity is growing as hardware speed increases over time. In this work, we examine this phenomenon through a series of case studies analyzing the effects of model design decisions, framework paradigms, and hardware platforms on total model latency. Code is available at https://github.com/JaredFern/Framework-Tax.',\n",
       " \"Author: Emma Strubell Title: Data-efficient Active Learning for Structured Prediction with Partial Annotation and Self-Training Publication year: 2023 Coauthors: Zhisong Zhang, Emma Strubell, E. Hovy Abstract: In this work we propose a pragmatic method that reduces the annotation cost for structured label spaces using active learning. Our approach leverages partial annotation, which reduces labeling costs for structured outputs by selecting only the most informative sub-structures for annotation. We also utilize self-training to incorporate the current model's automatic predictions as pseudo-labels for un-annotated sub-structures. A key challenge in effectively combining partial annotation with self-training to reduce annotation cost is determining which sub-structures to select to label. To address this challenge, we adopt an error estimator to adaptively decide the partial selection ratio according to the current model's capability. In evaluations spanning four structured prediction tasks, we show that our combination of partial annotation and self-training using an adaptive selection ratio reduces annotation cost over strong full annotation baselines under a fair comparison scheme that takes reading time into consideration.\",\n",
       " \"Author: Emma Strubell Title: How To Train Your (Compressed) Large Language Model Publication year: 2023 Coauthors: A. Jha, Dirk Groeneveld, Emma Strubell, Iz Beltagy Abstract: With the increase in the size of large language models (LLMs), we need compression methods that can reduce the model size while preserving the generality and zero-shot promptability of the model. This goal is more ambitious than the typical compression setup, which reduces the model's size at the expense of specializing it to a specific end-task. To study this, we develop a task-agnostic compression pipeline with a large-scale evaluation comprising language modeling perplexity and 12 zero-shot end-tasks. Our results show that a simple layer-wise pruning followed by continued language model pretraining matches or outperforms three existing state-of-the-art baselines while being 1.5x more computationally efficient. However, unlike typical task-specialized compression, our best-compressed model significantly underperforms a similar-sized model trained from scratch. We posit the half-sized pretrained model as an upper bound for task-agnostic compression and call for future work to bridge this gap under a reasonable token budget. Our findings highlight the inadequacy of existing compression methods for LLMs and establish a requirement for new methods that preserve a model's generality and zero-shot promptability under compression. We release our code and evaluation setup to facilitate reproducibility and help iterate on method design.\",\n",
       " 'Author: A. Waibel Title: Audio-driven Talking Face Generation by Overcoming Unintended Information Flow Publication year: 2023 Coauthors: Dogucan Yaman, Fevziye Irem Eyiokur, Leonard Barmann, H. K. Ekenel, A. Waibel Abstract: Audio-driven talking face generation is the task of creating a lip-synchronized, realistic face video from given audio and reference frames. This involves two major challenges: overall visual quality of generated images on the one hand, and audio-visual synchronization of the mouth part on the other hand. In this paper, we start by identifying several problematic aspects of synchronization methods in recent audio-driven talking face generation approaches. Specifically, this involves unintended flow of lip, pose and other information from the reference to the generated image, as well as instabilities during model training. Subsequently, we propose various techniques for obviating these issues: First, a silent-lip reference image generator prevents leaking of lips from the reference to the generated image. Second, an adaptive triplet loss handles the pose leaking problem. Finally, we propose a stabilized formulation of synchronization loss, circumventing aforementioned training instabilities while additionally further alleviating the lip leaking issue. Combining the individual improvements, we present state-of-the-art visual quality and synchronization performance on LRS2 in five out of seven and LRW in six out of seven metrics, and competitive results on the remaining ones. We further validate our design in various ablation experiments, confirming the individual contributions as well as their complementary effects.',\n",
       " 'Author: A. Waibel Title: AdapITN: A Fast, Reliable, and Dynamic Adaptive Inverse Text Normalization Publication year: 2023 Coauthors: T. Nguyen, Le Duc Minh Nhat, Quang Minh Nguyen, Quoc Truong Do, C. Luong, A. Waibel Abstract: Inverse text normalization (ITN) is the task that transforms text in spoken-form into written-form. While automatic speech recognition (ASR) produces text in spoken-form, human and natural language understanding systems prefer to consume text in written-form. ITN generally deals with semiotic phrases (e.g., numbers, date, time). However, lack of studies to deal with phonetization phrases, which is ASR’s output when it handles unseen data (e.g., foreign-named entities, domain names), although these exist in the same form in the spoken-form text. The reason is that phonetization phrases are infinite patterns and language-dependent. In this study, we introduce a novel end2end model that can handle both semiotic phrases (SEP) and phonetization phrases (PHP), named AdapITN. We call it \"Adap\" because it allows for handling unseen PHP. The model performs only when necessary by providing a mechanism to narrow normalized regions and external query knowledge, reducing the runtime significantly.',\n",
       " 'Author: A. Waibel Title: Train Global, Tailor Local: Minimalist Multilingual Translation into Endangered Languages Publication year: 2023 Coauthors: Zhong Zhou, J. Niehues, A. Waibel Abstract: In many humanitarian scenarios, translation into severely low resource languages often does not require a universal translation engine, but a dedicated text-specific translation engine. For example, healthcare records, hygienic procedures, government communication, emergency procedures and religious texts are all limited texts. While generic translation engines for all languages do not exist, translation of multilingually known limited texts into new, endangered languages may be possible and reduce human translation effort. We attempt to leverage translation resources from rich resource languages to efficiently produce best possible translation quality for well known texts, which is available in multiple languages, in a new, severely low resource language. We examine two approaches: 1.) best selection of seed sentences to jump start translations in a new language in view of best generalization to the remainder of a larger targeted text(s), and 2.) we adapt large general multilingual translation engines from many other languages to focus on a specific text in a new, unknown language. We find that adapting large pretrained multilingual models to the domain/text first and then to the severely low resource language works best. If we also select a best set of seed sentences, we can improve average chrF performance on new test languages from a baseline of 21.9 to 50.7, while reducing the number of seed sentences to only ∼1,000 in the new, unknown language.',\n",
       " 'Author: A. Waibel Title: Towards Efficient Simultaneous Speech Translation: CUNI-KIT System for Simultaneous Track at IWSLT 2023 Publication year: 2023 Coauthors: Peter Polák, Danni Liu, Ngoc-Quan Pham, J. Niehues, A. Waibel, Ondrej Bojar Abstract: In this paper, we describe our submission to the Simultaneous Track at IWSLT 2023. This year, we continue with the successful setup from the last year, however, we adopt the latest methods that further improve the translation quality. Additionally, we propose a novel online policy for attentional encoder-decoder models. The policy prevents the model to generate translation beyond the current speech input by using an auxiliary CTC output layer. We show that the proposed simultaneous policy can be applied to both streaming blockwise models and offline encoder-decoder models. We observe significant improvements in quality (up to 1.1 BLEU) and the computational footprint (up to 45% relative RTF).',\n",
       " 'Author: A. Waibel Title: SYNTACC : Synthesizing Multi-Accent Speech By Weight Factorization Publication year: 2023 Coauthors: Tuan-Nam Nguyen, Ngoc-Quan Pham, A. Waibel Abstract: Conventional multi-speaker text-to-speech synthesis (TTS) is known to be capable of synthesizing speech for multiple voices, yet it cannot generate speech in different accents. This limitation has motivated us to develop SYNTACC (Synthesizing speech with accents) which adapts conventional multi-speaker TTS to produce multi-accent speech. Our method uses the YourTTS model and involves a novel multi-accent training mechanism. The method works by decomposing each weight matrix into a shared component and an accent-dependent component, with the former being initialized by the pretrained multi-speaker TTS model and the latter being factorized into vectors using rank-1 matrices to reduce the number of training parameters per accent. This weight factorization method proves to be effective in fine-tuning the SYNTACC on multi-accent data sets in a low-resource condition. Our SYNTACC model eventually allows speech synthesis in not only different voices but also in different accents.',\n",
       " 'Author: A. Waibel Title: KIT’s Multilingual Speech Translation System for IWSLT 2023 Publication year: 2023 Coauthors: Danni Liu, T. Nguyen, Sai Koneru, Enes Yavuz Ugan, Ngoc-Quan Pham, Tuan-Nam Nguyen, Tu Anh Dinh, Carlos Mullov, A. Waibel, J. Niehues Abstract: Many existing speech translation benchmarks focus on native-English speech in high-quality recording conditions, which often do not match the conditions in real-life use-cases. In this paper, we describe our speech translation system for the multilingual track of IWSLT 2023, which focuses on the translation of scientific conference talks. The test condition features accented input speech and terminology-dense contents. The tasks requires translation into 10 languages of varying amounts of resources. In absence of training data from the target domain, we use a retrieval-based approach (kNN-MT) for effective adaptation (+0.8 BLEU for speech translation). We also use adapters to easily integrate incremental training data from data augmentation, and show that it matches the performance of re-training. We observe that cascaded systems are more easily adaptable towards specific target domains, due to their separate modules. Our cascaded speech system outperforms its end-to-end counterpart on scientific talk translation, although their performance remains similar on TED talks.',\n",
       " \"Author: A. Waibel Title: Convoifilter: A case study of doing cocktail party speech recognition Publication year: 2023 Coauthors: T. Nguyen, A. Waibel Abstract: This paper presents an end-to-end model designed to improve automatic speech recognition (ASR) for a particular speaker in a crowded, noisy environment. The model utilizes a single-channel speech enhancement module that isolates the speaker's voice from background noise (ConVoiFilter) and an ASR module. The model can decrease ASR's word error rate (WER) from 80% to 26.4% through this approach. Typically, these two components are adjusted independently due to variations in data requirements. However, speech enhancement can create anomalies that decrease ASR efficiency. By implementing a joint fine-tuning strategy, the model can reduce the WER from 26.4% in separate tuning to 14.5% in joint tuning. We openly share our pre-trained model to foster further research hf.co/nguyenvulebinh/voice-filter.\",\n",
       " 'Author: A. Waibel Title: Incremental Blockwise Beam Search for Simultaneous Speech Translation with Controllable Quality-Latency Tradeoff Publication year: 2023 Coauthors: Peter Polák, Brian Yan, Shinji Watanabe, A. Waibel, Ondrej Bojar Abstract: Blockwise self-attentional encoder models have recently emerged as one promising end-to-end approach to simultaneous speech translation. These models employ a blockwise beam search with hypothesis reliability scoring to determine when to wait for more input speech before translating further. However, this method maintains multiple hypotheses until the entire speech input is consumed -- this scheme cannot directly show a single \\\\textit{incremental} translation to users. Further, this method lacks mechanisms for \\\\textit{controlling} the quality vs. latency tradeoff. We propose a modified incremental blockwise beam search incorporating local agreement or hold-$n$ policies for quality-latency control. We apply our framework to models trained for online or offline translation and demonstrate that both types can be effectively used in online mode. Experimental results on MuST-C show 0.6-3.6 BLEU improvement without changing latency or 0.8-1.4 s latency improvement without changing quality.',\n",
       " 'Author: A. Waibel Title: End-to-End Evaluation for Low-Latency Simultaneous Speech Translation Publication year: 2023 Coauthors: Christian Huber, Tu Anh Dinh, Carlos Mullov, Ngoc-Quan Pham, T. Nguyen, Fabian Retkowski, Stefan Constantin, Enes Yavuz Ugan, Danni Liu, Zhaolin Li, Sai Koneru, J. Niehues, A. Waibel Abstract: The challenge of low-latency speech translation has recently draw significant interest in the research community as shown by several publications and shared tasks. Therefore, it is essential to evaluate these different approaches in realistic scenarios. However, currently only specific aspects of the systems are evaluated and often it is not possible to compare different approaches. In this work, we propose the first framework to perform and evaluate the various aspects of low-latency speech translation under realistic conditions. The evaluation is carried out in an end-to-end fashion. This includes the segmentation of the audio as well as the run-time of the different components. Secondly, we compare different approaches to low-latency speech translation using this framework. We evaluate models with the option to revise the output as well as methods with fixed output. Furthermore, we directly compare state-of-the-art cascaded as well as end-to-end systems. Finally, the framework allows to automatically evaluate the translation quality as well as latency and also provides a web interface to show the low-latency model outputs to the user.',\n",
       " 'Author: A. Waibel Title: FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN Publication year: 2023 Coauthors: Sweta Agrawal, Antonios Anastasopoulos, L. Bentivogli, Ondrej Bojar, Claudia Borg, Marine Carpuat, Roldano Cattoni, Mauro Cettolo, Mingda Chen, William Chen, K. Choukri, Alexandra Chronopoulou, Anna Currey, T. Declerck, Qianqian Dong, Kevin Duh, Y. Estève, Marcello Federico, Souhir Gahbiche, B. Haddow, B. Hsu, Phu Mon Htut, H. Inaguma, Dávid Javorský, J. Judge, Yasumasa Kano, Tom Ko, Rishu Kumar, Peng Li, Xutai Ma, Prashant Mathur, E. Matusov, Paul McNamee, John P. McCrae, Kenton Murray, Maria Nadejde, Satoshi Nakamura, Matteo Negri, H. Nguyen, J. Niehues, Xing Niu, Atul Kr. Ojha, John E. Ortega, Proyag Pal, J. Pino, Lonneke van der Plas, Peter Polák, Elijah Matthew Rippeth, Elizabeth Salesky, Jiatong Shi, Matthias Sperber, Sebastian Stüker, Katsuhito Sudoh, Yun Tang, Brian Thompson, Ke M. Tran, M. Turchi, A. Waibel, Mingxuan Wang, Shinji Watanabe, Rodolfo Zevallos Abstract: This paper reports on the shared tasks organized by the 20th IWSLT Conference. The shared tasks address 9 scientific challenges in spoken language translation: simultaneous and offline translation, automatic subtitling and dubbing, speech-to-speech translation, multilingual, dialect and low-resource speech translation, and formality control. The shared tasks attracted a total of 38 submissions by 31 teams. The growing interest towards spoken language translation is also witnessed by the constantly increasing number of shared task organizers and contributors to the overview paper, almost evenly distributed across industry and academia.',\n",
       " 'Author: Alexander Waibel Title: Modular Design of a Front-End and Back-End Speech-to-Speech Translation Application for Psychiatric Treatment of Refugees Publication year: 2023 Coauthors: Enes Yavuz Ugan, Mohammed Mediani, Omar Al Jawabra, Aya Khader, Yining Liu, Alexander Waibel Abstract: One of the inevitable impacts happening in areas with political conflicts is the significant influx of displaced individuals. The psychological consequences on individuals enduring such events are profound. Therefore, the imperative of providing adequate mental health care to refugees coming from conflict areas becomes apparent. However, providing this necessary care faces two obstacles. On the one hand, not all this target population is expected to have an acceptable level of proficiency of the hosting country’s local language. On the other hand, finding enough number of suitable interpreters is a very challenging task. Moreover, even when the availability of the human interpreters is no problem, the refugees may hesitate to share their experiences with interpreters due to the associated stigma. To address these challenges and enhance mental health care for refugees, we propose the design of a modular front-end and back-end Speech-to-Speech translation system, with a focus on safeguarding patient data privacy. As our system is Speech-to-Speech, it also enables dialogue with dyslexic people and removes barriers for their treatment as well.',\n",
       " 'Author: Alexander Waibel Title: Continuously Learning New Words in Automatic Speech Recognition Publication year: 2024 Coauthors: Christian Huber, Alexander Waibel Abstract: Despite recent advances, Automatic Speech Recognition (ASR) systems are still far from perfect. Typical errors include acronyms, named entities and domain-specific special words for which little or no data is available. To address the problem of recognizing these words, we propose an self-supervised continual learning approach. Given the audio of a lecture talk with corresponding slides, we bias the model towards decoding new words from the slides by using a memory-enhanced ASR model from previous work. Then, we perform inference on the talk, collecting utterances that contain detected new words into an adaptation dataset. Continual learning is then performed on this set by adapting low-rank matrix weights added to each weight matrix of the model. The whole procedure is iterated for many talks. We show that with this approach, we obtain increasing performance on the new words when they occur more frequently (more than 80% recall) while preserving the general performance of the model.',\n",
       " \"Author: Alexander H. Waibel Title: Incremental Learning of Humanoid Robot Behavior from Natural Interaction and Large Language Models Publication year: 2023 Coauthors: Leonard Bärmann, Rainer Kartmann, Fabian Peller-Konrad, Alexander H. Waibel, T. Asfour Abstract: Natural-language dialog is key for intuitive human-robot interaction. It can be used not only to express humans' intents, but also to communicate instructions for improvement if a robot does not understand a command correctly. Of great importance is to endow robots with the ability to learn from such interaction experience in an incremental way to allow them to improve their behaviors or avoid mistakes in the future. In this paper, we propose a system to achieve incremental learning of complex behavior from natural interaction, and demonstrate its implementation on a humanoid robot. Building on recent advances, we present a system that deploys Large Language Models (LLMs) for high-level orchestration of the robot's behavior, based on the idea of enabling the LLM to generate Python statements in an interactive console to invoke both robot perception and action. The interaction loop is closed by feeding back human instructions, environment observations, and execution results to the LLM, thus informing the generation of the next statement. Specifically, we introduce incremental prompt learning, which enables the system to interactively learn from its mistakes. For that purpose, the LLM can call another LLM responsible for code-level improvements of the current interaction based on human feedback. The improved interaction is then saved in the robot's memory, and thus retrieved on similar requests. We integrate the system in the robot cognitive architecture of the humanoid robot ARMAR-6 and evaluate our methods both quantitatively (in simulation) and qualitatively (in simulation and real-world) by demonstrating generalized incrementally-learned knowledge.\",\n",
       " 'Author: Alexander H. Waibel Title: Multimodal Error Correction with Natural Language and Pointing Gestures Publication year: 2023 Coauthors: Stefan Constantin, Fevziye Irem Eyiokur, Dogucan Yaman, Leonard Bärmann, Alexander H. Waibel Abstract: Error correction is crucial in human-computer interaction, as it can provide supervision for incrementally learning artificial intelligence. If a system maps entities like objects or persons with unknown class to inappropriate existing classes, or misrecognizes entities from known classes when there is too high train-test discrepancy, error correction is a natural way for a user to improve the system. Provided an agent with visual perception, if such entity is in the view of the system, pointing gestures can dramatically simplify the error correction. Therefore, we propose a modularized system for multimodal error correction using natural language and pointing gestures. First, pointing line generation and region proposal detects whether there is a pointing gesture, and if yes, which candidate objects (i.e. RoIs) are on the pointing line. Second, these RoIs (if any) and the user’s utterances are fed into a VL-T5 network to extract and link both the class name and the corresponding RoI of the referred entity, or to output that there is no error correction. In the latter case, the utterances can be passed to a downstream component for Natural Language Understanding. We use additional, challenging annotations for an existing real-world pointing gesture dataset to evaluate our proposed system. Furthermore, we demonstrate our approach by integrating it on a real-world steerable laser pointer robot, enabling interactive multimodal error correction and thus incremental learning of new objects.',\n",
       " 'Author: Shinji Watanabe Title: Saturation time of exposure interval for cross-neutralization response to SARS-CoV-2: Implications for vaccine dose interval Publication year: 2023 Coauthors: Sho Miyamoto, Y. Kuroda, T. Kanno, A. Ueno, N. Shiwa-Sudo, N. Iwata-Yoshikawa, Yusuke Sakai, N. Nagata, T. Arashiro, A. Ainai, Saya Moriyama, N. Kishida, Shinji Watanabe, K. Nojima, Y. Seki, T. Mizukami, H. Hasegawa, H. Ebihara, S. Fukushi, Yoshimasa Takahashi, Maeda Ken, Tadaki Suzuki ',\n",
       " 'Author: Shinji Watanabe Title: Joint Prediction and Denoising for Large-Scale Multilingual Self-Supervised Learning Publication year: 2023 Coauthors: William Chen, Jiatong Shi, Brian Yan, Dan Berrebbi, Wangyou Zhang, Yifan Peng, Xuankai Chang, Soumi Maiti, Shinji Watanabe Abstract: Multilingual self-supervised learning (SSL) has often lagged behind state-of-the-art (SOTA) methods due to the expenses and complexity required to handle many languages. This further harms the reproducibility of SSL, which is already limited to few research groups due to its resource usage. We show that more powerful techniques can actually lead to more efficient pre-training, opening SSL to more research groups. We propose WavLabLM, which extends WavLM’s joint prediction and denoising to 40k hours of data across 136 languages. To build WavLabLM, we devise a novel multi-stage pre-training method, designed to address the language imbalance of multilingual data. WavLabLM achieves comparable performance to XLS-R on ML-SUPERB with less than $10 \\\\%$ of the training data, making SSL realizable with academic compute. We show that further efficiency can be achieved with a vanilla HuBERT Base model, which can maintain $94 \\\\%$ of XLS-R’s performance with only $3 \\\\%$ of the data, 4 GPUs, and limited trials. We open-source all code and models in ESPnet.',\n",
       " 'Author: Shinji Watanabe Title: Tensor decomposition for minimization of E2E SLU model toward on-device processing Publication year: 2023 Coauthors: Yosuke Kashiwagi, Siddhant Arora, Hayato Futami, Jessica Huynh, Shih-Lun Wu, Yifan Peng, Brian Yan, E. Tsunoo, Shinji Watanabe Abstract: Spoken Language Understanding (SLU) is a critical speech recognition application and is often deployed on edge devices. Consequently, on-device processing plays a significant role in the practical implementation of SLU. This paper focuses on the end-to-end (E2E) SLU model due to its small latency property, unlike a cascade system, and aims to minimize the computational cost. We reduce the model size by applying tensor decomposition to the Conformer and E-Branchformer architectures used in our E2E SLU models. We propose to apply singular value decomposition to linear layers and the Tucker decomposition to convolution layers, respectively. We also compare COMP/PARFAC decomposition and Tensor-Train decomposition to the Tucker decomposition. Since the E2E model is represented by a single neural network, our tensor decomposition can flexibly control the number of parameters without changing feature dimensions. On the STOP dataset, we achieved 70.9% exact match accuracy under the tight constraint of only 15 million parameters.',\n",
       " 'Author: Shinji Watanabe Title: Decoder-only Architecture for Speech Recognition with CTC Prompts and Text Data Augmentation Publication year: 2023 Coauthors: E. Tsunoo, Hayato Futami, Yosuke Kashiwagi, Siddhant Arora, Shinji Watanabe Abstract: Collecting audio-text pairs is expensive; however, it is much easier to access text-only data. Unless using shallow fusion, end-to-end automatic speech recognition (ASR) models require architecture modifications or additional training schemes to use text-only data. Inspired by recent advances in decoder-only language models (LMs), such as GPT-3 and PaLM adopted for speech-processing tasks, we propose using a decoder-only architecture for ASR with simple text augmentation. To provide audio information, encoder features compressed by CTC prediction are used as prompts for the decoder, which can be regarded as refining CTC prediction using the decoder-only model. Because the decoder architecture is the same as an autoregressive LM, it is simple to enhance the model by leveraging external text data with LM training. An experimental comparison using LibriSpeech and Switchboard shows that our proposed models with text augmentation training reduced word error rates from ordinary CTC by 0.3% and 1.4% on LibriSpeech test-clean and testother set, respectively, and 2.9% and 5.0% on Switchboard and CallHome. The proposed model had advantage on computational efficiency compared with conventional encoder-decoder ASR models with a similar parameter setup, and outperformed them on the LibriSpeech 100h and Switchboard training scenarios.',\n",
       " 'Author: Shinji Watanabe Title: ML-SUPERB: Multilingual Speech Universal PERformance Benchmark Publication year: 2023 Coauthors: Jiatong Shi, Dan Berrebbi, William Chen, Ho-Lam Chung, En-Pei Hu, Wei Huang, Xuankai Chang, Shang-Wen Li, Abdel-rahman Mohamed, Hung-yi Lee, Shinji Watanabe Abstract: Speech processing Universal PERformance Benchmark (SUPERB) is a leaderboard to benchmark the performance of Self-Supervised Learning (SSL) models on various speech processing tasks. However, SUPERB largely considers English speech in its evaluation. This paper presents multilingual SUPERB (ML-SUPERB), covering 143 languages (ranging from high-resource to endangered), and considering both automatic speech recognition and language identification. Following the concept of SUPERB, ML-SUPERB utilizes frozen SSL features and employs a simple framework for multilingual tasks by learning a shallow downstream model. Similar to the SUPERB benchmark, we find speech SSL models can significantly improve performance compared to FBANK features. Furthermore, we find that multilingual models do not always perform better than their monolingual counterparts. We will release ML-SUPERB as a challenge with organized datasets and reproducible training scripts for future multilingual representation research.',\n",
       " 'Author: Shinji Watanabe Title: Structured Pruning of Self-Supervised Pre-Trained Models for Speech Recognition and Understanding Publication year: 2023 Coauthors: Yifan Peng, Kwangyoun Kim, Felix Wu, Prashant Sridhar, Shinji Watanabe Abstract: Self-supervised speech representation learning (SSL) has shown to be effective in various downstream tasks, but SSL models are usually large and slow. Model compression techniques such as pruning aim to reduce the model size and computation without degradation in accuracy. Prior studies focus on the pruning of Transformers; however, speech models not only utilize a stack of Transformer blocks, but also combine a frontend network based on multiple convolutional layers for low-level feature representation learning. This frontend has a small size but a heavy computational cost. In this work, we propose three task-specific structured pruning methods to deal with such heterogeneous networks. Experiments on LibriSpeech and SLURP show that the proposed method is more accurate than the original wav2vec2-base with 10% to 30% less computation, and is able to reduce the computation by 40% to 50% without any degradation.',\n",
       " 'Author: Shinji Watanabe Title: Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization Publication year: 2023 Coauthors: Puyuan Peng, Brian Yan, Shinji Watanabe, David F. Harwath Abstract: We investigate the emergent abilities of the recently proposed web-scale speech model Whisper, by adapting it to unseen tasks with prompt engineering. We selected three tasks: audio-visual speech recognition (AVSR), code-switched speech recognition (CS-ASR), and speech translation (ST) on unseen language pairs. We design task-specific prompts, by either leveraging another large-scale model, or simply manipulating the special tokens in the default prompts. Experiments show that compared to the default prompts, our proposed prompts improve performance by 10% to 45% on the three zero-shot tasks, and even outperform SotA supervised models on some datasets. In addition, our experiments reveal many interesting properties of Whisper, including its robustness to prompts, bias on accents, and the multilingual understanding in its latent space. Code is available at https://github.com/jasonppy/PromptingWhisper',\n",
       " 'Author: Shinji Watanabe Title: A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks Publication year: 2023 Coauthors: Yifan Peng, Kwangyoun Kim, Felix Wu, Brian Yan, Siddhant Arora, William Chen, Jiyang Tang, Suwon Shon, Prashant Sridhar, Shinji Watanabe Abstract: Conformer, a convolution-augmented Transformer variant, has become the de facto encoder architecture for speech processing due to its superior performance in various tasks, including automatic speech recognition (ASR), speech translation (ST) and spoken language understanding (SLU). Recently, a new encoder called E-Branchformer has outperformed Conformer in the LibriSpeech ASR benchmark, making it promising for more general speech applications. This work compares E-Branchformer and Conformer through extensive experiments using different types of end-to-end sequence-to-sequence models. Results demonstrate that E-Branchformer achieves comparable or better performance than Conformer in almost all evaluation sets across 15 ASR, 2 ST, and 3 SLU benchmarks, while being more stable during training. We will release our training configurations and pre-trained models for reproducibility, which can benefit the speech community.',\n",
       " 'Author: Shinji Watanabe Title: Efficient Sequence Transduction by Jointly Predicting Tokens and Durations Publication year: 2023 Coauthors: Hainan Xu, Fei Jia, Somshubra Majumdar, Hengguan Huang, Shinji Watanabe, Boris Ginsburg Abstract: This paper introduces a novel Token-and-Duration Transducer (TDT) architecture for sequence-to-sequence tasks. TDT extends conventional RNN-Transducer architectures by jointly predicting both a token and its duration, i.e. the number of input frames covered by the emitted token. This is achieved by using a joint network with two outputs which are independently normalized to generate distributions over tokens and durations. During inference, TDT models can skip input frames guided by the predicted duration output, which makes them significantly faster than conventional Transducers which process the encoder output frame by frame. TDT models achieve both better accuracy and significantly faster inference than conventional Transducers on different sequence transduction tasks. TDT models for Speech Recognition achieve better accuracy and up to 2.82X faster inference than conventional Transducers. TDT models for Speech Translation achieve an absolute gain of over 1 BLEU on the MUST-C test compared with conventional Transducers, and its inference is 2.27X faster. In Speech Intent Classification and Slot Filling tasks, TDT models improve the intent accuracy by up to over 1% (absolute) over conventional Transducers, while running up to 1.28X faster. Our implementation of the TDT model will be open-sourced with the NeMo (https://github.com/NVIDIA/NeMo) toolkit.',\n",
       " 'Author: Shinji Watanabe Title: Speech Summarization of Long Spoken Document: Improving Memory Efficiency of Speech/Text Encoders Publication year: 2023 Coauthors: Takatomo Kano, A. Ogawa, Marc Delcroix, Roshan Sharma, Kohei Matsuura, Shinji Watanabe Abstract: Speech summarization requires processing several minute-long speech sequences to allow exploiting the whole context of a spoken document. A conventional approach is a cascade of automatic speech recognition (ASR) and text summarization (TS). However, the cascade systems are sensitive to ASR errors. Moreover, the cascade system cannot be optimized for input speech and utilize para-linguistic information. Recently, there has been an increased interest in end-to-end (E2E) approaches optimized to output summaries directly from speech. Such systems can thus mitigate the ASR errors of cascade approaches. However, E2E speech summarization requires massive computational resources because it needs to encode long speech sequences. We propose a speech summarization system that enables E2E summarization from 100 seconds, which is the limit of the conventional method, to up to 10 minutes (i.e., the duration of typical instructional videos on YouTube). However, the modeling capability of this model for minute-long speech sequences is weaker than the conventional approach. We thus exploit auxiliary text information from ASR transcriptions to improve the modeling capabilities. The resultant system consists of a dual speech/text encoder decoder-based summarization system. We perform experiments on the How2 dataset showing the proposed system improved METEOR scores by up to 2.7 points by fully exploiting the long spoken documents.',\n",
       " 'Author: Shinji Watanabe Title: UNSSOR: Unsupervised Neural Speech Separation by Leveraging Over-determined Training Mixtures Publication year: 2023 Coauthors: Zhong-Qiu Wang, Shinji Watanabe Abstract: In reverberant conditions with multiple concurrent speakers, each microphone acquires a mixture signal of multiple speakers at a different location. In over-determined conditions where the microphones out-number speakers, we can narrow down the solutions to speaker images and realize unsupervised speech separation by leveraging each mixture signal as a constraint (i.e., the estimated speaker images at a microphone should add up to the mixture). Equipped with this insight, we propose UNSSOR, an algorithm for $\\\\textbf{u}$nsupervised $\\\\textbf{n}$eural $\\\\textbf{s}$peech $\\\\textbf{s}$eparation by leveraging $\\\\textbf{o}$ver-determined training mixtu$\\\\textbf{r}$es. At each training step, we feed an input mixture to a deep neural network (DNN) to produce an intermediate estimate for each speaker, linearly filter the estimates, and optimize a loss so that, at each microphone, the filtered estimates of all the speakers can add up to the mixture to satisfy the above constraint. We show that this loss can promote unsupervised separation of speakers. The linear filters are computed in each sub-band based on the mixture and DNN estimates through the forward convolutive prediction (FCP) algorithm. To address the frequency permutation problem incurred by using sub-band FCP, a loss term based on minimizing intra-source magnitude scattering is proposed. Although UNSSOR requires over-determined training mixtures, we can train DNNs to achieve under-determined separation (e.g., unsupervised monaural speech separation). Evaluation results on two-speaker separation in reverberant conditions show the effectiveness and potential of UNSSOR.',\n",
       " 'Author: Shinji Watanabe Title: Segment-Level Vectorized Beam Search Based on Partially Autoregressive Inference Publication year: 2023 Coauthors: Masao Someki, N. Eng, Yosuke Higuchi, Shinji Watanabe Abstract: Attention-based encoder-decoder models with autoregressive (AR) decoding have proven to be the dominant approach for automatic speech recognition (ASR) due to their superior accuracy. However, they often suffer from slow inference. This is primarily attributed to the incremental calculation of the decoder. This work proposes a partially AR framework, which employs segment-level vectorized beam search for improving the inference speed of an ASR model based on the hybrid connectionist temporal classification (CTC) attention-based architecture. It first generates an initial hypothesis using greedy CTC decoding, identifying low-confidence tokens based on their output probabilities. We then utilize the decoder to perform segment-level vectorized beam search on these tokens, re-predicting in parallel with minimal decoder calculations. Experimental results show that our method is 12 to 13 times faster in inference on the LibriSpeech corpus over AR decoding whilst preserving high accuracy.',\n",
       " 'Author: Shinji Watanabe Title: Unsupervised Data Selection for TTS: Using Arabic Broadcast News as a Case Study Publication year: 2023 Coauthors: Massa Baali, Tomoki Hayashi, Hamdy Mubarak, Soumi Maiti, Shinji Watanabe, W. El-Hajj, Ahmed Ali Abstract: .',\n",
       " 'Author: Shinji Watanabe Title: BASS: Block-wise Adaptation for Speech Summarization Publication year: 2023 Coauthors: Roshan Sharma, Kenneth Zheng, Siddhant Arora, Shinji Watanabe, Rita Singh, B. Raj Abstract: End-to-end speech summarization has been shown to improve performance over cascade baselines. However, such models are difficult to train on very large inputs (dozens of minutes or hours) owing to compute restrictions and are hence trained with truncated model inputs. Truncation leads to poorer models, and a solution to this problem rests in block-wise modeling, i.e., processing a portion of the input frames at a time. In this paper, we develop a method that allows one to train summarization models on very long sequences in an incremental manner. Speech summarization is realized as a streaming process, where hypothesis summaries are updated every block based on new acoustic information. We devise and test strategies to pass semantic context across the blocks. Experiments on the How2 dataset demonstrate that the proposed block-wise training method improves by 3 points absolute on ROUGE-L over a truncated input baseline.',\n",
       " 'Author: Shinji Watanabe Title: I3D: Transformer Architectures with Input-Dependent Dynamic Depth for Speech Recognition Publication year: 2023 Coauthors: Yifan Peng, Jaesong Lee, Shinji Watanabe Abstract: Transformer-based end-to-end speech recognition has achieved great success. However, the large footprint and computational overhead make it difficult to deploy these models in some real-world applications. Model compression techniques can reduce the model size and speed up inference, but the compressed model has a fixed architecture which might be suboptimal. We propose a novel Transformer encoder with Input-Dependent Dynamic Depth (I3D) to achieve strong performance-efficiency trade-offs. With a similar number of layers at inference time, I3D-based models outperform the vanilla Transformer and the static pruned model via iterative layer pruning. We also present interesting analysis on the gate probabilities and the input-dependency, which helps us better understand deep encoders.',\n",
       " 'Author: Shinji Watanabe Title: A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech Publication year: 2023 Coauthors: Li-Wei Chen, Shinji Watanabe, Alexander I. Rudnicky Abstract: Recent Text-to-Speech (TTS) systems trained on reading or acted corpora have achieved near human-level naturalness. The diversity of human speech, however, often goes beyond the coverage of these corpora. We believe the ability to handle such diversity is crucial for AI systems to achieve human-level communication. Our work explores the use of more abundant real-world data for building speech synthesizers. We train TTS systems using real-world speech from YouTube and podcasts. We observe the mismatch between training and inference alignments in mel-spectrogram based autoregressive models, leading to unintelligible synthesis, and demonstrate that learned discrete codes within multiple code groups effectively resolves this issue. We introduce our MQTTS system whose architecture is designed for multiple code generation and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality. We conduct ablation analyses to identify the efficacy of our methods. We show that MQTTS outperforms existing TTS systems in several objective and subjective measures.',\n",
       " 'Author: Shinji Watanabe Title: A New Benchmark of Aphasia Speech Recognition and Detection Based on E-Branchformer and Multi-task Learning Publication year: 2023 Coauthors: Jiyang Tang, William Chen, Xuankai Chang, Shinji Watanabe, B. MacWhinney Abstract: Aphasia is a language disorder that affects the speaking ability of millions of patients. This paper presents a new benchmark for Aphasia speech recognition and detection tasks using state-of-the-art speech recognition techniques with the AphsiaBank dataset. Specifically, we introduce two multi-task learning methods based on the CTC/Attention architecture to perform both tasks simultaneously. Our system achieves state-of-the-art speaker-level detection accuracy (97.3%), and a relative WER reduction of 11% for moderate Aphasia patients. In addition, we demonstrate the generalizability of our approach by applying it to another disordered speech database, the DementiaBank Pitt corpus. We will make our all-in-one recipes and pre-trained model publicly available to facilitate reproducibility. Our standardized data preprocessing pipeline and open-source recipes enable researchers to compare results directly, promoting progress in disordered speech processing.',\n",
       " 'Author: Shinji Watanabe Title: Antiviral Susceptibilities of Distinct Lineages of Influenza C and D Viruses Publication year: 2023 Coauthors: E. Takashita, S. Murakami, Y. Matsuzaki, Seiichiro Fujisaki, H. Morita, Shiho Nagata, Misa Katayama, K. Mizuta, H. Nishimura, Shinji Watanabe, T. Horimoto, H. Hasegawa Abstract: The emergence and spread of antiviral-resistant influenza viruses are of great concern. To minimize the public health risk, it is important to monitor antiviral susceptibilities of influenza viruses. Analyses of the antiviral susceptibilities of influenza A and B viruses have been conducted globally; however, those of influenza C and D viruses are limited. Here, we determined the susceptibilities of influenza C viruses representing all six lineages (C/Taylor, C/Yamagata, C/Sao Paulo, C/Aichi, C/Kanagawa, and C/Mississippi) and influenza D viruses representing four lineages (D/OK, D/660, D/Yama2016, and D/Yama2019) to RNA polymerase inhibitors (baloxavir and favipiravir) by using a focus reduction assay. All viruses tested were susceptible to both drugs. We then performed a genetic analysis to check for amino acid substitutions associated with baloxavir and favipiravir resistance and found that none of the viruses tested possessed these substitutions. Use of the focus reduction assay with the genotypic assay has proven valuable for monitoring the antiviral susceptibilities of influenza C and D viruses as well as influenza A and B viruses. Antiviral susceptibility monitoring of all influenza virus types should continue in order to assess the public health risks posed by these viruses.',\n",
       " 'Author: Shinji Watanabe Title: Paaploss: A Phonetic-Aligned Acoustic Parameter Loss for Speech Enhancement Publication year: 2023 Coauthors: Muqiao Yang, Joseph Konan, David Bick, YUNYANG ZENG, Shuo Han, Anurag Kumar, Shinji Watanabe, B. Raj Abstract: Despite rapid advancement in recent years, current speech enhancement models often produce speech that differs in perceptual quality from real clean speech. We propose a learning objective that formalizes differences in perceptual quality, by using domain knowledge of acoustic-phonetics. We identify temporal acoustic parameters – such as spectral tilt, spectral flux, shimmer, etc. – that are non-differentiable, and we develop a neural network estimator that can accurately predict their time-series values across an utterance. We also model phoneme-specific weights for each feature, as the acoustic parameters are known to show different behavior in different phonemes. We can add this criterion as an auxiliary loss to any model that produces speech, to optimize speech outputs to match the values of clean speech in these features. Experimentally we show that it improves speech enhancement workflows in both time-domain and time-frequency domain, as measured by standard evaluation metrics. We also provide an analysis of phoneme-dependent improvement on acoustic parameters, demonstrating the additional interpretability that our method provides. This analysis can suggest which features are currently the bottleneck for improvement.',\n",
       " 'Author: Shinji Watanabe Title: SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing Publication year: 2023 Coauthors: Taiqi He, Lindia Tjuatja, Nathaniel R. Robinson, Shinji Watanabe, David R. Mortensen, Graham Neubig, L. Levin Abstract: In our submission to the SIGMORPHON 2023 Shared Task on interlinear glossing (IGT), we explore approaches to data augmentation and modeling across seven low-resource languages. For data augmentation, we explore two approaches: creating artificial data from the provided training data and utilizing existing IGT resources in other languages. On the modeling side, we test an enhanced version of the provided token classification baseline as well as a pretrained multilingual seq2seq model. Additionally, we apply post-correction using a dictionary for Gitksan, the language with the smallest amount of data. We find that our token classification models are the best performing, with the highest word-level accuracy for Arapaho and highest morpheme-level accuracy for Gitksan out of all submissions. We also show that data augmentation is an effective strategy, though applying artificial data pretraining has very different effects across both models tested.',\n",
       " 'Author: Shinji Watanabe Title: Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation Publication year: 2023 Coauthors: Yoshiki Masuyama, Xuankai Chang, Wangyou Zhang, Samuele Cornell, Zhongqiu Wang, Nobutaka Ono, Y. Qian, Shinji Watanabe Abstract: Neural speech separation has made remarkable progress and its integration with automatic speech recognition (ASR) is an important direction towards realizing multi-speaker ASR. This work provides an insightful investigation of speech separation in reverberant and noisy-reverberant scenarios as an ASR front-end. In detail, we explore multi-channel separation methods, mask-based beamforming and complex spectral mapping, as well as the best features to use in the ASR back-end model. We employ the recent self-supervised learning representation (SSLR) as a feature and improve the recognition performance from the case with filterbank features. To further improve multi-speaker recognition performance, we present a carefully designed training strategy for integrating speech separation and recognition with SSLR. The proposed integration using TF-GridNet-based complex spectral mapping and WavLM-based SSLR achieves a 2.5% word error rate in reverberant WHAMR! test set, significantly outperforming an existing mask-based MVDR beamforming and filterbank integration (28.9%).',\n",
       " 'Author: Shinji Watanabe Title: Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive Instruction-Tuning Benchmark for Speech Publication year: 2023 Coauthors: Chien-yu Huang, Ke-Han Lu, Shi Wang, Chi-Yuan Hsiao, Chun-Yi Kuan, Haibin Wu, Siddhant Arora, Kai-Wei Chang, Jiatong Shi, Yifan Peng, Roshan Sharma, Shinji Watanabe, Bhiksha Ramakrishnan, Shady Shehata, Hung-yi Lee Abstract: Text language models have shown remarkable zero-shot capability in generalizing to unseen tasks when provided with well-formulated instructions. However, existing studies in speech processing primarily focus on limited or specific tasks. Moreover, the lack of standardized benchmarks hinders a fair comparison across different approaches. Thus, we present Dynamic-SUPERB, a benchmark designed for building universal speech models capable of leveraging instruction tuning to perform multiple tasks in a zero-shot fashion. To achieve comprehensive coverage of diverse speech tasks and harness instruction tuning, we invite the community to collaborate and contribute, facilitating the dynamic growth of the benchmark. To initiate, Dynamic-SUPERB features 55 evaluation instances by combining 33 tasks and 22 datasets. This spans a broad spectrum of dimensions, providing a comprehensive platform for evaluation. Additionally, we propose several approaches to establish benchmark baselines. These include the utilization of speech models, text language models, and the multimodal encoder. Evaluation results indicate that while these baselines perform reasonably on seen tasks, they struggle with unseen ones. We also conducted an ablation study to assess the robustness and seek improvements in the performance. We release all materials to the public and welcome researchers to collaborate on the project, advancing technologies in the field together.',\n",
       " 'Author: Shinji Watanabe Title: Speaker-Independent Acoustic-to-Articulatory Speech Inversion Publication year: 2023 Coauthors: Peter Wu, Li-Wei Chen, Cheol Jun Cho, Shinji Watanabe, L. Goldstein, A. Black, G. Anumanchipalli Abstract: To build speech processing methods that can handle speech as naturally as humans, researchers have explored multiple ways of building an invertible mapping from speech to an interpretable space. The articulatory space is a promising inversion target, since this space captures the mechanics of speech production. To this end, we build an acoustic-to-articulatory inversion (AAI) model that leverages autoregression, adversarial training, and self supervision to generalize to unseen speakers. Our approach obtains 0.784 correlation on an electromagnetic articulography (EMA) dataset, improving the state-of-the-art by 12.5%. Additionally, we show the interpretability of these representations through directly com-paring the behavior of estimated representations with speech production behavior. Finally, we propose a resynthesis-based AAI evaluation metric that does not rely on articulatory labels, demonstrating its efficacy with an 18-speaker dataset.',\n",
       " 'Author: Shinji Watanabe Title: Towards Practical and Efficient Image-to-Speech Captioning with Vision-Language Pre-training and Multi-modal Tokens Publication year: 2023 Coauthors: Minsu Kim, J. Choi, Soumi Maiti, Jeong Hun Yeo, Shinji Watanabe, Y. Ro Abstract: In this paper, we propose methods to build a powerful and efficient Image-to-Speech captioning (Im2Sp) model. To this end, we start with importing the rich knowledge related to image comprehension and language modeling from a large-scale pre-trained vision-language model into Im2Sp. We set the output of the proposed Im2Sp as discretized speech units, i.e., the quantized speech features of a self-supervised speech model. The speech units mainly contain linguistic information while suppressing other characteristics of speech. This allows us to incorporate the language modeling capability of the pre-trained vision-language model into the spoken language modeling of Im2Sp. With the vision-language pre-training strategy, we set new state-of-the-art Im2Sp performances on two widely used benchmark databases, COCO and Flickr8k. Then, we further improve the efficiency of the Im2Sp model. Similar to the speech unit case, we convert the original image into image units, which are derived through vector quantization of the raw image. With these image units, we can drastically reduce the required data storage for saving image data to just 0.8% when compared to the original image data in terms of bits. Demo page: https://ms-dot-k.github.io/Image-to-Speech-Captioning.',\n",
       " 'Author: Shinji Watanabe Title: Enhancing Speech-To-Speech Translation with Multiple TTS Targets Publication year: 2023 Coauthors: Jiatong Shi, Yun Tang, Ann Lee, H. Inaguma, Changhan Wang, J. Pino, Shinji Watanabe Abstract: It has been known that direct speech-to-speech translation (S2ST) models usually suffer from the data scarcity issue because of the limited existing parallel materials for both source and target speech. Therefore to train a direct S2ST system, previous works usually utilize text-to-speech (TTS) systems to generate samples in the target language by augmenting the data from speech-to-text translation (S2TT). However, there is a limited investigation into how the synthesized target speech would affect the S2ST models. In this work, we analyze the effect of changing synthesized target speech for direct S2ST models. We find that simply combining the target speech from different TTS systems can potentially improve the S2ST performances. Following that, we also propose a multi-task framework that jointly optimizes the S2ST system with multiple targets from different TTS systems. Extensive experiments demonstrate that our proposed framework achieves consistent improvements (2.8 BLEU) over the baselines on the Fisher Spanish-English dataset.',\n",
       " 'Author: Shinji Watanabe Title: AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head Publication year: 2023 Coauthors: Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jia-Bin Huang, Jinglin Liu, Yixiang Ren, Zhou Zhao, Shinji Watanabe Abstract: Large language models (LLMs) have exhibited remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. Despite the recent success, current LLMs are not capable of processing complex audio information or conducting spoken conversations (like Siri or Alexa). In this work, we propose a multi-modal AI system named AudioGPT, which complements LLMs (i.e., ChatGPT) with 1) foundation models to process complex audio information and solve numerous understanding and generation tasks; and 2) the input/output interface (ASR, TTS) to support spoken dialogue. With an increasing demand to evaluate multi-modal LLMs of human intention understanding and cooperation with foundation models, we outline the principles and processes and test AudioGPT in terms of consistency, capability, and robustness. Experimental results demonstrate the capabilities of AudioGPT in solving AI tasks with speech, music, sound, and talking head understanding and generation in multi-round dialogues, which empower humans to create rich and diverse audio content with unprecedented ease. Our system is publicly available at \\\\url{https://github.com/AIGC-Audio/AudioGPT}.',\n",
       " 'Author: Shinji Watanabe Title: Improving Audio Captioning Models with Fine-grained Audio Features, Text Embedding Supervision, and LLM Mix-up Augmentation Publication year: 2023 Coauthors: Shih-Lun Wu, Xuankai Chang, G. Wichern, Jee-weon Jung, Franccois G. Germain, Jonathan Le Roux, Shinji Watanabe Abstract: Automated audio captioning (AAC) aims to generate informative descriptions for various sounds from nature and/or human activities. In recent years, AAC has quickly attracted research interest, with state-of-the-art systems now relying on a sequence-to-sequence (seq2seq) backbone powered by strong models such as Transformers. Following the macro-trend of applied machine learning research, in this work, we strive to improve the performance of seq2seq AAC models by extensively leveraging pretrained models and large language models (LLMs). Specifically, we utilize BEATs to extract fine-grained audio features. Then, we employ Instructor LLM to fetch text embeddings of captions, and infuse their language-modality knowledge into BEATs audio features via an auxiliary InfoNCE loss function. Moreover, we propose a novel data augmentation method that uses ChatGPT to produce caption mix-ups (i.e., grammatical and compact combinations of two captions) which, together with the corresponding audio mixtures, increase not only the amount but also the complexity and diversity of training data. During inference, we propose to employ nucleus sampling and a hybrid reranking algorithm, which has not been explored in AAC research. Combining our efforts, our model achieves a new state-of-the-art 32.6 SPIDEr-FL score on the Clotho evaluation split, and wins the 2023 DCASE AAC challenge.',\n",
       " 'Author: Shinji Watanabe Title: Voxtlm: unified decoder-only models for consolidating speech recognition/synthesis and speech/text continuation tasks Publication year: 2023 Coauthors: Soumi Maiti, Yifan Peng, Shukjae Choi, Jee-weon Jung, Xuankai Chang, Shinji Watanabe Abstract: We propose a decoder-only language model, VoxtLM, that can perform four tasks: speech recognition, speech synthesis, text generation, and speech continuation. VoxtLM integrates text vocabulary with discrete speech tokens from self-supervised speech features and uses special tokens to enable multitask learning. Compared to a single-task model, VoxtLM exhibits a significant improvement in speech synthesis, with improvements in both speech intelligibility from 28.9 to 5.6 and objective quality from 2.68 to 3.90. VoxtLM also improves speech generation and speech recognition performance over the single-task counterpart. Further, VoxtLM is trained with publicly available data and training recipes and model checkpoints are open-sourced to make fully reproducible work.',\n",
       " 'Author: Shinji Watanabe Title: Learning to Speak from Text: Zero-Shot Multilingual Text-to-Speech with Unsupervised Text Pretraining Publication year: 2023 Coauthors: Takaaki Saeki, Soumi Maiti, Xinjian Li, Shinji Watanabe, Shinnosuke Takamichi, H. Saruwatari Abstract: While neural text-to-speech (TTS) has achieved human-like natural synthetic speech, multilingual TTS systems are limited to resource-rich languages due to the need for paired text and studio-quality audio data. This paper proposes a method for zero-shot multilingual TTS using text-only data for the target language. The use of text-only data allows the development of TTS systems for low-resource languages for which only textual resources are available, making TTS accessible to thousands of languages. Inspired by the strong cross-lingual transferability of multilingual language models, our framework first performs masked language model pretraining with multilingual text-only data. Then we train this model with a paired data in a supervised manner, while freezing a language-aware embedding layer. This allows inference even for languages not included in the paired data but present in the text-only data. Evaluation results demonstrate highly intelligible zero-shot TTS with a character error rate of less than 12% for an unseen language.',\n",
       " 'Author: Shinji Watanabe Title: FNeural Speech Enhancement with Very Low Algorithmic Latency and Complexity via Integrated full- and sub-band Modeling Publication year: 2023 Coauthors: Zhongqiu Wang, Samuele Cornell, Shukjae Choi, Younglo Lee, Byeonghak Kim, Shinji Watanabe Abstract: We propose FSB-LSTM, a novel long short-term memory (LSTM) based architecture that integrates full- and sub-band (FSB) modeling, for single- and multi-channel speech enhancement in the short-time Fourier transform (STFT) domain. The model maintains an information highway to flow an over-complete input representation through multiple FSB-LSTM modules. Each FSB-LSTM module consists of a full-band block to model spectro-temporal patterns at all frequencies and a sub-band block to model patterns within each sub-band, where each of the two blocks takes a down-sampled representation as input and returns an up-sampled discriminative representation to be added to the block input via a residual connection. The model is designed to have a low algorithmic complexity, a small run-time buffer and a very low algorithmic latency, at the same time producing a strong enhancement performance on a noisy-reverberant speech enhancement task even if the hop size is as low as 2 ms.',\n",
       " 'Author: Shinji Watanabe Title: Improving Massively Multilingual ASR with Auxiliary CTC Objectives Publication year: 2023 Coauthors: William Chen, Brian Yan, Jiatong Shi, Yifan Peng, Soumi Maiti, Shinji Watanabe Abstract: Multilingual Automatic Speech Recognition (ASR) models have extended the usability of speech technologies to a wide variety of languages. With how many languages these models have to handle, however, a key to understanding their imbalanced performance across different languages is to examine if the model actually knows which language it should transcribe. In this paper, we introduce our work on improving performance on FLEURS, a 102-language open ASR benchmark, by conditioning the entire model on language identity (LID). We investigate techniques inspired from recent Connectionist Temporal Classification (CTC) studies to help the model handle the large number of languages, conditioning on the LID predictions of auxiliary tasks. Our experimental results demonstrate the effectiveness of our technique over standard CTC/Attention-based hybrid models. Furthermore, our state-of-the-art systems using self-supervised models with the Conformer architecture improve over the results of prior work on FLEURS by a relative 28.4% CER. Trained models are reproducible recipes are available at https://github.com/espnet/espnet/tree/master/egs2/fleurs/asr1.',\n",
       " 'Author: Shinji Watanabe Title: Toward Universal Speech Enhancement For Diverse Input Conditions Publication year: 2023 Coauthors: Wangyou Zhang, Kohei Saijo, Zhong-Qiu Wang, Shinji Watanabe, Yanmin Qian Abstract: The past decade has witnessed substantial growth of data-driven speech enhancement (SE) techniques thanks to deep learning. While existing approaches have shown impressive performance in some common datasets, most of them are designed only for a single condition (e.g., single-channel, multi-channel, or a fixed sampling frequency) or only consider a single task (e.g., denoising or dereverberation). Currently, there is no universal SE approach that can effectively handle diverse input conditions with a single model. In this paper, we make the first attempt to investigate this line of research. First, we devise a single SE model that is independent of microphone channels, signal lengths, and sampling frequencies. Second, we design a universal SE benchmark by combining existing public corpora with multiple conditions. Our experiments on a wide range of datasets show that the proposed single model can successfully handle diverse conditions with strong performance.',\n",
       " 'Author: Shinji Watanabe Title: The Pipeline System of ASR and NLU with MLM-based data Augmentation Toward Stop Low-Resource Challenge Publication year: 2023 Coauthors: Hayato Futami, Jessica Huynh, Siddhant Arora, Shih-Lun Wu, Yosuke Kashiwagi, Yifan Peng, Brian Yan, E. Tsunoo, Shinji Watanabe Abstract: This paper describes our system for the low-resource domain adaptation track (Track 3) in Spoken Language Understanding Grand Challenge, which is a part of ICASSP Signal Processing Grand Challenge 2023. In the track, we adopt a pipeline approach of ASR and NLU. For ASR, we fine-tune Whisper for each domain with upsampling. For NLU, we fine-tune BART on all the Track3 data and then on low-resource domain data. We apply masked LM (MLM) -based data augmentation, where some of input tokens and corresponding target labels are replaced using MLM. We also apply a retrieval-based approach, where model input is augmented with similar training samples. As a result, we achieved exact match (EM) accuracy 63.3/75.0 (average: 69.15) for reminder/weather domain, and won the 1st place at the challenge.',\n",
       " 'Author: Shinji Watanabe Title: A Study on the Integration of Pipeline and E2E SLU Systems for Spoken Semantic Parsing Toward Stop Quality Challenge Publication year: 2023 Coauthors: Siddhant Arora, Hayato Futami, Shih-Lun Wu, Jessica Huynh, Yifan Peng, Yosuke Kashiwagi, E. Tsunoo, Brian Yan, Shinji Watanabe Abstract: Recently there have been efforts to introduce new benchmark tasks for spoken language understanding (SLU), like semantic parsing. In this paper, we describe our proposed spoken semantic parsing system for the quality track (Track 1) in Spoken Language Understanding Grand Challenge which is part of ICASSP Signal Processing Grand Challenge 2023. We experiment with both end-to-end and pipeline systems for this task. Strong automatic speech recognition (ASR) models like Whisper and pretrained Language models (LM) like BART are utilized inside our SLU framework to boost performance. We also investigate the output level combination of various models to get an exact match accuracy of 80.8, which won the 1st place at the challenge.',\n",
       " 'Author: Shinji Watanabe Title: A community cluster of influenza A(H3N2) virus infection with reduced susceptibility to baloxavir due to a PA E199G substitution in Japan, February to March 2023 Publication year: 2023 Coauthors: E. Takashita, Seiichiro Fujisaki, H. Morita, Shiho Nagata, H. Miura, Yuki Matsuura, Saya Yamamoto, Shoko Chiba, Yumiko Inoue, Iori Minami, Sayaka Yoshikawa, Seiko Yamazaki, N. Kishida, Kazuya Nakamura, Masayuki Shirakura, Shinji Watanabe, Hideki Hasegawa Abstract: A community cluster of influenza A(H3N2) caused by viruses with an E199G substitution in PA was detected in Nara, Japan, between February and March 2023. The three patients with these mutant viruses had not received antiviral treatment before specimen collection but patients in the same hospital had. The sequences of the mutant viruses were closely related, suggesting clonal spread in Nara. They showed reduced susceptibility to baloxavir in vitro; however, the clinical significance of the PA E199G substitution remains unclear.',\n",
       " 'Author: Shinji Watanabe Title: Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study Publication year: 2023 Coauthors: Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma, Jiatong Shi, Jinchuan Tian, Shinji Watanabe, Yuya Fujita, Takashi Maekaku, Pengcheng Guo, Yao-Fei Cheng, Pavel Denisov, Kohei Saijo, Hsiu-Hsuan Wang Abstract: Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of discrete speech units derived from self-supervised learning representations, which significantly compresses the size of speech data. Applying various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech processing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good results in almost all the settings. We intend to release our configurations and trained models to foster future research efforts.',\n",
       " 'Author: Shinji Watanabe Title: Joint Modelling of Spoken Language Understanding Tasks with Integrated Dialog History Publication year: 2023 Coauthors: Siddhant Arora, Hayato Futami, E. Tsunoo, Brian Yan, Shinji Watanabe Abstract: Most human interactions occur in the form of spoken conversations where the semantic meaning of a given utterance depends on the context. Each utterance in spoken conversation can be represented by many semantic and speaker attributes, and there has been an interest in building Spoken Language Understanding (SLU) systems for automatically predicting these attributes. Recent work has shown that incorporating dialogue history can help advance SLU performance. However, separate models are used for each SLU task, leading to an increase in inference time and computation cost. Motivated by this, we aim to ask: can we jointly model all the SLU tasks while incorporating context to facilitate low-latency and lightweight inference? To answer this, we propose a novel model architecture that learns dialog context to jointly predict the intent, dialog act, speaker role, and emotion for the spoken utterance. Note that our joint prediction is based on an autoregressive model and we need to decide the prediction order of dialog attributes, which is not trivial. To mitigate the issue, we also propose an order agnostic training method. Our experiments show that our joint model achieves similar results to task-specific classifiers and can effectively integrate dialog context to further improve the SLU performance.1',\n",
       " 'Author: Shinji Watanabe Title: Enhancing End-to-End Conversational Speech Translation Through Target Language Context Utilization Publication year: 2023 Coauthors: A. Hussein, Brian Yan, Antonios Anastasopoulos, Shinji Watanabe, S. Khudanpur Abstract: Incorporating longer context has been shown to benefit machine translation, but the inclusion of context in end-to-end speech translation (E2E-ST) remains under-studied. To bridge this gap, we introduce target language context in E2E-ST, enhancing coherence and overcoming memory constraints of extended audio segments. Additionally, we propose context dropout to ensure robustness to the absence of context, and further improve performance by adding speaker information. Our proposed contextual E2E-ST outperforms the isolated utterance-based E2E-ST approach. Lastly, we demonstrate that in conversational speech, contextual information primarily contributes to capturing context style, as well as resolving anaphora and named entities.',\n",
       " 'Author: Shinji Watanabe Title: Challenges of Corporate Alliance CLOMA toward Plastic Litter Publication year: 2023 Coauthors: Shinji Watanabe ',\n",
       " 'Author: Shinji Watanabe Title: The Multimodal Information Based Speech Processing (MISP) 2023 Challenge: Audio-Visual Target Speaker Extraction Publication year: 2023 Coauthors: Shilong Wu, Chenxi Wang, Hang Chen, Yusheng Dai, Chenyue Zhang, Ruoyu Wang, Hongbo Lan, Jun Du, Chin-Hui Lee, Jingdong Chen, Shinji Watanabe, Sabato Marco Siniscalchi, O. Scharenborg, Zhong-Qiu Wang, Jia Pan, Jianqing Gao Abstract: Previous Multimodal Information based Speech Processing (MISP) challenges mainly focused on audio-visual speech recognition (AVSR) with commendable success. However, the most advanced back-end recognition systems often hit performance limits due to the complex acoustic environments. This has prompted a shift in focus towards the Audio-Visual Target Speaker Extraction (AVTSE) task for the MISP 2023 challenge in ICASSP 2024 Signal Processing Grand Challenges. Unlike existing audio-visual speech enhance-ment challenges primarily focused on simulation data, the MISP 2023 challenge uniquely explores how front-end speech processing, combined with visual clues, impacts back-end tasks in real-world scenarios. This pioneering effort aims to set the first benchmark for the AVTSE task, offering fresh insights into enhancing the ac-curacy of back-end speech recognition systems through AVTSE in challenging and real acoustic environments. This paper delivers a thorough overview of the task setting, dataset, and baseline system of the MISP 2023 challenge. It also includes an in-depth analysis of the challenges participants may encounter. The experimental results highlight the demanding nature of this task, and we look forward to the innovative solutions participants will bring forward.',\n",
       " 'Author: Shinji Watanabe Title: Intrusion of Coastal Oyashio water to Funka Bay and Tsugaru Strait occasionally disturbed by Kuroshio-originating warm core ring Publication year: 2023 Coauthors: H. Abe, Y. Yahiro, T. Hasegawa, T. Hirawake, H. Onishi, A. Ooki, T. Takatsu, K. Sasaki, M. Wakita, H. Kaneko, Shinji Watanabe, T. Tanaka, T. Okunishi, S. Ohno, S. Hashizume ',\n",
       " 'Author: Shinji Watanabe Title: Reproducing Whisper-Style Training Using An Open-Source Toolkit And Publicly Available Data Publication year: 2023 Coauthors: Yifan Peng, Jinchuan Tian, Brian Yan, Dan Berrebbi, Xuankai Chang, Xinjian Li, Jiatong Shi, Siddhant Arora, William Chen, Roshan Sharma, Wangyou Zhang, Yui Sudo, Muhammad Shakeel, Jee-weon Jung, Soumi Maiti, Shinji Watanabe Abstract: Pre-training speech models on large volumes of data has achieved remarkable success. OpenAI Whisper is a multilingual multitask model trained on 680k hours of supervised speech data. It generalizes well to various speech recognition and translation benchmarks even in a zero-shot setup. However, the full pipeline for developing such models (from data collection to training) is not publicly accessible, which makes it difficult for researchers to further improve its performance and address training-related issues such as efficiency, robustness, fairness, and bias. This work presents an Open Whisper-style Speech Model (OWSM), which reproduces Whisperstyle training using an open-source toolkit and publicly available data. OWSM even supports more translation directions and can be more efficient to train. We will publicly release all scripts used for data preparation, training, inference, and scoring as well as pretrained models and training logs to promote open science. 11https://github.com/espnet/espnet',\n",
       " 'Author: Shinji Watanabe Title: Visual Speech Recognition for Languages with Limited Labeled Data using Automatic Labels from Whisper Publication year: 2023 Coauthors: Jeong Hun Yeo, Minsu Kim, Shinji Watanabe, Y. Ro Abstract: This paper proposes a powerful Visual Speech Recognition (VSR) method for multiple languages, especially for low-resource languages that have a limited number of labeled data. Different from previous methods that tried to improve the VSR performance for the target language by using knowledge learned from other languages, we explore whether we can increase the amount of training data itself for the different languages without human intervention. To this end, we employ a Whisper model which can conduct both language identification and audio-based speech recognition. It serves to filter data of the desired languages and transcribe labels from the unannotated, multilingual audio-visual data pool. By comparing the performances of VSR models trained on automatic labels and the human-annotated labels, we show that we can achieve similar VSR performance to that of human-annotated labels even without utilizing human annotations. Through the automated labeling process, we label large-scale unlabeled multilingual databases, VoxCeleb2 and AVSpeech, producing 1,002 hours of data for four low VSR resource languages, French, Italian, Spanish, and Portuguese. With the automatic labels, we achieve new state-of-the-art performance on mTEDx in four languages, significantly surpassing the previous methods. The automatic labels are available online: https://github.com/JeongHun0716/Visual-Speech-Recognition-for-Low-Resource-Languages',\n",
       " 'Author: Shinji Watanabe Title: The CHiME-7 DASR Challenge: Distant Meeting Transcription with Multiple Devices in Diverse Scenarios Publication year: 2023 Coauthors: Samuele Cornell, Matthew Wiesner, Shinji Watanabe, Desh Raj, Xuankai Chang, Paola García, Yoshiki Masuyama, Zhong-Qiu Wang, S. Squartini, S. Khudanpur Abstract: The CHiME challenges have played a significant role in the development and evaluation of robust automatic speech recognition (ASR) systems. We introduce the CHiME-7 distant ASR (DASR) task, within the 7th CHiME challenge. This task comprises joint ASR and diarization in far-field settings with multiple, and possibly heterogeneous, recording devices. Different from previous challenges, we evaluate systems on 3 diverse scenarios: CHiME-6, DiPCo, and Mixer 6. The goal is for participants to devise a single system that can generalize across different array geometries and use cases with no a-priori information. Another departure from earlier CHiME iterations is that participants are allowed to use open-source pre-trained models and datasets. In this paper, we describe the challenge design, motivation, and fundamental research questions in detail. We also present the baseline system, which is fully array-topology agnostic and features multi-channel diarization, channel selection, guided source separation and a robust ASR model that leverages self-supervised speech representations (SSLR).',\n",
       " 'Author: Shinji Watanabe Title: Cross-Modal Multi-Tasking for Speech-to-Text Translation via Hard Parameter Sharing Publication year: 2023 Coauthors: Brian Yan, Xuankai Chang, Antonios Anastasopoulos, Yuya Fujita, Shinji Watanabe Abstract: Recent works in end-to-end speech-to-text translation (ST) have proposed multi-tasking methods with soft parameter sharing which leverage machine translation (MT) data via secondary encoders that map text inputs to an eventual cross-modal representation. In this work, we instead propose a ST/MT multi-tasking framework with hard parameter sharing in which all model parameters are shared cross-modally. Our method reduces the speech-text modality gap via a pre-processing stage which converts speech and text inputs into two discrete token sequences of similar length -- this allows models to indiscriminately process both modalities simply using a joint vocabulary. With experiments on MuST-C, we demonstrate that our multi-tasking framework improves attentional encoder-decoder, Connectionist Temporal Classification (CTC), transducer, and joint CTC/attention models by an average of +0.5 BLEU without any external MT data. Further, we show that this framework incorporates external MT data, yielding +0.8 BLEU, and also improves transfer learning from pre-trained textual models, yielding +1.8 BLEU.',\n",
       " \"Author: Shinji Watanabe Title: ESPnet-ST-v2: Multipurpose Spoken Language Translation Toolkit Publication year: 2023 Coauthors: Brian Yan, Jiatong Shi, Yun Tang, H. Inaguma, Yifan Peng, Siddharth Dalmia, Peter Pol'ak, Patrick Fernandes, Dan Berrebbi, Tomoki Hayashi, Xiaohui Zhang, Zhaoheng Ni, Moto Hira, Soumi Maiti, J. Pino, Shinji Watanabe Abstract: ESPnet-ST-v2 is a revamp of the open-source ESPnet-ST toolkit necessitated by the broadening interests of the spoken language translation community. ESPnet-ST-v2 supports 1) offline speech-to-text translation (ST), 2) simultaneous speech-to-text translation (SST), and 3) offline speech-to-speech translation (S2ST) – each task is supported with a wide variety of approaches, differentiating ESPnet-ST-v2 from other open source spoken language translation toolkits. This toolkit offers state-of-the-art architectures such as transducers, hybrid CTC/attention, multi-decoders with searchable intermediates, time-synchronous blockwise CTC/attention, Translatotron models, and direct discrete unit models. In this paper, we describe the overall design, example models for each task, and performance benchmarking behind ESPnet-ST-v2, which is publicly available at https://github.com/espnet/espnet.\",\n",
       " 'Author: Shinji Watanabe Title: Multi-Channel Target Speaker Extraction with Refinement: The WavLab Submission to the Second Clarity Enhancement Challenge Publication year: 2023 Coauthors: Samuele Cornell, Zhongqiu Wang, Yoshiki Masuyama, Shinji Watanabe, Manuel Pariente, Nobutaka Ono Abstract: This paper describes our submission to the Second Clarity Enhancement Challenge (CEC2), which consists of target speech enhancement for hearing-aid (HA) devices in noisy-reverberant environments with multiple interferers such as music and competing speakers. Our approach builds upon the powerful iterative neural/beamforming enhancement (iNeuBe) framework introduced in our recent work, and this paper extends it for target speaker extraction. We therefore name the proposed approach as iNeuBe-X, where the X stands for extraction. To address the challenges encountered in the CEC2 setting, we introduce four major novelties: (1) we extend the state-of-the-art TF-GridNet model, originally designed for monaural speaker separation, for multi-channel, causal speech enhancement, and large improvements are observed by replacing the TCNDenseNet used in iNeuBe with this new architecture; (2) we leverage a recent dual window size approach with future-frame prediction to ensure that iNueBe-X satisfies the 5 ms constraint on algorithmic latency required by CEC2; (3) we introduce a novel speaker-conditioning branch for TF-GridNet to achieve target speaker extraction; (4) we propose a fine-tuning step, where we compute an additional loss with respect to the target speaker signal compensated with the listener audiogram. Without using external data, on the official development set our best model reaches a hearing-aid speech perception index (HASPI) score of 0.942 and a scale-invariant signal-to-distortion ratio improvement (SI-SDRi) of 18.8 dB. These results are promising given the fact that the CEC2 data is extremely challenging (e.g., on the development set the mixture SI-SDR is -12.3 dB). A demo of our submitted system is available at WAVLab CEC2 demo.',\n",
       " 'Author: Shinji Watanabe Title: Fully Unsupervised Topic Clustering of Unlabelled Spoken Audio Using Self-Supervised Representation Learning and Topic Model Publication year: 2023 Coauthors: Takashi Maekaku, Yuya Fujita, Xuankai Chang, Shinji Watanabe Abstract: Unsupervised topic clustering of spoken audio is an important research topic for zero-resourced unwritten languages. A classical approach is to find a set of spoken terms from only the audio based on dynamic time warping or generative modeling (e.g., hidden Markov model), and apply a topic model to classify topics. The spoken term discovery is the most important and difficult part. In this paper, we propose to combine self-supervised representation learning (SSRL) methods as a component of spoken term discovery and probabilistic topic models. Most SSRL methods pre-train a model which predicts high-quality pseudo labels generated from an audio-only corpus. These pseudo labels can be used to produce a sequence of pseudo subwords by applying deduplication and a subword model. Then, we apply a topic model based on latent Dirichlet allocation for these pseudo-subword sequences in an unsupervised manner. The clustering performance is evaluated on the Fisher corpus using normalized mutual information. We confirm the improvement of the proposed method and its effectiveness compared to an existing approach using dynamic time warping and topic models although the experimental setups are not directly comparable.',\n",
       " 'Author: Shinji Watanabe Title: TAPLoss: A Temporal Acoustic Parameter Loss for Speech Enhancement Publication year: 2023 Coauthors: YUNYANG ZENG, Joseph Konan, Shuo Han, David Bick, Muqiao Yang, Anurag Kumar, Shinji Watanabe, B. Raj Abstract: Speech enhancement models have greatly progressed in recent years, but still show limits in perceptual quality of their speech outputs. We propose an objective for perceptual quality based on temporal acoustic parameters. These are fundamental speech features that play an essential role in various applications, including speaker recognition and paralinguistic analysis. We provide a differentiable estimator for four categories of low-level acoustic descriptors involving: frequency-related parameters, energy or amplitude-related parameters, spectral balance parameters, and temporal features. Un-like prior work that looks at aggregated acoustic parameters or a few categories of acoustic parameters, our temporal acoustic parameter (TAP) loss enables auxiliary optimization and improvement of many fine-grained speech characteristics in enhancement workflows. We show that adding TAPLoss as an auxiliary objective in speech enhancement produces speech with improved perceptual quality and intelligibility. We use data from the Deep Noise Suppression 2020 Challenge to demonstrate that both time-domain models and time-frequency domain models can benefit from our method.',\n",
       " 'Author: Shinji Watanabe Title: End-to-End Speech Recognition: A Survey Publication year: 2023 Coauthors: Rohit Prabhavalkar, Takaaki Hori, Tara N. Sainath, R. Schluter, Shinji Watanabe Abstract: In the last decade of automatic speech recognition (ASR) research, the introduction of deep learning has brought considerable reductions in word error rate of more than 50% relative, compared to modeling without deep learning. In the wake of this transition, a number of all-neural ASR architectures have been introduced. These so-called end-to-end (E2E) models provide highly integrated, completely neural ASR models, which rely strongly on general machine learning knowledge, learn more consistently from data, with lower dependence on ASR domain-specific experience. The success and enthusiastic adoption of deep learning, accompanied by more generic model architectures has led to E2E models now becoming the prominent ASR approach. The goal of this survey is to provide a taxonomy of E2E ASR models and corresponding improvements, and to discuss their properties and their relationship to classical hidden Markov model (HMM) based ASR architectures. All relevant aspects of E2E ASR are covered in this work: modeling, training, decoding, and external language model integration, discussions of performance and deployment opportunities, as well as an outlook into potential future developments.',\n",
       " 'Author: Shinji Watanabe Title: The Multimodal Information Based Speech Processing (Misp) 2022 Challenge: Audio-Visual Diarization And Recognition Publication year: 2023 Coauthors: Zhe Wang, Shilong Wu, Hang Chen, Maokui He, Jun Du, Chin-Hui Lee, Jingdong Chen, Shinji Watanabe, Sabato Marco Siniscalchi, O. Scharenborg, Diyuan Liu, Baocai Yin, Jia Pan, Jianqing Gao, Cong Liu Abstract: The Multi-modal Information based Speech Processing (MISP) challenge aims to extend the application of signal processing technology in specific scenarios by promoting the research into wake-up words, speaker diarization, speech recognition, and other technologies. The MISP2022 challenge has two tracks: 1) audio-visual speaker diarization (AVSD), aiming to solve \"who spoken when\" using both audio and visual data; 2) a novel audio-visual diarization and recognition (AVDR) task that focuses on addressing \"who spoken what when\" with audio-visual speaker diarization results. Both tracks focus on the Chinese language, and use far-field audio and video in real home-tv scenarios: 2-6 people communicating each other with TV noise in the background. This paper introduces the dataset, track settings, and baselines of the MISP2022 challenge. Our analyses of experiments and examples indicate the good performance of AVDR baseline system, and the potential difficulties in this challenge due to, e.g., the far-field video quality, the presence of TV noise in the background, and the indistinguishable speakers.',\n",
       " 'Author: Shinji Watanabe Title: DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models Publication year: 2023 Coauthors: Yifan Peng, Yui Sudo, Muhammad Shakeel, Shinji Watanabe Abstract: Self-supervised learning (SSL) has achieved notable success in many speech processing tasks, but the large model size and heavy computational cost hinder the deployment. Knowledge distillation trains a small student model to mimic the behavior of a large teacher model. However, the student architecture usually needs to be manually designed and will remain fixed during training, which requires prior knowledge and can lead to suboptimal performance. Inspired by recent success of task-specific structured pruning, we propose DPHuBERT, a novel task-agnostic compression method for speech SSL based on joint distillation and pruning. Experiments on SUPERB show that DPHuBERT outperforms pure distillation methods in almost all tasks. Moreover, DPHuBERT requires little training time and performs well with limited training data, making it suitable for resource-constrained applications. Our method can also be applied to various speech SSL models. Our code and models will be publicly available.',\n",
       " 'Author: Shinji Watanabe Title: An external quality assessment feasibility study; cross laboratory comparison of haemagglutination inhibition assay and microneutralization assay performance for seasonal influenza serology testing: A FLUCOP study Publication year: 2023 Coauthors: J. Waldock, C. Weiss, Wei Wang, M. Levine, Stacie N. Jefferson, S. Ho, K. Hoschler, B. Londt, E. Masat, Louise A. Carolan, Stephany Sánchez-Ovando, A. Fox, Shinji Watanabe, Miki Akimoto, Aya Sato, N. Kishida, A. Buys, Lorens Maake, Cardia Fourie, Catherine Caillet, Sandrine Raynaud, R. Webby, J. Debeauchamp, R. Cox, Sarah Lartey, C. Trombetta, S. Marchi, E. Montomoli, I. Sanz-Muñoz, J. Eiros, Javier Sánchez-Martínez, D. Duijsings, O. Engelhardt Abstract: Introduction External Quality Assessment (EQA) schemes are designed to provide a snapshot of laboratory proficiency, identifying issues and providing feedback to improve laboratory performance and inter-laboratory agreement in testing. Currently there are no international EQA schemes for seasonal influenza serology testing. Here we present a feasibility study for conducting an EQA scheme for influenza serology methods. Methods We invited participant laboratories from industry, contract research organizations (CROs), academia and public health institutions who regularly conduct hemagglutination inhibition (HAI) and microneutralization (MN) assays and have an interest in serology standardization. In total 16 laboratories returned data including 19 data sets for HAI assays and 9 data sets for MN assays. Results Within run analysis demonstrated good laboratory performance for HAI, with intrinsically higher levels of intra-assay variation for MN assays. Between run analysis showed laboratory and strain specific issues, particularly with B strains for HAI, whilst MN testing was consistently good across labs and strains. Inter-laboratory variability was higher for MN assays than HAI, however both assays showed a significant reduction in inter-laboratory variation when a human sera pool is used as a standard for normalization. Discussion This study has received positive feedback from participants, highlighting the benefit such an EQA scheme would have on improving laboratory performance, reducing inter laboratory variation and raising awareness of both harmonized protocol use and the benefit of biological standards for seasonal influenza serology testing.',\n",
       " 'Author: Shinji Watanabe Title: Multi-Channel Speaker Extraction with Adversarial Training: The Wavlab Submission to The Clarity ICASSP 2023 Grand Challenge Publication year: 2023 Coauthors: Samuele Cornell, Zhongqiu Wang, Yoshiki Masuyama, Shinji Watanabe, Manuel Pariente, Nobutaka Ono, S. Squartini Abstract: In this work we detail our submission to the Clarity ICASSP 2023 grand challenge, in which participants have to develop a strong target speech enhancement system for hearing-aid (HA) devices in noisy-reverberant environments. Our system builds on our previous submission at the Second Clarity Enhancement Challenge (CEC2): iNeuBe-X, which consists in an iterative neural/conventional beamforming enhancement pipeline, guided by an enrollment utterance from the target speaker. This model, which won by a large margin the CEC2, is an extension of the state-of-the-art TF-GridNet model for multi-channel, streamable target-speaker speech enhancement. Here, this approach is extended and further improved by leveraging generative adversarial training, which we show proves especially useful when the training data is limited. Using only the official 6k training scenes data, our best model achieves 0.80 hearing-aid speech perception index (HASPI) and 0.41 hearing-aid speech quality index (HASQI) scores on the synthetic evaluation set. However, our model generalized poorly on the semi-real evaluation set. This highlights the fact that our community should focus more on real-world evaluation and less on fully synthetic datasets.',\n",
       " 'Author: Shinji Watanabe Title: AV-SUPERB: A Multi-Task Evaluation Benchmark for Audio-Visual Representation Models Publication year: 2023 Coauthors: Yuan Tseng, Layne Berry, Yi-Ting Chen, I-Hsiang Chiu, Hsuan-Hao Lin, Max Liu, Puyuan Peng, Yi-Jen Shih, Hung-Yu Wang, Haibin Wu, Po-Yao Huang, Chun-Mao Lai, Shang-Wen Li, David F. Harwath, Yu Tsao, Shinji Watanabe, Abdel-rahman Mohamed, Chi-Luen Feng, Hung-yi Lee Abstract: Audio-visual representation learning aims to develop systems with human-like perception by utilizing correlation between auditory and visual information. However, current models often focus on a limited set of tasks, and generalization abilities of learned representations are unclear. To this end, we propose the AV-SUPERB benchmark that enables general-purpose evaluation of unimodal audio/visual and bimodal fusion representations on 7 datasets covering 5 audio-visual tasks in speech and audio processing. We evaluate 5 recent self-supervised models and show that none of these models generalize to all tasks, emphasizing the need for future study on improving universal model performance. In addition, we show that representations may be improved with intermediate-task fine-tuning and audio event classification with AudioSet serves as a strong intermediate task. We release our benchmark with evaluation code and a model submission platform to encourage further research in audio-visual learning.',\n",
       " 'Author: Shinji Watanabe Title: Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation Publication year: 2023 Coauthors: Yu-Kuan Fu, Liang-Hsuan Tseng, Jiatong Shi, Chen-An Li, Tsung-Yuan Hsu, Shinji Watanabe, Hung-yi Lee Abstract: Most of the speech translation models heavily rely on parallel data, which is hard to collect especially for low-resource languages. To tackle this issue, we propose to build a cascaded speech translation system without leveraging any kind of paired data. We use fully unpaired data to train our unsupervised systems and evaluate our results on CoVoST 2 and CVSS. The results show that our work is comparable with some other early supervised methods in some language pairs. While cascaded systems always suffer from severe error propagation problems, we proposed denoising back-translation (DBT), a novel approach to building robust unsupervised neural machine translation (UNMT). DBT successfully increases the BLEU score by 0.7--0.9 in all three translation directions. Moreover, we simplified the pipeline of our cascaded system to reduce inference latency and conducted a comprehensive analysis of every part of our work. We also demonstrate our unsupervised speech translation results on the established website.',\n",
       " 'Author: Shinji Watanabe Title: Deep Speech Synthesis from MRI-Based Articulatory Representations Publication year: 2023 Coauthors: Peter Wu, Tingle Li, Yijingxiu Lu, Yubin Zhang, Jiachen Lian, A. Black, L. Goldstein, Shinji Watanabe, G. Anumanchipalli Abstract: In this paper, we study articulatory synthesis, a speech synthesis method using human vocal tract information that offers a way to develop efficient, generalizable and interpretable synthesizers. While recent advances have enabled intelligible articulatory synthesis using electromagnetic articulography (EMA), these methods lack critical articulatory information like excitation and nasality, limiting generalization capabilities. To bridge this gap, we propose an alternative MRI-based feature set that covers a much more extensive articulatory space than EMA. We also introduce normalization and denoising procedures to enhance the generalizability of deep learning methods trained on MRI data. Moreover, we propose an MRI-to-speech model that improves both computational efficiency and speech fidelity. Finally, through a series of ablations, we show that the proposed MRI representation is more comprehensive than EMA and identify the most suitable MRI feature subset for articulatory synthesis.',\n",
       " 'Author: Shinji Watanabe Title: FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN Publication year: 2023 Coauthors: Sweta Agrawal, Antonios Anastasopoulos, L. Bentivogli, Ondrej Bojar, Claudia Borg, Marine Carpuat, Roldano Cattoni, Mauro Cettolo, Mingda Chen, William Chen, K. Choukri, Alexandra Chronopoulou, Anna Currey, T. Declerck, Qianqian Dong, Kevin Duh, Y. Estève, Marcello Federico, Souhir Gahbiche, B. Haddow, B. Hsu, Phu Mon Htut, H. Inaguma, Dávid Javorský, J. Judge, Yasumasa Kano, Tom Ko, Rishu Kumar, Peng Li, Xutai Ma, Prashant Mathur, E. Matusov, Paul McNamee, John P. McCrae, Kenton Murray, Maria Nadejde, Satoshi Nakamura, Matteo Negri, H. Nguyen, J. Niehues, Xing Niu, Atul Kr. Ojha, John E. Ortega, Proyag Pal, J. Pino, Lonneke van der Plas, Peter Polák, Elijah Matthew Rippeth, Elizabeth Salesky, Jiatong Shi, Matthias Sperber, Sebastian Stüker, Katsuhito Sudoh, Yun Tang, Brian Thompson, Ke M. Tran, M. Turchi, A. Waibel, Mingxuan Wang, Shinji Watanabe, Rodolfo Zevallos Abstract: This paper reports on the shared tasks organized by the 20th IWSLT Conference. The shared tasks address 9 scientific challenges in spoken language translation: simultaneous and offline translation, automatic subtitling and dubbing, speech-to-speech translation, multilingual, dialect and low-resource speech translation, and formality control. The shared tasks attracted a total of 38 submissions by 31 teams. The growing interest towards spoken language translation is also witnessed by the constantly increasing number of shared task organizers and contributors to the overview paper, almost evenly distributed across industry and academia.',\n",
       " \"Author: Shinji Watanabe Title: Speech collage: code-switched audio generation by collaging monolingual corpora Publication year: 2023 Coauthors: A. Hussein, Dorsa Zeinali, Ondrej Klejch, Matthew Wiesner, Brian Yan, Shammur A. Chowdhury, Ahmed Ali, Shinji Watanabe, S. Khudanpur Abstract: Designing effective automatic speech recognition (ASR) systems for Code-Switching (CS) often depends on the availability of the transcribed CS resources. To address data scarcity, this paper introduces Speech Collage, a method that synthesizes CS data from monolingual corpora by splicing audio segments. We further improve the smoothness quality of audio generation using an overlap-add approach. We investigate the impact of generated data on speech recognition in two scenarios: using in-domain CS text and a zero-shot approach with synthesized CS text. Empirical results highlight up to 34.4% and 16.2% relative reductions in Mixed-Error Rate and Word-Error Rate for in-domain and zero-shot scenarios, respectively. Lastly, we demonstrate that CS augmentation bolsters the model's code-switching inclination and reduces its monolingual bias.\",\n",
       " 'Author: Shinji Watanabe Title: Exploration on HuBERT with Multiple Resolutions Publication year: 2023 Coauthors: Jiatong Shi, Yun Tang, H. Inaguma, Hongyu Gong, J. Pino, Shinji Watanabe Abstract: Hidden-unit BERT (HuBERT) is a widely-used self-supervised learning (SSL) model in speech processing. However, we argue that its fixed 20ms resolution for hidden representations would not be optimal for various speech-processing tasks since their attributes (e.g., speaker characteristics and semantics) are based on different time scales. To address this limitation, we propose utilizing HuBERT representations at multiple resolutions for downstream tasks. We explore two approaches, namely the parallel and hierarchical approaches, for integrating HuBERT features with different resolutions. Through experiments, we demonstrate that HuBERT with multiple resolutions outperforms the original model. This highlights the potential of utilizing multiple resolutions in SSL models like HuBERT to capture diverse information from speech signals.',\n",
       " 'Author: Shinji Watanabe Title: Integrating Pretrained ASR and LM to Perform Sequence Generation for Spoken Language Understanding Publication year: 2023 Coauthors: Siddhant Arora, Hayato Futami, Yosuke Kashiwagi, E. Tsunoo, Brian Yan, Shinji Watanabe Abstract: There has been an increased interest in the integration of pretrained speech recognition (ASR) and language models (LM) into the SLU framework. However, prior methods often struggle with a vocabulary mismatch between pretrained models, and LM cannot be directly utilized as they diverge from its NLU formulation. In this study, we propose a three-pass end-to-end (E2E) SLU system that effectively integrates ASR and LM subnetworks into the SLU formulation for sequence generation tasks. In the first pass, our architecture predicts ASR transcripts using the ASR subnetwork. This is followed by the LM subnetwork, which makes an initial SLU prediction. Finally, in the third pass, the deliberation subnetwork conditions on representations from the ASR and LM subnetworks to make the final prediction. Our proposed three-pass SLU system shows improved performance over cascaded and E2E SLU models on two benchmark SLU datasets, SLURP and SLUE, especially on acoustically challenging utterances.',\n",
       " 'Author: Shinji Watanabe Title: Reducing Barriers to Self-Supervised Learning: HuBERT Pre-training with Academic Compute Publication year: 2023 Coauthors: William Chen, Xuankai Chang, Yifan Peng, Zhaoheng Ni, Soumi Maiti, Shinji Watanabe Abstract: Self-supervised learning (SSL) has led to great strides in speech processing. However, the resources needed to train these models has become prohibitively large as they continue to scale. Currently, only a few groups with substantial resources are capable of creating SSL models, which harms reproducibility. In this work, we optimize HuBERT SSL to fit in academic constraints. We reproduce HuBERT independently from the original implementation, with no performance loss. Our code and training optimizations make SSL feasible with only 8 GPUs, instead of the 32 used in the original work. We also explore a semi-supervised route, using an ASR model to skip the first pre-training iteration. Within one iteration of pre-training, our models improve over HuBERT on several tasks. Furthermore, our HuBERT Large variant requires only 8 GPUs, achieving similar performance to the original trained on 128. As our contribution to the community, all models, configurations, and code are made open-source in ESPnet.',\n",
       " 'Author: Shinji Watanabe Title: FindAdaptNet: Find and Insert Adapters by Learned Layer Importance Publication year: 2023 Coauthors: Junwei Huang, Karthik Ganesan, Soumi Maiti, Young Min Kim, Xuankai Chang, Paul Liang, Shinji Watanabe Abstract: Adapters are lightweight bottleneck modules introduced to assist pre-trained self-supervised learning (SSL) models to be customized to new tasks. However, searching the appropriate layers to insert adapters on large models has become difficult due to the large number of possible layers and thus a vast search space (2N possibilities for N layers). In this paper, we propose a technique that achieves automatic insertion of adapters for downstream automatic speech recognition (ASR) and spoken language understanding (SLU) tasks. Our approach is based on two-stage training. First, we train our model for a specific downstream task with additional shallow learnable layers and weight parameters to obtain the weighted summation over the output of each layer in SSL. This training method is established by the SUPERB baseline [1]. This first-stage training determines the most important layers given their respective weights. In the second stage, we proceed to insert adapters to the most important layers, retaining both performance and neural architecture search efficiency. On the CommonVoice dataset[2] we obtain 20.6% absolute improvement in Word Error Rate (WER) on the Welsh language against the conventional method, which inserts the adapter modules into the highest layers without search. In the SLURP SLU task, our method yields 4.0% intent accuracy improvement against the same conventional baseline.',\n",
       " 'Author: Shinji Watanabe Title: CMU’s IWSLT 2023 Simultaneous Speech Translation System Publication year: 2023 Coauthors: Brian Yan, Jiatong Shi, Soumi Maiti, William Chen, Xinjian Li, Yifan Peng, Siddhant Arora, Shinji Watanabe Abstract: This paper describes CMU’s submission to the IWSLT 2023 simultaneous speech translation shared task for translating English speech to both German text and speech in a streaming fashion. We first build offline speech-to-text (ST) models using the joint CTC/attention framework. These models also use WavLM front-end features and mBART decoder initialization. We adapt our offline ST models for simultaneous speech-to-text translation (SST) by 1) incrementally encoding chunks of input speech, re-computing encoder states for each new chunk and 2) incrementally decoding output text, pruning beam search hypotheses to 1-best after processing each chunk. We then build text-to-speech (TTS) models using the VITS framework and achieve simultaneous speech-to-speech translation (SS2ST) by cascading our SST and TTS models.',\n",
       " 'Author: Shinji Watanabe Title: Exploration of Efficient End-to-End ASR using Discretized Input from Self-Supervised Learning Publication year: 2023 Coauthors: Xuankai Chang, Brian Yan, Yuya Fujita, Takashi Maekaku, Shinji Watanabe Abstract: Self-supervised learning (SSL) of speech has shown impressive results in speech-related tasks, particularly in automatic speech recognition (ASR). While most methods employ the output of intermediate layers of the SSL model as real-valued features for downstream tasks, there is potential in exploring alternative approaches that use discretized token sequences. This approach offers benefits such as lower storage requirements and the ability to apply techniques from natural language processing. In this paper, we propose a new protocol that utilizes discretized token sequences in ASR tasks, which includes de-duplication and sub-word modeling to enhance the input sequence. It reduces computational cost by decreasing the length of the sequence. Our experiments on the LibriSpeech dataset demonstrate that our proposed protocol performs competitively with conventional ASR systems using continuous input features, while reducing computational and storage costs.',\n",
       " 'Author: Shinji Watanabe Title: Integration of Frame- and Label-synchronous Beam Search for Streaming Encoder-decoder Speech Recognition Publication year: 2023 Coauthors: E. Tsunoo, Hayato Futami, Yosuke Kashiwagi, Siddhant Arora, Shinji Watanabe Abstract: Although frame-based models, such as CTC and transducers, have an affinity for streaming automatic speech recognition, their decoding uses no future knowledge, which could lead to incorrect pruning. Conversely, label-based attention encoder-decoder mitigates this issue using soft attention to the input, while it tends to overestimate labels biased towards its training domain, unlike CTC. We exploit these complementary attributes and propose to integrate the frame- and label-synchronous (F-/L-Sync) decoding alternately performed within a single beam-search scheme. F-Sync decoding leads the decoding for block-wise processing, while L-Sync decoding provides the prioritized hypotheses using look-ahead future frames within a block. We maintain the hypotheses from both decoding methods to perform effective pruning. Experiments demonstrate that the proposed search algorithm achieves lower error rates compared to the other search methods, while being robust against out-of-domain situations.',\n",
       " 'Author: Shinji Watanabe Title: E-Branchformer-Based E2E SLU Toward Stop on-Device Challenge Publication year: 2023 Coauthors: Yosuke Kashiwagi, Siddhant Arora, Hayato Futami, Jessica Huynh, Shih-Lun Wu, Yifan Peng, Brian Yan, E. Tsunoo, Shinji Watanabe Abstract: In this paper, we report our team’s study on track 2 of the Spoken Language Understanding Grand Challenge, which is a component of the ICASSP Signal Processing Grand Challenge 2023. The task is intended for on-device processing and involves estimating semantic parse labels from speech using a model with 15 million parameters. We use E2E E-Branchformer-based spoken language understanding model, which is more parameter controllable than cascade models, and reduced the parameter size through sequential distillation and tensor decomposition techniques. On the STOP dataset, we achieved an exact match accuracy of 70.9% under the tight constraint of 15 million parameters.',\n",
       " 'Author: Shinji Watanabe Title: Incremental Blockwise Beam Search for Simultaneous Speech Translation with Controllable Quality-Latency Tradeoff Publication year: 2023 Coauthors: Peter Polák, Brian Yan, Shinji Watanabe, A. Waibel, Ondrej Bojar Abstract: Blockwise self-attentional encoder models have recently emerged as one promising end-to-end approach to simultaneous speech translation. These models employ a blockwise beam search with hypothesis reliability scoring to determine when to wait for more input speech before translating further. However, this method maintains multiple hypotheses until the entire speech input is consumed -- this scheme cannot directly show a single \\\\textit{incremental} translation to users. Further, this method lacks mechanisms for \\\\textit{controlling} the quality vs. latency tradeoff. We propose a modified incremental blockwise beam search incorporating local agreement or hold-$n$ policies for quality-latency control. We apply our framework to models trained for online or offline translation and demonstrate that both types can be effectively used in online mode. Experimental results on MuST-C show 0.6-3.6 BLEU improvement without changing latency or 0.8-1.4 s latency improvement without changing quality.',\n",
       " 'Author: Shinji Watanabe Title: Bayes Risk Transducer: Transducer with Controllable Alignment Prediction Publication year: 2023 Coauthors: Jinchuan Tian, Jianwei Yu, Hangting Chen, Brian Yan, Chao Weng, Dong Yu, Shinji Watanabe Abstract: Automatic speech recognition (ASR) based on transducers is widely used. In training, a transducer maximizes the summed posteriors of all paths. The path with the highest posterior is commonly defined as the predicted alignment between the speech and the transcription. While the vanilla transducer does not have a prior preference for any of the valid paths, this work intends to enforce the preferred paths and achieve controllable alignment prediction. Specifically, this work proposes Bayes Risk Transducer (BRT), which uses a Bayes risk function to set lower risk values to the preferred paths so that the predicted alignment is more likely to satisfy specific desired properties. We further demonstrate that these predicted alignments with intentionally designed properties can provide practical advantages over the vanilla transducer. Experimentally, the proposed BRT saves inference cost by up to 46% for non-streaming ASR and reduces overall system latency by 41% for streaming ASR.',\n",
       " 'Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe ',\n",
       " \"Author: Shinji Watanabe Title: ESPnet-SPK: full pipeline speaker embedding toolkit with reproducible recipes, self-supervised front-ends, and off-the-shelf models Publication year: 2024 Coauthors: Jee-weon Jung, Wangyou Zhang, Jiatong Shi, Zakaria Aldeneh, Takuya Higuchi, B. Theobald, A. H. Abdelaziz, Shinji Watanabe Abstract: This paper introduces ESPnet-SPK, a toolkit designed with several objectives for training speaker embedding extractors. First, we provide an open-source platform for researchers in the speaker recognition community to effortlessly build models. We provide several models, ranging from x-vector to recent SKA-TDNN. Through the modularized architecture design, variants can be developed easily. We also aspire to bridge developed models with other domains, facilitating the broad research community to effortlessly incorporate state-of-the-art embedding extractors. Pre-trained embedding extractors can be accessed in an off-the-shelf manner and we demonstrate the toolkit's versatility by showcasing its integration with two tasks. Another goal is to integrate with diverse self-supervised learning features. We release a reproducible recipe that achieves an equal error rate of 0.39% on the Vox1-O evaluation protocol using WavLM-Large with ECAPA-TDNN.\",\n",
       " 'Author: Shinji Watanabe Title: AugSumm: towards generalizable speech summarization using synthetic labels from large language model Publication year: 2024 Coauthors: Jee-weon Jung, Roshan Sharma, William Chen, Bhiksha Raj, Shinji Watanabe Abstract: Abstractive speech summarization (SSUM) aims to generate human-like summaries from speech. Given variations in information captured and phrasing, recordings can be summarized in multiple ways. Therefore, it is more reasonable to consider a probabilistic distribution of all potential summaries rather than a single summary. However, conventional SSUM models are mostly trained and evaluated with a single ground-truth (GT) human-annotated deterministic summary for every recording. Generating multiple human references would be ideal to better represent the distribution statistically, but is impractical because annotation is expensive. We tackle this challenge by proposing AugSumm, a method to leverage large language models (LLMs) as a proxy for human annotators to generate augmented summaries for training and evaluation. First, we explore prompting strategies to generate synthetic summaries from ChatGPT. We validate the quality of synthetic summaries using multiple metrics including human evaluation, where we find that summaries generated using AugSumm are perceived as more valid to humans. Second, we develop methods to utilize synthetic summaries in training and evaluation. Experiments on How2 demonstrate that pre-training on synthetic summaries and fine-tuning on GT summaries improves ROUGE-L by 1 point on both GT and AugSumm-based test sets. AugSumm summaries are available at https://github.com/Jungjee/AugSumm.',\n",
       " \"Author: Shinji Watanabe Title: OWSM v3.1: Better and Faster Open Whisper-Style Speech Models based on E-Branchformer Publication year: 2024 Coauthors: Yifan Peng, Jinchuan Tian, William Chen, Siddhant Arora, Brian Yan, Yui Sudo, Muhammad Shakeel, Kwanghee Choi, Jiatong Shi, Xuankai Chang, Jee-weon Jung, Shinji Watanabe Abstract: Recent studies have advocated for fully open foundation models to promote transparency and open science. As an initial step, the Open Whisper-style Speech Model (OWSM) reproduced OpenAI's Whisper using publicly available data and open-source toolkits. With the aim of reproducing Whisper, the previous OWSM v1 through v3 models were still based on Transformer, which might lead to inferior performance compared to other state-of-the-art speech encoders. In this work, we aim to improve the performance and efficiency of OWSM without extra training data. We present E-Branchformer based OWSM v3.1 models at two scales, i.e., 100M and 1B. The 1B model is the largest E-Branchformer based speech model that has been made publicly available. It outperforms the previous OWSM v3 in a vast majority of evaluation benchmarks, while demonstrating up to 25% faster inference speed. We publicly release the data preparation scripts, pre-trained models and training logs.\",\n",
       " 'Author: Shinji Watanabe Title: Can you Remove the Downstream Model for Speaker Recognition with Self-Supervised Speech Features? Publication year: 2024 Coauthors: Zakaria Aldeneh, Takuya Higuchi, Jee-weon Jung, Skyler Seto, T. Likhomanenko, Stephen Shum, A. H. Abdelaziz, Shinji Watanabe, B. Theobald Abstract: Self-supervised features are typically used in place of filter-banks in speaker verification models. However, these models were originally designed to ingest filter-banks as inputs, and thus, training them on top of self-supervised features assumes that both feature types require the same amount of learning for the task. In this work, we observe that pre-trained self-supervised speech features inherently include information required for downstream speaker verification task, and therefore, we can simplify the downstream model without sacrificing performance. To this end, we revisit the design of the downstream model for speaker verification using self-supervised features. We show that we can simplify the model to use 97.51% fewer parameters while achieving a 29.93% average improvement in performance on SUPERB. Consequently, we show that the simplified downstream model is more data efficient compared to baseline--it achieves better performance with only 60% of the training data.',\n",
       " 'Author: Shinji Watanabe Title: UniverSLU: Universal Spoken Language Understanding for Diverse Classification and Sequence Generation Tasks with a Single Network Publication year: 2023 Coauthors: Siddhant Arora, Hayato Futami, Jee-weon Jung, Yifan Peng, Roshan Sharma, Yosuke Kashiwagi, E. Tsunoo, Shinji Watanabe Abstract: Recent studies have demonstrated promising outcomes by employing large language models with multi-tasking capabilities. They utilize prompts to guide the model\\'s behavior and surpass performance of task-specific models. Motivated by this, we ask: can we build a single model that jointly perform various spoken language understanding (SLU) tasks? To address this, we utilize pre-trained automatic speech recognition (ASR) models and employ various task and dataset specifiers as discrete prompts. We demonstrate efficacy of our single multi-task learning (MTL) model\"UniverSLU\"for 12 different speech classification and sequence generation tasks across 17 datasets and 9 languages. Results show that UniverSLU achieves competitive performance and even surpasses task-specific models. We also conduct preliminary investigations into enabling human-interpretable natural phrases instead of task specifiers as discrete prompts and test the model\\'s generalization capabilities to new paraphrases.',\n",
       " 'Author: Shinji Watanabe Title: One model to rule them all ? Towards End-to-End Joint Speaker Diarization and Speech Recognition Publication year: 2023 Coauthors: Samuele Cornell, Jee-weon Jung, Shinji Watanabe, S. Squartini Abstract: This paper presents a novel framework for joint speaker diarization (SD) and automatic speech recognition (ASR), named SLIDAR (sliding-window diarization-augmented recognition). SLIDAR can process arbitrary length inputs and can handle any number of speakers, effectively solving ``who spoke what, when\\'\\' concurrently. SLIDAR leverages a sliding window approach and consists of an end-to-end diarization-augmented speech transcription (E2E DAST) model which provides, locally, for each window: transcripts, diarization and speaker embeddings. The E2E DAST model is based on an encoder-decoder architecture and leverages recent techniques such as serialized output training and ``Whisper-style\"prompting. The local outputs are then combined to get the final SD+ASR result by clustering the speaker embeddings to get global speaker identities. Experiments performed on monaural recordings from the AMI corpus confirm the effectiveness of the method in both close-talk and far-field speech scenarios.',\n",
       " 'Author: Shinji Watanabe Title: Understanding Probe Behaviors through Variational Bounds of Mutual Information Publication year: 2023 Coauthors: Kwanghee Choi, Jee-weon Jung, Shinji Watanabe Abstract: With the success of self-supervised representations, researchers seek a better understanding of the information encapsulated within a representation. Among various interpretability methods, we focus on classification-based linear probing. We aim to foster a solid understanding and provide guidelines for linear probing by constructing a novel mathematical framework leveraging information theory. First, we connect probing with the variational bounds of mutual information (MI) to relax the probe design, equating linear probing with fine-tuning. Then, we investigate empirical behaviors and practices of probing through our mathematical framework. We analyze the layer-wise performance curve being convex, which seemingly violates the data processing inequality. However, we show that the intermediate representations can have the biggest MI estimate because of the tradeoff between better separability and decreasing MI. We further suggest that the margin of linearly separable representations can be a criterion for measuring the\"goodness of representation.\"We also compare accuracy with MI as the measuring criteria. Finally, we empirically validate our claims by observing the self-supervised speech models on retaining word and phoneme information.',\n",
       " 'Author: Shinji Watanabe Title: Improving ASR Contextual Biasing with Guided Attention Publication year: 2024 Coauthors: Jiyang Tang, Kwangyoun Kim, Suwon Shon, Felix Wu, Prashant Sridhar, Shinji Watanabe Abstract: In this paper, we propose a Guided Attention (GA) auxiliary training loss, which improves the effectiveness and robustness of automatic speech recognition (ASR) contextual biasing without introducing additional parameters. A common challenge in previous literature is that the word error rate (WER) reduction brought by contextual biasing diminishes as the number of bias phrases increases. To address this challenge, we employ a GA loss as an additional training objective besides the Transducer loss. The proposed GA loss aims to teach the cross attention how to align bias phrases with text tokens or audio frames. Compared to studies with similar motivations, the proposed loss operates directly on the cross attention weights and is easier to implement. Through extensive experiments based on Conformer Transducer with Contextual Adapter, we demonstrate that the proposed method not only leads to a lower WER but also retains its effectiveness as the number of bias phrases increases. Specifically, the GA loss decreases the WER of rare vocabularies by up to 19.2% on LibriSpeech compared to the contextual biasing baseline, and up to 49.3% compared to a vanilla Transducer.',\n",
       " 'Author: Shinji Watanabe Title: Contextualized Automatic Speech Recognition with Attention-Based Bias Phrase Boosted Beam Search Publication year: 2024 Coauthors: Yui Sudo, Muhammad Shakeel, Yosuke Fukumoto, Yifan Peng, Shinji Watanabe Abstract: End-to-end (E2E) automatic speech recognition (ASR) methods exhibit remarkable performance. However, since the performance of such methods is intrinsically linked to the context present in the training data, E2E-ASR methods do not perform as desired for unseen user contexts (e.g., technical terms, personal names, and playlists). Thus, E2E-ASR methods must be easily contextualized by the user or developer. This paper proposes an attention-based contextual biasing method that can be customized using an editable phrase list (referred to as a bias list). The proposed method can be trained effectively by combining a bias phrase index loss and special tokens to detect the bias phrases in the input speech data. In addition, to improve the contextualization performance during inference further, we propose a bias phrase boosted (BPB) beam search algorithm based on the bias phrase index probability. Experimental results demonstrate that the proposed method consistently improves the word error rate and the character error rate of the target phrases in the bias list on both the Librispeech-960 (English) and our in-house (Japanese) dataset, respectively.',\n",
       " 'Author: Shinji Watanabe Title: Generative Context-aware Fine-tuning of Self-supervised Speech Models Publication year: 2023 Coauthors: Suwon Shon, Kwangyoun Kim, Prashant Sridhar, Yi-Te Hsu, Shinji Watanabe, Karen Livescu Abstract: When performing tasks like automatic speech recognition or spoken language understanding for a given utterance, access to preceding text or audio provides contextual information can improve performance. Considering the recent advances in generative large language models (LLM), we hypothesize that an LLM could generate useful context information using the preceding text. With appropriate prompts, LLM could generate a prediction of the next sentence or abstractive text like titles or topics. In this paper, we study the use of LLM-generated context information and propose an approach to distill the generated information during fine-tuning of self-supervised speech models, which we refer to as generative context-aware fine-tuning. This approach allows the fine-tuned model to make improved predictions without access to the true surrounding segments or to the LLM at inference time, while requiring only a very small additional context module. We evaluate the proposed approach using the SLUE and Libri-light benchmarks for several downstream tasks: automatic speech recognition, named entity recognition, and sentiment analysis. The results show that generative context-aware fine-tuning outperforms a context injection fine-tuning approach that accesses the ground-truth previous text, and is competitive with a generative context injection fine-tuning approach that requires the LLM at inference time.',\n",
       " 'Author: Shinji Watanabe Title: SpeechComposer: Unifying Multiple Speech Tasks with Prompt Composition Publication year: 2024 Coauthors: Yihan Wu, Soumi Maiti, Yifan Peng, Wangyou Zhang, Chenda Li, Yuyue Wang, Xihua Wang, Shinji Watanabe, Ruihua Song Abstract: Recent advancements in language models have significantly enhanced performance in multiple speech-related tasks. Existing speech language models typically utilize task-dependent prompt tokens to unify various speech tasks in a single model. However, this design omits the intrinsic connections between different speech tasks, which can potentially boost the performance of each task. In this work, we propose a novel decoder-only speech language model, SpeechComposer, that can unify common speech tasks by composing a fixed set of prompt tokens. Built upon four primary tasks -- speech synthesis, speech recognition, speech language modeling, and text language modeling -- SpeechComposer can easily extend to more speech tasks via compositions of well-designed prompt tokens, like voice conversion and speech enhancement. The unification of prompt tokens also makes it possible for knowledge sharing among different speech tasks in a more structured manner. Experimental results demonstrate that our proposed SpeechComposer can improve the performance of both primary tasks and composite tasks, showing the effectiveness of the shared prompt tokens. Remarkably, the unified decoder-only model achieves a comparable and even better performance than the baselines which are expert models designed for single tasks.',\n",
       " 'Author: Shinji Watanabe Title: SpeechBERTScore: Reference-Aware Automatic Evaluation of Speech Generation Leveraging NLP Evaluation Metrics Publication year: 2024 Coauthors: Takaaki Saeki, Soumi Maiti, Shinnosuke Takamichi, Shinji Watanabe, H. Saruwatari Abstract: While subjective assessments have been the gold standard for evaluating speech generation, there is a growing need for objective metrics that are highly correlated with human subjective judgments due to their cost efficiency. This paper proposes reference-aware automatic evaluation methods for speech generation inspired by evaluation metrics in natural language processing. The proposed SpeechBERTScore computes the BERTScore for self-supervised dense speech features of the generated and reference speech, which can have different sequential lengths. We also propose SpeechBLEU and SpeechTokenDistance, which are computed on speech discrete tokens. The evaluations on synthesized speech show that our method correlates better with human subjective ratings than mel cepstral distortion and a recent mean opinion score prediction model. Also, they are effective in noisy speech evaluation and have cross-lingual applicability.',\n",
       " 'Author: Shinji Watanabe Title: Multilingual TTS Accent Impressions for Accented ASR Publication year: 2023 Coauthors: G. Karakasidis, Nathaniel R. Robinson, Yaroslav Getman, Atieno Ogayo, Ragheb Al-Ghezi, Ananya Ayasi, Shinji Watanabe, David R. Mortensen, M. Kurimo ',\n",
       " 'Author: Shinji Watanabe Title: Domain Adaptation by Data Distribution Matching Via Submodularity For Speech Recognition Publication year: 2023 Coauthors: Yusuke Shinohara, Shinji Watanabe Abstract: We study the problem of building a domain-specific speech recognition model given some text from the target domain. One of the most popular approaches to this problem is shallow fusion, which incorporates a domain-specific language model build from the given text. However, shallow fusion significantly increases the model size and inference cost, which makes its deployment harder. In this paper, we propose domain adaptation by data distribution matching, where a subset is selected from an existing multi-domain training data to match the target-domain distribution, and a model is fine-tuned on the subset. A submodular optimization algorithm with a novel extension is employed for the subset selection. Experiments on LibriSpeech, a corpus of audiobooks, where we treat each book as a domain, show that the proposed distribution-matching approach achieves WERs equivalent with the conventional shallow-fusion approach, without any increase in the model size and inference cost.',\n",
       " 'Author: Shinji Watanabe Title: Synthetic Data Augmentation for ASR with Domain Filtering Publication year: 2023 Coauthors: Tuan Vu Ho, Shota Horiguchi, Shinji Watanabe, Paola Garcia, Takashi Sumiyoshi Abstract: Recent studies have shown that synthetic speech can effectively serve as training data for automatic speech recognition models. Text data for synthetic speech is mostly obtained from in-domain text or generated text using augmentation. However, obtaining large amounts of in-domain text data with diverse lexical contexts is difficult, especially in low-resource scenarios. This paper proposes using text from a large generic-domain source and applying a domain filtering method to choose the relevant text data. This method involves two filtering steps: 1) selecting text based on its semantic similarity to the available in-domain text and 2) diversifying the vocabulary of the selected text using a greedy-search algorithm. Experimental results show that our proposed method outperforms the conventional text augmentation approach, with the relative reduction of word-error-rate ranging from 6% to 25% on the LibriSpeech dataset and 15% on a low-resource Vietnamese dataset.',\n",
       " 'Author: Shinji Watanabe Title: TorchAudio 2.1: Advancing Speech Recognition, Self-Supervised Learning, and Audio Processing Components for Pytorch Publication year: 2023 Coauthors: Jeff Hwang, Moto Hira, Caroline Chen, Xiaohui Zhang, Zhaoheng Ni, Guangzhi Sun, Pingchuan Ma, Ruizhe Huang, Vineel Pratap, Yuekai Zhang, Anurag Kumar, Chin-Yun Yu, Chuang Zhu, Chunxi Liu, Jacob Kahn, M. Ravanelli, Peng Sun, Shinji Watanabe, Yangyang Shi, Yumeng Tao, Robin Scheibler, Samuele Cornell, Sean Kim, Stavros Petridis Abstract: TorchAudio is an open-source audio and speech processing library built for PyTorch. It aims to accelerate the research and development of audio and speech technologies by providing well-designed, easy-to-use, and performant PyTorch components. Its contributors routinely engage with users to understand their needs and fulfill them by developing impactful features. Here, we survey TorchAudio’s development principles and contents and highlight key features we include in its latest version (2.1): self-supervised learning pre-trained pipelines and training recipes, high-performance CTC decoders, speech recognition models and training recipes, advanced media I/O capabilities, and tools for performing forced alignment, multi-channel speech enhancement, and reference-less speech assessment. For a selection of these features, through empirical studies, we demonstrate their efficacy and show that they achieve competitive or state-of-the-art performance.',\n",
       " 'Author: Shinji Watanabe Title: Phoneme-aware Encoding for Prefix-tree-based Contextual ASR Publication year: 2023 Coauthors: Hayato Futami, E. Tsunoo, Yosuke Kashiwagi, Hiroaki Ogawa, Siddhant Arora, Shinji Watanabe Abstract: In speech recognition applications, it is important to recognize context-specific rare words, such as proper nouns. Tree-constrained Pointer Generator (TCPGen) has shown promise for this purpose, which efficiently biases such words with a prefix tree. While the original TCPGen relies on grapheme-based encoding, we propose extending it with phoneme-aware encoding to better recognize words of unusual pronunciations. As TCPGen handles biasing words as subword units, we propose obtaining subword-level phoneme-aware encoding by using alignment between phonemes and subwords. Furthermore, we propose injecting phoneme-level predictions from CTC into queries of TCPGen so that the model better interprets the phoneme-aware encodings. We conducted ASR experiments with TCPGen for RNN transducer. We observed that proposed phoneme-aware encoding outperformed ordinary grapheme-based encoding on both the English LibriSpeech and Japanese CSJ datasets, demonstrating the robustness of our approach across linguistically diverse languages.',\n",
       " 'Author: Shinji Watanabe Title: Semi-Autoregressive Streaming ASR With Label Context Publication year: 2023 Coauthors: Siddhant Arora, G. Saon, Shinji Watanabe, Brian Kingsbury Abstract: Non-autoregressive (NAR) modeling has gained significant interest in speech processing since these models achieve dramatically lower inference time than autoregressive (AR) models while also achieving good transcription accuracy. Since NAR automatic speech recognition (ASR) models must wait for the completion of the entire utterance before processing, some works explore streaming NAR models based on blockwise attention for low-latency applications. However, streaming NAR models significantly lag in accuracy compared to streaming AR and non-streaming NAR models. To address this, we propose a streaming\"semi-autoregressive\"ASR model that incorporates the labels emitted in previous blocks as additional context using a Language Model (LM) subnetwork. We also introduce a novel greedy decoding algorithm that addresses insertion and deletion errors near block boundaries while not significantly increasing the inference time. Experiments show that our method outperforms the existing streaming NAR model by 19% relative on Tedlium2, 16%/8% on Librispeech-100 clean/other test sets, and 19%/8% on the Switchboard(SWB)/Callhome(CH) test sets. It also reduced the accuracy gap with streaming AR and non-streaming NAR models while achieving 2.5x lower latency. We also demonstrate that our approach can effectively utilize external text data to pre-train the LM subnetwork to further improve streaming ASR accuracy.',\n",
       " 'Author: Shinji Watanabe Title: Summary on the Multimodal Information Based Speech Processing (MISP) 2022 Challenge Publication year: 2023 Coauthors: Hang Chen, Shilong Wu, Yusheng Dai, Zhe Wang, Jun Du, Chin-Hui Lee, Jingdong Chen, Shinji Watanabe, Sabato Marco Siniscalchi, O. Scharenborg, Diyuan Liu, Baocai Yin, Jia Pan, Jianqing Gao, Cong Liu Abstract: The Multimodal Information based Speech Processing (MISP) 2022 challenge aimed to enhance speech processing performance in harsh acoustic environments by leveraging additional modalities such as video or text. The challenge included two tracks: audio-visual speaker diarization (AVSD) and audio-visual diarization and recognition (AVDR). The training material was based on previous MISP 2021 recordings, but we have accurately synchronized audio and visual data. Additionally, a new evaluation set was provided. This paper gives an overview of the challenge setup, presents the results, and summarizes the effective techniques employed by the participants. We also analyze the current technical challenges and suggest directions for future research in AVSD and AVDR.',\n",
       " 'Author: Shinji Watanabe Title: Singing Voice Data Scaling-up: An Introduction to ACE-Opencpop and KiSing-v2 Publication year: 2024 Coauthors: Jiatong Shi, Yueqian Lin, Xinyi Bai, Keyi Zhang, Yuning Wu, Yuxun Tang, Yifeng Yu, Qin Jin, Shinji Watanabe Abstract: In singing voice synthesis (SVS), generating singing voices from musical scores faces challenges due to limited data availability, a constraint less common in text-to-speech (TTS). This study proposes a new approach to address this data scarcity. We utilize an existing singing voice synthesizer for data augmentation and apply precise manual tuning to reduce unnatural voice synthesis. Our development of two extensive singing voice corpora, ACE-Opencpop and KiSing-v2, facilitates large-scale, multi-singer voice synthesis. Utilizing pre-trained models derived from these corpora, we achieve notable improvements in voice quality, evident in both in-domain and out-of-domain scenarios. The corpora, pre-trained models, and their related training recipes are publicly available at Muskits-ESPnet (https://github.com/espnet/espnet).',\n",
       " 'Author: Shinji Watanabe Title: LV-CTC: Non-Autoregressive ASR With CTC and Latent Variable Models Publication year: 2023 Coauthors: Yuya Fujita, Shinji Watanabe, Xuankai Chang, Takashi Maekaku Abstract: Non-autoregressive (NAR) models for automatic speech recognition (ASR) aim to achieve high accuracy and fast inference by simplifying the autoregressive (AR) generation process of conventional models. Connectionist temporal classification (CTC) is one of the key techniques used in NAR ASR models. In this paper, we propose a new model combining CTC and a latent variable model, which is one of the state-of-the-art models in the neural machine translation research field. A new neural network architecture and formulation specialized for ASR application are introduced. In the proposed model, CTC alignment is assumed to be dependent on the latent variables that are expected to capture dependencies between tokens. Experimental results on a 100 hours subset of Librispeech corpus showed the best recognition accuracy among CTC-based NAR models. On the TED-LIUM2 corpus, the best recognition accuracy is achieved including AR E2E models with faster inference speed.',\n",
       " \"Author: Shinji Watanabe Title: EFFUSE: Efficient Self-Supervised Feature Fusion for E2E ASR in Multilingual and Low Resource Scenarios Publication year: 2023 Coauthors: Tejes Srivastava, Jiatong Shi, William Chen, Shinji Watanabe Abstract: Self-Supervised Learning (SSL) models have demonstrated exceptional performance in various speech tasks, particularly in low-resource and multilingual domains. Recent works show that fusing SSL models could achieve superior performance compared to using one SSL model. However, fusion models have increased model parameter size, leading to longer inference times. In this paper, we propose a novel approach of predicting other SSL models' features from a single SSL model, resulting in a light-weight framework with competitive performance. Our experiments show that SSL feature prediction models outperform individual SSL models in multilingual speech recognition tasks. The leading prediction model achieves an average SUPERB score increase of 135.4 in ML-SUPERB benchmarks. Moreover, our proposed framework offers an efficient solution, as it reduces the resulting model parameter size and inference times compared to previous fusion models.\",\n",
       " \"Author: Shinji Watanabe Title: HuBERTopic: Enhancing Semantic Representation of HuBERT through Self-supervision Utilizing Topic Model Publication year: 2023 Coauthors: Takashi Maekaku, Jiatong Shi, Xuankai Chang, Yuya Fujita, Shinji Watanabe Abstract: Recently, the usefulness of self-supervised representation learning (SSRL) methods has been confirmed in various downstream tasks. Many of these models, as exemplified by HuBERT and WavLM, use pseudo-labels generated from spectral features or the model's own representation features. From previous studies, it is known that the pseudo-labels contain semantic information. However, the masked prediction task, the learning criterion of HuBERT, focuses on local contextual information and may not make effective use of global semantic information such as speaker, theme of speech, and so on. In this paper, we propose a new approach to enrich the semantic representation of HuBERT. We apply topic model to pseudo-labels to generate a topic label for each utterance. An auxiliary topic classification task is added to HuBERT by using topic labels as teachers. This allows additional global semantic information to be incorporated in an unsupervised manner. Experimental results demonstrate that our method achieves comparable or better performance than the baseline in most tasks, including automatic speech recognition and five out of the eight SUPERB tasks. Moreover, we find that topic labels include various information about utterance, such as gender, speaker, and its theme. This highlights the effectiveness of our approach in capturing multifaceted semantic nuances.\",\n",
       " 'Author: Shinji Watanabe Title: Findings of the 2023 ML-Superb Challenge: Pre-Training And Evaluation Over More Languages And Beyond Publication year: 2023 Coauthors: Jiatong Shi, William Chen, Dan Berrebbi, Hsiu-Hsuan Wang, Wei-Ping Huang, En-Pei Hu, Ho-Lam Chuang, Xuankai Chang, Yuxun Tang, Shang-Wen Li, Abdelrahman Mohamed, Hung-yi Lee, Shinji Watanabe Abstract: The 2023 Multilingual Speech Universal Performance Benchmark (ML-SUPERB) Challenge expands upon the acclaimed SUPERB framework, emphasizing self-supervised models in multilingual speech recognition and language identification. The challenge comprises a research track focused on applying ML-SUPERB to specific multilingual subjects, a Challenge Track for model submissions, and a New Language Track where language resource researchers can contribute and evaluate their low-resource language data in the context of the latest progress in multilingual speech recognition. The challenge garnered 12 model submissions and 54 language corpora, resulting in a comprehensive benchmark encompassing 154 languages. The findings indicate that merely scaling models is not the definitive solution for multilingual speech tasks, and a variety of speech/voice types present significant challenges in multilingual speech processing.',\n",
       " 'Author: Shinji Watanabe Title: Boosting Unknown-number Speaker Separation with Transformer Decoder-based Attractor Publication year: 2024 Coauthors: Younglo Lee, Shukjae Choi, Byeonghak Kim, Zhong-Qiu Wang, Shinji Watanabe Abstract: We propose a novel speech separation model designed to separate mixtures with an unknown number of speakers. The proposed model stacks 1) a dual-path processing block that can model spectro-temporal patterns, 2) a transformer decoder-based attractor (TDA) calculation module that can deal with an unknown number of speakers, and 3) triple-path processing blocks that can model inter-speaker relations. Given a fixed, small set of learned speaker queries and the mixture embedding produced by the dual-path blocks, TDA infers the relations of these queries and generates an attractor vector for each speaker. The estimated attractors are then combined with the mixture embedding by feature-wise linear modulation conditioning, creating a speaker dimension. The mixture embedding, conditioned with speaker information produced by TDA, is fed to the final triple-path blocks, which augment the dual-path blocks with an additional pathway dedicated to inter-speaker processing. The proposed approach outperforms the previous best reported in the literature, achieving 24.0 and 23.7 dB SI-SDR improvement (SI-SDRi) on WSJ0-2 and 3mix respectively, with a single model trained to separate 2- and 3-speaker mixtures. The proposed model also exhibits strong performance and generalizability at counting sources and separating mixtures with up to 5 speakers.',\n",
       " 'Author: Shinji Watanabe Title: Improving Design of Input Condition Invariant Speech Enhancement Publication year: 2024 Coauthors: Wangyou Zhang, Jee-weon Jung, Shinji Watanabe, Yanmin Qian Abstract: Building a single universal speech enhancement (SE) system that can handle arbitrary input is a demanded but underexplored research topic. Towards this ultimate goal, one direction is to build a single model that handles diverse audio duration, sampling frequencies, and microphone variations in noisy and reverberant scenarios, which we define here as\"input condition invariant SE\". Such a model was recently proposed showing promising performance; however, its multi-channel performance degraded severely in real conditions. In this paper we propose novel architectures to improve the input condition invariant SE model so that performance in simulated conditions remains competitive while real condition degradation is much mitigated. For this purpose, we redesign the key components that comprise such a system. First, we identify that the channel-modeling module\\'s generalization to unseen scenarios can be sub-optimal and redesign this module. We further introduce a two-stage training strategy to enhance training efficiency. Second, we propose two novel dual-path time-frequency blocks, demonstrating superior performance with fewer parameters and computational costs compared to the existing method. All proposals combined, experiments on various public datasets validate the efficacy of the proposed model, with significantly improved performance on real conditions. Recipe with full model details is released at https://github.com/espnet/espnet.',\n",
       " 'Author: Shinji Watanabe Title: Software Design and User Interface of ESPnet-SE++: Speech Enhancement for Robust Speech Processing Publication year: 2023 Coauthors: Yen-Ju Lu, Xuankai Chang, Chenda Li, Wangyou Zhang, Samuele Cornell, Zhaoheng Ni, Yoshiki Masuyama, Brian Yan, Robin Scheibler, Zhong-Qiu Wang, Yu Tsao, Yanmin Qian, Shinji Watanabe ',\n",
       " 'Author: Shinji Watanabe Title: A Single Speech Enhancement Model Unifying Dereverberation, Denoising, Speaker Counting, Separation, And Extraction Publication year: 2023 Coauthors: Kohei Saijo, Wangyou Zhang, Zhong-Qiu Wang, Shinji Watanabe, Tetsunori Kobayashi, Tetsuji Ogawa Abstract: We propose a multi-task universal speech enhancement (MUSE) model that can perform five speech enhancement (SE) tasks: dereverberation, denoising, speech separation (SS), target speaker extraction (TSE), and speaker counting. This is achieved by integrating two modules into an SE model: 1) an internal separation module that does both speaker counting and separation; and 2) a TSE module that extracts the target speech from the internal separation outputs using target speaker cues. The model is trained to perform TSE if the target speaker cue is given and SS otherwise. By training the model to remove noise and reverberation, we allow the model to tackle the five tasks mentioned above with a single model, which has not been accomplished yet. Evaluation results demonstrate that the proposed MUSE model can successfully handle multiple tasks with a single model.',\n",
       " 'Author: Shinji Watanabe Title: Summarize While Translating: Universal Model With Parallel Decoding for Summarization and Translation Publication year: 2023 Coauthors: Takatomo Kano, A. Ogawa, Marc Delcroix, Kohei Matsuura, Takanori Ashihara, William Chen, Shinji Watanabe Abstract: Recently, multi-decoder and universal models have attracted increased interest in speech and language processing as they allow learning common representations across tasks. These models learn a common representation by sharing a part of or all network parameters. Moreover, such a universal model can handle tasks unseen during training (zero-shot tasks). However, these models do not fully exploit inter-dependencies between tasks during decoding since they usually perform decoding for each task independently. In this paper, we propose to address this issue by extending the universal model to perform multi-task parallel decoding with a cross-attention module between decoders to capture task inter-dependencies explicitly. We also introduce a novel multi-stream beam search algorithm to allow such parallel decoding. We test our proposed model on multi-lingual (English and Portuguese) text/speech translation and summarization, confirming its potential, especially in zero-shot tasks.',\n",
       " 'Author: Shinji Watanabe Title: Yodas: Youtube-Oriented Dataset for Audio and Speech Publication year: 2023 Coauthors: Xinjian Li, Shinnosuke Takamichi, Takaaki Saeki, William Chen, Sayaka Shiota, Shinji Watanabe Abstract: In this study, we introduce YODAS (YouTube-Oriented Dataset for Audio and Speech), a large-scale, multilingual dataset comprising currently over 500k hours of speech data in more than 100 languages, sourced from both labeled and unlabeled YouTube speech datasets. The labeled subsets, including manual or automatic subtitles, facilitate supervised model training. Conversely, the unlabeled subsets are apt for self-supervised learning applications. YODAS is distinctive as the first publicly available dataset of its scale, and it will be distributed under a Creative Commons license. We introduce the collection methodology utilized for YODAS, which contributes to the large-scale speech dataset construction. Subsequently, we provide a comprehensive analysis of speech, text contained within the dataset. Finally, we describe the speech recognition baselines over the top-15 languages.',\n",
       " 'Author: Shinji Watanabe Title: Espnet-Summ: Introducing a Novel Large Dataset, Toolkit, and a Cross-Corpora Evaluation of Speech Summarization Systems Publication year: 2023 Coauthors: Roshan Sharma, William Chen, Takatomo Kano, Ruchira Sharma, Siddhant Arora, Shinji Watanabe, A. Ogawa, Marc Delcroix, Rita Singh, Bhiksha Raj Abstract: Speech summarization has garnered significant interest and progressed rapidly over the past few years. In particular, end-to-end models have recently emerged as a competitive alternative to cascade systems for abstractive video summarization. This paper aims to establish progress in this rapidly evolving research field, by introducing ESPNet-SUMM, a new open-source toolkit that facilitates a comprehensive comparison of end-to-end and cascade speech summarization models on 4 different speech summarization tasks spanning diverse applications. Experiments demonstrate that end-to-end models perform better for larger corpora with shorter inputs. This work also introduces Interview, the largest public open-domain multiparty interview corpus with $4400 \\\\mathrm{~h}$ of conversations between radio hosts and guests. Finally, this work explores the use of multiple datasets to improve end-to-end summarization, and experiments demonstrate the benefit of multi-style training over fine-tuning. 1',\n",
       " 'Author: S. Welleck Title: Self-Refine: Iterative Refinement with Self-Feedback Publication year: 2023 Coauthors: Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, S. Welleck, Bodhisattwa Prasad Majumder, Shashank Gupta, A. Yazdanbakhsh, Peter Clark Abstract: Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ~20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach.',\n",
       " 'Author: S. Welleck Title: D RAFT , S KETCH , AND P ROVE : G UIDING F ORMAL T HEOREM P ROVERS WITH I NFORMAL P ROOFS Publication year: 2023 Coauthors: Albert Qiaochu Jiang, S. Welleck, J. Zhou, Wen-Ding Li, Jiacheng Liu, M. Jamnik, Timothée Lacroix, Guillaume Lample, Yuhuai Wu Abstract: The formalization of existing mathematical proofs is a notoriously difficult process. Despite decades of research on automation and proof assistants, writing formal proofs remains arduous and only accessible to a few experts. While previous studies to automate formalization focused on powerful search algorithms, no attempts were made to take advantage of available informal proofs. In this work, we introduce Draft, Sketch, and Prove (DSP), a method that maps informal proofs to formal proof sketches, and uses the sketches to guide an automated prover by directing its search to easier sub-problems. We investigate two relevant setups where informal proofs are either written by humans or generated by a language model. Our experiments and ablation studies show that large language models are able to produce wellstructured formal sketches that follow the same reasoning steps as the informal proofs. Guiding an automated prover with these sketches enhances its performance from 20.9% to 39.3% on a collection of mathematical competition problems. Figure 1: Draft, Sketch, and Prove. Starting with an informal statement, our framework yields a formal proof through a three-stage process: drafting informal proofs, mapping them into formal sketches, and proving the remaining conjectures. Concretely, an informal statement is a mathematical problem described in a mixture of natural and mathematical languages (e.g., formulae in LTEX). Then, we use a large language model to autoformalize each informal proof into a formal sketch, which is a skeleton of the formal proof with open conjectures left unproven (indicated by the <proof> blocks). The formal sketch mirrors the structure of the informal proof. Finally, the open conjectures/gaps inside each formal sketch are proved by an off-the-shelf prover. †Equal contributions as leading authors. Correspondence to: qj213@cam.ac.uk. ‡Equal contributions as senior authors.',\n",
       " \"Author: S. Welleck Title: Faith and Fate: Limits of Transformers on Compositionality Publication year: 2023 Coauthors: Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jian, Bill Yuchen Lin, Peter West, Chandra Bhagavatula, Ronan Le Bras, Jena D. Hwang, Soumya Sanyal, S. Welleck, Xiang Ren, Allyson Ettinger, Zaïd Harchaoui, Yejin Choi Abstract: Transformer large language models (LLMs) have sparked admiration for their exceptional performance on tasks that demand intricate multi-step reasoning. Yet, these models simultaneously show failures on surprisingly trivial problems. This begs the question: Are these errors incidental, or do they signal more substantial limitations? In an attempt to demystify transformer LLMs, we investigate the limits of these models across three representative compositional tasks -- multi-digit multiplication, logic grid puzzles, and a classic dynamic programming problem. These tasks require breaking problems down into sub-steps and synthesizing these steps into a precise answer. We formulate compositional tasks as computation graphs to systematically quantify the level of complexity, and break down reasoning steps into intermediate sub-procedures. Our empirical findings suggest that transformer LLMs solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, without necessarily developing systematic problem-solving skills. To round off our empirical study, we provide theoretical arguments on abstract multi-step reasoning problems that highlight how autoregressive generations' performance can rapidly decay with\\\\,increased\\\\,task\\\\,complexity.\",\n",
       " 'Author: S. Welleck Title: STEER: Unified Style Transfer with Expert Reinforcement Publication year: 2023 Coauthors: Skyler Hallinan, Faeze Brahman, Ximing Lu, Jaehun Jung, S. Welleck, Yejin Choi Abstract: While text style transfer has many applications across natural language processing, the core premise of transferring from a single source style is unrealistic in a real-world setting. In this work, we focus on arbitrary style transfer: rewriting a text from an arbitrary, unknown style to a target style. We propose STEER: Unified Style Transfer with Expert Reinforcement, a unified frame-work developed to overcome the challenge of limited parallel data for style transfer. STEER involves automatically generating a corpus of style-transfer pairs using a product of experts during decoding. The generated offline data is then used to pre-train an initial policy before switching to online, off-policy reinforcement learning for further improvements via fine-grained reward signals. STEER is unified and can transfer to multiple target styles from an arbitrary, unknown source style, making it particularly flexible and efficient. Experimental results on a challenging dataset with text from a diverse set of styles demonstrate state-of-the-art results compared to competitive baselines. Remarkably, STEER outperforms the 175B parameter instruction-tuned GPT-3 on overall style transfer quality, despite being 226 times smaller in size. We also show STEER is robust, maintaining its style transfer capabilities on out-of-domain data, and surpassing nearly all baselines across various styles. The success of our method highlights the potential of RL algorithms when augmented with controllable decoding to overcome the challenge of limited data supervision.',\n",
       " 'Author: S. Welleck Title: Llemma: An Open Language Model For Mathematics Publication year: 2023 Coauthors: Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q. Jiang, Jia Deng, Stella Biderman, S. Welleck Abstract: We present Llemma, a large language model for mathematics. We continue pretraining Code Llama on the Proof-Pile-2, a mixture of scientific papers, web data containing mathematics, and mathematical code, yielding Llemma. On the MATH benchmark Llemma outperforms all known open base models, as well as the unreleased Minerva model suite on an equi-parameter basis. Moreover, Llemma is capable of tool use and formal theorem proving without any further finetuning. We openly release all artifacts, including 7 billion and 34 billion parameter models, the Proof-Pile-2, and code to replicate our experiments.',\n",
       " 'Author: S. Welleck Title: Inference-Time Policy Adapters (IPA): Tailoring Extreme-Scale LMs without Fine-tuning Publication year: 2023 Coauthors: Ximing Lu, Faeze Brahman, Peter West, Jaehun Jang, Khyathi Raghavi Chandu, Abhilasha Ravichander, Lianhui Qin, Prithviraj Ammanabrolu, Liwei Jiang, Sahana Ramnath, Nouha Dziri, Jillian R. Fisher, Bill Yuchen Lin, Skyler Hallinan, Xiang Ren, S. Welleck, Yejin Choi Abstract: While extreme-scale language models have demonstrated exceptional performance on a variety of language tasks, the degree of control over these language models through pure prompting can often be limited. Directly fine-tuning such language models can be effective for tailoring them, but it can be either extremely costly (e.g., GPT-3) or not even feasible for the broader community (e.g., GPT-4). We propose Inference-time Policy Adapters (IPA), which efficiently tailors a language model such as GPT-3 without fine-tuning it. IPA guides a large base model during decoding time through a lightweight policy adapter trained to optimize an arbitrary user objective with reinforcement learning. On five challenging text generation tasks, such as toxicity reduction and lexically constrained generation, IPA consistently brings significant improvements over off-the-shelf language models. It outperforms competitive baseline methods, sometimes even including expensive fine-tuning. In particular, tailoring GPT-2 with IPA can outperform GPT-3, while tailoring GPT-3 with IPA brings a major performance boost over GPT-3 (and sometimes even over GPT-4). Our promising results highlight the potential of IPA as a lightweight alternative to tailoring extreme-scale language models.',\n",
       " \"Author: Sean Welleck Title: LLMSTEP: LLM proofstep suggestions in Lean Publication year: 2023 Coauthors: Sean Welleck, Rahul Saha Abstract: We present LLMSTEP, a tool for integrating a language model into the Lean proof assistant. LLMSTEP is a Lean 4 tactic that sends a user's proof state to a server hosting a language model. The language model generates suggestions, which are checked in Lean and displayed to a user in their development environment. We provide a baseline language model, along with code for fine-tuning and evaluation to support further development. We provide server implementations that run on CPU, a CUDA GPU, or a Google Colab notebook, as a step towards fast, effective language model suggestions for any user.\",\n",
       " 'Author: Eric P. Xing Title: TrustLLM: Trustworthiness in Large Language Models Publication year: 2024 Coauthors: Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, Zheng Liu, Yixin Liu, Yijue Wang, Zhikun Zhang, B. Kailkhura, Caiming Xiong, Chaowei Xiao, Chun-Yan Li, Eric P. Xing, Furong Huang, Haodong Liu, Heng Ji, Hongyi Wang, Huan Zhang, Huaxiu Yao, M. Kellis, M. Zitnik, Meng Jiang, Mohit Bansal, James Zou, Jian Pei, Jian Liu, Jianfeng Gao, Jiawei Han, Jieyu Zhao, Jiliang Tang, Jindong Wang, John Mitchell, Kai Shu, Kaidi Xu, Kai-Wei Chang, Lifang He, Lifu Huang, M. Backes, Neil Zhenqiang Gong, Philip S. Yu, Pin-Yu Chen, Quanquan Gu, Ran Xu, Rex Ying, Shuiwang Ji, S. Jana, Tian-Xiang Chen, Tianming Liu, Tianying Zhou, William Wang, Xiang Li, Xiang-Yu Zhang, Xiao Wang, Xingyao Xie, Xun Chen, Xuyu Wang, Yan Liu, Yanfang Ye, Yinzhi Cao, Yue Zhao Abstract: Large language models (LLMs), exemplified by ChatGPT, have gained considerable attention for their excellent natural language processing capabilities. Nonetheless, these LLMs present many challenges, particularly in the realm of trustworthiness. Therefore, ensuring the trustworthiness of LLMs emerges as an important topic. This paper introduces TrustLLM, a comprehensive study of trustworthiness in LLMs, including principles for different dimensions of trustworthiness, established benchmark, evaluation, and analysis of trustworthiness for mainstream LLMs, and discussion of open challenges and future directions. Specifically, we first propose a set of principles for trustworthy LLMs that span eight different dimensions. Based on these principles, we further establish a benchmark across six dimensions including truthfulness, safety, fairness, robustness, privacy, and machine ethics. We then present a study evaluating 16 mainstream LLMs in TrustLLM, consisting of over 30 datasets. Our findings firstly show that in general trustworthiness and utility (i.e., functional effectiveness) are positively related. Secondly, our observations reveal that proprietary LLMs generally outperform most open-source counterparts in terms of trustworthiness, raising concerns about the potential risks of widely accessible open-source LLMs. However, a few open-source LLMs come very close to proprietary ones. Thirdly, it is important to note that some LLMs may be overly calibrated towards exhibiting trustworthiness, to the extent that they compromise their utility by mistakenly treating benign prompts as harmful and consequently not responding. Finally, we emphasize the importance of ensuring transparency not only in the models themselves but also in the technologies that underpin trustworthiness. Knowing the specific trustworthy technologies that have been employed is crucial for analyzing their effectiveness.',\n",
       " \"Author: Eric P. Xing Title: SlimPajama-DC: Understanding Data Combinations for LLM Training Publication year: 2023 Coauthors: Zhiqiang Shen, Tianhua Tao, Liqun Ma, W. Neiswanger, Zhengzhong Liu, Hongyi Wang, Bowen Tan, Joel Hestness, Natalia Vassilieva, Daria Soboleva, Eric P. Xing Abstract: This paper aims to understand the impacts of various data combinations (e.g., web text, wikipedia, github, books) on the training of large language models using SlimPajama. SlimPajama is a rigorously deduplicated, multi-source dataset, which has been refined and further deduplicated to 627B tokens from the extensive 1.2T tokens RedPajama dataset contributed by Together. We've termed our research as SlimPajama-DC, an empirical analysis designed to uncover fundamental characteristics and best practices associated with employing SlimPajama in the training of large language models. During our research with SlimPajama, two pivotal observations emerged: (1) Global deduplication vs. local deduplication. We analyze and discuss how global (across different sources of datasets) and local (within the single source of dataset) deduplications affect the performance of trained models. (2) Proportions of high-quality/highly-deduplicated multi-source datasets in the combination. To study this, we construct six configurations of SlimPajama dataset and train individual ones using 1.3B Cerebras-GPT model with Alibi and SwiGLU. Our best configuration outperforms the 1.3B model trained on RedPajama using the same number of training tokens by a significant margin. All our 1.3B models are trained on Cerebras 16$\\\\times$ CS-2 cluster with a total of 80 PFLOP/s in bf16 mixed precision. We further extend our discoveries (such as increasing data diversity is crucial after global deduplication) on a 7B model with large batch-size training. Our models and the separate SlimPajama-DC datasets are available at: https://huggingface.co/MBZUAI-LLM and https://huggingface.co/datasets/cerebras/SlimPajama-627B.\",\n",
       " 'Author: Eric P. Xing Title: LLM360: Towards Fully Transparent Open-Source LLMs Publication year: 2023 Coauthors: Zhengzhong Liu, Aurick Qiao, W. Neiswanger, Hongyi Wang, Bowen Tan, Tianhua Tao, Junbo Li, Yuqi Wang, Suqi Sun, Omkar Pangarkar, Richard Fan, Yi Gu, Victor Miller, Yonghao Zhuang, Guowei He, Haonan Li, Fajri Koto, Liping Tang, Nikhil Ranjan, Zhiqiang Shen, Xuguang Ren, Roberto Iriondo, Cun Mu, Zhiting Hu, Mark Schulze, Preslav Nakov, Timothy Baldwin, Eric P. Xing Abstract: The recent surge in open-source Large Language Models (LLMs), such as LLaMA, Falcon, and Mistral, provides diverse options for AI practitioners and researchers. However, most LLMs have only released partial artifacts, such as the final model weights or inference code, and technical reports increasingly limit their scope to high-level design choices and surface statistics. These choices hinder progress in the field by degrading transparency into the training of LLMs and forcing teams to rediscover many details in the training process. We present LLM360, an initiative to fully open-source LLMs, which advocates for all training code and data, model checkpoints, and intermediate results to be made available to the community. The goal of LLM360 is to support open and collaborative AI research by making the end-to-end LLM training process transparent and reproducible by everyone. As a first step of LLM360, we release two 7B parameter LLMs pre-trained from scratch, Amber and CrystalCoder, including their training code, data, intermediate checkpoints, and analyses (at https://www.llm360.ai). We are committed to continually pushing the boundaries of LLMs through this open-source effort. More large-scale and stronger models are underway and will be released in the future.',\n",
       " 'Author: Eric P. Xing Title: Fusing Models with Complementary Expertise Publication year: 2023 Coauthors: Hongyi Wang, Felipe Maia Polo, Yuekai Sun, Souvik Kundu, Eric P. Xing, M. Yurochkin Abstract: Training AI models that generalize across tasks and domains has long been among the open problems driving AI research. The emergence of Foundation Models made it easier to obtain expert models for a given task, but the heterogeneity of data that may be encountered at test time often means that any single expert is insufficient. We consider the Fusion of Experts (FoE) problem of fusing outputs of expert models with complementary knowledge of the data distribution and formulate it as an instance of supervised learning. Our method is applicable to both discriminative and generative tasks and leads to significant performance improvements in image and text classification, text summarization, multiple-choice QA, and automatic evaluation of generated text. We also extend our method to the\"frugal\"setting where it is desired to reduce the number of expert model evaluations at test time.',\n",
       " 'Author: Eric P. Xing Title: Making Scalable Meta Learning Practical Publication year: 2023 Coauthors: Sang Keun Choe, Sanket Vaibhav Mehta, Hwijeen Ahn, W. Neiswanger, Pengtao Xie, Emma Strubell, Eric P. Xing Abstract: Despite its flexibility to learn diverse inductive biases in machine learning programs, meta learning (i.e., learning to learn) has long been recognized to suffer from poor scalability due to its tremendous compute/memory costs, training instability, and a lack of efficient distributed training support. In this work, we focus on making scalable meta learning practical by introducing SAMA, which combines advances in both implicit differentiation algorithms and systems. Specifically, SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients. Evaluated on multiple large-scale meta learning benchmarks, SAMA showcases up to 1.7/4.8x increase in throughput and 2.0/3.8x decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms. Furthermore, we show that SAMA-based data optimization leads to consistent improvements in text classification accuracy with BERT and RoBERTa large language models, and achieves state-of-the-art results in both small- and large-scale data pruning on image classification tasks, demonstrating the practical applicability of scalable meta learning across language and vision domains.',\n",
       " 'Author: Eric P. Xing Title: PromptAgent: Strategic Planning with Language Models Enables Expert-level Prompt Optimization Publication year: 2023 Coauthors: Xinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Haotian Luo, Jiayou Zhang, Nebojsa Jojic, Eric P. Xing, Zhiting Hu Abstract: Highly effective, task-specific prompts are often heavily engineered by experts to integrate detailed instructions and domain insights based on a deep understanding of both instincts of large language models (LLMs) and the intricacies of the target task. However, automating the generation of such expert-level prompts remains elusive. Existing prompt optimization methods tend to overlook the depth of domain knowledge and struggle to efficiently explore the vast space of expert-level prompts. Addressing this, we present PromptAgent, an optimization method that autonomously crafts prompts equivalent in quality to those handcrafted by experts. At its core, PromptAgent views prompt optimization as a strategic planning problem and employs a principled planning algorithm, rooted in Monte Carlo tree search, to strategically navigate the expert-level prompt space. Inspired by human-like trial-and-error exploration, PromptAgent induces precise expert-level insights and in-depth instructions by reflecting on model errors and generating constructive error feedback. Such a novel framework allows the agent to iteratively examine intermediate prompts (states), refine them based on error feedbacks (actions), simulate future rewards, and search for high-reward paths leading to expert prompts. We apply PromptAgent to 12 tasks spanning three practical domains: BIG-Bench Hard (BBH), as well as domain-specific and general NLP tasks, showing it significantly outperforms strong Chain-of-Thought and recent prompt optimization baselines. Extensive analyses emphasize its capability to craft expert-level, detailed, and domain-insightful prompts with great efficiency and generalizability.',\n",
       " 'Author: Eric P. Xing Title: Cappy: Outperforming and Boosting Large Multi-Task LMs with a Small Scorer Publication year: 2023 Coauthors: Bowen Tan, Yun Zhu, Lijuan Liu, Eric P. Xing, Zhiting Hu, Jindong Chen Abstract: Large language models (LLMs) such as T0, FLAN, and OPT-IML, excel in multi-tasking under a unified instruction-following paradigm, where they also exhibit remarkable generalization abilities to unseen tasks. Despite their impressive performance, these LLMs, with sizes ranging from several billion to hundreds of billions of parameters, demand substantial computational resources, making their training and inference expensive and inefficient. Furthermore, adapting these models to downstream applications, particularly complex tasks, is often unfeasible due to the extensive hardware requirements for finetuning, even when utilizing parameter-efficient approaches such as prompt tuning. Additionally, the most powerful multi-task LLMs, such as OPT-IML-175B and FLAN-PaLM-540B, are not publicly accessible, severely limiting their customization potential. To address these challenges, we introduce a pretrained small scorer, Cappy, designed to enhance the performance and efficiency of multi-task LLMs. With merely 360 million parameters, Cappy functions either independently on classification tasks or serve as an auxiliary component for LLMs, boosting their performance. Moreover, Cappy enables efficiently integrating downstream supervision without requiring LLM finetuning nor the access to their parameters. Our experiments demonstrate that, when working independently on 11 language understanding tasks from PromptSource, Cappy outperforms LLMs that are several orders of magnitude larger. Besides, on 45 complex tasks from BIG-Bench, Cappy boosts the performance of the advanced multi-task LLM, FLAN-T5, by a large margin. Furthermore, Cappy is flexible to cooperate with other LLM adaptations, including finetuning and in-context learning, offering additional performance enhancement.',\n",
       " \"Author: Eric P. Xing Title: RedCoast: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU/TPUs Publication year: 2023 Coauthors: Bowen Tan, Yun Zhu, Lijuan Liu, Hongyi Wang, Yonghao Zhuang, Jindong Chen, Eric P. Xing, Zhiting Hu Abstract: The recent progress of AI can be largely attributed to large language models (LLMs). However, their escalating memory requirements introduce challenges for machine learning (ML) researchers and engineers. Addressing this requires developers to partition a large model to distribute it across multiple GPUs or TPUs. This necessitates considerable coding and intricate configuration efforts with existing model parallel tools, such as Megatron-LM, DeepSpeed, and Alpa. These tools require users' expertise in machine learning systems (MLSys), creating a bottleneck in LLM development, particularly for developers without MLSys background. In this work, we present RedCoast(Redco), a lightweight and user-friendly tool crafted to automate distributed training and inference for LLMs, as well as to simplify ML pipeline development. The design of Redco emphasizes two key aspects. Firstly, to automate model parallism, our study identifies two straightforward rules to generate tensor parallel strategies for any given LLM. Integrating these rules into Redco facilitates effortless distributed LLM training and inference, eliminating the need of additional coding or complex configurations. We demonstrate the effectiveness by applying Redco on a set of LLM architectures, such as GPT-J, LLaMA, T5, and OPT, up to the size of 66B. Secondly, we propose a mechanism that allows for the customization of diverse ML pipelines through the definition of merely three functions, avoiding redundant and formulaic code like multi-host related processing. This mechanism proves adaptable across a spectrum of ML algorithms, from foundational language modeling to complex algorithms like meta-learning and reinforcement learning. Consequently, Redco implementations exhibit much fewer code lines compared to their official counterparts.\",\n",
       " 'Author: Eric P. Xing Title: Neural-Symbolic Interaction and Co-Evolving Publication year: 2023 Coauthors: Bowen Tan, Shibo Hao, Eric P. Xing, Zhiting Hu ',\n",
       " 'Author: Eric P. Xing Title: Redco: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU/TPUs Publication year: 2023 Coauthors: Bowen Tan, Yun Zhu, Lijuan Liu, Hongyi Wang, Yonghao Zhuang, Jindong Chen, Eric P. Xing, Zhiting Hu Abstract: .',\n",
       " 'Author: Eric P. Xing Title: Learning to Prompt Segment Anything Models Publication year: 2024 Coauthors: Jiaxing Huang, Kai Jiang, Jingyi Zhang, Han Qiu, Lewei Lu, Shijian Lu, Eric P. Xing Abstract: Segment Anything Models (SAMs) like SEEM and SAM have demonstrated great potential in learning to segment anything. The core design of SAMs lies with Promptable Segmentation, which takes a handcrafted prompt as input and returns the expected segmentation mask. SAMs work with two types of prompts including spatial prompts (e.g., points) and semantic prompts (e.g., texts), which work together to prompt SAMs to segment anything on downstream datasets. Despite the important role of prompts, how to acquire suitable prompts for SAMs is largely under-explored. In this work, we examine the architecture of SAMs and identify two challenges for learning effective prompts for SAMs. To this end, we propose spatial-semantic prompt learning (SSPrompt) that learns effective semantic and spatial prompts for better SAMs. Specifically, SSPrompt introduces spatial prompt learning and semantic prompt learning, which optimize spatial prompts and semantic prompts directly over the embedding space and selectively leverage the knowledge encoded in pre-trained prompt encoders. Extensive experiments show that SSPrompt achieves superior image segmentation performance consistently across multiple widely adopted datasets.',\n",
       " 'Author: Eric P. Xing Title: 3D Open-vocabulary Segmentation with Foundation Models Publication year: 2023 Coauthors: Kunhao Liu, Fangneng Zhan, Jiahui Zhang, Muyu Xu, Yingchen Yu, Abdulmotaleb El-Saddik, Christian Theobalt, Eric P. Xing, Shijian Lu Abstract: Open-vocabulary segmentation of 3D scenes is a fundamental function of human perception and thus a crucial objective in computer vision research. However, this task is heavily impeded by the lack of large-scale and diverse 3D open-vocabulary segmentation datasets for training robust and generalizable models. Distilling knowledge from pre-trained 2D open-vocabulary segmentation models helps but it compromises the open-vocabulary feature significantly as the 2D models are mostly finetuned with close-vocabulary datasets. We tackle the challenges in 3D open-vocabulary segmentation by exploiting the open-vocabulary multimodal knowledge and object reasoning capability of pre-trained foundation models CLIP and DINO, without necessitating any fine-tuning. Specifically, we distill open-vocabulary visual and textual knowledge from CLIP into a neural radiance field (NeRF) which effectively lifts 2D features into view-consistent 3D segmentation. Furthermore, we introduce the Relevancy-Distribution Alignment loss and Feature-Distribution Alignment loss to respectively mitigate the ambiguities of CLIP features and distill precise object boundaries from DINO features, eliminating the need for segmentation annotations during training. Extensive experiments show that our method even outperforms fully supervised models trained with segmentation annotations, suggesting that 3D open-vocabulary segmentation can be effectively learned from 2D images and text-image pairs.',\n",
       " 'Author: Eric P. Xing Title: Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale From A New Perspective Publication year: 2023 Coauthors: Zeyuan Yin, Eric P. Xing, Zhiqiang Shen Abstract: We present a new dataset condensation framework termed Squeeze, Recover and Relabel (SRe$^2$L) that decouples the bilevel optimization of model and synthetic data during training, to handle varying scales of datasets, model architectures and image resolutions for efficient dataset condensation. The proposed method demonstrates flexibility across diverse dataset scales and exhibits multiple advantages in terms of arbitrary resolutions of synthesized images, low training cost and memory consumption with high-resolution synthesis, and the ability to scale up to arbitrary evaluation network architectures. Extensive experiments are conducted on Tiny-ImageNet and full ImageNet-1K datasets. Under 50 IPC, our approach achieves the highest 42.5% and 60.8% validation accuracy on Tiny-ImageNet and ImageNet-1K, outperforming all previous state-of-the-art methods by margins of 14.5% and 32.9%, respectively. Our approach also surpasses MTT in terms of speed by approximately 52$\\\\times$ (ConvNet-4) and 16$\\\\times$ (ResNet-18) faster with less memory consumption of 11.6$\\\\times$ and 6.4$\\\\times$ during data synthesis. Our code and condensed datasets of 50, 200 IPC with 4K recovery budget are available at https://github.com/VILA-Lab/SRe2L.',\n",
       " 'Author: Eric P. Xing Title: One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning Publication year: 2023 Coauthors: Arnav Chavan, Zhuang Liu, D. Gupta, Eric P. Xing, Zhiqiang Shen Abstract: We present Generalized LoRA (GLoRA), an advanced approach for universal parameter-efficient fine-tuning tasks. Enhancing Low-Rank Adaptation (LoRA), GLoRA employs a generalized prompt module to optimize pre-trained model weights and adjust intermediate activations, providing more flexibility and capability across diverse tasks and datasets. Moreover, GLoRA facilitates efficient parameter adaptation by employing a scalable, modular, layer-wise structure search that learns individual adapter of each layer. Originating from a unified mathematical formulation, GLoRA exhibits strong transfer learning, few-shot learning and domain generalization abilities, as it adapts to new tasks through not only weights but also additional dimensions like activations. Comprehensive experiments demonstrate that GLoRA outperforms all previous methods in natural, specialized, and structured vision benchmarks, achieving superior accuracy with fewer parameters and computations. The proposed method on LLaMA-1 and LLaMA-2 also show considerable enhancements compared to the original LoRA in the language domain. Furthermore, our structural re-parameterization design ensures that GLoRA incurs no extra inference cost, rendering it a practical solution for resource-limited applications. Code and models are available at: https://github.com/Arnav0400/ViT-Slim/tree/master/GLoRA.',\n",
       " 'Author: Eric P. Xing Title: Defending Against Malicious Behaviors in Federated Learning with Blockchain Publication year: 2023 Coauthors: Nanqing Dong, Zhipeng Wang, Jiahao Sun, Michael C. Kampffmeyer, Yizhe Wen, Shuoying Zhang, W. Knottenbelt, Eric P. Xing Abstract: In the era of deep learning, federated learning (FL) presents a promising approach that allows multi-institutional data owners, or clients, to collaboratively train machine learning models without compromising data privacy. However, most existing FL approaches rely on a centralized server for global model aggregation, leading to a single point of failure. This makes the system vulnerable to malicious attacks when dealing with dishonest clients. In this work, we address this problem by proposing a secure and reliable FL system based on blockchain and distributed ledger technology. Our system incorporates a peer-to-peer voting mechanism and a reward-and-slash mechanism, which are powered by on-chain smart contracts, to detect and deter malicious behaviors. Both theoretical and empirical analyses are presented to demonstrate the effectiveness of the proposed approach, showing that our framework is robust against malicious client-side behaviors.',\n",
       " 'Author: Eric P. Xing Title: Weakly Supervised 3D Open-vocabulary Segmentation Publication year: 2023 Coauthors: Kunhao Liu, Fangneng Zhan, Jiahui Zhang, Muyu Xu, Yingchen Yu, A. E. Saddik, C. Theobalt, Eric P. Xing, Shijian Lu Abstract: Open-vocabulary segmentation of 3D scenes is a fundamental function of human perception and thus a crucial objective in computer vision research. However, this task is heavily impeded by the lack of large-scale and diverse 3D open-vocabulary segmentation datasets for training robust and generalizable models. Distilling knowledge from pre-trained 2D open-vocabulary segmentation models helps but it compromises the open-vocabulary feature as the 2D models are mostly finetuned with close-vocabulary datasets. We tackle the challenges in 3D open-vocabulary segmentation by exploiting pre-trained foundation models CLIP and DINO in a weakly supervised manner. Specifically, given only the open-vocabulary text descriptions of the objects in a scene, we distill the open-vocabulary multimodal knowledge and object reasoning capability of CLIP and DINO into a neural radiance field (NeRF), which effectively lifts 2D features into view-consistent 3D segmentation. A notable aspect of our approach is that it does not require any manual segmentation annotations for either the foundation models or the distillation process. Extensive experiments show that our method even outperforms fully supervised models trained with segmentation annotations in certain scenes, suggesting that 3D open-vocabulary segmentation can be effectively learned from 2D images and text-image pairs. Code is available at \\\\url{https://github.com/Kunhao-Liu/3D-OVS}.',\n",
       " 'Author: Eric P. Xing Title: Supplementary Material for KD-DLGAN: Data Limited Image Generation via Knowledge Distillation Publication year: 2023 Coauthors: Kaiwen Cui, Yingchen Yu, Fangneng Zhan, Shengcai Liao, Shijian Lu, Eric P. Xing Abstract: We conduct experiments over multiple widely adopted datasets including: 100-shot, AFHQ, CIFAR-10, CIFAR100 and ImageNet. 100-shot: 100-shot contains three datasets each of which has 100 samples of resolution 256 × 256. The three datasets are 100-shot Obama, 100-shot Grumpy Cat and 100-shot Panda. AFHQ: AFHQ consists of face images of three types of animals including Cat, Dog and Wildlife, each of which has 5k training images. We follow DA [9] and use 160 AFHQCat images and 389 AFHQ-Dog images (at a resolution of 256 × 256) for training. CIFAR-10: CIFAR-10 contains 50k training images and 10k validation images with 10 classes. The image resolution is 32 × 32. In our experiments, three networks are trained with 100%, 20% or 10% training images, respectively, and the trained models are valuated over all the validation images. CIFAR-100: CIFAR-100 contains 50k training images and 10k validation images of 100 classes. The image resolution is 32 × 32. In our experiments, three networks are trained with 100%, 20% or 10% training images, respectively, and the trained models are evaluated over all the validation data.',\n",
       " 'Author: Eric P. Xing Title: Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming Publication year: 2023 Coauthors: Hanlin Zhang, Jiani Huang, Ziyang Li, M. Naik, Eric P. Xing Abstract: Pre-trained large language models (LMs) struggle to perform logical reasoning reliably despite advances in scale and compositionality. In this work, we tackle this challenge through the lens of symbolic programming. We propose DSR-LM, a Differentiable Symbolic Reasoning framework where pre-trained LMs govern the perception of factual knowledge, and a symbolic module performs deductive reasoning. In contrast to works that rely on hand-crafted logic rules, our differentiable symbolic reasoning framework efficiently learns weighted rules and applies semantic loss to further improve LMs. DSR-LM is scalable, interpretable, and allows easy integration of prior knowledge, thereby supporting extensive symbolic programming to robustly derive a logical conclusion. The results of our experiments suggest that DSR-LM improves the logical reasoning abilities of pre-trained language models, resulting in a significant increase in accuracy of over 20% on deductive reasoning benchmarks. Furthermore, DSR-LM outperforms a variety of competitive baselines when faced with systematic changes in sequence length.',\n",
       " 'Author: Eric P. Xing Title: Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models Publication year: 2023 Coauthors: Neha Sengupta, Sunil Kumar Sahu, Bokang Jia, Satheesh Katipomu, Haonan Li, Fajri Koto, Osama Mohammed Afzal, Samta Kamboj, O. Pandit, Rahul Pal, Lalit Pradhan, Zainul Mujahid, Massa Baali, Xudong Han, Alham Fikri Aji, Zhengzhong Liu, Andy Hock, Andrew Feldman, Jonathan Lee, A. Jackson, Preslav Nakov, Timothy Baldwin, Eric P. Xing Abstract: We introduce Jais and Jais-chat, new state-of-the-art Arabic-centric foundation and instruction-tuned open generative large language models (LLMs). The models are based on the GPT-3 decoder-only architecture and are pretrained on a mixture of Arabic and English texts, including source code in various programming languages. With 13 billion parameters, they demonstrate better knowledge and reasoning capabilities in Arabic than any existing open Arabic and multilingual models by a sizable margin, based on extensive evaluation. Moreover, the models are competitive in English compared to English-centric open models of similar size, despite being trained on much less English data. We provide a detailed description of the training, the tuning, the safety alignment, and the evaluation of the models. We release two open versions of the model -- the foundation Jais model, and an instruction-tuned Jais-chat variant -- with the aim of promoting research on Arabic LLMs. Available at https://huggingface.co/inception-mbzuai/jais-13b-chat',\n",
       " 'Author: Eric P. Xing Title: KD-DLGAN: Data Limited Image Generation via Knowledge Distillation Publication year: 2023 Coauthors: Kaiwen Cui, Yingchen Yu, Fangneng Zhan, Shengcai Liao, Shijian Lu1, Eric P. Xing Abstract: Generative Adversarial Networks (GANs) rely heavily on large-scale training data for training high-quality image generation models. With limited training data, the GAN discriminator often suffers from severe overfitting which directly leads to degraded generation especially in generation diversity. Inspired by the recent advances in knowledge distillation (KD), we propose KD-DLGAN, a knowledge-distillation based generation framework that introduces pre-trained vision-language models for training effective data-limited generation models. KD-DLGAN consists of two innovative designs. The first is aggregated generative KD that mitigates the discriminator overfitting by challenging the discriminator with harder learning tasks and distilling more generalizable knowledge from the pre-trained models. The second is correlated generative KD that improves the generation diversity by distilling and preserving the diverse image-text correlation within the pre-trained models. Extensive experiments over multiple benchmarks show that KD-DLGAN achieves superior image generation with limited training data. In addition, KD-DLGAN complements the state-of-the-art with consistent and substantial performance gains. Note that codes will be released.',\n",
       " 'Author: Eric P. Xing Title: 3D Semantic Segmentation in the Wild: Learning Generalized Models for Adverse-Condition Point Clouds Publication year: 2023 Coauthors: Aoran Xiao, Jiaxing Huang, Weihao Xuan, Ruijie Ren, Kangcheng Liu, Dayan Guan, A. E. Saddik, Shijian Lu, Eric P. Xing Abstract: Robust point cloud parsing under all-weather conditions is crucial to level-5 autonomy in autonomous driving. However, how to learn a universal 3D semantic segmentation (3DSS) model is largely neglected as most existing benchmarks are dominated by point clouds captured under normal weather. We introduce SemanticSTF, an adverse-weather point cloud dataset that provides dense point-level annotations and allows to study 3DSS under various adverse weather conditions. We study all-weather 3DSS modeling under two setups: 1) domain adaptive 3DSS that adapts from normal-weather data to adverse-weather data; 2) domain generalizable 3DSS that learns all-weather 3DSS models from normal-weather data. Our studies reveal the challenge while existing 3DSS methods encounter adverse-weather data, showing the great value of SemanticSTF in steering the future endeavor along this very meaningful research direction. In addition, we design a domain randomization technique that alternatively randomizes the geometry styles of point clouds and aggregates their embeddings, ultimately leading to a generalizable model that can improve 3DSS under various adverse weather effectively. The SemanticSTF and related codes are available at https://github.com/xiaoaoran/SemanticSTF.',\n",
       " 'Author: Eric P. Xing Title: Cuttlefish: Low-Rank Model Training without All the Tuning Publication year: 2023 Coauthors: Hongyi Wang, Saurabh Agarwal, Pongsakorn U-chupala, Yoshiki Tanaka, Eric P. Xing, Dimitris Papailiopoulos Abstract: Recent research has shown that training low-rank neural networks can effectively reduce the total number of trainable parameters without sacrificing predictive accuracy, resulting in end-to-end speedups. However, low-rank model training necessitates adjusting several additional factorization hyperparameters, such as the rank of the factorization at each layer. In this paper, we tackle this challenge by introducing Cuttlefish, an automated low-rank training approach that eliminates the need for tuning factorization hyperparameters. Cuttlefish leverages the observation that after a few epochs of full-rank training, the stable rank (i.e., an approximation of the true rank) of each layer stabilizes at a constant value. Cuttlefish switches from full-rank to low-rank training once the stable ranks of all layers have converged, setting the dimension of each factorization to its corresponding stable rank. Our results show that Cuttlefish generates models up to 5.6 times smaller than full-rank models, and attains up to a 1.2 times faster end-to-end training process while preserving comparable accuracy. Moreover, Cuttlefish outperforms state-of-the-art low-rank model training methods and other prominent baselines. The source code for our implementation can be found at: https://github.com/hwang595/Cuttlefish.',\n",
       " 'Author: Eric P. Xing Title: Does compressing activations help model parallel training? Publication year: 2023 Coauthors: S. Bian, Dacheng Li, Hongyi Wang, Eric P. Xing, S. Venkataraman Abstract: Large-scale Transformer models are known for their exceptional performance in a range of tasks, but training them can be difficult due to the requirement for communication-intensive model parallelism. One way to improve training speed is to compress the message size in communication. Previous approaches have primarily focused on compressing gradients in a data parallelism setting, but compression in a model-parallel setting is an understudied area. We have discovered that model parallelism has fundamentally different characteristics than data parallelism. In this work, we present the first empirical study on the effectiveness of compression methods for model parallelism. We implement and evaluate three common classes of compression algorithms - pruning-based, learning-based, and quantization-based - using a popular Transformer training framework. We evaluate these methods across more than 160 settings and 8 popular datasets, taking into account different hyperparameters, hardware, and both fine-tuning and pre-training stages. We also provide analysis when the model is scaled up. Finally, we provide insights for future development of model parallelism compression algorithms.',\n",
       " 'Author: Eric P. Xing Title: Federated Learning as Variational Inference: A Scalable Expectation Propagation Approach Publication year: 2023 Coauthors: Han Guo, P. Greengard, Hongyi Wang, A. Gelman, Yoon Kim, Eric P. Xing Abstract: The canonical formulation of federated learning treats it as a distributed optimization problem where the model parameters are optimized against a global loss function that decomposes across client loss functions. A recent alternative formulation instead treats federated learning as a distributed inference problem, where the goal is to infer a global posterior from partitioned client data (Al-Shedivat et al., 2021). This paper extends the inference view and describes a variational inference formulation of federated learning where the goal is to find a global variational posterior that well-approximates the true posterior. This naturally motivates an expectation propagation approach to federated learning (FedEP), where approximations to the global posterior are iteratively refined through probabilistic message-passing between the central server and the clients. We conduct an extensive empirical study across various algorithmic considerations and describe practical strategies for scaling up expectation propagation to the modern federated setting. We apply FedEP on standard federated learning benchmarks and find that it outperforms strong baselines in terms of both convergence speed and accuracy.',\n",
       " \"Author: Eric P. Xing Title: Memory-adaptive Depth-wise Heterogenous Federated Learning Publication year: 2023 Coauthors: Kai Zhang, Yutong Dai, Hongyi Wang, Eric P. Xing, Xun Chen, Lichao Sun Abstract: Federated learning is a promising paradigm that allows multiple clients to collaboratively train a model without sharing the local data. However, the presence of heterogeneous devices in federated learning, such as mobile phones and IoT devices with varying memory capabilities, would limit the scale and hence the performance of the model could be trained. The mainstream approaches to address memory limitations focus on width-slimming techniques, where different clients train subnetworks with reduced widths locally and then the server aggregates the subnetworks. The global model produced from these methods suffers from performance degradation due to the negative impact of the actions taken to handle the varying subnetwork widths in the aggregation phase. In this paper, we introduce a memory-adaptive depth-wise learning solution in FL called FeDepth, which adaptively decomposes the full model into blocks according to the memory budgets of each client and trains blocks sequentially to obtain a full inference model. Our method outperforms state-of-the-art approaches, achieving 5% and more than 10% improvements in top-1 accuracy on CIFAR-10 and CIFAR-100, respectively. We also demonstrate the effectiveness of depth-wise fine-tuning on ViT. Our findings highlight the importance of memory-aware techniques for federated learning with heterogeneous devices and the success of depth-wise training strategy in improving the global model's performance.\",\n",
       " \"Author: E. Xing Title: Identification of Nonlinear Latent Hierarchical Models Publication year: 2023 Coauthors: Lingjing Kong, Biwei Huang, Feng Xie, E. Xing, Yuejie Chi, Kun Zhang Abstract: Identifying latent variables and causal structures from observational data is essential to many real-world applications involving biological data, medical data, and unstructured data such as images and languages. However, this task can be highly challenging, especially when observed variables are generated by causally related latent variables and the relationships are nonlinear. In this work, we investigate the identification problem for nonlinear latent hierarchical causal models in which observed variables are generated by a set of causally related latent variables, and some latent variables may not have observed children. We show that the identifiability of causal structures and latent variables (up to invertible transformations) can be achieved under mild assumptions: on causal structures, we allow for multiple paths between any pair of variables in the graph, which relaxes latent tree assumptions in prior work; on structural functions, we permit general nonlinearity and multi-dimensional continuous variables, alleviating existing work's parametric assumptions. Specifically, we first develop an identification criterion in the form of novel identifiability guarantees for an elementary latent variable model. Leveraging this criterion, we show that both causal structures and latent variables of the hierarchical model can be identified asymptotically by explicitly constructing an estimation procedure. To the best of our knowledge, our work is the first to establish identifiability guarantees for both causal structures and latent variables in nonlinear latent hierarchical models.\",\n",
       " 'Author: E. Xing Title: StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields Publication year: 2023 Coauthors: Kunhao Liu, Fangneng Zhan, Yiwen Chen, Jiahui Zhang, Yingchen Yu, Abdulmotaleb El Saddik, Shijian Lu, E. Xing Abstract: 3D style transfer aims to render stylized novel views of a 3D scene with multiview consistency. However, most existing work suffers from a three-way dilemma over accurate geometry reconstruction, high-quality stylization, and being generalizable to arbitrary new styles. We propose StyleRF (Style Radiance Fields), an innovative 3D style transfer technique that resolves the three-way dilemma by performing style transformation within the feature space of a radiance field. StyleRF employs an explicit grid of high-level features to represent 3D scenes, with which highfidelity geometry can be reliably restored via volume rendering. In addition, it transforms the grid features according to the reference style which directly leads to high-quality zero-shot style transfer. StyleRF consists of two innovative designs. The first is sampling-invariant content transformation that makes the transformation invariant to the holistic statistics of the sampled 3D points and accordingly ensures multi-view consistency. The second is deferred style transformation of 2D feature maps which is equivalent to the transformation of 3D points but greatly reduces memory footprint without degrading multi-view consistency. Extensive experiments show that StyleRF achieves superior 3D stylization quality with precise geometry reconstruction and it can generalize to various new styles in a zero-shot manner. Project website: https://kunhao-liu.github.io/StyleRF/',\n",
       " 'Author: E. Xing Title: Judging LLM-as-a-judge with MT-Bench and Chatbot Arena Publication year: 2023 Coauthors: Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, E. Xing, Haotong Zhang, Joseph Gonzalez, I. Stoica Abstract: Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.',\n",
       " 'Author: E. Xing Title: Understanding Masked Autoencoders via Hierarchical Latent Variable Models Publication year: 2023 Coauthors: Lingjing Kong, Martin Q. Ma, Guan-Hong Chen, E. Xing, Yuejie Chi, Louis-Philippe Morency, Kun Zhang Abstract: Masked autoencoder (MAE), a simple and effective self-supervised learning framework based on the reconstruction of masked image regions, has recently achieved prominent success in a variety of vision tasks. Despite the emergence of intriguing empirical observations on MAE, a theoretically principled understanding is still lacking. In this work, we formally characterize and justify existing empirical in-sights and provide theoretical guarantees of MAE. We formulate the underlying data-generating process as a hierarchical latent variable model, and show that under reasonable assumptions, MAE provably identifies a set of latent variables in the hierarchical model, explaining why MAE can extract high-level information from pixels. Further, we show how key hyperparameters in MAE (the masking ratio and the patch size) determine which true latent variables to be recovered, therefore influencing the level of semantic information in the representation. Specifically, extremely large or small masking ratios inevitably lead to low-level representations. Our theory offers coherent explanations of existing empirical observations and provides insights for potential empirical improvements and fundamental limitations of the masked-reconstruction paradigm. We conduct extensive experiments to validate our theoretical insights.',\n",
       " 'Author: Eric P. Xing Title: LightSeq: Sequence Level Parallelism for Distributed Training of Long Context Transformers Publication year: 2023 Coauthors: Dacheng Li, Rulin Shao, Anze Xie, Eric P. Xing, Joseph E. Gonzalez, I. Stoica, Xuezhe Ma, Hao Zhang Abstract: Increasing the context length of large language models (LLMs) unlocks fundamentally new capabilities, but also significantly increases the memory footprints of training. Previous model-parallel systems such as Megatron-LM partition and compute different attention heads in parallel, resulting in large communication volumes, so they cannot scale beyond the number of attention heads, thereby hindering its adoption. In this paper, we introduce a new approach, LightSeq, for long-context LLMs training. LightSeq has many notable advantages. First, LightSeq partitions over the sequence dimension, hence is agnostic to model architectures and readily applicable for models with varying numbers of attention heads, such as Multi-Head, Multi-Query and Grouped-Query attention. Second, LightSeq not only requires up to 4.7x less communication than Megatron-LM on popular LLMs but also overlaps the communication with computation. To further reduce the training time, LightSeq features a novel gradient checkpointing scheme to bypass an forward computation for memory-efficient attention. We evaluate LightSeq on Llama-7B and its variants with sequence lengths from 32K to 512K. Through comprehensive experiments on single and cross-node training, we show that LightSeq achieves up to 1.24-2.01x end-to-end speedup, and a 2-8x longer sequence length on models with fewer heads, compared to Megatron-LM. Codes will be available at https://github.com/RulinShao/LightSeq.',\n",
       " \"Author: Eric P. Xing Title: LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset Publication year: 2023 Coauthors: Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric P. Xing, Joseph E. Gonzalez, I. Stoica, Haotong Zhang Abstract: Studying how people interact with large language models (LLMs) in real-world scenarios is increasingly important due to their widespread use in various applications. In this paper, we introduce LMSYS-Chat-1M, a large-scale dataset containing one million real-world conversations with 25 state-of-the-art LLMs. This dataset is collected from 210K unique IP addresses in the wild on our Vicuna demo and Chatbot Arena website. We offer an overview of the dataset's content, including its curation process, basic statistics, and topic distribution, highlighting its diversity, originality, and scale. We demonstrate its versatility through four use cases: developing content moderation models that perform similarly to GPT-4, building a safety benchmark, training instruction-following models that perform similarly to Vicuna, and creating challenging benchmark questions. We believe that this dataset will serve as a valuable resource for understanding and advancing LLM capabilities. The dataset is publicly available at https://huggingface.co/datasets/lmsys/lmsys-chat-1m.\",\n",
       " \"Author: Eric Xing Title: ALISON: Fast and Effective Stylometric Authorship Obfuscation Publication year: 2024 Coauthors: Eric Xing, Saranya Venkatraman, Thai Le, Dongwon Lee Abstract: Authorship Attribution (AA) and Authorship Obfuscation (AO) are two competing tasks of increasing importance in privacy research. Modern AA leverages an author's consistent writing style to match a text to its author using an AA classifier. AO is the corresponding adversarial task, aiming to modify a text in such a way that its semantics are preserved, yet an AA model cannot correctly infer its authorship. To address privacy concerns raised by state-of-the-art (SOTA) AA methods, new AO methods have been proposed but remain largely impractical to use due to their prohibitively slow training and obfuscation speed, often taking hours. To this challenge, we propose a practical AO method, ALISON, that (1) dramatically reduces training/obfuscation time, demonstrating more than 10x faster obfuscation than SOTA AO methods, (2) achieves better obfuscation success through attacking three transformer-based AA methods on two benchmark datasets, typically performing 15% better than competing methods, (3) does not require direct signals from a target AA classifier during obfuscation, and (4) utilizes unique stylometric features, allowing sound model interpretation for explainable obfuscation. We also demonstrate that ALISON can effectively prevent four SOTA AA methods from accurately determining the authorship of ChatGPT-generated texts, all while minimally changing the original text semantics. To ensure the reproducibility of our findings, our code and data are available at: https://github.com/EricX003/ALISON.\",\n",
       " 'Author: Eric Xing Title: A Study on the Calibration of In-context Learning Publication year: 2023 Coauthors: Hanlin Zhang, Yi-Fan Zhang, Yaodong Yu, Dhruv Madeka, Dean Foster, Eric Xing, Hima Lakkaraju, S. Kakade Abstract: Accurate uncertainty quantification is crucial for the safe deployment of language models (LMs), and prior research has demonstrated improvements in the calibration of modern LMs. Our study focuses on in-context learning (ICL), a prevalent method for adapting static LMs through tailored prompts, and examines the balance between performance and calibration across a broad spectrum of natural language understanding and reasoning tasks. Through comprehensive experiments, we observe that, with an increasing number of ICL examples, models initially exhibit increased miscalibration before achieving better calibration and miscalibration tends to arise in low-shot settings. Moreover, we find that methods aimed at improving usability, such as fine-tuning and chain-of-thought (CoT) prompting, can lead to miscalibration and unreliable natural language explanations, suggesting that new methods may be required for scenarios where models are expected to be reliable.',\n",
       " \"Author: Eric Xing Title: SegMix: A Simple Structure-Aware Data Augmentation Method Publication year: 2023 Coauthors: Yuxin Pei, Pushkar Bhuse, Zhengzhong Liu, Eric Xing Abstract: Interpolation-based Data Augmentation (DA) methods (Mixup) linearly interpolate the inputs and labels of two or more training examples. Mixup has more recently been adapted to the field of Natural Language Processing (NLP), mainly for sequence labeling tasks. However, such a simple adoption yields mixed or unstable improvements over the baseline models. We argue that the direct-adoption methods do not account for structures in NLP tasks. To this end, we propose SegMix, a collection of interpolation-based DA algorithms that can adapt to task-specific structures. SegMix poses fewer constraints on data structures, is robust to various hyperparameter settings, applies to more task settings, and adds little computational overhead. In the algorithm's core, we apply interpolation methods on task-specific meaningful segments, in contrast to applying them on sequences as in prior work. We find SegMix to be a flexible framework that combines rule-based DA methods with interpolation-based methods, creating interesting mixtures of DA techniques. We show that SegMix consistently improves performance over strong baseline models in Named Entity Recognition (NER) and Relation Extraction (RE) tasks, especially under data-scarce settings. Furthermore, this method is easy to implement and adds negligible training overhead.\",\n",
       " \"Author: Eric Xing Title: Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large Language Models Publication year: 2024 Coauthors: Loka Li, Guan-Hong Chen, Yusheng Su, Zhenhao Chen, Yixuan Zhang, Eric Xing, Kun Zhang Abstract: The recent success of Large Language Models (LLMs) has catalyzed an increasing interest in their self-correction capabilities. This paper presents a comprehensive investigation into the intrinsic self-correction of LLMs, attempting to address the ongoing debate about its feasibility. Our research has identified an important latent factor - the ``confidence'' of LLMs - during the self-correction process. Overlooking this factor may cause the models to over-criticize themselves, resulting in unreliable conclusions regarding the efficacy of self-correction. We have experimentally observed that LLMs possess the capability to understand the ``confidence'' in their own responses. It motivates us to develop an ``If-or-Else'' (IoE) prompting framework, designed to guide LLMs in assessing their own ``confidence'', facilitating intrinsic self-corrections. We conduct extensive experiments and demonstrate that our IoE-based Prompt can achieve a consistent improvement regarding the accuracy of self-corrected responses over the initial answers. Our study not only sheds light on the underlying factors affecting self-correction in LLMs, but also introduces a practical framework that utilizes the IoE prompting principle to efficiently improve self-correction capabilities with ``confidence''. The code is available at \\\\url{https://github.com/MBZUAI-CLeaR/IoE-Prompting.git}.\",\n",
       " 'Author: Eric Xing Title: Temporally Disentangled Representation Learning under Unknown Nonstationarity Publication year: 2023 Coauthors: Xiangchen Song, Weiran Yao, Yewen Fan, Xinshuai Dong, Guan-Hong Chen, Juan Carlos Niebles, Eric Xing, Kun Zhang Abstract: In unsupervised causal representation learning for sequential data with time-delayed latent causal influences, strong identifiability results for the disentanglement of causally-related latent variables have been established in stationary settings by leveraging temporal structure. However, in nonstationary setting, existing work only partially addressed the problem by either utilizing observed auxiliary variables (e.g., class labels and/or domain indexes) as side information or assuming simplified latent causal dynamics. Both constrain the method to a limited range of scenarios. In this study, we further explored the Markov Assumption under time-delayed causally related process in nonstationary setting and showed that under mild conditions, the independent latent components can be recovered from their nonlinear mixture up to a permutation and a component-wise transformation, without the observation of auxiliary variables. We then introduce NCTRL, a principled estimation framework, to reconstruct time-delayed latent causal variables and identify their relations from measured sequential data only. Empirical evaluations demonstrated the reliable identification of time-delayed latent causal influences, with our methodology substantially outperforming existing baselines that fail to exploit the nonstationarity adequately and then, consequently, cannot distinguish distribution shifts.',\n",
       " 'Author: Eric P. Xing Title: LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning Publication year: 2023 Coauthors: Han Guo, P. Greengard, Eric P. Xing, Yoon Kim Abstract: We propose a simple approach for memory-efficient adaptation of pretrained language models. Our approach uses an iterative algorithm to decompose each pretrained matrix into a high-precision low-rank component and a memory-efficient quantized component. During finetuning, the quantized component remains fixed and only the low-rank component is updated. We present an integer linear programming formulation of the quantization component which enables dynamic configuration of quantization parameters (e.g., bit-width, block size) for each matrix given an overall target memory budget. We further explore a data-aware version of the algorithm which uses an approximation of the Fisher information matrix to weight the reconstruction objective during matrix decomposition. Experiments on finetuning RoBERTa and LLaMA-2 (7B and 70B) demonstrate that our low-rank plus quantized matrix decomposition approach (LQ-LoRA) outperforms strong QLoRA and GPTQ-LoRA baselines and enables aggressive quantization to sub-3 bits with only minor performance degradations. When finetuned on a language modeling calibration dataset, LQ-LoRA can also be used for model compression; in this setting our 2.75-bit LLaMA-2-70B model (which has 2.85 bits on average when including the low-rank components and requires 27GB of GPU memory) performs respectably compared to the 16-bit baseline.',\n",
       " 'Author: Chenyan Xiong Title: Text Matching Improves Sequential Recommendation by Reducing Popularity Biases Publication year: 2023 Coauthors: Zhenghao Liu, Senkun Mei, Chenyan Xiong, Xiaohua Li, Shi Yu, Zhiyuan Liu, Yu Gu, Ge Yu Abstract: This paper proposes Text mAtching based SequenTial rEcommenda-tion model (TASTE), which maps items and users in an embedding space and recommends items by matching their text representations. TASTE verbalizes items and user-item interactions using identifiers and attributes of items. To better characterize user behaviors, TASTE additionally proposes an attention sparsity method, which enables TASTE to model longer user-item interactions by reducing the self-attention computations during encoding. Our experiments show that TASTE outperforms the state-of-the-art methods on widely used sequential recommendation datasets. TASTE alleviates the cold start problem by representing long-tail items using full-text modeling and bringing the benefits of pretrained language models to recommendation systems. Our further analyses illustrate that TASTE significantly improves the recommendation accuracy by reducing the popularity bias of previous item id based recommendation models and returning more appropriate and text-relevant items to satisfy users. All codes are available at https://github.com/OpenMatch/TASTE.',\n",
       " 'Author: Chenyan Xiong Title: Fusion-in-T5: Unifying Document Ranking Signals for Improved Information Retrieval Publication year: 2023 Coauthors: S. Yu, Cheng-Chung Fan, Chenyan Xiong, David Jin, Zhiyuan Liu, Zhenghao Liu Tsinghua University, Huazhong University of Science, Technology, Microsoft Research, M. I. O. Technology, N. University Abstract: Common IR pipelines are typically cascade systems that may involve multiple rankers and/or fusion models to integrate different information step-by-step. In this paper, we propose a novel re-ranker named Fusion-in-T5 (FiT5), which integrates document text information, retrieval features, and global document information into a single unified model using templated-based input and global attention. Experiments on passage ranking benchmarks MS MARCO and TREC DL show that FiT5 significantly improves ranking performance over prior pipelines. Analyses find that through global attention, FiT5 is able to jointly utilize the ranking features via gradually attending to related documents, and thus improve the detection of subtle nuances between them. Our code will be open-sourced.',\n",
       " 'Author: Chenyan Xiong Title: OpenMatch-v2: An All-in-one Multi-Modality PLM-based Information Retrieval Toolkit Publication year: 2023 Coauthors: Shi Yu, Zhenghao Liu, Chenyan Xiong, Zhiyuan Liu Abstract: Pre-trained language models (PLMs) have emerged as the foundation of the most advanced Information Retrieval (IR) models. Powered by PLMs, the latest IR research has proposed novel models, new domain adaptation algorithms as well as enlarged datasets. In this paper, we present a Python-based IR toolkit OpenMatch-v2. As a full upgrade of OpenMatch proposed in 2021, OpenMatch-v2 incorporates the most recent advancements of PLM-based IR research, providing support for new, cross-modality models and enhanced domain adaptation techniques with a streamlined, optimized infrastructure. The code of OpenMatch is publicly available at https://github.com/OpenMatch/OpenMatch.',\n",
       " \"Author: Chenyan Xiong Title: An In-depth Look at Gemini's Language Abilities Publication year: 2023 Coauthors: Syeda Nahida Akter, Zichun Yu, Aashiq Muhamed, Tianyue Ou, Alex Bauerle, Ángel Alexander Cabrera, Krish Dholakia, Chenyan Xiong, Graham Neubig Abstract: The recently released Google Gemini class of models are the first to comprehensively report results that rival the OpenAI GPT series across a wide variety of tasks. In this paper, we do an in-depth exploration of Gemini's language abilities, making two contributions. First, we provide a third-party, objective comparison of the abilities of the OpenAI GPT and Google Gemini models with reproducible code and fully transparent results. Second, we take a closer look at the results, identifying areas where one of the two model classes excels. We perform this analysis over 10 datasets testing a variety of language abilities, including reasoning, answering knowledge-based questions, solving math problems, translating between languages, generating code, and acting as instruction-following agents. From this analysis, we find that Gemini Pro achieves accuracy that is close but slightly inferior to the corresponding GPT 3.5 Turbo on all tasks that we benchmarked. We further provide explanations for some of this under-performance, including failures in mathematical reasoning with many digits, sensitivity to multiple-choice answer ordering, aggressive content filtering, and others. We also identify areas where Gemini demonstrates comparably high performance, including generation into non-English languages, and handling longer and more complex reasoning chains. Code and data for reproduction can be found at https://github.com/neulab/gemini-benchmark\",\n",
       " 'Author: Chenyan Xiong Title: CompleQA: Benchmarking the Impacts of Knowledge Graph Completion Methods on Question Answering Publication year: 2023 Coauthors: Donghan Yu, Yu Gu, Chenyan Xiong, Yiming Yang ',\n",
       " 'Author: Chenyan Xiong Title: ActiveRAG: Revealing the Treasures of Knowledge via Active Learning Publication year: 2024 Coauthors: Zhipeng Xu, Zhenghao Liu, Yibin Liu, Chenyan Xiong, Yukun Yan, Shuo Wang, Shi Yu, Zhiyuan Liu, Ge Yu Abstract: Retrieval Augmented Generation (RAG) has introduced a new paradigm for Large Language Models (LLMs), aiding in the resolution of knowledge-intensive tasks. However, current RAG models position LLMs as passive knowledge receptors, thereby restricting their capacity for learning and comprehending external knowledge. In this paper, we present ActiveRAG, an innovative RAG framework that shifts from passive knowledge acquisition to an active learning mechanism. This approach utilizes the Knowledge Construction mechanism to develop a deeper understanding of external knowledge by associating it with previously acquired or memorized knowledge. Subsequently, it designs the Cognitive Nexus mechanism to incorporate the outcomes from both chains of thought and knowledge construction, thereby calibrating the intrinsic cognition of LLMs. Our experimental results demonstrate that ActiveRAG surpasses previous RAG models, achieving a 5% improvement on question-answering datasets. All data and codes are available at https://github.com/OpenMatch/ActiveRAG.',\n",
       " 'Author: Chenyan Xiong Title: Cleaner Pretraining Corpus Curation with Neural Web Scraping Publication year: 2024 Coauthors: Zhipeng Xu, Zhenghao Liu, Yukun Yan, Zhiyuan Liu, Chenyan Xiong, Ge Yu Abstract: The web contains large-scale, diverse, and abundant information to satisfy the information-seeking needs of humans. Through meticulous data collection, preprocessing, and curation, webpages can be used as a fundamental data resource for language model pretraining. However, when confronted with the progressively revolutionized and intricate nature of webpages, rule-based/feature-based web scrapers are becoming increasingly inadequate. This paper presents a simple, fast, and effective Neural web Scraper (NeuScraper) to help extract primary and clean text contents from webpages. Experimental results show that NeuScraper surpasses the baseline scrapers by achieving more than a 20% improvement, demonstrating its potential in extracting higher-quality data to facilitate the language model pretraining. All of the code is available at https://github.com/OpenMatch/NeuScraper.',\n",
       " 'Author: Chenyan Xiong Title: Improving Multitask Retrieval by Promoting Task Specialization Publication year: 2023 Coauthors: Wenzheng Zhang, Chenyan Xiong, K. Stratos, Arnold Overwijk Abstract: Abstract In multitask retrieval, a single retriever is trained to retrieve relevant contexts for multiple tasks. Despite its practical appeal, naive multitask retrieval lags behind task-specific retrieval, in which a separate retriever is trained for each task. We show that it is possible to train a multitask retriever that outperforms task-specific retrievers by promoting task specialization. The main ingredients are: (1) a better choice of pretrained model—one that is explicitly optimized for multitasking—along with compatible prompting, and (2) a novel adaptive learning method that encourages each parameter to specialize in a particular task. The resulting multitask retriever is highly performant on the KILT benchmark. Upon analysis, we find that the model indeed learns parameters that are more task-specialized compared to naive multitasking without prompting or adaptive learning.1',\n",
       " 'Author: Chenyan Xiong Title: Unsupervised Dense Retrieval Training with Web Anchors Publication year: 2023 Coauthors: Yiqing Xie, X. Liu, Chenyan Xiong Abstract: In this work, we present an unsupervised retrieval method with contrastive learning on web anchors. The anchor text describes the content that is referenced from the linked page. This shows similarities to search queries that aim to retrieve pertinent information from relevant documents. Based on their commonalities, we train an unsupervised dense retriever, Anchor-DR, with a contrastive learning task that matches the anchor text and the linked document. To filter out uninformative anchors (such as \"homepage\" or other functional anchors), we present a novel filtering technique to only select anchors that contain similar types of information as search queries. Experiments show that Anchor-DR outperforms state-of-the-art methods on unsupervised dense retrieval by a large margin (e.g., by 5.3% NDCG@10 on MSMARCO). The gain of our method is especially significant for search and question answering tasks. Our analysis further reveals that the pattern of anchor-document pairs is similar to that of search query-document pairs. Code available at https://github.com/Veronicium/AnchorDR.',\n",
       " 'Author: Chenyan Xiong Title: Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In Publication year: 2023 Coauthors: Zichun Yu, Chenyan Xiong, S. Yu, Zhiyuan Liu Abstract: Retrieval augmentation can aid language models (LMs) in knowledge-intensive tasks by supplying them with external information. Prior works on retrieval augmentation usually jointly fine-tune the retriever and the LM, making them closely coupled. In this paper, we explore the scheme of generic retrieval plug-in: the retriever is to assist target LMs that may not be known beforehand or are unable to be fine-tuned together. To retrieve useful documents for unseen target LMs, we propose augmentation-adapted retriever (AAR), which learns LM’s preferences obtained from a known source LM. Experiments on the MMLU and PopQA datasets demonstrate that our AAR trained with a small source LM is able to significantly improve the zero-shot generalization of larger target LMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates that the preferences of different LMs overlap, enabling AAR trained with a single source LM to serve as a generic plug-in for various target LMs. Our code is open-sourced at https://github.com/OpenMatch/Augmentation-Adapted-Retriever.',\n",
       " 'Author: Chenyan Xiong Title: Model-Generated Pretraining Signals Improves Zero-Shot Generalization of Text-to-Text Transformers Publication year: 2023 Coauthors: Linyuan Gong, Chenyan Xiong, Xiaodong Liu, Payal Bajaj, Yiqing Xie, Alvin Cheung, Jianfeng Gao, Xia Song Abstract: This paper explores the effectiveness of model-generated signals in improving zero-shot generalization of text-to-text Transformers such as T5. We study various designs to pretrain T5 using an auxiliary model to construct more challenging token replacements for the main model to denoise. Key aspects under study include the decoding target, the location of the RTD head, and the masking pattern. Based on these studies, we develop a new model, METRO-T0, which is pretrained using the redesigned ELECTRA-Style pretraining strategies and then prompt-finetuned on a mixture of NLP tasks. METRO-T0 outperforms all similar-sized baselines on prompted NLP benchmarks, such as _T0 Eval_ and MMLU, and rivals the state-of-the-art T0-11B model with only **8%** of its parameters. Our analysis on model’s neural activation and parameter sensitivity reveals that the effectiveness of METRO-T0 stems from more balanced contribution of parameters and better utilization of their capacity. The code and model checkpoints are available at [https://github.com/gonglinyuan/metro_t0](https://github.com/gonglinyuan/metro_t0).',\n",
       " \"Author: Chenyan Xiong Title: MARVEL: Unlocking the Multi-Modal Capability of Dense Retrieval via Visual Module Plugin Publication year: 2023 Coauthors: Tianshuo Zhou, Senkun Mei, Xinze Li, Zhenghao Liu, Chenyan Xiong, Zhiyuan Liu, Yu Gu, Ge Yu Abstract: This paper proposes Multi-modAl Retrieval model via Visual modulE pLugin (MARVEL), which learns an embedding space for queries and multi-modal documents to conduct retrieval. MARVEL encodes queries and multi-modal documents with a unified encoder model, which helps to alleviate the modality gap between images and texts. Specifically, we enable the image understanding ability of the well-trained dense retriever, T5-ANCE, by incorporating the visual module's encoded image features as its inputs. To facilitate the multi-modal retrieval tasks, we build the ClueWeb22-MM dataset based on the ClueWeb22 dataset, which regards anchor texts as queries, and exacts the related text and image documents from anchor-linked web pages. Our experiments show that MARVEL significantly outperforms the state-of-the-art methods on the multi-modal retrieval dataset WebQA and ClueWeb22-MM. MARVEL provides an opportunity to broaden the advantages of text retrieval to the multi-model scenario. Besides, we also illustrate that the language model has the ability to extract image semantics and partly map the image features to the input word embedding space. All codes are available at https://github.com/OpenMatch/MARVEL.\",\n",
       " 'Author: Chenyan Xiong Title: Distributionally Robust Unsupervised Dense Retrieval Training on Web Graphs Publication year: 2023 Coauthors: Peixuan Han, Zhenghao Liu, Zhiyuan Liu, Chenyan Xiong Abstract: This paper introduces Web-DRO, an unsupervised dense retrieval model, which clusters documents based on web structures and reweights the groups during contrastive training. Specifically, we first leverage web graph links and contrastively train an embedding model for clustering anchor-document pairs. Then we use Group Distributional Robust Optimization to reweight different clusters of anchor-document pairs, which guides the model to assign more weights to the group with higher contrastive loss and pay more attention to the worst case during training. Our experiments on MS MARCO and BEIR show that our model, Web-DRO, significantly improves the retrieval effectiveness in unsupervised scenarios. A comparison of clustering techniques shows that training on the web graph combining URL information reaches optimal performance on clustering. Further analysis confirms that group weights are stable and valid, indicating consistent model preferences as well as effective up-weighting of valuable groups and down-weighting of uninformative ones. The code of this paper can be obtained from https://github.com/OpenMatch/Web-DRO.',\n",
       " 'Author: Chenyan Xiong Title: Structure-Aware Language Model Pretraining Improves Dense Retrieval on Structured Data Publication year: 2023 Coauthors: Xinze Li, Zhenghao Liu, Chenyan Xiong, Shi Yu, Yu Gu, Zhiyuan Liu, Ge Yu Abstract: This paper presents Structure Aware Dense Retrieval (SANTA) model, which encodes user queries and structured data in one universal embedding space for retrieving structured data. SANTA proposes two pretraining methods to make language models structure-aware and learn effective representations for structured data: 1) Structured Data Alignment, which utilizes the natural alignment relations between structured data and unstructured data for structure-aware pretraining. It contrastively trains language models to represent multi-modal text data and teaches models to distinguish matched structured data for unstructured texts. 2) Masked Entity Prediction, which designs an entity-oriented mask strategy and asks language models to fill in the masked entities. Our experiments show that SANTA achieves state-of-the-art on code search and product search and conducts convincing results in the zero-shot setting. SANTA learns tailored representations for multi-modal text data by aligning structured and unstructured data pairs and capturing structural semantics by masking and predicting entities in the structured data. All codes are available at https://github.com/OpenMatch/OpenMatch.',\n",
       " \"Author: Chenyan Xiong Title: Toolink: Linking Toolkit Creation and Using through Chain-of-Solving on Open-Source Model Publication year: 2023 Coauthors: Cheng Qian, Chenyan Xiong, Zhenghao Liu, Zhiyuan Liu Abstract: Large Language Models (LLMs) have demonstrated remarkable progress in utilizing tools, but their closed-source nature and high inference costs pose limitations on their adaptability, necessitating a valid method that leverages smaller, open-sourced models. In this paper, we introduce Toolink, a comprehensive framework that performs task-solving by first creating a toolkit and then integrating the planning and calling of tools through a chain-of-solving (CoS) approach. We first validate the efficacy of Toolink in harnessing the model's creativity and CoS ability on ChatGPT. Subsequently, we curate CoS-GPT, a chain-of-solving dataset designed for tool-using, and finetune the LLaMA-7B model. It results in LLaMA-CoS, a powerful open-source model with advanced tool-planning and tool-calling capabilities. Evaluation on diverse tasks from BIG-bench demonstrates its CoS ability matches that of ChatGPT while its performance surpasses the chain-of-thought approach. Further studies highlight the generalization of LLaMA-CoS to unseen tasks and showcase its capability in using toolkits not explicitly tailored for the target task, affirming its robustness in real-world scenarios. All codes and data are released.\",\n",
       " 'Author: Chenyan Xiong Title: Augmenting Zero-Shot Dense Retrievers with Plug-in Mixture-of-Memories Publication year: 2023 Coauthors: Suyu Ge, Chenyan Xiong, Corby Rosset, Arnold Overwijk, Jiawei Han, Paul N. Bennett Abstract: In this paper we improve the zero-shot generalization ability of language models via Mixture-Of-Memory Augmentation (MoMA), a mechanism that retrieves augmentation documents from multiple information corpora (\"external memories\"), with the option to\"plug in\"new memory at inference time. We develop a joint learning mechanism that trains the augmentation component with latent labels derived from the end retrieval task, paired with hard negatives from the memory mixture. We instantiate the model in a zero-shot dense retrieval setting by augmenting a strong T5-based retriever with MoMA. Our model, MoMA, obtains strong zero-shot retrieval accuracy on the eighteen tasks included in the standard BEIR benchmark. It outperforms systems that seek generalization from increased model parameters and computation steps. Our analysis further illustrates the necessity of augmenting with mixture-of-memory for robust generalization, the benefits of augmentation learning, and how MoMA utilizes the plug-in memory at inference time without changing its parameters. We plan to open source our code.',\n",
       " 'Author: Yiming Yang Title: Accurate detection of reactive oxygen species by tuning an elastic motif (GPGGA)4 in nanopores. Publication year: 2023 Coauthors: Cunli Wang, Yiming Yang, Shuai Shao, Hangyu Zhang, Na Li, Zheng-Zhu Zhang, Bo Liu Abstract: We have developed a reactive oxygen species (ROS) sensor based on nanopores modified with GGGCEG(GPGGA)4CEG. The formation of an intramolecular disulfide bond oxidized by ROS leads to conformation changes in GGGCEG(GPGGA)4CEG, which then induces an obvious change in the size of the nanopores and a corresponding ionic current change. This work allows the accurate and dynamic monitoring of ROS through the combination of (GPGGA)4 and nanopores.',\n",
       " 'Author: Yiming Yang Title: Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation Publication year: 2023 Coauthors: Junwei Huang, Zhiqing Sun, Yiming Yang Abstract: Graph-based diffusion models have shown promising results in terms of generating high-quality solutions to NP-complete (NPC) combinatorial optimization (CO) problems. However, those models are often inefficient in inference, due to the iterative evaluation nature of the denoising diffusion process. This paper proposes to use progressive distillation to speed up the inference by taking fewer steps (e.g., forecasting two steps ahead within a single step) during the denoising process. Our experimental results show that the progressively distilled model can perform inference 16 times faster with only 0.019% degradation in performance on the TSP-50 dataset.',\n",
       " 'Author: Yiming Yang Title: Automatic synchrotron tomographic alignment schemes based on genetic algorithms and human-in-the-loop software Publication year: 2023 Coauthors: Zhen Zhang, Xiaoxue Bi, Pengcheng Li, Chenglong Zhang, Yiming Yang, Yu Liu, Gang Chen, Yuhui Dong, Gongfa Liu, Yi Zhang Abstract: A highly automatic alignment scheme is proposed to address the pressing challenge in tomographic alignment of future scanning tomography experiments. The results show that the proposed method exhibits excellent sub-pixel alignment accuracy and high time efficiency.',\n",
       " 'Author: Yiming Yang Title: High CD8+tumor-infiltrating lymphocytes indicate severe exhaustion and poor prognosis in angioimmunoblastic T-cell lymphoma Publication year: 2023 Coauthors: Qiqi Zhu, Yiming Yang, Xueqin Deng, Ningning Chao, Zihang Chen, Y. Ye, Wenyan Zhang, Weiping Liu, Sha Zhao Abstract: Background Exhaustion of CD8+ tumor-infiltrating lymphocytes (TILs), characterized by the overexpression of immune checkpoints (IC), is a major impediment to anti-tumor immunity. However, the exhaustion status of CD8+TILs in angioimmunoblastic T cell lymphoma (AITL) remains unclear. Therefore, we aimed to elucidate the exhaustion status of CD8+TILs in AITL and its influence on prognosis. Methods The correlation between CD8+TILs and IC expression in AITL was analyzed using single-cell RNA sequencing (n = 2), flow cytometry (n = 20), and RNA sequencing (n = 20). Biological changes related to CD8+TILs exhaustion at different cytotoxic T lymphocyte (CTL) levels (mean expression levels of CD8A, CD8B, GZMA, GZMB, and PRF1) in AITL were evaluated using RNA sequencing (n = 20) and further validated using the GEO dataset (n = 51). The impact of CD8 protein expression and CTL levels on patient prognosis was analyzed using flow cytometry and RNA sequencing, respectively. Results Our findings demonstrated that the higher the infiltration of CD8+TILs, the higher was the proportion of exhausted CD8+TILs characterized by the overexpression of multiple IC. This was accompanied by extensive exhaustion-related biological changes, which suggested severe exhaustion in CD8+TILs and may be one of the main reasons for the poor prognosis of patients with high CD8+TILs and CTL. Conclusion Our study comprehensively reveals the exhaustion status of CD8+TILs and their potential negative impact on AITL prognosis, which facilitates further mechanistic studies and is valuable for guiding immunotherapy strategies.',\n",
       " 'Author: Yiming Yang Title: Robust Cross-Domain Pseudo-Labeling and Contrastive Learning for Unsupervised Domain Adaptation NIR-VIS Face Recognition Publication year: 2023 Coauthors: Yiming Yang, Weipeng Hu, Haiqi Lin, Haifeng Hu Abstract: Near-infrared and visible face recognition (NIR-VIS) is attracting increasing attention because of the need to achieve face recognition in low-light conditions to enable 24-hour secure retrieval. However, annotating identity labels for a large number of heterogeneous face images is time-consuming and expensive, which limits the application of the NIR-VIS face recognition system to larger scale real-world scenarios. In this paper, we attempt to achieve NIR-VIS face recognition in an unsupervised domain adaptation manner. To get rid of the reliance on manual annotations, we propose a novel Robust cross-domain Pseudo-labeling and Contrastive learning (RPC) network which consists of three key components, i.e., NIR cluster-based Pseudo labels Sharing (NPS), Domain-specific cluster Contrastive Learning (DCL) and Inter-domain cluster Contrastive Learning (ICL). Firstly, NPS is presented to generate pseudo labels by exploring robust NIR clusters and sharing reliable label knowledge with VIS domain. Secondly, DCL is designed to learn intra-domain compact yet discriminative representations. Finally, ICL dynamically combines and refines intrinsic identity relationships to guide the instance-level features to learn robust and domain-independent representations. Extensive experiments are conducted to verify an accuracy of over 99% in pseudo label assignment and the advanced performance of RPC network on four mainstream NIR-VIS datasets.',\n",
       " 'Author: Yiming Yang Title: Numerical Investigation of Fatigue Crack Propagation Behaviour of 550E High-Performance Steel Publication year: 2023 Coauthors: Linfa Xiao, Heng Lin, Yongxiang Wang, Yiming Yang, Huapeng Chen Abstract: The fatigue crack propagation behaviour of Q550E high-performance steel (HPS) is studied in this paper. Static tensile testing and fatigue crack propagation testing were carried out, and the results were compared with those of Q235. Finite element models were developed and verified against the experimental results. The impacts of the initial crack angle, crack depth ratio, stress ratio, thickness, and corrosion pitting on the fatigue crack propagation behaviour of the HPS were analysed. The results show that the fatigue life of Q550 was reduced by 18% due to the corrosion pitting, but it did not change the crack propagation path. When the stress intensity factor is higher than a certain value, the fatigue performance of Q235 is better than that of Q550E. The initial crack angle of 52.5° is the critical angle of the crack stress intensity factor. The steel tends to fracture as the crack depth ratio increases, and more attention should be paid to the effective crack length in engineering practice. An increasing stress ratio leads to a smaller stress intensity factor, and the thickness affects the stress intensity factor in the later stage. The crack stress intensity factor around the corrosion pits gradually decreases along the thickness direction, and the crack tips around the corrosion pits tend to reach the yield state initially, accelerating the fatigue fracture of the specimen and ultimately leading to a decrease in fatigue life.',\n",
       " \"Author: Yiming Yang Title: Phase Behaviors of Charged Macromolecules in Aqueous Solutions Publication year: 2023 Coauthors: Yiming Yang, Di Jia Abstract: Compared to the charge–charge interaction, the role of the dipole‐dipole interaction has long been ignored in the phase behaviors of charged macromolecules in solutions. Charged macromolecules in solutions exhibit rich phase behaviors due to their complexity and they have been studied extensively. Phase separation can happen for charged macromolecules in the presence of monovalent salt, multivalent salt, and oppositely charged polymers, surfactants, etc., and for more advanced charged macromolecules such as polyzwitterions and polyampholytes, the phase diagram is even richer. In this perspective, the unacknowledged role of dipole‐dipole interaction in the phase behaviors of charged macromolecular solutions will be introduced. Dipolar polymers can form complex, self‐regulating structures which can be employed in various fields from drug‐delivery systems to next‐generation polymers. More importantly, it will shed light on how some of the life's basic and coherent structures such as biomolecular condensates and membrane‐less organelles are assembled and built by charged biomacromolecules such as DNA, RNA, and proteins.\",\n",
       " 'Author: Yiming Yang Title: Association between albumin-to-globulin ratio and the risk of overall survival in advanced non-small cell lung cancer patients with anlotinib treatment: a retrospective cohort study Publication year: 2023 Coauthors: Jinzhan Chen, Cong-jun Xie, Yiming Yang, Shuwen Yang, Jin-xiang Huang, Feiyang Ye, Zhenyang Lin, L. Tong, Jiaxin Liu ',\n",
       " 'Author: Yiming Yang Title: Modification Effect of Pt on the Active Sites of Sulfated CeO2 Nanorods for the Selective Catalytic Reduction of NO Publication year: 2023 Coauthors: Hao Fan, Yiming Yang, Xu Yang, Xuefeng He, Jian Sun, Liu Yang, Jiao Li, Zhenxing Shen ',\n",
       " 'Author: Yiming Yang Title: Analysis of Volatile Components in Dried Fruits and Branch Exudates of Schisandra chinensis with Different Fruit Colors Using GC-IMS Technology Publication year: 2023 Coauthors: Yiping Yan, Wenpeng Lu, Taiping Tian, Nan Shu, Yiming Yang, Shutian Fan, Xianyan Han, Yunhua Ge, Peilei Xu Abstract: To investigate the volatile components of Schisandra chinensis (Turcz.) Bail (commonly known as northern Schisandra) of different colors and to explore their similarities and differences, to identify the main flavor substances in the volatile components of the branch exudates of northern schisandra, and finally to establish a fingerprint map of the volatile components of the dried fruits and branch exudates of northern Schisandra of different colors, we used GC-IMS technology to analyze the volatile components of the dried fruits and branch exudates of three different colors of northern Schisandra and established a fingerprint spectra. The results showed that a total of 60 different volatile chemical components were identified in the branch exudates and dried fruits of Schisandra. The components of germplasm resources with different fruit colors were significantly different. The ion mobility spectrum and OPLS-DA results showed that white and yellow fruits were more similar compared to red fruits. The volatile components in dried fruits were significantly higher than those in branch exudates. After VIP (variable importance in projection) screening, 41 key volatile substances in dried fruits and 30 key volatile substances in branch exudates were obtained. After screening by odor activity value (OAV), there were 24 volatile components greater than 1 in both dried fruits and branch exudates. The most important contributing volatile substance was 3-methyl-butanal, and the most important contributing volatile substance in white fruit was (E)-2-hexenal.',\n",
       " 'Author: Yiming Yang Title: Secreted endogenous macrosomes reduce Aβ burden and ameliorate Alzheimer’s disease Publication year: 2023 Coauthors: Cunli Wang, Yiming Yang, Xiaoyu Zhang, Zhenqiang Shi, Huiling Gao, Manli Zhong, Yong-gang Fan, Hongyan Zhang, Bo Liu, Guangyan Qing Abstract: Innovative therapeutic strategies are urgently needed for Alzheimer’s disease (AD) due to the increasing size of the aging population and the lack of effective drug treatment. Here, we report the therapeutic effects of extracellular vesicles (EVs) secreted by microglia, including macrosomes and small EVs, on AD-associated pathology. Macrosomes strongly inhibited β-amyloid (Aβ) aggregation and rescued cells from Aβ misfolding–induced cytotoxicity. Furthermore, macrosome administration reduced Aβ plaques and ameliorated cognitive impairment in mice with AD. In contrast, small EVs slightly promoted Aβ aggregation and did not improve AD pathology. Proteomic analysis of small EVs and macrosomes revealed that macrosomes harbor several important neuroprotective proteins that inhibit Aβ misfolding. In particular, the small integral membrane protein 10–like protein 2B in macrosomes has been shown to inhibit Aβ aggregation. Our observations provide an alternative therapeutic strategy for the treatment of AD over conventional ineffective drug treatments.',\n",
       " 'Author: Yiming Yang Title: Neutral Face Learning and Progressive Fusion Synthesis Network for NIR-VIS Face Recognition Publication year: 2023 Coauthors: Yiming Yang, Weipeng Hu, Haifeng Hu Abstract: To meet the strong demand for deploying face recognition systems in low-light scenarios, the Near-InfraRed and VISible (NIR-VIS) face recognition task is receiving increasing attention. However, heterogeneous faces have the characteristics of heterogeneity and non-neutrality. Heterogeneity refers to the fact that the matching images are in different modalities, and non-neutrality means that the matching images are significantly different in pose, expression, lighting, etc. Both situations pose challenges for NIR-VIS face matching. To address this problem, we propose a novel Neutral face Learning and Progressive Fusion synthesis (NLPF) network to disentangle the latent attributes of heterogeneous faces and learn neutral face representations. Our approach naturally integrates Identity-related Neutral face Learning (INL) and Attribute Progressive Fusion (APF) into a joint framework. Firstly, INL eliminates modal variations and residual variations by guiding the network to learn homogeneous neutral face feature representations, which tackles the challenge of heterogeneity and non-neutrality by mapping cross-modal images to a common neutral representation subspace. Besides, APF is presented to perform the disentanglement and reintegration of identity-related features, modality-related features and residual features in a progressive fusion manner, which helps to further purify identity-related features. Comprehensive evaluations are carried out on three mainstream NIR-VIS datasets to verify the robustness and effectiveness of the NLPF model. In particular, NLPF has competitive recognition performance on LAMP-HQ, the most challenging NIR-VIS dataset so far.',\n",
       " 'Author: Yiming Yang Title: DIFUSCO: Graph-based Diffusion Solvers for Combinatorial Optimization Publication year: 2023 Coauthors: Zhiqing Sun, Yiming Yang Abstract: Neural network-based Combinatorial Optimization (CO) methods have shown promising results in solving various NP-complete (NPC) problems without relying on hand-crafted domain knowledge. This paper broadens the current scope of neural solvers for NPC problems by introducing a new graph-based diffusion framework, namely DIFUSCO. Our framework casts NPC problems as discrete {0, 1}-vector optimization problems and leverages graph-based denoising diffusion models to generate high-quality solutions. We investigate two types of diffusion models with Gaussian and Bernoulli noise, respectively, and devise an effective inference schedule to enhance the solution quality. We evaluate our methods on two well-studied NPC combinatorial optimization problems: Traveling Salesman Problem (TSP) and Maximal Independent Set (MIS). Experimental results show that DIFUSCO strongly outperforms the previous state-of-the-art neural solvers, improving the performance gap between ground-truth and neural solvers from 1.76% to 0.46% on TSP-500, from 2.46% to 1.17% on TSP-1000, and from 3.19% to 2.58% on TSP10000. For the MIS problem, DIFUSCO outperforms the previous state-of-the-art neural solver on the challenging SATLIB benchmark.',\n",
       " 'Author: Yiming Yang Title: Effects of S Content on Inclusion Formation in the Al and Ti–Mg Complex Deoxidized Steel Publication year: 2023 Coauthors: Pengliang Jin, Yiming Yang, Lei Cao, Xinghu Yuan, Guocheng Wang Abstract: Two groups of Al and Ti–Mg complex deoxidized steels with different S contents are designed, and inclusion characteristics of two groups of steel samples are observed by field‐emission scanning electron microscopy–energy‐dispersive spectroscopy. The results show that there are single TiS inclusions, MgAl2O4 (Al2O3)–TiS, and MgAl2O4–TiN–TiS complex inclusions in No. 1 steel (low‐sulfur content) and No. 2 steel (high‐sulfur content). However, there are also complex inclusions containing MnS in the No. 2 steel but not in No. 1 steel. In order to reveal the precipitation mechanism of MnS, equilibrium phase of inclusion from 1873\\u2009K to liquidus temperature is further analyzed, and the mass fractions of different inclusions from liquidus to solidus temperature are quantitatively calculated using the element segregation model combined with FactSage 7.2 thermodynamic software. Furthermore, the mismatch values between different crystal planes of MnS, TiS, and TiN are calculated. The results show that MnS (110) is most likely to precipitate on TiS (001), which is consistent with the observation that there is TiS–MnS interface in the complex inclusions containing MnS in No. 2 steel. This study could be helpful to the controlling sulfide precipitation in Al and Ti–Mg complex deoxidized steel.',\n",
       " 'Author: Yiming Yang Title: Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs Publication year: 2023 Coauthors: Qingyang Zhang, Yiming Yang, Jingqing Ruan, Xuantang Xiong, Dengpeng Xing, Bo Xu Abstract: Goal-Conditioned Hierarchical Reinforcement Learning (GCHRL) is a promising paradigm to address the exploration-exploitation dilemma in reinforcement learning. It decomposes the source task into sub goal conditional subtasks and conducts exploration and exploitation in the subgoal space. The effectiveness of GCHRL heavily relies on sub goal representation functions and sub goal selection strategy. However, existing works often overlook the temporal coherence in GCHRL when learning latent sub goal representations and lack an efficient sub goal selection strategy that balances exploration and exploitation. This paper proposes HIerarchical reinforcement learning via dynamically building Latent Landmark graphs (HILL) to overcome these limitations. HILL learns latent subgoal representations that satisfy temporal coherence using a contrastive representation learning objective. Based on these representations, HILL dynamically builds latent landmark graphs and employs a novelty measure on nodes and a utility measure on edges. Finally, HILL develops a subgoal selection strategy that balances exploration and exploitation by jointly considering both measures. Experimental results demonstrate that HILL outperforms state-of-the-art baselines on continuous control tasks with sparse rewards in sample efficiency and asymptotic performance. Our code is available at https://github.com/papercode2022/HILL.',\n",
       " 'Author: Yiming Yang Title: Comprehensive evaluation of nine grape varieties based on fundamental physical and chemical indicators, color and volatile compounds Publication year: 2023 Coauthors: W. Cao, Nan Shu, Yiming Yang, Jinli Wen, Wenpeng Lu Abstract: BACKGROUND: In todays’ society, the rapid development of the wine industry and the improvement of peoples’ living standards make people pay more and more attention to wine grape quality. OBJECTIVE: To evaluate the wine grape varieties in Northeast Chinas’ grape growing regions for better wine grape quality, we evaluated the quality of different varieties of wine grapes. METHODS: The grape varieties ‘Hassan’ ‘Zuoshaner’ ‘Beibinghong’ ‘Zuoyouhong’ ‘Beta’ ‘Shuanghong’ ‘Zijingganlu’ ‘Cabernet Sauvignon’ and ‘Syrah’ were planted in the grape growing area of Jilin, Northeast China, were used as the subjects of this study. The grape berries were analyzed and tested for morphological indicators, basic physicochemical indicators, color, and phenolic and aromatic composition. RESULTS: According to lab results, ‘Hassan’ contained the highest amount of total phenolics; ‘Zuoyouhong’ had the highest solids and total sugar content; ‘Shuanghong’ had the most elevated total acid and anthocyanin content; ‘Zijngganlu’ had the highest tannin content and acid fixation ratio; Seventy-one volatile compounds were detected in nine grape varieties. CONCLUSIONS: Each of the nine grape varieties has a distinctive flavor, and because of this, grape processing products with regional flavors can be created. The same offer valuable data for future scientific grape resource collection, conservation, and exploitation.',\n",
       " 'Author: Yiming Yang Title: Aligning Large Multimodal Models with Factually Augmented RLHF Publication year: 2023 Coauthors: Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liangyan Gui, Yu-Xiong Wang, Yiming Yang, K. Keutzer, Trevor Darrell Abstract: Large Multimodal Models (LMM) are built across modalities and the misalignment between two modalities can result in\"hallucination\", generating textual outputs that are not grounded by the multimodal information in context. To address the multimodal misalignment issue, we adapt the Reinforcement Learning from Human Feedback (RLHF) from the text domain to the task of vision-language alignment, where human annotators are asked to compare two responses and pinpoint the more hallucinated one, and the vision-language model is trained to maximize the simulated human rewards. We propose a new alignment algorithm called Factually Augmented RLHF that augments the reward model with additional factual information such as image captions and ground-truth multi-choice options, which alleviates the reward hacking phenomenon in RLHF and further improves the performance. We also enhance the GPT-4-generated training data (for vision instruction tuning) with previously available human-written image-text pairs to improve the general capabilities of our model. To evaluate the proposed approach in real-world scenarios, we develop a new evaluation benchmark MMHAL-BENCH with a special focus on penalizing hallucinations. As the first LMM trained with RLHF, our approach achieves remarkable improvement on the LLaVA-Bench dataset with the 94% performance level of the text-only GPT-4 (while previous best methods can only achieve the 87% level), and an improvement by 60% on MMHAL-BENCH over other baselines. We opensource our code, model, data at https://llava-rlhf.github.io.',\n",
       " 'Author: Yiming Yang Title: Experimental and numerical research on the static behavior of locally corroded OSBD Publication year: 2023 Coauthors: J. Peng, Yi Liu, Yiming Yang, Yadong Zhou, Longzhen Xie ',\n",
       " 'Author: Yiming Yang Title: Syncretic Space Learning Network for NIR-VIS Face Recognition Publication year: 2023 Coauthors: Yiming Yang, Weipeng Hu, Haifeng Hu Abstract: To overcome the technical bottleneck of face recognition in low-light scenarios, Near-InfraRed and VISible (NIR-VIS) heterogeneous face recognition is proposed for matching well-lit VIS faces with poorly lit NIR faces. Current cross-modal synthesis methods visually convert the NIR modality to the VIS modality and then perform face matching in the VIS modality. However, using a heavyweight GAN network on unpaired NIR-VIS faces may lead to high synthesis difficulty, low inference efficiency, and other problems. To alleviate the above problems, we simultaneously synthesize NIR and VIS images into modality-independent syncretic images and propose a novel syncretic space learning (SSL) model to eliminate the modal gap. First, Syncretic Modality Generator (SMG) synthesizes NIR and VIS images into syncretic images using channel-level convolution with a shallow CNN. In particular, the discriminative structural information is well preserved and the face quality can be further improved with small modal variations in a self-supervised learning manner. Second, Modality-adversarial Syncretic space Learning (MSL) projects NIR and VIS images into the syncretic space by a syncretic-modality adversarial learning strategy with syncretic pattern guided objective, so the modal gap of NIR-VIS faces can be effectively reduced. Finally, the Syncretic Distribution Consistency (SDC) constructed by NIR-syncretic, syncretic-syncretic, and VIS-syncretic consistency can enhance the intra-class compactness and learn discriminative representations. Extensive experiments on three challenging datasets demonstrate the effectiveness of the SSL method.',\n",
       " 'Author: Yiming Yang Title: An Experimental Study on Secondary Transfer Performances of Prestress after Anchoring Failure of Steel Wire Strands Publication year: 2023 Coauthors: Rihua Yang, Yiming Yang, Xuhui Zhang, Xinzhong Wang Abstract: To understand the secondary transfer performances of residual prestress after the anchoring failure of end-anchored steel wire strands due to corrosion fracture, six steel wire strand components of post-tensioning prestress were designed and fabricated. One-side fast corrosion was applied to the steel wire strand components using the electrochemical method until anchoring failure was reached. The sphere of influence, stress changes, and the retraction and swelling effect of broken beams after failure were investigated. The influences of factors such as concrete strength, stirrup area, and the length of the component on the secondary transfer length of residual prestress were discussed. Based on the deformation relationship between prestressed steel wire strands and concrete in the stress transfer zone, a stress equation was established and solved through a bond constitutive model. A prediction model of the effective stress transfer length of prestressed steel wire strand after failure was proposed. The results demonstrated that residual prestress can have a secondary transfer after the corrosion fracture of end-anchored steel wire strands, but some effective prestress may be lost. Moreover, the loss of prestress is inversely proportional to concrete compressive strength. When the specimens are relatively short, the prestress loss increases significantly. Concrete strength has significant influences on the length of secondary transfer. The proposed simplified calculation method of the secondary transfer length of residual prestress has a relatively high accuracy, with an average error of 2.9% and a maximum error of 5.2%.',\n",
       " 'Author: Yiming Yang Title: Core loss analysis of soft magnetic composite under non-sinusoidal excitation based on finite element models Publication year: 2023 Coauthors: Lei Zhao, Chengcheng Liu, Youhua H. Wang, Yiming Yang Abstract: Due to the effect of higher harmonics on magnetic properties under actual complex operating conditions, the accurate calculation of core losses of soft magnetic composites (SMC) is complicated. First, this paper improves the existing SMC model by introducing a correction factor to correct the hysteresis loss coefficient so that the model can consider the local variation characteristics of the magnetic density waveform and then calculate the core loss under different harmonic excitation. Then, the influence of skin effect and inhomogeneous flux density within the ring sample model is analyzed. Finally, to validate the improved model, it is compared with other models in the reference based on experimental measurements, respectively. The results show that the core loss calculated by the improved model is closer to the experimental results under different harmonic excitations. In addition, the applicability of the improved SMC model under triangular and square wave excitations is also verified by the derivation of the equations.',\n",
       " 'Author: Yiming Yang Title: Distributed cooperative dual closed loop velocity-attitude consensus controller for rendezvous of the underactuated AUV swarm in 3-dimensional space Publication year: 2023 Coauthors: Yu Zhang, W. Zhang, G. Xia, Yiming Yang, Yan-luan Zheng, Peiyu Han ',\n",
       " 'Author: Yiming Yang Title: MRI Features for Predicting Microvascular Invasion and Postoperative Recurrence in Hepatocellular Carcinoma Without Peritumoral Hypointensity Publication year: 2023 Coauthors: Zhiyuan Chen, Xiaohuan Li, Yu Zhang, Yiming Yang, Yan Zhang, Dongjing Zhou, Yu Yang, Shuping Zhang, Yupin Liu Abstract: Purpose To identify MRI features of hepatocellular carcinoma (HCC) that predict microvascular invasion (MVI) and postoperative intrahepatic recurrence in patients without peritumoral hepatobiliary phase (HBP) hypointensity. Patients and Methods One hundred and thirty patients with HCC who underwent preoperative gadoxetate-enhanced MRI and curative hepatic resection were retrospectively reviewed. Two radiologists reviewed all preoperative MR images and assessed the radiological features of HCCs. The ability of peritumoral HBP hypointensity to identify MVI and intrahepatic recurrence was analyzed. We then assessed the MRI features of HCC that predicted the MVI and intrahepatic recurrence-free survival (RFS) in the subgroup without peritumoral HBP hypointensity. Finally, a two-step flowchart was constructed to assist in clinical decision-making. Results Peritumoral HBP hypointensity (odds ratio, 3.019; 95% confidence interval: 1.071–8.512; P=0.037) was an independent predictor of MVI. The sensitivity, specificity, positive predictive value, negative predictive value, and AUROC of peritumoral HBP hypointensity in predicting MVI were 23.80%, 91.04%, 71.23%, 55.96%, and 0.574, respectively. Intrahepatic RFS was significantly shorter in patients with peritumoral HBP hypointensity (P<0.001). In patients without peritumoral HBP hypointensity, the only significant difference between MVI-positive and MVI-negative HCCs was the presence of a radiological capsule (P=0.038). Satellite nodule was an independent risk factor for intrahepatic RFS (hazard ratio,3.324; 95% CI: 1.733–6.378; P<0.001). The high-risk HCC detection rate was significantly higher when using the two-step flowchart that incorporated peritumoral HBP hypointensity and satellite nodule than when using peritumoral HBP hypointensity alone (P<0.001). Conclusion In patients without peritumoral HBP hypointensity, a radiological capsule is useful for identifying MVI and satellite nodule is an independent risk factor for intrahepatic RFS.',\n",
       " 'Author: Yiming Yang Title: Impact of local governments’ construction land allocation strategies on innovation-driven development of China Publication year: 2023 Coauthors: Jian Wang, Shangui Peng, Yuhao Feng, Yiming Yang, Qun Wu ',\n",
       " 'Author: Yiming Yang Title: PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification Publication year: 2023 Coauthors: Yau-Shian Wang, Ta-Chung Chi, Ruohong Zhang, Yiming Yang Abstract: We present PESCO, a novel contrastive learning framework that substantially improves the performance of zero-shot text classification. We formulate text classification as a neural text retrieval problem where each document is treated as a query, and the system learns the mapping from each query to the relevant class labels by (1) adding prompts to enhance label retrieval, and (2) using retrieved labels to enrich the training set in a self-training loop of contrastive learning. PESCO achieves state-of-the-art performance on four benchmark text classification datasets. On DBpedia, we achieve 98.5% accuracy without any labeled data, which is close to the fully-supervised result. Extensive experiments and analyses show all the components of PESCO are necessary for improving the performance of zero-shot text classification.',\n",
       " \"Author: Yiming Yang Title: Learning Performance-Improving Code Edits Publication year: 2023 Coauthors: Aman Madaan, Alex Shypula, Uri Alon, Milad Hashemi, Parthasarathy Ranganathan, Yiming Yang, Graham Neubig, A. Yazdanbakhsh Abstract: The waning of Moore's Law has shifted the focus of the tech industry towards alternative methods for continued performance gains. While optimizing compilers are a standard tool to help increase program efficiency, programmers continue to shoulder much responsibility in crafting and refactoring code with better performance characteristics. In this paper, we investigate the ability of large language models (LLMs) to suggest functionally correct, performance improving code edits. We hypothesize that language models can suggest such edits in ways that would be impractical for static analysis alone. We investigate these questions by curating a large-scale dataset of Performance-Improving Edits, PIE. PIE contains trajectories of programs, where a programmer begins with an initial, slower version and iteratively makes changes to improve the program's performance. We use PIE to evaluate and improve the capacity of large language models. Specifically, use examples from PIE to fine-tune multiple variants of CODEGEN, a billion-scale Transformer-decoder model. Additionally, we use examples from PIE to prompt OpenAI's CODEX using a few-shot prompting. By leveraging PIE, we find that both CODEX and CODEGEN can generate performance-improving edits, with speedups of more than 2.5x for over 25% of the programs, for C++ and Python, even after the C++ programs were compiled using the O3 optimization level. Crucially, we show that PIE allows CODEGEN, an open-sourced and 10x smaller model than CODEX, to match the performance of CODEX on this challenging task. Overall, this work opens new doors for creating systems and methods that can help programmers write efficient code.\",\n",
       " 'Author: Yiming Yang Title: Generation-driven Contrastive Self-training for Zero-shot Text Classification with Instruction-tuned GPT Publication year: 2023 Coauthors: Ruohong Zhang, Yau-Shian Wang, Yiming Yang Abstract: Moreover, GPT-based zero-shot classification models tend to make independent predictions over test instances, which can be sub-optimal as the instance correlations and the decision boundaries in the target space are ignored. To address these difficulties and limitations, we propose a new approach to zero-shot text classification, namely \\\\ourmodelshort, which leverages the strong generative power of GPT to assist in training a smaller, more adaptable, and efficient sentence encoder classifier with contrastive self-training. Specifically, GenCo applies GPT in two ways: firstly, it generates multiple augmented texts for each input instance to enhance the semantic embedding of the instance and improve the mapping to relevant labels; secondly, it generates augmented texts conditioned on the predicted label during self-training, which makes the generative process tailored to the decision boundaries in the target space. In our experiments, GenCo outperforms previous state-of-the-art methods on multiple benchmark datasets, even when only limited in-domain text data is available.',\n",
       " 'Author: Yiming Yang Title: Self-Refine: Iterative Refinement with Self-Feedback Publication year: 2023 Coauthors: Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, S. Welleck, Bodhisattwa Prasad Majumder, Shashank Gupta, A. Yazdanbakhsh, Peter Clark Abstract: Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ~20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach.',\n",
       " 'Author: Yiming Yang Title: Long-tailed Extreme Multi-label Text Classification by the Retrieval of Generated Pseudo Label Descriptions Publication year: 2023 Coauthors: Ruohong Zhang, Yau-Shian Wang, Yiming Yang, Donghan Yu, Tom Vu, Li Lei Abstract: Extreme Multi-label Text Classification (XMTC) has been a tough challenge in machine learning research and applications due to the sheer sizes of the label spaces and the severe data scarcity problem associated with the long tail of rare labels in highly skewed distributions. This paper addresses the challenge of tail label prediction by leveraging the power of dense neural retrieval model in mapping input documents (as queries) to relevant label descriptions. To further enhance the quality of label descriptions, we propose to generate pseudo label descriptions from a trained bag-of-words (BoW) classifier, which demonstrates better classification performance under severe scarce data conditions.The proposed approach achieves the state-of-the-art (SOTA) performance of overall label prediction on XMTC benchmark datasets and especially outperforms the SOTA models in the tail label prediction. We also provide a theoretical analysis for relating the BoW and neural models w.r.t. performance lower bound.',\n",
       " 'Author: Yiming Yang Title: Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers Publication year: 2023 Coauthors: K. Choromanski, Shanda Li, Valerii Likhosherstov, Kumar Avinava Dubey, Shengjie Luo, Di He, Yiming Yang, Tamás Sarlós, Thomas Weingarten, Adrian Weller Abstract: We propose a new class of linear Transformers called FourierLearner-Transformers (FLTs), which incorporate a wide range of relative positional encoding mechanisms (RPEs). These include regular RPE techniques applied for nongeometric data, as well as novel RPEs operating on the sequences of tokens embedded in higher-dimensional Euclidean spaces (e.g. point clouds). FLTs construct the optimal RPE mechanism implicitly by learning its spectral representation. As opposed to other architectures combining efficient low-rank linear attention with RPEs, FLTs remain practical in terms of their memory usage and do not require additional assumptions about the structure of the RPE-mask. FLTs allow also for applying certain structural inductive bias techniques to specify masking strategies, e.g. they provide a way to learn the so-called local RPEs introduced in this paper and providing accuracy gains as compared with several other linear Transformers for language modeling. We also thoroughly tested FLTs on other data modalities and tasks, such as: image classification and 3D molecular modeling. For 3D-data FLTs are, to the best of our knowledge, the first Transformers architectures providing RPE-enhanced linear attention.',\n",
       " 'Author: Yiming Yang Title: Efficient Temporal Sentence Grounding in Videos with Multi-Teacher Knowledge Distillation Publication year: 2023 Coauthors: Renjie Liang, Yiming Yang, Hui Lu, Li Li Abstract: Temporal Sentence Grounding in Videos (TSGV) aims to detect the event timestamps described by the natural language query from untrimmed videos. This paper discusses the challenge of achieving efficient computation in TSGV models while maintaining high performance. Most existing approaches exquisitely design complex architectures to improve accuracy with extra layers and loss, suffering from inefficiency and heaviness. Although some works have noticed that, they only make an issue of feature fusion layers, which can hardly enjoy the highspeed merit in the whole clunky network. To tackle this problem, we propose a novel efficient multi-teacher model (EMTM) based on knowledge distillation to transfer diverse knowledge from both heterogeneous and isomorphic networks. Specifically, We first unify different outputs of the heterogeneous models into one single form. Next, a Knowledge Aggregation Unit (KAU) is built to acquire high-quality integrated soft labels from multiple teachers. After that, the KAU module leverages the multi-scale video and global query information to adaptively determine the weights of different teachers. A Shared Encoder strategy is then proposed to solve the problem that the student shallow layers hardly benefit from teachers, in which an isomorphic teacher is collaboratively trained with the student to align their hidden states. Extensive experimental results on three popular TSGV benchmarks demonstrate that our method is both effective and efficient without bells and whistles.',\n",
       " 'Author: Yiming Yang Title: Active Retrieval Augmented Generation Publication year: 2023 Coauthors: Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig Abstract: Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.',\n",
       " 'Author: Yiming Yang Title: Retrieval-Enhanced Generative Model for Large-Scale Knowledge Graph Completion Publication year: 2023 Coauthors: Donghan Yu, Yiming Yang Abstract: The task of knowledge graph completion (KGC) is of great importance. To achieve scalability when dealing with large-scale knowledge graphs, recent works formulate KGC as a sequence-to-sequence process, where the incomplete triplet (input) and the missing entity (output) are both verbalized as text sequences. However, inference with these methods relies solely on the model parameters for implicit reasoning and neglects the use of KG itself, which limits the performance since the model lacks the capacity to memorize a vast number of triplets. To tackle this issue, we introduce ReSKGC, a Retrieval-enhanced Seq2seq KGC model, which selects semantically relevant triplets from the KG and uses them as evidence to guide output generation with explicit reasoning. Our method has demonstrated state-of-the-art performance on benchmark datasets Wikidata5M and WikiKG90Mv2, which contain about 5M and 90M entities, respectively.',\n",
       " \"Author: Yiming Yang Title: Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs Publication year: 2023 Coauthors: Pranjal Aggarwal, Aman Madaan, Yiming Yang, Mausam Abstract: A popular approach for improving the correctness of output from large language models (LLMs) is Self-Consistency - poll the LLM multiple times and output the most frequent solution. Existing Self-Consistency techniques always generate a constant number of samples per question, where a better approach will be to non-uniformly distribute the available budget based on the amount of agreement in the samples generated so far. In response, we introduce Adaptive-Consistency, a cost-efficient, model-agnostic technique that dynamically adjusts the number of samples per question using a lightweight stopping criterion. Our experiments over 17 reasoning and code generation datasets and three LLMs demonstrate that Adaptive-Consistency reduces sample budget by up to 7.9 times with an average accuracy drop of less than 0.1%. Our code and data are available at https://www.sample-step-by-step.info\",\n",
       " \"Author: Yiming Yang Title: Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision Publication year: 2023 Coauthors: Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David D. Cox, Yiming Yang, Chuang Gan Abstract: Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised fine-tuning (SFT) with human annotations and reinforcement learning from human feedback (RLHF) to align the output of large language models (LLMs) with human intentions, ensuring they are helpful, ethical, and reliable. However, this dependence can significantly constrain the true potential of AI-assistant agents due to the high cost of obtaining human supervision and the related issues on quality, reliability, diversity, self-consistency, and undesirable biases. To address these challenges, we propose a novel approach called SELF-ALIGN, which combines principle-driven reasoning and the generative power of LLMs for the self-alignment of AI agents with minimal human supervision. Our approach encompasses four stages: first, we use an LLM to generate synthetic prompts, and a topic-guided method to augment the prompt diversity; second, we use a small set of human-written principles for AI models to follow, and guide the LLM through in-context learning from demonstrations (of principles application) to produce helpful, ethical, and reliable responses to user's queries; third, we fine-tune the original LLM with the high-quality self-aligned responses so that the resulting model can generate desirable responses for each query directly without the principle set and the demonstrations anymore; and finally, we offer a refinement step to address the issues of overly-brief or indirect responses. Applying SELF-ALIGN to the LLaMA-65b base language model, we develop an AI assistant named Dromedary. With fewer than 300 lines of human annotations (including<200 seed prompts, 16 generic principles, and 5 exemplars for in-context learning). Dromedary significantly surpasses the performance of several state-of-the-art AI systems, including Text-Davinci-003 and Alpaca, on benchmark datasets with various settings.\",\n",
       " 'Author: Yiming Yang Title: Policy Representation via Diffusion Probability Model for Reinforcement Learning Publication year: 2023 Coauthors: Long Yang, Zhixiong Huang, Fenghao Lei, Yucun Zhong, Yiming Yang, Cong Fang, Shiting Wen, Binbin Zhou, Zhouchen Lin Abstract: Popular reinforcement learning (RL) algorithms tend to produce a unimodal policy distribution, which weakens the expressiveness of complicated policy and decays the ability of exploration. The diffusion probability model is powerful to learn complicated multimodal distributions, which has shown promising and potential applications to RL. In this paper, we formally build a theoretical foundation of policy representation via the diffusion probability model and provide practical implementations of diffusion policy for online model-free RL. Concretely, we character diffusion policy as a stochastic process, which is a new approach to representing a policy. Then we present a convergence guarantee for diffusion policy, which provides a theory to understand the multimodality of diffusion policy. Furthermore, we propose the DIPO which is an implementation for model-free online RL with DIffusion POlicy. To the best of our knowledge, DIPO is the first algorithm to solve model-free online RL problems with the diffusion model. Finally, extensive empirical results show the effectiveness and superiority of DIPO on the standard continuous control Mujoco benchmark.',\n",
       " \"Author: Yiming Yang Title: Self-Imagine: Effective Unimodal Reasoning with Multimodal Models using Self-Imagination Publication year: 2024 Coauthors: Syeda Nahida Akter, Aman Madaan, Sangwu Lee, Yiming Yang, Eric Nyberg Abstract: The potential of Vision-Language Models (VLMs) often remains underutilized in handling complex text-based problems, particularly when these problems could benefit from visual representation. Resonating with humans' ability to solve complex text-based problems by (1) creating a visual diagram from the problem and (2) deducing what steps they need to take to solve it, we propose Self-Imagine. We leverage a single Vision-Language Model (VLM) to generate a structured representation of the question using HTML, then render the HTML as an image, and finally use the same VLM to answer the question using both the question and the image. Our approach does not require any additional training data or training. We evaluate our approach on three mathematics tasks and nine general-purpose reasoning tasks using state-of-the-art (LLAVA-1.5 and GEMINI PRO) VLMs. Our approach boosts the performance of LLAVA-1.5 and GEMINI PRO on all math tasks (on average GSM8K: +3.1%; ASDIV: +3.2%; SVAMP: +6.9%) and the majority of the general-purpose reasoning tasks by 3.2% to 6.0% on average.\",\n",
       " \"Author: Yiming Yang Title: Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning with LLMs Publication year: 2023 Coauthors: Pranjal Aggarwal, Aman Madaan, Yiming Yang, Mausam Abstract: A popular approach for improving the correctness of output from large language models (LLMs) is Self-Consistency – poll the LLM multiple times and output the most frequent so-lution. Existing Self-Consistency techniques always draw a constant number of samples per question, where a better approach will be to non-uniformly distribute the available bud-get based on the amount of agreement in the samples drawn so far. In response, we introduce Adaptive-Consistency, a cost-efﬁcient, model-agnostic technique that dynamically adjusts the number of samples per question using a lightweight stopping criterion. Our experiments over 13 datasets and two LLMs demonstrate that Adaptive-Consistency reduces sample budget by up to 6.0 times with an average accuracy drop of less than 0.1%. 1\",\n",
       " 'Author: Yiming Yang Title: AutoMix: Automatically Mixing Language Models Publication year: 2023 Coauthors: Aman Madaan, Pranjal Aggarwal, Ankit Anand, Srividya Pranavi Potharaju, Swaroop Mishra, Pei Zhou, Aditya Gupta, Dheeraj Rajagopal, Karthik Kappaganthu, Yiming Yang, Shyam Upadhyay, Mausam, Manaal Faruqui Abstract: Large language models (LLMs) are now available in various sizes and configurations from cloud API providers. While this diversity offers a broad spectrum of choices, effectively leveraging the options to optimize computational cost and performance remains challenging. In this work, we present AutoMix, an approach that strategically routes queries to larger LMs, based on the approximate correctness of outputs from a smaller LM. Central to AutoMix is a few-shot self-verification mechanism, which estimates the reliability of its own outputs without requiring training. Given that verifications can be noisy, we employ a meta verifier in AutoMix to refine the accuracy of these assessments. Our experiments using LLAMA2-13/70B, on five context-grounded reasoning datasets demonstrate that AutoMix surpasses established baselines, improving the incremental benefit per cost by up to 89%. Our code and data are available at https://github.com/automix-llm/automix.',\n",
       " 'Author: Yiming Yang Title: CompleQA: Benchmarking the Impacts of Knowledge Graph Completion Methods on Question Answering Publication year: 2023 Coauthors: Donghan Yu, Yu Gu, Chenyan Xiong, Yiming Yang ',\n",
       " 'Author: Yiming Yang Title: KEEC: Embed to Control on An Equivariant Geometry Publication year: 2023 Coauthors: Xiaoyuan Cheng, Yiming Yang, Wei Jiang, Yukun Hu Abstract: This paper investigates how representation learning can enable optimal control in unknown and complex dynamics, such as chaotic and non-linear systems, without relying on prior domain knowledge of the dynamics. The core idea is to establish an equivariant geometry that is diffeomorphic to the manifold defined by a dynamical system and to perform optimal control within this corresponding geometry, which is a non-trivial task. To address this challenge, Koopman Embed to Equivariant Control (KEEC) is proposed for model learning and control. Inspired by Lie theory, KEEC begins by learning a non-linear dynamical system defined on a manifold and embedding trajectories into a Lie group. Subsequently, KEEC formulates an equivariant value function equation in reinforcement learning on the equivariant geometry, ensuring an invariant effect as the value function on the original manifold. By deriving analytical-form optimal actions on the equivariant value function, KEEC theoretically achieves quadratic convergence for the optimal equivariant value function by leveraging the differential information on the equivariant geometry. The effectiveness of KEEC is demonstrated in challenging dynamical systems, including chaotic ones like Lorenz-63. Notably, our results show that isometric functions, which maintain the compactness and completeness of geometry while preserving metric and differential information, consistently outperform loss functions lacking these characteristics.',\n",
       " 'Author: Yiming Yang Title: A Study on Semantic Understanding of Large Language Models from the Perspective of Ambiguity Resolution Publication year: 2023 Coauthors: Shuguang Yang, Feipeng Chen, Yiming Yang, Zude Zhu ',\n",
       " \"Author: Yiming Yang Title: Functional Interpolation for Relative Positions Improves Long Context Transformers Publication year: 2023 Coauthors: Shanda Li, Chong You, Guru Guruganesh, J. Ainslie, Santiago Ontanon, M. Zaheer, Sumit K. Sanghai, Yiming Yang, Sanjiv Kumar, Srinadh Bhojanapalli Abstract: Preventing the performance decay of Transformers on inputs longer than those used for training has been an important challenge in extending the context length of these models. Though the Transformer architecture has fundamentally no limits on the input sequence lengths it can process, the choice of position encoding used during training can limit the performance of these models on longer inputs. We propose a novel functional relative position encoding with progressive interpolation, FIRE, to improve Transformer generalization to longer contexts. We theoretically prove that this can represent some of the popular relative position encodings, such as T5's RPE, Alibi, and Kerple. We next empirically show that FIRE models have better generalization to longer contexts on both zero-shot language modeling and long text benchmarks.\",\n",
       " 'Author: Yiming Yang Title: In-Context Principle Learning from Mistakes Publication year: 2024 Coauthors: Tianjun Zhang, Aman Madaan, Luyu Gao, Steven Zheng, Swaroop Mishra, Yiming Yang, Niket Tandon, Uri Alon Abstract: In-context learning (ICL, also known as few-shot prompting) has been the standard method of adapting LLMs to downstream tasks, by learning from a few input-output examples. Nonetheless, all ICL-based approaches only learn from correct input-output pairs. In this paper, we revisit this paradigm, by learning more from the few given input-output examples. We introduce Learning Principles (LEAP): First, we intentionally induce the model to make mistakes on these few examples; then we reflect on these mistakes, and learn explicit task-specific\"principles\"from them, which help solve similar problems and avoid common mistakes; finally, we prompt the model to answer unseen test questions using the original few-shot examples and these learned general principles. We evaluate LEAP on a wide range of benchmarks, including multi-hop question answering (Hotpot QA), textual QA (DROP), Big-Bench Hard reasoning, and math problems (GSM8K and MATH); in all these benchmarks, LEAP improves the strongest available LLMs such as GPT-3.5-turbo, GPT-4, GPT-4 turbo and Claude-2.1. For example, LEAP improves over the standard few-shot prompting using GPT-4 by 7.5% in DROP, and by 3.3% in HotpotQA. Importantly, LEAP does not require any more input or examples than the standard few-shot prompting settings.',\n",
       " 'Author: Yiming Yang Title: A Self-enhancement Approach for Domain-specific Chatbot Training via Knowledge Mining and Digest Publication year: 2023 Coauthors: Ruohong Zhang, Luyu Gao, Chen Zheng, Zhen Fan, Guokun Lai, Zheng Zhang, Fangzhou Ai, Yiming Yang, Hongxia Yang Abstract: Large Language Models (LLMs), despite their great power in language generation, often encounter challenges when dealing with intricate and knowledge-demanding queries in specific domains. This paper introduces a novel approach to enhance LLMs by effectively extracting the relevant knowledge from domain-specific textual sources, and the adaptive training of a chatbot with domain-specific inquiries. Our two-step approach starts from training a knowledge miner, namely LLMiner, which autonomously extracts Question-Answer pairs from relevant documents through a chain-of-thought reasoning process. Subsequently, we blend the mined QA pairs with a conversational dataset to fine-tune the LLM as a chatbot, thereby enriching its domain-specific expertise and conversational capabilities. We also developed a new evaluation benchmark which comprises four domain-specific text corpora and associated human-crafted QA pairs for testing. Our model shows remarkable performance improvement over generally aligned LLM and surpasses domain-adapted models directly fine-tuned on domain corpus. In particular, LLMiner achieves this with minimal human intervention, requiring only 600 seed instances, thereby providing a pathway towards self-improvement of LLMs through model-synthesized training data.',\n",
       " 'Author: Yiming Yang Title: SALMON: Self-Alignment with Principle-Following Reward Models Publication year: 2023 Coauthors: Zhiqing Sun, Yikang Shen, Hongxin Zhang, Qinhong Zhou, Zhenfang Chen, David D. Cox, Yiming Yang, Chuang Gan Abstract: Supervised Fine-Tuning (SFT) on response demonstrations combined with Reinforcement Learning from Human Feedback (RLHF) constitutes a powerful paradigm for aligning LLM-based AI agents. However, a significant limitation of such an approach is its dependency on high-quality human annotations, making its application to intricate tasks challenging due to difficulties in obtaining consistent response demonstrations and in-distribution response preferences. This paper presents a novel approach, namely SALMON (Self-ALignMent with principle-fOllowiNg reward models), to align base language models with minimal human supervision, using only a small set of human-defined principles, yet achieving superior performance. Central to our approach is a principle-following reward model. Trained on synthetic preference data, this model can generate reward scores based on arbitrary human-defined principles. By merely adjusting these principles during the RL training phase, we gain full control over the preferences with the reward model, subsequently influencing the behavior of the RL-trained policies, and eliminating the reliance on the collection of online human preferences. Applying our method to the LLaMA-2-70b base language model, we developed an AI assistant named Dromedary-2. With only 6 exemplars for in-context learning and 31 human-defined principles, Dromedary-2 significantly surpasses the performance of several state-of-the-art AI systems, including LLaMA-2-Chat-70b, on various benchmark datasets. We have open-sourced the code and model weights to encourage further research into aligning LLM-based AI agents with enhanced supervision efficiency, improved controllability, and scalable oversight.',\n",
       " 'Author: Yiming Yang Title: Generalized Robot Dynamics Learning and Gen2Real Transfer Publication year: 2023 Coauthors: Dengpeng Xing, Yiming Yang, Zechang Wang, Jiale Li, Bo Xu Abstract: Acquiring dynamics is critical for robot learning and is fundamental to planning and control. This paper concerns two fundamental questions: How can we learn a model that covers massive, diverse robot dynamics? Can we construct a model that lifts the data-collection pain and domain expertise required for building specific robot models? We learn the dynamics involved in a dataset containing a large number of serial articulated robots and propose a new concept, “Gen2Real”, to transfer simulated, generalized models to physical, specific robots. We generate a large-scale dataset by randomizing dynamics parameters, topology configurations, and model dimensions, which, in sequence, correspond to different properties, connections, and numbers of robot links. A structure modified from the generative pre-trained transformer is applied to approximate the dynamics of massive heterogeneous robots. In Gen2Real, we transfer the pre-trained model to a target robot using distillation, for the sake of real-time computation. The results demonstrate the superiority of the proposed method in terms of its accuracy in learning a tremendous amount of robot dynamics and its generality to transfer to different robots.',\n",
       " 'Author: Yiming Yang Title: Cardsformer: Grounding Language to Learn a Generalizable Policy in Hearthstone Publication year: 2023 Coauthors: Wannian Xia, Yiming Yang, Jingqing Ruan, Dengpeng Xing, Bo Xu ']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "publications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "with open('/zfsauton2/home/yifuc/11711-RAG/data/cmu/faculty_publication.json', 'w') as f:\n",
    "    json.dump(publications, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMU Schedule of class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1699649/1010705020.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "urls = [r'https://enr-apps.as.cmu.edu/assets/SOC/sched_layout_spring.htm', r'https://enr-apps.as.cmu.edu/assets/SOC/sched_layout_summer_1.htm', r'https://enr-apps.as.cmu.edu/assets/SOC/sched_layout_summer_2.htm', r'https://enr-apps.as.cmu.edu/assets/SOC/sched_layout_fall.htm']\n",
    "datas = []\n",
    "for url in urls:\n",
    "    tables = pd.read_html(url) # Returns list of all tables on page\n",
    "    datas.append(tables[0]) # Select table of interest\n",
    "order = ['Spring', 'Summer 1/Summer All', 'Summer 2', 'Fall']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "days_expand = {\n",
    "    'M': 'Monday',\n",
    "    'T': 'Tuesday',\n",
    "    'W': 'Wednesday',\n",
    "    'R': 'Thursday',\n",
    "    'F': 'Friday',\n",
    "    'S': 'Saturday',\n",
    "    'U': 'Sunday'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1699649/116906015.py:10: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  table.fillna(method='ffill', inplace=True)\n",
      "/tmp/ipykernel_1699649/116906015.py:10: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  table.fillna(method='ffill', inplace=True)\n",
      "/tmp/ipykernel_1699649/116906015.py:10: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  table.fillna(method='ffill', inplace=True)\n",
      "/tmp/ipykernel_1699649/116906015.py:10: FutureWarning: DataFrame.fillna with 'method' is deprecated and will raise in a future version. Use obj.ffill() or obj.bfill() instead.\n",
      "  table.fillna(method='ffill', inplace=True)\n"
     ]
    }
   ],
   "source": [
    "courses = []\n",
    "for table, semester in zip(datas, order):\n",
    "    #drop first row and last column\n",
    "    table = table.iloc[1:, :-1]\n",
    "    table.columns = table.iloc[0]\n",
    "    table = table.iloc[1:, :]\n",
    "\n",
    "    #exclude rows where location and title is nan\n",
    "    table = table[table['Location'].notna() | table['Title'].notna()]\n",
    "    table.fillna(method='ffill', inplace=True)\n",
    "    table.reset_index(inplace=True, drop=True)\n",
    "\n",
    "    for i in range(len(table)):\n",
    "        course_info = table.iloc[i].to_dict()\n",
    "        s = semester + ' offering: '\n",
    "        days_str = ''\n",
    "        for k, v in course_info.items():\n",
    "            #expand days symbol\n",
    "            if k == 'Days':\n",
    "                if v == 'TBA':\n",
    "                    v = 'To be announced'\n",
    "                else:\n",
    "                    for day in v:\n",
    "                        days_str += days_expand[day] + ', '\n",
    "                    v = days_str[:-2]\n",
    "\n",
    "            #expand lec/sec \n",
    "            if k == 'Lec/Sec':\n",
    "                if v == 'Lec':\n",
    "                    v = 'Lecture'\n",
    "                else:\n",
    "                    v = 'Section ' + v\n",
    "            s += f'{k}: {v} '\n",
    "        courses.append(s)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Spring offering: Course: 48105 Title: Architecture Design Studio: Poiesis Studio 2 Units: 15.0 Lec/Sec: Lecture Days: Monday, Wednesday, Friday,  Begin: 02:00PM End: 04:50PM Bldg/Room: MM A14 Location: Pittsburgh, Pennsylvania Instructor(s): Yang '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "courses[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "with open('/zfsauton2/home/yifuc/11711-RAG/data/cmu/course_schedule.json', 'w') as f:\n",
    "    json.dump(courses, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMU Academic calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "semesters = ['Fall 2023', 'Spring 2024', 'Summer One_All 2024', 'Summer Two 2024']\n",
    "events = []\n",
    "for semester in semesters:\n",
    "    df = pd.read_excel(f'/zfsauton2/home/yifuc/11711-RAG/data/cmu/raw/{semester}.xlsx')\n",
    "    for i in range(len(df)):\n",
    "        date_dict = df.iloc[i].to_dict()\n",
    "        #convert timestamp to string and remove hour, minute, and second\n",
    "        date1 = date_dict['Date'].strftime('%Y-%m-%d')\n",
    "        date2 = date_dict['Date2']\n",
    "        #check for consecutive date\n",
    "        if not str(date2) == 'nan' and not str(date2) == 'NaT':\n",
    "            date2 = date2.strftime('%Y-%m-%d')\n",
    "            date = f'{date1}-{date2}'\n",
    "        else:\n",
    "            date = date1\n",
    "        #create date string \n",
    "        date_string = f'Date: {date}'\n",
    "        for k, v in date_dict.items():\n",
    "            if k not in ['Date', 'Date2', '-']:\n",
    "                date_string += f' {k}: {v}'\n",
    "        date_string = semester + ': ' + date_string\n",
    "        events.append(date_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "with open('/zfsauton2/home/yifuc/11711-RAG/data/cmu/university_calendar.json', 'w') as f:\n",
    "    json.dump(events, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LTI Academic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_521683/3081452667.py:1: DeprecationWarning: \n",
      "Pyarrow will become a required dependency of pandas in the next major release of pandas (pandas 3.0),\n",
      "(to allow more performant data types, such as the Arrow string type, and better interoperability with other libraries)\n",
      "but was not found to be installed on your system.\n",
      "If this would cause problems for you,\n",
      "please provide us feedback at https://github.com/pandas-dev/pandas/issues/54466\n",
      "        \n",
      "  import pandas as pd\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "urls = [r'https://lti.cs.cmu.edu/learn']\n",
    "datas = []\n",
    "for url in urls:\n",
    "    tables = pd.read_html(url) # Returns list of all tables on page\n",
    "    datas.append(tables[0]) # Select table of interest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = r'https://lti.cs.cmu.edu/learn'\n",
    "#parse the first chunk of the html about phd program in lti \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "phd = soup.find_all('div', class_='field-item even')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Carnegie Mellon\\'s School of Computer Science has a centralized online application process. Applications and all supporting documentation for fall admission to any of the LTI\\'s graduate programs must be received by the application deadline. Incomplete applications will not be considered. The application period for Fall 2024 will open on September 6, 2023.\\nFinal Application Deadline\\nDecember 13, 2023 at 3 p.m. EST.\\nCost\\n$100 per program and $80 if the applicant applies before November 29, 2023 at 3 p.m. EST (early deadline).\\nFee Waivers\\nFee waivers may be available in cases of financial hardship. For more information, please refer to the\\xa0School of Computer Science Fee Waiver page.\\nRequirements\\nThe School of Computer Science requires the following for all Ph.D. applications.\\nGRE scores:\\xa0GREs are now optional, but if you want to submit GRE scores:These must be less than five years old. The GRE Subject Test is not required, but is recommended. Our Institution Code is 2074; Department Code is 0402.\\nEnglish Proficiency Requirement:\\xa0If you will be studying on an F-1 or J-1 visa, and English is not a native language for you (native language…meaning spoken at home and from birth), an official copy of an English proficiency score report is required.\\xa0The English proficiency requirement cannot be waived for any reason.\\xa0Find more information under \"Test Scores\" on our\\xa0FAQ\\xa0page.\\nSuccessful applicants will have a minimum TOEFL score of 100. Our Institution Code is 4256; the Department Code is 78.\\nOfficial transcripts from each university you have attended, regardless of whether you received your degree there.\\nCurrent resume.\\nStatement of Purpose.\\nThree letters of recommendation.\\nFor more details on these requirements, please see the SCS Doctoral Admissions page.\\nIn addition to the SCS guidelines, the LTI requires:\\nA short (1-3 minute) video of yourself. There will be a prompt question that you will respond to. You will have three attempts.\\xa0This is not a required part of the application process, but it\\'s strongly suggested.\\nAny outside funding you are receiving must be accompanied by an official award letter.\\nNo incomplete applications will be eligible for consideration.\\nFor specific application/admissions questions, please contact\\xa0Stacey Young.\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phd[3].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "''' \n",
    "Ph.D. in Language and Information Technology: The Ph.D. in LTI focuses on developing the next generation of scientific and entrepreneurial leaders. The first two years of the Ph.D. program are similar to our MLT program. After the second year, you will spend most of your time working closely with your faculty advisor on research that advances the state-of-the-art in computer science.\n",
    "\n",
    "Ph.D. students are expected to publish papers about original research in the most competitive scientific journals and international conference proceedings, and to present their research at conferences and workshops. Most of our Ph.D. graduates become professors and research scientists, while a few have started their own companies.\n",
    "''',\n",
    "''' \n",
    "Ph.D. in Language and Information Technology Requirements: In general, students pursuing a Ph.D. in Language and Information Technologies must\n",
    "\n",
    "    Pass at least 96 units of graduate-level courses.\n",
    "    Satisfy proficiencies in writing, presentation, programming and teaching; and\n",
    "    Propose, write and defend a Ph.D. dissertation (thesis).\n",
    "\n",
    "Students must also attend the LTI Colloquium each semester and satisfy our Research Speaking Requirement.\n",
    "\n",
    "For a detailed breakdown of the above requirements, download and read the PhD Handbook.\n",
    "''',\n",
    "''' \n",
    "Ph.D. in Language and Information Technology Curriculum: In order to obtain your Ph.D. in Language and Information Technologies, you need to pass 96 units (generally, eight courses) of graduate courses that fulfill these requirements:\n",
    "\n",
    "    At least 72 units of LTI courses: Must include one class in each LTI focus area.\n",
    "    At least 24 units of SCS courses.\n",
    "    At least two lab courses in two different research areas.\n",
    "''',\n",
    "''' \n",
    "Ph.D. in Language and Information Technology Admission: \n",
    "\n",
    "Carnegie Mellon's School of Computer Science has a centralized online application process. Applications and all supporting documentation for fall admission to any of the LTI's graduate programs must be received by the application deadline. Incomplete applications will not be considered. The application period for Fall 2024 will open on September 6, 2023.\n",
    "\n",
    "Final Application Deadline\n",
    "\n",
    "December 13, 2023 at 3 p.m. EST.\n",
    "\n",
    "Cost\n",
    "\n",
    "    $100 per program and $80 if the applicant applies before November 29, 2023 at 3 p.m. EST (early deadline).\n",
    "\n",
    "Fee Waivers\n",
    "\n",
    "Fee waivers may be available in cases of financial hardship. For more information, please refer to the School of Computer Science Fee Waiver page.\n",
    "\n",
    "Requirements\n",
    "\n",
    "The School of Computer Science requires the following for all Ph.D. applications.\n",
    "\n",
    "    GRE scores: GREs are now optional, but if you want to submit GRE scores:\n",
    "    These must be less than five years old. The GRE Subject Test is not required, but is recommended. Our Institution Code is 2074; Department Code is 0402.\n",
    "    English Proficiency Requirement: If you will be studying on an F-1 or J-1 visa, and English is not a native language for you (native language…meaning spoken at home and from birth), an official copy of an English proficiency score report is required. The English proficiency requirement cannot be waived for any reason. Find more information under \"Test Scores\" on our FAQ page.\n",
    "    Successful applicants will have a minimum TOEFL score of 100. Our Institution Code is 4256; the Department Code is 78.\n",
    "    Official transcripts from each university you have attended, regardless of whether you received your degree there.\n",
    "    Current resume.\n",
    "    Statement of Purpose.\n",
    "    Three letters of recommendation.\n",
    "\n",
    "For more details on these requirements, please see the SCS Doctoral Admissions page.\n",
    "\n",
    "In addition to the SCS guidelines, the LTI requires:\n",
    "\n",
    "    A short (1-3 minute) video of yourself. There will be a prompt question that you will respond to. You will have three attempts. This is not a required part of the application process, but it's strongly suggested.\n",
    "    Any outside funding you are receiving must be accompanied by an official award letter.\n",
    "\n",
    "No incomplete applications will be eligible for consideration.\n",
    "\n",
    "For specific application/admissions questions, please contact Stacey Young.\n",
    "''',\n",
    "''' \n",
    "Dual-Degree Ph.D. in Language and Information Technologies (Portugal Partnership): The LTI offers a dual-degree Ph.D. in Language and Information Technologies in cooperation with:\n",
    "\n",
    "    Universidade de Aveiro (Ph.D. in Computer Engineering), Universidade do Minho (Ph.D. in Informatics)  and the Universidade do Porto (FCUP, Ph.D. in Computer Science and FEUP, Ph.D. in Computer Science) as part of MAPi;\n",
    "    Universidade de Lisboa, Faculdade de Ciências – FCUL (Ph.D. in Informatics) \n",
    "\n",
    "    Universidade de Lisboa, Instituto Superior Técnico – IST  (Ph.D. in Computer Science and Engineering, Ph.D. in Electrical and Computer Engineering, Ph.D. in Information Security)\n",
    "\n",
    "    Universidade Nova de Lisboa, Faculdade de Ciências e Tecnologia – FCTUNL (Ph.D. in Computer Science)\n",
    "\n",
    "    Universidade de Coimbra, Faculdade de Ciências e Tecnologia – FCTUC (Ph.D. in Information Science and Technology)\n",
    "\n",
    "Students jointly enrolled in the LTI Ph.D program spend a year in Portugal, then two years at Carnegie Mellon taking classes in linguistics, computer science, statistical learning and task orientation.\n",
    "\n",
    "After completing the majority of their academic requirements, students return to Portugal for the next two years to conduct extensive research, ultimately leading to a dissertation topic that will be publicly defended. One adviser from each institution co-supervises their student’s progress and helps to define their final thesis topic.\n",
    "''',\n",
    "''' \n",
    "Dual-Degree Ph.D. in Language and Information Technologies (Portugal Partnership) Requirments: Students participating in the dual-degree program will spend their first year in Portugal, followed by two years in Pittsburgh to complete their coursework. They will complete a maximum of eight courses with a proper balance of focus areas (linguistics, computer science, statistical/learning and task orientation). After that, they will return to Portugal for their last two years, pursuing research and completing their dissertation. For more, see the Carnegie Mellon | Portugal page.\n",
    "''',\n",
    "''' \n",
    "Dual-Degree Ph.D. in Language and Information Technologies (Portugal Partnership) Admission: \n",
    "\n",
    "Students applying to the dual degree program must apply through Carnegie Mellon's online application. In addition to the requirements listed below, prospective students must also contact Stacey Young when applying.\n",
    "\n",
    "Carnegie Mellon's School of Computer Science has a centralized online application process. Applications and all supporting documentation for fall admission to any of the LTI's graduate programs must be received by the application deadline. Incomplete applications will not be considered. The application period for Fall 2024 will open on September 6, 2023.\n",
    "\n",
    "Final Application Deadline\n",
    "\n",
    "December 13, 2023 3:00 p.m. EST.\n",
    "\n",
    "Cost\n",
    "\n",
    "    $100 per program, $80 for applications submitted by November 29, 2023 at 3:00 p.m. EST (early deadline).\n",
    "\n",
    "Fee Waivers\n",
    "\n",
    "Fee waivers may be available in cases of financial hardship, or for participants in select programs. For more information, please refer to the School of Computer Science Fee Waiver page.\n",
    "\n",
    "Requirements\n",
    "\n",
    "The School of Computer Science requires the following for all Ph.D. applications.\n",
    "\n",
    "    GRE scores: These must be less than five years old. The GRE Subject Test is not required, but is recommended. Our Institution Code is 2074; Department Code is 0402.\n",
    "    English Proficiency Requirement: If you will be studying on an F-1 or J-1 visa, and English is not a native language for you (native language…meaning spoken at home and from birth), an official copy of an English proficiency score report is required. The English proficiency requirement cannot be waived for any reason. Find more information under \"Test Scores\" on our FAQ page.\n",
    "    Successful applicants will have a minimum TOEFL score of 100. Our Institution Code is 4256; the Department Code is 78.\n",
    "    Official transcripts from each university you have attended, regardless of whether you received your degree there.\n",
    "    Current resume.\n",
    "    Statement of Purpose.\n",
    "    Three letters of recommendation.\n",
    "\n",
    "For more details on these requirements, please see the SCS Doctoral Admissions page.\n",
    "\n",
    "In addition to the SCS guidelines, the LTI requires:\n",
    "\n",
    "    A short (1-3 minute) video of yourself. There will be a prompt question that you will respond to. You will have three attempts. This is not a required part of the application process, but it's strongly suggested.\n",
    "\n",
    "    Any outside funding you are receiving must be accompanied by an official award letter.\n",
    "\n",
    "No incomplete applications will be eligible for consideration.\n",
    "\n",
    "For specific application/admissions questions, please contact Stacey Young.\n",
    "\n",
    "''',\n",
    "''' \n",
    "Master of Language Technologies: The MLT program prepares students for a research career in academia or industry. In this program, you’ll be immersed in research for two full years. During the academic year, your time will be evenly split between taking courses and doing research with your faculty advisor. Your summer will be devoted entirely to research. Many MLT grads continue on to Ph.D. programs at CMU and other top institutions, while others pursue careers at companies emphasizing research and rapid innovation.\n",
    "''',\n",
    "''' \n",
    "Master of Language Technologies Requirements: \n",
    "\n",
    "The MLT program lasts two years (24 months), and students must complete two summers of research. Students should usually expect to graduate in August of their second year.\n",
    "\n",
    "MLT students take 120 or more course units (about 10 courses), at least 72 of which are LTI courses, and 24 of which are School of Computer Science (SCS) courses. Most of these are 12-unit courses, although lab courses are typically 6 units. Our courses generally assume knowledge of programming and data structures. The remaining units may also be taken from the LTI, or with approval from the faculty advisor, any other senior- or graduate-level course offered at CMU or Pitt.\n",
    "\n",
    "Directed research is another integral part of the MLT program; MLT students carry out directed research during their studies, with guidance from their faculty advisors.\n",
    "\n",
    "Students may also choose to complete an optional MLT thesis. Guidelines can be found in the PDF iconMLT Handbook.\n",
    "\n",
    "''',\n",
    "''' \n",
    "Master of Language Technologies Admission: \n",
    "\n",
    "Carnegie Mellon's School of Computer Science has a centralized online application process. Applications and all supporting documentation for fall admission to any of the LTI's graduate programs must be received by the application deadline. Incomplete applications will not be considered. The application period for Fall 2024 will open on September 6, 2023.\n",
    "\n",
    "*Please note, we no longer require mailed, hard versions of transcripts or test scores at the time of application. Do not mail anything to the admissions office. If you are accepted to a program, you will be given instruction to then mail your materials.\n",
    "\n",
    "Final Application Deadline\n",
    "\n",
    "December 13, 2023 at 3 p.m. EST.\n",
    "\n",
    "Cost\n",
    "\n",
    "    $100 per program, $80 for applications submitted before November 29, 2023 at 3PM EST (early deadline).\n",
    "\n",
    "Fee Waivers\n",
    "\n",
    "Fee waivers may be available in cases of financial hardship, or for participants in select \"pipeline\" programs. For more information, please refer to the School of Computer Science Fee Waiver page.\n",
    "\n",
    "Requirements\n",
    "\n",
    "The School of Computer Science requires the following for all graduate program applications:\n",
    "\n",
    "    GRE scores: GREs are now optional, but if you want to submit GRE scores:\n",
    "    These must be less than five years old. A GRE subject test in science, engineering, computer science, math, etc. is not required, but you may complete one and submit the scores if you wish. Our Institution Code is 2074; Department Code is 0402.\n",
    "    If you will be studying on an F-1 or J-1 visa, and English is not a native language for you (native language…meaning spoken at home and from birth), an official copy of an English proficiency score report is required. The English proficiency requirement cannot be waived for any reason. Find more information under \"Test Scores\" on our FAQ page.\n",
    "    Successful applicants will have a minimum TOEFL score of 100. Our Institution Code is 4256; the Department Code is 78.\n",
    "    Official transcripts from each university you have attended, regardless of whether you received your degree there.\n",
    "    Current resume.\n",
    "    Statement of Purpose.\n",
    "    Three letters of recommendation.\n",
    "\n",
    "For more details on these requirements, please see the SCS Master's Admissions page.\n",
    "\n",
    "In addition to the SCS guidelines, the LTI requires:\n",
    "\n",
    "    A short (1-3 minute) video of yourself. There will be a prompt question that you will respond to. You will have three attempts. This is not a required part of the application process, but it's strongly suggested.\n",
    "\n",
    "    Any outside funding you are receiving must be accompanied by an official award letter.\n",
    "\n",
    "No incomplete applications will be eligible for consideration.\n",
    "\n",
    "For specific application/admissions questions, please contact Kate Schaich.\n",
    "\n",
    "Tuition Rates\n",
    "\n",
    "Tuition is set by the School of Computer Science and can vary by year. Current tuition rates can be found on the Graduate Tuition section of the Student Financial Services website.\n",
    "\n",
    "Financial Aid Resources\n",
    "\n",
    "Research Assistant-ships are occasionally offered by research advisors to current MLT students. These are not guaranteed and vary from semester-to-semester. They fluctuate and are dependent on the funding source, research advisor and MLT student.\n",
    "\n",
    "Student Financial Services has additional information on financial aid and billing / payments. They have a detailed outline of how to apply for financial aid on the Graduate Financial Aid Process page.\n",
    "\n",
    "Enrollment & Finances has additional resource links to assist with financial aid and tuition payments.\n",
    "\n",
    "Graduate Education – Financial Assistance provides resources for current students regarding emergency loans and conference travel grants.\n",
    "\n",
    "''',\n",
    "''' \n",
    "Master of Science in Intelligent Information Systems (MIIS): The Master's in Intelligent Information Systems degree focuses on recognizing and extracting meaning from text, spoken language and video. As an MIIS student, you’ll receive the department’s deepest exposure to content analysis and machine learning. In addition to completing the program’s coursework, you’ll work on directed study projects with your faculty advisor for two semesters; participate in a summer internship; and collaborate with your peers on a semester-long, group-oriented capstone project. This combination of classroom instruction, professional experience, and using new skills in significant projects with world-class colleagues will help prepare you for a successful career in industry or government. Our alumni have gone on to exciting careers at places like Apple, IBM and Google, and most have job offers within six weeks of graduation.\n",
    "''',\n",
    "''' \n",
    "Master of Science in Intelligent Information Systems (MIIS) Requirements: \n",
    "\n",
    "The Intelligent Information Systems degree offers students the flexibility to create their own course of study in consultation with their advisor. \n",
    "\n",
    "MIIS students gain three types of practical experience: software development supervised by their advisor (24 units equivalent to two courses); a summer internship (which can be waived for students that have sufficient prior professional experience); and a capstone project executed in a group of peers (42 units equivalent to three 12-unit courses and one 6-unit course). This combination is proven to help IIS students to broaden their skills quickly. The MIIS degree is offered in two options:\n",
    "\n",
    "Option 1. Standard MIIS degree (MIIS-16) - A 16-month track that is completed in three academic semesters (fall, spring, fall) and a summer internship.  \n",
    "\n",
    "Option 2. MIIS: Advanced Study degree (MIIS-21) - A 21-month track that is completed in four academic semesters (fall, spring, fall, spring) and a summer internship.\n",
    "\n",
    "MIIS: Advanced Study track offers indepth degree in one of the following areas of concentration:\n",
    "\n",
    "    Human Language for Language Technologies\n",
    "    Language Technology Application\n",
    "    Machine Learning for Language Technologies\n",
    "\n",
    "Part-time education option is available in some cases. \n",
    "\n",
    "MIIS-16 students must take at least 84 units (typically 7 courses) of qualifying and elective courses that satisfy human language, machine learning, and language technology applications breadth requirements. MIIS-21 students have to take at least two more courses from the selected concentration area to satisfy their degree requirements, making it total of 108 units (typically 9 courses) of qualifying and elective courses, that also satisfy breadth requirements.\n",
    "\n",
    "For a full list of requirements, read the MIIS Handbook.\n",
    "\n",
    "''',\n",
    "''' \n",
    "Master of Science in Intelligent Information Systems (MIIS) Admission: \n",
    "\n",
    "Carnegie Mellon's School of Computer Science has a centralized online application process. Applications and all supporting documentation for fall admission to any of the LTI's graduate programs must be received by the application deadline. Incomplete applications will not be considered. The application period for Fall 2024 will open on September 6, 2023.\n",
    "\n",
    "*Please note, we no longer require mailed, hard versions of transcripts or test scores at the time of application. Do not mail anything to the admissions office. If you are accepted to a program, you will be given instruction to then mail your materials.\n",
    "\n",
    "Final Application Deadline\n",
    "\n",
    "December 13, 2023 at 3:00 p.m. EST.\n",
    "\n",
    "Cost\n",
    "\n",
    "    $100 per program, $80 for applications submitted before November 29, 2023 at 3:00 p.m. EST (early deadline).\n",
    "\n",
    "Fee Waivers\n",
    "\n",
    "Fee waivers may be available in cases of financial hardship, or for participants in select \"pipeline\" programs. For more information, please refer to the School of Computer Science Fee Waiver page.\n",
    "\n",
    "Requirements\n",
    "\n",
    "The School of Computer Science requires the following for all Master's applications.\n",
    "\n",
    "    GRE scores: MIIS applicants must submit their GRE scores. The scores must be less than five years old. The GRE Subject Test is not required, but is recommended. Our Institution Code is 2074; Department Code is 0402.\n",
    "\n",
    "    Proof of English Language Proficiency:\n",
    "\n",
    "If you will be studying on an F-1 or J-1 visa, and English is not a native language for you (native language…meaning spoken at home and from birth), we are required to formally evaluate your English proficiency. We require applicants who will be studying on an F-1 or J-1 visa, and for whom English is not a native language, to demonstrate English proficiency via one of these standardized tests: TOEFL (preferred), IELTS, or Duolingo.  We discourage the use of the \"TOEFL ITP Plus for China,\" since speaking is not scored.\n",
    "\n",
    "We do not issue waivers for non-native speakers of English.  In particular, we do not issue waivers based on previous study at a U.S. high school, college, or university.  We also do not issue waivers based on previous study at an English-language high school, college, or university outside of the United States.  No amount of educational experience in English, regardless of which country it occurred in, will result in a test waiver. Submit valid, recent scores: If as described above you are required to submit proof of English proficiency, your TOEFL, IELTS or Duolingo test scores will be considered valid as follows: If you have not received a bachelor’s degree in the U.S., you will need to submit an English proficiency score no older than two years. (scores from exams taken before Sept. 1, 2021, will not be accepted.)\n",
    "\n",
    "If you are currently working on or have received a bachelor's and/or a master's degree in the U.S., you may submit an expired test score up to five years old. (scores from exams taken before Sept. 1, 2018, will not be accepted.)Additional details about English proficiency requirements are provided on the FAQ page.\n",
    "\n",
    "    Official transcripts from each university you have attended, regardless of whether you received your degree there.\n",
    "    Current resume.\n",
    "    Statement of Purpose.\n",
    "    Three letters of recommendation.\n",
    "\n",
    "For more details on these requirements, please see the SCS Master's Admissions page.\n",
    "\n",
    "In addition to the SCS guidelines, the MIIS requires:\n",
    "\n",
    "    A short (1-3 minute) video of yourself. There will be a prompt question that you will respond to. You will have three attempts.\n",
    "\n",
    "No incomplete applications will be eligible for consideration.\n",
    "\n",
    "For specific application/admissions questions, please contact Brianna Eriksen.\n",
    "\n",
    "''', \n",
    "''' \n",
    "Master of Computational Data Science (MCDS): The MCDS degree focuses on engineering and deploying large-scale information systems. Our comprehensive curriculum equips you with the skills and knowledge to develop the layers of technology involved in the next generation of massive information system deployments and analyze the data these systems generate. When you graduate, you’ll have a unified vision of these systems from your core courses; internship experience; and semester-long, group-oriented capstone project. MCDS graduates are sought-after software engineers, data scientists and project managers at leading information technology, software services and social media companies.\n",
    "''',\n",
    "''' \n",
    "\n",
    "Master of Computational Data Science (MCDS) Requirements:\n",
    "\n",
    "The MCDS program offers three majors: Systems, Analytics, and Human-Centered Data Science. All three require the same total number of course credits, split among required core courses, electives, data science seminar and capstone courses specifically defined for each major. The degree can also be earned in two different ways, depending on the length of time you spend working on it. Regardless of the timing option, all MCDS students must complete a minimum of 144 units to graduate.\n",
    "\n",
    "Here are the options:\n",
    "\n",
    "    Standard Timing — a 16-month degree consisting of study for fall and spring semesters, a summer internship, and fall semester of study. Each semester comprises a minimum of 48 units. This timing is typical for most students. Students graduate in December.\n",
    "    Extended Timing — a 20-month degree consisting of study for fall and spring semesters, a summer internship, and a second year of fall and spring study. Each semester comprises a minimum of 36 units. Students graduate in May.\n",
    "\n",
    "For a complete overview of the MCDS requirements, visit the MCDS website or read the MCDS Handbook.\n",
    "\n",
    "''',\n",
    "''' \n",
    "Master of Computational Data Science (MCDS) Admission:\n",
    "\n",
    "\n",
    "Carnegie Mellon's School of Computer Science has a centralized online application process. Applications and all supporting documentation for fall admission to any of the LTI's graduate programs must be received by the application deadline. Incomplete applications will not be considered. The application period for Fall 2024 will open on September 6, 2023.\n",
    "\n",
    "*Please note, we no longer require mailed, hard versions of transcripts or test scores at the time of application. Do not mail anything to the admissions office. If you are accepted to a program, you will be given instruction to then mail your materials.\n",
    "\n",
    "Final Application Deadline\n",
    "\n",
    "December 13, 2023 at 3:00 p.m. EST.\n",
    "\n",
    "Cost\n",
    "\n",
    "    $100 per program, $80 for applications submitted before November 29,2023 at 3:00 p.m. EST (early deadline).\n",
    "\n",
    "Fee Waivers\n",
    "\n",
    "Fee waivers may be available in cases of financial hardship, or for participants in select \"pipeline\" programs. For more information, please refer to the School of Computer Science Fee Waiver page.\n",
    "\n",
    "Requirements\n",
    "\n",
    "The School of Computer Science requires the following for all Ph.D. applications.\n",
    "\n",
    "    GRE scores: These must be less than five years old. The GRE Subject Test is not required, but is recommended. Our Institution Code is 2074; Department Code is 0402.\n",
    "\n",
    "    Proof of English Language Proficiency:\n",
    "    If you will be studying on an F-1 or J-1 visa, and English is not a native language for you (native language…meaning spoken at home and from birth), we are required to formally evaluate your English proficiency. We require applicants who will be studying on an F-1 or J-1 visa, and for whom English is not a native language, to demonstrate English proficiency via one of these standardized tests: TOEFL (preferred), IELTS, or Duolingo. We discourage the use of the \"TOEFL ITP Plus for China,\" since speaking is not scored.\n",
    "\n",
    "    We do not issue waivers for non-native speakers of English. In particular, we do not issue waivers based on previous study at a U.S. high school, college, or university. We also do not issue waivers based on previous study at an English-language high school, college, or university outside of the United States. No amount of educational experience in English, regardless of which country it occurred in, will result in a test waiver.\n",
    "\n",
    "    Applicants applying to MCDS are required to submit scores from an English proficiency exam taken within the last two years. Scores taken before Sept. 1, 2021, will not be accepted regardless of whether you have previously studied in the U.S. For more information about their English proficiency score policies, visit the MCDS admission website. \n",
    "\n",
    "    Successful applicants will have a minimum TOEFL score of 100 (Reading, Listening, Speaking, Writing scores all 25 or above), IELTS score of 7.5 (Reading 7 or above, Listening 7 or above, Speaking 7.5 or above, Writing 6.5 or above), or DuoLingo score of 120 (Literacy 115 or above, Comprehension 125 or above, Production 100 or above, Conversation 105 or above). Our Institution Code is 4256; the Department Code is 78. Additional details about English proficiency requirements are provided on the FAQ page. \n",
    "    Official transcripts from each university you have attended, regardless of whether you received your degree there.\n",
    "    A short (1-3 minutes) video of yourself. Tell us about you and why you are interested in the MCDS program. This is not a required part of the application process, but it is STRONGLY suggested.  \n",
    "    Current resume.\n",
    "    Statement of Purpose.\n",
    "    Three letters of recommendation.\n",
    "\n",
    "For more details on these requirements, please see the SCS Master's Admissions page.\n",
    "\n",
    "In addition to the SCS guidelines, the LTI requires:\n",
    "\n",
    "    Any outside funding you are receiving must be accompanied by an official award letter.\n",
    "\n",
    "No incomplete applications will be eligible for consideration.\n",
    "\n",
    "For specific application/admissions questions, please contact Jennifer Lucas or Caitlin Korpus.\n",
    "\n",
    "''',\n",
    "''' \n",
    "Master of Computational Data Science (MCDS) Curriculum:\n",
    "\n",
    "To earn an MCDS degree, student must pass courses in the core curriculum, the MCDS seminar, a concentration area and electives. Students must also complete a capstone project in which they work on a research project at CMU or on an industry-sponsored project.\n",
    "\n",
    "In total, students must complete 144 eligible units of study, including eight 12-unit courses, two 12-unit seminar courses and one 24-unit capstone course. Students must choose at minimum five core courses. The remainder of the 12-unit courses with course numbers 600 or greater can be electives chosen from the SCS course catalog. Any additional non-prerequisite units taken beyond the 144 units are also considered electives.\n",
    "\n",
    "MCDS students must also pass the undergraduate course 15-513 Introduction to Computer Systems (6 units), typically in the summer before their program commences. The student must pass with a grade of B- or better. Failure to pass the course means that the student takes 15-213 during either the fall or spring semester. Note that in both cases the units do not count toward the 144 eligible units of study.\n",
    "''',\n",
    "''' \n",
    "Master of Science in Artificial Intelligence and Innovation (MSAII):\n",
    "The Master of Science in Artificial Intelligence and Innovation (MSAII) program is a successor to the M.S. in Biotechnology, Innovation and Computing (MSBIC). It combines a rigorous AI and machine learning curriculum with real-world team experience in identifying an AI market niche and developing a responsive product in cooperation with external stakeholders. The core program, which lasts four semesters and leads to a capstone project, focuses on both intrapreneurship and entrepreneurship, equipping graduates to either begin a startup or develop a new organization within an existing company. Students will also gain critical practical skills, such as making persuasive technical presentations, assembling development teams, and evaluating the potential of new market ideas.\n",
    "''',\n",
    "''' \n",
    "Master of Science in Artificial Intelligence and Innovation (MSAII) Requirements:\n",
    "\n",
    "\n",
    "Incoming students generally hold undergraduate degrees in computer science, software engineering, bioinformatics or bioengineering. To earn the MSAII degree, you must pass courses in the Core Curriculum, the Knowledge Requirements and Electives. You must also complete a capstone project in which you work on a development project as part of the Core Curriculum. In total, you will complete 192 eligible units of study, including 84 units of Core Curriculum (including the 36-unit Capstone), 72 units of Knowledge Requirements and at least 36 units of approved Electives.\n",
    "\n",
    "For full requirements and program details, read the MSAII Handbook.\n",
    "\n",
    "''',\n",
    "''' \n",
    "\n",
    "Master of Science in Artificial Intelligence and Innovation (MSAII) Admission: \n",
    "\n",
    "Carnegie Mellon's School of Computer Science has a centralized online application process. Applications and all supporting documentation for fall admission to any of the LTI's graduate programs must be received by the application deadline. Incomplete applications will not be considered. The application period for Fall 2024 will open on September 6, 2023.\n",
    "\n",
    "Final Application Deadline\n",
    "\n",
    "December 13, 2023 at 3:00 p.m. EST.\n",
    "\n",
    "Cost\n",
    "\n",
    "    $100 for one program, $80 if application is submitted before November 29, 2023 at 3PM EST (early deadline).\n",
    "\n",
    "Fee Waivers\n",
    "\n",
    "Fee waivers may be available in cases of financial hardship, or for participants in select \"pipeline\" programs. For more information, please refer to the School of Computer Science Fee Waiver page.\n",
    "\n",
    "Requirements\n",
    "\n",
    "The School of Computer Science requires the following for all applications:\n",
    "\n",
    "    A GPA of 3.0 or higher. (Students should report raw university GPA scores and NOT converted scores. Please DO NOT convert your international score to a US GPA or weighted GPA or other system).\n",
    "\n",
    "    GRE scores: GRE is required. Our Institution Code is 2074; Department Code is 0402. \n",
    "\n",
    "    English Language Proficiency: If you will be studying on an F-1 or J-1 visa, and English is not a native language for you (native language…meaning spoken at home and from birth), an official copy of an English proficiency score report is required. The English proficiency requirement cannot be waived for any reason. Find more information under \"Test Scores\" on our FAQ page.Unofficial transcripts from each university you have attended, regardless of whether you received a degree.\n",
    "\n",
    "    Current resume.\n",
    "\n",
    "    Statement of Purpose. A Statement of Purpose is not a resume. It should discuss your reasons for choosing the MSAII program and indicate your intended career path. \n",
    "\n",
    "    Three letters of recommendation.\n",
    "\n",
    "    A short (1-3 minutes) video of yourself. Tell us about you and why you are interested in the MSAII program. This is not a required part of the application process, but it is STRONGLY suggested.  \n",
    "\n",
    "For specific application/admissions questions, please contact Amber Vivis. \n",
    "''',\n",
    "\n",
    "''' \n",
    "Undergraduate LT Concentration: Human language technologies have become an increasingly central component of computer science. Information retrieval, machine translation and speech technology are used daily by the general public, while text mining, natural language processing and language-based tutoring are common within more specialized professional or educational environments. The LTI prepares students for this world by offering a minor that gives you the opportunity to not only learn about language technologies, but to also apply that knowledge through a directed project.\n",
    "''',\n",
    "\n",
    "''' \n",
    "Undergraduate LT Concentration Requirements: \n",
    "\n",
    "\n",
    "Students interested in the language technologies minor must complete our prerequisite courses with an average grade of B (3.0) or better before applying to the program. (Students who do not meet this average must submit a letter of explanation along with their application.) Prerequisites include:\n",
    "\n",
    "    Principles of Imperative Computation (15-122)\n",
    "    Principles of Functional Programming (15-150)\n",
    "\n",
    "We also strongly encourage candidates to take\n",
    "\n",
    "    Differential and Integral Calculus (21-120) and Integration and Approximation (21-122)\n",
    "    Matrices and Linear Transformations (21-241) or Matrix Theory (21-242)\n",
    "    Probability and Computing (15-259) or Probability (21-325) or Probability Theory for Computer Scientists (36-218) or Introduction to Probability Theory (36-225)\n",
    "\n",
    "\n",
    "''',\n",
    "'''\n",
    "Undergraduate LT Concentration Admission:\n",
    "Students interested in earning a minor in language technologies must apply for admission no later than September 30 of their senior year. An admission decision will usually be made within one month. Students may petition the LTI undergraduate program director to be admitted to the minor earlier or later in their undergraduate careers. To apply, contact the program's director, Carolyn Rosé.\n",
    "''',\n",
    "''' \n",
    "Undergraduate LT Concentration Curriculum:\n",
    "The Language Technologies Concentration requires that SCS students complete one core course and their choice of three elective courses of at least 9 units each. The electives can be chosen from a specific set of stand-alone courses. In addition to the four courses, students are required to do an undergraduate research project for at least 9 units to complete their concentration.\n",
    "'''\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "with open('/zfsauton2/home/yifuc/11711-RAG/data/cmu/LTI_Programs.json', 'w') as f:\n",
    "    json.dump(texts, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCS 25 great things and history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = r'https://www.cs.cmu.edu/scs25/25things'\n",
    "#parse the whole webpage's text\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "text = soup.get_text()\n",
    "text = text.strip().split('\\n')[-25:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"What’s so great about computer science at Carnegie Mellon?We're glad you asked! Here are 25 great ideas from CMU computer scientists to think about as we celebrate the birthday of the School of Computer Science.1. Artificial intelligence, 1955-56\\xa0Can you write a working computer program without a computer? Herb Simon (H’90), at left, Allen Newell (IA’57), at right, and Cliff Shaw did. The team created the first artificial intelligence program, Logic Theorist, which could solve logic puzzles in the same way that a human might solve them. Newell demonstrated that it worked by writing the instructions on 3-by-5 index cards that were manipulated on the kitchen table by Newell, his wife, and a group of Carnegie Tech grad students.\",\n",
       " '2. Multi-core processors, 1971\\xa0Multi-core processors are common in today’s computers, but they were still science fiction in the early 1970s. But when CMU researchers found their existing machines too slow to keep pace with the advance of speech and graphics programs, they knew they had to do something. They solved the problem by ganging together 16 processors to build a pioneering computer called C.mmp—then topped the feat by linking 50 processors into Cm*.',\n",
       " '3. Tutoring machines, 1973Games and drills, such as flash cards, have long been used to help students learn tough subjects. But the cognitive tutoring programs developed at Carnegie Mellon, beginning in the 1970s, did more than simply drill students on math problems. Cognitive tutors were able to adapt, presenting harder or easier problems as students learned or stumbled. Today, cognitive tutors teach subjects such as algebra to hundreds of thousands of students every year.',\n",
       " '4. Speech recognition, 1976If you have an iPhone, ask Siri to look up “Hearsay I,” the first computer system capable of continuous speech recognition. It was developed by future Turing Award winner and future SCS dean Raj Reddy along with his students. Their work on subsequent systems established many of the principles that still underlie speech recognition software.',\n",
       " '5. Emoticons, 1982Sure, it was just a joke, but (for better or worse) it’s endured for more than three decades. CMU researcher Scott Fahlman created the emoticon to clear up misunderstandings on computer message boards. We’ve been looking at the world sideways ever since. :-)',\n",
       " '6. Andrew project, 1982It was long the dream of computer scientists to put a workstation in every home and office, but no one had actually tried to accomplish it until researchers from Carnegie Mellon University and IBM launched the Andrew Project. Soon, every student, faculty member and employee had access to email, word processing, file-transfer services and graphics programs, and CMU was the most-wired campus in the world.',\n",
       " \"7. Autonomous robots, 1983Thanks to Red Whittaker (E'75,'79), robots moved off of the assembly lines and into places no human ever could go. His Robotic Reconnaissance Vehicle spent four years inspecting and cleaning up the contaminated reactor building at the crippled Three Mile Island nuclear plant.\",\n",
       " '8. User interfaces, 1983Why should humans adapt to fit computers? Shouldn’t computers adapt to fit humans? That was the attitude of CMU researchers, who applied design principles to computer science to develop better, easier-to-use interfaces. They called the new field “human-computer interaction.”',\n",
       " '9. Machine translation, 1984Every “Star Trek” fan knows about the universal translator. Scientists in the Language Technologies Institute are moving those gadgets from science fiction to real life. Their pioneering systems include handheld, portable speech-to-speech translators, just like those depicted on the USS Enterprise.',\n",
       " '10. Mach kernel, 1985In computer parlance, a “kernel” is the heart of an operating system, passing input and output requests to and from the processor. At the core of all modern Apple devices --- including iPhones, iPads and MacBooks --- is the Mach kernel, developed at CMU under the leadership of then-professor Rick Rashid.',\n",
       " '11. Computer chess, 1990Could a computer play chess at the level of the world’s best players? For many years, it was considered the “holy grail” of artificial intelligence. Hitech, developed by CMU researcher Hans Berliner (CS’74), was the first computer to achieve grandmaster status. CMU alumni played key roles in developing “Deep Blue,” the IBM machine that beat human chess champion Garry Kasparov in 1997.',\n",
       " '12. Java, 1991 As a CMU grad student, James Gosling (CS’83) worked on the Andrew project, which stressed interoperability between computers, whether they were Macs, IBMs or Unix machines. Those lessons served Gosling well when he developed Java, the first programming language able to run on almost any platform.',\n",
       " '13. Email attachments, 1992Steve Jobs liked the email system built into CMU’s Andrew so much that he tried to hire Nathaniel Borenstein (CS’81,’85) and the rest of his team to create a similar program for Apple. Borenstein didn’t take the offer, but he did like Jobs’ idea about attaching documents to email. Borenstein went on to develop the MIME standard that’s used by all email programs to send photos and other files over the Internet.',\n",
       " '14. Web search engines, 1994The World Wide Web was still in its toddler stage when CMU researcher Michael “Fuzzy” Mauldin (CS’83,’89) developed one of the first successful search engines, Lycos. It was the most-visited site on the Web by 1999.',\n",
       " '15. Model checking, 1994CMU professor Edmund Clarke had long stressed the importance of verifying computer hardware and software through a formal problem-solving technique called “model checking.” In 1994, his arguments gained new weight with the discovery that Intel’s amazing new Pentium chip made errors on certain math problems. Clarke would go on to receive the Turing Award for his role in the development of model checking.',\n",
       " '16. CAPTCHAs, 2000 “Spam” and malicious attacks were a growing problem on the Internet when hackers developed automated “bots” that could sign up for email and other Web services without human intervention. Luis von Ahn (CS’03,’05), Nick Hopper (CS’04), John Langford (CS’02) and CMU professor Manuel Blum invented a “Completely Automated Public Turing Test to tell Computers and Humans Apart,” or CAPTCHA, to help foil the bots. A later variation, reCAPTCHA, is helping digitize old books and newspapers.',\n",
       " '17. Robotic video cameras, 2001When Baltimore Ravens quarterback Trent Dilfer dropped back to pass, TV viewers of Super Bowl XXXV saw something they’d never seen before --- the motion froze and the view suddenly rotated to show Dilfer’s opposite side. CBS called it Eyevision. The synchronized system of robotic cameras and advanced image processing was the brainchild of CMU’s Takeo Kanade, one of many advances he pioneered in computer vision.',\n",
       " '18. Self-driving vehicles, 2007Carnegie Mellon’s early attempts at self-driving vehicles progressed slowly, creeping around Pittsburgh’s Schenley Park in the late 1990s. But they were going full-throttle by the time CMU’s self-driving SUV, named BOSS, won the 2007 DARPA Urban Challenge road race.',\n",
       " '19. Thought reading programs, 2007Your brain reacts in different ways, depending on what words you’re thinking about --- ways that are measurable with magnetic-resonance imaging, or MRI, machines. CMU researchers Tom Mitchell and Marcel Just are decoding those brain scans and are making progress at reading people’s thoughts.',\n",
       " \"20. Kidney donor matching, 2008 Organ transplants save lives every day, but more people could likely be saved if it was easier to match recipients with donors who are unrelated. An algorithm developed by CMU scientists is close to enabling a nationwide network that would match living kidney donors with potential recipients whom they've never met in real life.\",\n",
       " '21. RNA sequencing via videogames, 2010 Thanks to crowdsourcing, science isn’t just for scientists any more. People without formal training in molecular biology are producing new insights into genetic encoding through a videogame called EteRNA, developed by researchers at CMU and Stanford, that allows players to fold and shift RNA molecules to solve on-screen puzzles.',\n",
       " '22. Language learning software, 2010Learning a second language has always been challenging, but a CMU spinoff called Duolingo is proving that it doesn’t have to be expensive. Duolingo has developed language tutoring software that enables users to learn Spanish, English, Italian, German, Portuguese or French for free through its website and mobile apps. In the process, Duolingo users are helping to translate the Web.',\n",
       " '23. Question-answering computers, 2011 Searching the Web for information is rarely as simple as asking a question in plain English. So-called “question-answering” machines moved from laboratories to TV screens when an IBM computer called “Watson” defeated two human champions on the game show “Jeopardy!” At the heart of Watson was computer architecture developed by CMU’s Eric Nyberg and his students.',\n",
       " '24. Encrypting online information, 2012 Credit card numbers and other data used online is safer thanks to an encryption scheme developed by CMU alumna Shafi Goldwasser (S’79). She shared the 2012 Turing Award for her role in developing practical encoding schemes that are difficult to break.',\n",
       " '25. Smart, adaptable traffic signals, 2012 Smart traffic lights developed at CMU’s Robotics Institute are saving time and energy, and cutting down on the amount of air pollution created by idling cars. First rolled out in Pittsburgh’s East Liberty neighborhood, the new signals are being studied around the country.']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [t.strip() for t in text]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "with open('/zfsauton2/home/yifuc/11711-RAG/data/cmu/scs_25_great_thing.json', 'w') as f:\n",
    "    json.dump(text, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A history of SCS | SCS25 - Carnegie Mellon University School of Computer Science\n",
      "For an expanded history of the School of Computer Science and its predecessors at CMU, read \"Institutional Memories\" in the Summer 2014 issue of The Link magazine.In 2014, the School of Computer Science celebrated its 25th year as a stand-alone college within Carnegie Mellon University. It was arguably the first college devoted solely to computer science in the United States, and a model for others that followed. But CMU’s computer science era begins much earlier—in 1956, with the arrival of an IBM 650 computer on the campus of what was then known as Carnegie Institute of Technology. The IBM 650 had magnetic-drum memory and a processing speed of approximately 60 instructions per second. Herb Simon (H’90), associate dean of the Graduate School of Industrial Administration—now known as CMU’s Tepper School of Business—established Carnegie Tech’s first Computation Center with the help of its first director, Alan Perlis (S’42).First freshman-level computer science courseIn 1956 and 1957, Simon, Allen Newell (IA’57) and Cliff Shaw of RAND designed the Logic Theorist, a computer program that could develop proofs for theorems in much the same way a human would work. They also developed linked-list data structures, the foundation of computer programming. Perlis, Simon and Newell are credited with defining the term “computer science” as “the theory and design of computers,” as well as (in Newell’s words) “the study of the phenomena arising from them.” In 1958, Perlis began teaching the first freshman-level computer programming course in the United States at Carnegie Tech.Computer science Ph.D. program createdIn 1961, the Computation Center and its newest computer, a Bendix G-20, were moved to recently completed Scaife Hall. That same year, Carnegie Tech created an interdisciplinary Ph.D. program called Systems and Communications Sciences, combining elements of computer science, mathematics, psychology, business and electrical engineering. The university’s first computer science Ph.D.s were graduates of this program.Computer Science Department establishedIn 1965, Carnegie Tech established its Computer Science Department, or CSD, with a $5 million grant from the R.K. Mellon Foundation. Perlis was the first department head. There were no undergraduates; only Ph.D. students were admitted, and the department’s focus was on research, much of it funded by the federal government through the Defense Department’s Advanced Research Projects Agency, or DARPA. In 1967, Carnegie Institute of Technology merged with the nearby Mellon Institute of Industrial Research to form Carnegie-Mellon University. The Department of Computer Science moved into the newly created Mellon Institute of Science, later renamed Mellon College of Science, or MCS. Future SCS dean Raj Reddy joined the CSD in 1969 after three years as an assistant professor at Stanford. He brought with him research in speech, language and computer vision. But in 1970 and 1971, the new Computer Science Department faced its first crisis, as half of its tenured faculty members—including department head Perlis—left for other universities. Joe Traub was recruited from Bell Labs to CMU to become the new department head.Multi-processor machines emergeCSD emerged from the brief crisis as a highly agile, interdisciplinary entity, with many new faculty members taking joint appointments with other CMU departments. Several large projects emerged in the Computer Science Department, including C.mmp, the first shared-memory multiprocessor computer, with 16 processing units, and Cm*, a 50-processor computer. These computers were the forerunners of today’s ubiquitous multi-core desktops and laptops.Turing Awards and a Nobel PrizeIn 1975, Simon and Newell were awarded the A.M. Turing Award for their work in artificial intelligence. (As of 2014, 12 CMU alumni or faculty have been awarded Turings, sometimes considered the Nobel Prize of computing.) Three years later, Simon received the Nobel Prize in Economics for his work on decision-making theory. As the 1970s progressed, Newell became interested in human-computer interaction, and began a long relationship with Xerox’s Palo Alto Research Center, or PARC, which released the Xerox Alto in 1973. Considered a forerunner to many of the computing environments that followed, Alto featured a graphical-user interface and was among the first commercially available workstations controlled with a mouse. Inspired by Alto, Reddy launched a drive for development of CMU’s own “three-M” machine—a personal workstation with a megabyte of memory, a megapixel display and at least one million instructions per second of processing power.Launching a Robotics InstituteIn 1979, an executive at Pittsburgh’s Westinghouse Electric Corp., Tom Murrin, collaborated with Jordan and Reddy to create the Robotics Institute, with Reddy as its first director. By 1982, the Computer Science Department included more than 30 faculty members and 100 graduate students.The best-wired campus in the worldWorking with IBM in the early 1980s, the university and the Computer Science Department established another new research frontier: Development of a high-speed computer network that would reach virtually every room on campus, along with a GUI-based computing environment, and providing networked PCs or workstations for 7,000 students, faculty members and employees. Called the Andrew Project, it turned Carnegie Mellon into the best-connected, most-wired university in the world—a process Newell called “greening up the campus with computer science.” CMU also became home to a new Software Engineering Institute, funded by the Defense Department, to study computer security and develop best practices in the design of operating systems. Between 1982 and 1985, the amount of sponsored research in the Computer Science Department doubled, from $7.2 million to $15.3 million—more than the other four departments in the Mellon College of Science combined.A “school of computer science” is proposedFeeling that CSD’s needs were inadequately represented in MCS, CSD head A. Nico Habermann and then-CMU provost Angel Jordan in 1986 wrote a white paper proposing the creation of “a School of Computer Science.” Responding to concerns from the faculty that the change might be taking place too quickly, the university first established a free-floating Department of Computer Science. The experiment, which lasted two years, was an unqualified success. Separately, and also in 1986, the Pittsburgh Supercomputing Center was created as a joint effort between CMU, the University of Pittsburgh and Westinghouse Electric Corp.SCS is officially formedCMU’s Faculty Senate in the fall of 1988 agreed to President Richard Cyert’s plan to elevate the Department of Computer Science to college status. In addition to the Computer Science Department, SCS also incorporated the Robotics Institute, the Center for Machine Translation, and researchers from the Information Technology Center, which had developed Andrew. On Dec. 13, 1988, Cyert (H’89) told faculty and staff that Habermann had been appointed CMU’s first Dean of Computer Science, effective Dec. 1, and that the School of Computer Science would soon begin operations. SCS made its formal debut on Dec. 22, 1988, with a reception in the Wherrett Room of Skibo Hall, CMU’s student union. The official announcement of CMU’s new “graduate School of Computer Science” was made Jan. 3, 1989.Undergraduate degrees beginFor several years, undergraduates interested in computer science pursued an “applied math/CS” bachelor’s degree offered by the Mathematics Department. CSD professor Mary Shaw (CS’72) led CMU’s first effort to design an undergraduate curriculum solely in computer science. She and her colleagues were guided by the Carnegie Plan—guidelines established in 1938 under Carnegie Tech President Robert Doherty (A’40, E’48, H’50), outlining the principles of a sound professional education. Drawing on Shaw’s plan and also on the work of other faculty members, an undergraduate program in computer science was created during the 1989-90 academic year. Seven CS majors were admitted to the program as sophomores. Another 73 undergraduates were admitted in 1990–91. By 1995, there were 401 undergraduates in the School of Computer Science; in fall 2013, more than 600 undergraduates made up about 37 percent of student enrollment at SCS, along with more than 600 master’s degree students.New departments, new areas of studyAlong the way, the Center for Machine Translation became the Language Technologies Institute, and other new departments were formed, including the Human-Computer Interaction Institute (1993), the Institute for Software Research (1999), the Machine Learning Department (2006) and the Ray and Stephanie Lane Center for Computational Biology (2009). SCS’s seven degree-granting departments draw faculty and students from a wide variety of disciplines, including engineering, mathematics, social sciences, linguistics and design.Committed to extending our founders’ visionThe School of Computer Science at Carnegie Mellon University enters its second quarter century as a world-leading educational and research institution, embracing all facets of computing. Its graduate programs are consisted ranked with the best in the world by a leading U.S. magazine, while its undergraduate programs are also rated the best in the U.S. by corporate recruiters. In 2013, SCS had 284 faculty members and a total student enrollment of nearly 1,700, including undergraduate, master’s and Ph.D. students, and conducted $124 million in research. Indeed, by itself, the Robotics Institute is the largest university robotics research group in the world, with more than 500 people and more than 100 ongoing research projects. A half-century ago, Perlis, Simon and Newell outlined a vision for computer science. The School of Computer Science at CMU remains committed to continuing and extending their vision in the context of big data and connected computing in the 21st century.\n"
     ]
    }
   ],
   "source": [
    "url = r'https://www.cs.cmu.edu/scs25/history'\n",
    "#parse the whole webpage's text\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "text = soup.get_text()\n",
    "text = text.strip().split('\\n')[0] + '\\n' + text.strip().split('\\n')[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\n",
    "'''\n",
    "A history of SCS\n",
    "\n",
    "For an expanded history of the School of Computer Science and its predecessors at CMU, read \"Institutional Memories\" in the Summer 2014 issue of The Link magazine.\n",
    "\n",
    "In 2014, the School of Computer Science celebrated its 25th year as a stand-alone college within Carnegie Mellon University. It was arguably the first college devoted solely to computer science in the United States, and a model for others that followed. But CMU’s computer science era begins much earlier—in 1956, with the arrival of an IBM 650 computer on the campus of what was then known as Carnegie Institute of Technology. The IBM 650 had magnetic-drum memory and a processing speed of approximately 60 instructions per second. Herb Simon (H’90), associate dean of the Graduate School of Industrial Administration—now known as CMU’s Tepper School of Business—established Carnegie Tech’s first Computation Center with the help of its first director, Alan Perlis (S’42).\n",
    "''',\n",
    "'''\n",
    "First freshman-level computer science course\n",
    "\n",
    "In 1956 and 1957, Simon, Allen Newell (IA'57) and Cliff Shaw of RAND designed the Logic Theorist, a computer program that could develop proofs for theorems in much the same way a human would work. They also developed linked-list data structures, the foundation of computer programming. Perlis, Simon and Newell are credited with defining the term “computer science” as “the theory and design of computers,” as well as (in Newell’s words) “the study of the phenomena arising from them.” In 1958, Perlis began teaching the first freshman-level computer programming course in the United States at Carnegie Tech.\n",
    "''',\n",
    "''' \n",
    "Computer science Ph.D. program created\n",
    "\n",
    "In 1961, the Computation Center and its newest computer, a Bendix G-20, were moved to recently completed Scaife Hall. That same year, Carnegie Tech created an interdisciplinary Ph.D. program called Systems and Communications Sciences, combining elements of computer science, mathematics, psychology, business and electrical engineering. The university’s first computer science Ph.D.s were graduates of this program.\n",
    "''',\n",
    "''' \n",
    "Computer Science Department established\n",
    "\n",
    "In 1965, Carnegie Tech established its Computer Science Department, or CSD, with a $5 million grant from the R.K. Mellon Foundation. Perlis was the first department head. There were no undergraduates; only Ph.D. students were admitted, and the department’s focus was on research, much of it funded by the federal government through the Defense Department’s Advanced Research Projects Agency, or DARPA. In 1967, Carnegie Institute of Technology merged with the nearby Mellon Institute of Industrial Research to form Carnegie-Mellon University. The Department of Computer Science moved into the newly created Mellon Institute of Science, later renamed Mellon College of Science, or MCS. Future SCS dean Raj Reddy joined the CSD in 1969 after three years as an assistant professor at Stanford. He brought with him research in speech, language and computer vision. But in 1970 and 1971, the new Computer Science Department faced its first crisis, as half of its tenured faculty members—including department head Perlis—left for other universities. Joe Traub was recruited from Bell Labs to CMU to become the new department head.\n",
    "''',\n",
    "''' \n",
    "Multi-processor machines emerge\n",
    "\n",
    "CSD emerged from the brief crisis as a highly agile, interdisciplinary entity, with many new faculty members taking joint appointments with other CMU departments. Several large projects emerged in the Computer Science Department, including C.mmp, the first shared-memory multiprocessor computer, with 16 processing units, and Cm*, a 50-processor computer. These computers were the forerunners of today’s ubiquitous multi-core desktops and laptops.\n",
    "''',\n",
    "''' \n",
    "Turing Awards and a Nobel Prize\n",
    "\n",
    "In 1975, Simon and Newell were awarded the A.M. Turing Award for their work in artificial intelligence. (As of 2014, 12 CMU alumni or faculty have been awarded Turings, sometimes considered the Nobel Prize of computing.) Three years later, Simon received the Nobel Prize in Economics for his work on decision-making theory. As the 1970s progressed, Newell became interested in human-computer interaction, and began a long relationship with Xerox’s Palo Alto Research Center, or PARC, which released the Xerox Alto in 1973. Considered a forerunner to many of the computing environments that followed, Alto featured a graphical-user interface and was among the first commercially available workstations controlled with a mouse. Inspired by Alto, Reddy launched a drive for development of CMU’s own “three-M” machine—a personal workstation with a megabyte of memory, a megapixel display and at least one million instructions per second of processing power.\n",
    "''',\n",
    "''' \n",
    "Launching a Robotics Institute\n",
    "\n",
    "In 1979, an executive at Pittsburgh’s Westinghouse Electric Corp., Tom Murrin, collaborated with Jordan and Reddy to create the Robotics Institute, with Reddy as its first director. By 1982, the Computer Science Department included more than 30 faculty members and 100 graduate students.\n",
    "''',\n",
    "''' \n",
    "The best-wired campus in the world\n",
    "\n",
    "Working with IBM in the early 1980s, the university and the Computer Science Department established another new research frontier: Development of a high-speed computer network that would reach virtually every room on campus, along with a GUI-based computing environment, and providing networked PCs or workstations for 7,000 students, faculty members and employees. Called the Andrew Project, it turned Carnegie Mellon into the best-connected, most-wired university in the world—a process Newell called “greening up the campus with computer science.” CMU also became home to a new Software Engineering Institute, funded by the Defense Department, to study computer security and develop best practices in the design of operating systems. Between 1982 and 1985, the amount of sponsored research in the Computer Science Department doubled, from $7.2 million to $15.3 million—more than the other four departments in the Mellon College of Science combined\n",
    "''',\n",
    "''' \n",
    "A “school of computer science” is proposed\n",
    "\n",
    "Feeling that CSD’s needs were inadequately represented in MCS, CSD head A. Nico Habermann and then-CMU provost Angel Jordan in 1986 wrote a white paper proposing the creation of “a School of Computer Science.” Responding to concerns from the faculty that the change might be taking place too quickly, the university first established a free-floating Department of Computer Science. The experiment, which lasted two years, was an unqualified success. Separately, and also in 1986, the Pittsburgh Supercomputing Center was created as a joint effort between CMU, the University of Pittsburgh and Westinghouse Electric Corp.\n",
    "''',\n",
    "''' \n",
    "SCS is officially formed\n",
    "\n",
    "CMU’s Faculty Senate in the fall of 1988 agreed to President Richard Cyert’s plan to elevate the Department of Computer Science to college status. In addition to the Computer Science Department, SCS also incorporated the Robotics Institute, the Center for Machine Translation, and researchers from the Information Technology Center, which had developed Andrew. On Dec. 13, 1988, Cyert (H’89) told faculty and staff that Habermann had been appointed CMU’s first Dean of Computer Science, effective Dec. 1, and that the School of Computer Science would soon begin operations. SCS made its formal debut on Dec. 22, 1988, with a reception in the Wherrett Room of Skibo Hall, CMU’s student union. The official announcement of CMU’s new “graduate School of Computer Science” was made Jan. 3, 1989.\n",
    "''',\n",
    "''' \n",
    "Undergraduate degrees begin\n",
    "\n",
    "For several years, undergraduates interested in computer science pursued an “applied math/CS” bachelor’s degree offered by the Mathematics Department. CSD professor Mary Shaw (CS’72) led CMU’s first effort to design an undergraduate curriculum solely in computer science. She and her colleagues were guided by the Carnegie Plan—guidelines established in 1938 under Carnegie Tech President Robert Doherty (A’40, E’48, H’50), outlining the principles of a sound professional education. Drawing on Shaw’s plan and also on the work of other faculty members, an undergraduate program in computer science was created during the 1989-90 academic year. Seven CS majors were admitted to the program as sophomores. Another 73 undergraduates were admitted in 1990–91. By 1995, there were 401 undergraduates in the School of Computer Science; in fall 2013, more than 600 undergraduates made up about 37 percent of student enrollment at SCS, along with more than 600 master’s degree students.\n",
    "''',\n",
    "''' \n",
    "New departments, new areas of study\n",
    "\n",
    "Along the way, the Center for Machine Translation became the Language Technologies Institute, and other new departments were formed, including the Human-Computer Interaction Institute (1993), the Institute for Software Research (1999), the Machine Learning Department (2006) and the Ray and Stephanie Lane Center for Computational Biology (2009). SCS’s seven degree-granting departments draw faculty and students from a wide variety of disciplines, including engineering, mathematics, social sciences, linguistics and design.\n",
    "''',\n",
    "''' \n",
    "Committed to extending our founders’ vision\n",
    "\n",
    "The School of Computer Science at Carnegie Mellon University enters its second quarter century as a world-leading educational and research institution, embracing all facets of computing. Its graduate programs are consisted ranked with the best in the world by a leading U.S. magazine, while its undergraduate programs are also rated the best in the U.S. by corporate recruiters. In 2013, SCS had 284 faculty members and a total student enrollment of nearly 1,700, including undergraduate, master’s and Ph.D. students, and conducted $124 million in research. Indeed, by itself, the Robotics Institute is the largest university robotics research group in the world, with more than 500 people and more than 100 ongoing research projects. A half-century ago, Perlis, Simon and Newell outlined a vision for computer science. The School of Computer Science at CMU remains committed to continuing and extending their vision in the context of big data and connected computing in the 21st century.\n",
    "'''\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "with open('/zfsauton2/home/yifuc/11711-RAG/data/cmu/scs_history.json', 'w') as f:\n",
    "    json.dump(text, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMU Fact Sheet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use pypdf to parse an online pdf\n",
    "url = r'https://www.cmu.edu/about/cmu_fact_sheet_02.pdf'\n",
    "import io\n",
    "import requests\n",
    "from PyPDF2 import PdfReader\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (X11; Windows; Windows x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.5060.114 Safari/537.36'}\n",
    "\n",
    "response = requests.get(url=url, headers=headers, timeout=120)\n",
    "on_fly_mem_obj = io.BytesIO(response.content)\n",
    "pdf_file = PdfReader(on_fly_mem_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = ''\n",
    "for p in pdf_file.pages:\n",
    "    text += p.extract_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "with open('/zfsauton2/home/yifuc/11711-RAG/data/cmu/fact_sheet.json', 'w') as f:\n",
    "    json.dump([text], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CMU History and Buggy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "url = r'https://www.cmu.edu/about/history.html'\n",
    "#parse the whole webpage's text\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "text = soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "326"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.index('Andrew Carnegie')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6674"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.index('COVID-19')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Andrew Carnegie \\nA self-educated \"working boy\" who loved books, Andrew Carnegie emigrated from Scotland in 1848 and settled in Pittsburgh, Pa. Attending night school and borrowing books, Carnegie went from factory worker in a textile mill to successful entrepreneur and industrialist. He rose to prominence by founding what became the world\\'s largest steel producing company by the end of the 19th century.\\n \\n Carnegie Technical Schools \\nAt one point the richest man in the world, Carnegie believed that \"to die rich is to die disgraced.\" He turned his attention to writing, social activism and philanthropy, determined to establish educational opportunities for the general public where few existed.\\nIn 1900, he donated $1 million for the creation of a technical institute for the city of Pittsburgh, envisioning a school where working-class men and women of Pittsburgh could learn practical skills, trades and crafts that would enhance their careers, lives and communities.\\n\"My heart is in the work,\" he stated, which would become part of the school\\'s official motto.\\nThe Carnegie Technical Schools offered two- and three-year certificates in the arts as well as in engineering disciplines and included a college for women, Margaret Morrison Carnegie College.\\n \\n Carnegie Tech – Early Years \\nSoon faced with the demand for baccalaureate programs, Carnegie Technical Schools began offering bachelor\\'s degrees through its College of Engineering and College of Fine Arts, becoming the Carnegie Institute of Technology, or \"Carnegie Tech.\"\\nDuring the first half of the 20th century, with support from Andrew Carnegie and other funders, Carnegie Tech laid the foundation for a school on the cutting edge. Some key developments were:\\nIt expanded from two buildings into an elegant 20th century campus designed in the beaux arts architectural style, housing a wealth of machine shops, studios and laboratories — the hands-on center of learning that persists today.\\nIt pioneered conservatory degree programs in music and drama, in addition to visual art and design programs. The first U.S. drama degree was awarded in 1914 at Carnegie Tech.\\nIt began offering graduate degrees. In 1919, the first doctorate (in civil engineering) was awarded to Mao Yisheng, a student from China.\\nIt laid the groundwork for a research institution, recruiting leading scientists, offering sponsored fellowships with government and industry leaders and pioneering nontraditional interdisciplinary research, which brought together physicists, chemists and metallurgists, for example. Interdisciplinary research would become the hallmark of Carnegie Mellon research.\\nIt initiated the \\'Carnegie Plan\\' in 1938, a new curriculum that required science and engineer students to take courses in humanities and social sciences in order to better understand the needs of society.\\nCarnegie died in 1919, but his vision for an educated public lived on after him.\\n \\n Carnegie Tech - Post-war Years \\nWith the end of World War II, the latter half of the 20th century brought unprecedented growth to Carnegie Tech. In 1956, the arrival of the first IBM computer to campus was revolutionary, initiating a university culture where information technology pervaded virtually all areas of study.\\nUniversity culture also changed in 1973 when Margaret Morrison closed and women joined their male peers in classrooms and dorms.\\nThe times were changing, and Tech positioned itself at the forefront, opening three new schools:\\n1948:\\xa0The Graduate School of Industrial Administration, later renamed the\\xa0David A. Tepper School of Business, focusing on quantitative analysis and pioneering the field of management science.\\n1968:\\xa0School of Urban and Public Affairs, later renamed the\\xa0H. John Heinz III College, providing graduate training for work in the public sector.\\n1986:\\xa0School of Computer Science, pioneering computing and artificial intelligence, led by interdisciplinary efforts of Allen Newell and Herbert Simon.\\n \\n Carnegie Mellon University \\nIn 1967, Carnegie Tech merged with the Mellon Institute, a science research center founded by the Mellon family of Pittsburgh, to become known as Carnegie Mellon University. The merger built upon a long history of support from the Mellons.\\nIt allowed Carnegie Mellon to establish the last of its current pillars: the\\xa0Mellon College of Science\\xa0and the College of Humanities and Social Sciences, now known as\\xa0Marianna Brown Dietrich College of Humanities and Social Sciences.\\nIn 2017, Carnegie Mellon celebrated the 50th anniversary of the Carnegie Tech-Mellon Institute merger, revisiting the shared vision of the founders and recognizing the impact it has had, and will continue to have, in the world of higher education, research and discovery.\\n \\n A Global Impact \\nIn its 115 years, Carnegie Mellon has soared to national and international leadership in higher education and research. A birthplace of innovation since its founding, it continues to be known for innovation, for solving real-world problems and for interdisciplinary collaboration.\\nIts\\xa0alumni can be found across the globe\\xa0— from Tony Award winners to Nobel Prize and Turing Award winners, from CEOs to entrepreneurs, from professors to artists.\\nIn the 2000s, in response to demand for expanded international educational opportunities, Carnegie Mellon began offering degree programs outside of Pittsburgh.\\nToday its global presence includes campuses in Qatar and Silicon Valley, Calif., more than a dozen degree-granting locations and more than 20 research partnerships such as Los Angeles; New York City; Washington, D.C.; Australia; China; Portugal and Rwanda.\\n \\n The Future \\nCMU is positioned like never before to meet the challenges of the 21st century. In the coming years, the university will see the largest expansion to the Pittsburgh campus since 1900.\\nAt the intersection of technology and humanity, CMU research, innovation and creativity will continue to guide our future as a world-class university.\\nAs outlined in the Strategic Plan 2025, the university will focus on advancing the individual student experience, the broader Carnegie Mellon community experience, and the social impact of Carnegie Mellon throughout the world.\\n \\nCarnegie Mellon University challenges the curious and passionate to deliver work that matters.\\nCalendar Careers'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text[326:6674].split('\\n')\n",
    "text = [t for t in text if t != '']\n",
    "text = '\\n'.join(text)\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "with open('/zfsauton2/home/yifuc/11711-RAG/data/cmu/cmu_history.json', 'w') as f:\n",
    "    json.dump([text], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = r'https://www.cmu.edu/news/stories/archives/2019/april/spring-carnival-buggy.html'\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.content, 'html.parser')\n",
    "text = soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 6938)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.index('Buggy Races Keep Rolling at Carnegie Mellon'), text.index('around the course')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Buggy Races Keep Rolling at Carnegie Mellon -     News - Carnegie Mellon University\\nCarnegie Mellon University\\n——— \\nSearch\\n Search\\n Search this site only \\nNews\\n \\nNews\\n                                     \\xa0›\\xa0             Stories\\n    \\n                                                 \\xa0›\\xa0             Archives\\n    \\n                                                 \\xa0›\\xa0             2019\\n    \\n                                                 \\xa0›\\xa0             April\\n    \\n                                                 \\xa0›\\xa0             Buggy Races Keep Rolling at Carnegie Mellon\\n                            \\nApril 10, 2019\\nBuggy Races Keep Rolling at Carnegie Mellon\\nIn its 99th year, the tradition is a Spring Carnival treat\\nBy Heidi Opdyke opdyke(through)andrew.cmu.edu\\nMedia Inquiries \\nJulie Mattera\\nMarketing and Communications\\n jmattera(through)cmu.edu\\n412-268-2902\\n \\nSweepstakes, also known as the Buggy Races, has come a long way at Carnegie Mellon University. The slick, torpedo-like vessels carrying drivers with nerves of steel are a far cry from the two-man teams that once changed places halfway through a race and rode in everything from rain barrels with bicycle wheels to three-wheeled ash cans 99 years ago.\\nToday, it takes six people to maneuver the .84 -mile course around Schenley Park\\'s Flagstaff Hill.\\nBut while five pushers and a driver navigate the course\\'s hills, dozens of people are needed to make a successful race happen. A year of planning goes into just over two minutes of racing.\\nThe Machine\\nThe basics of a buggy are straightforward, but teams are often secretive in how they build the machines, in particular the way they brake, steer and what types of wheels are used.\\nEach has a body, pushbar for runners to move the machine up the hills, wheels, a safety harness and driving and braking mechanisms. Some also include fairings, a type of housing around the wheels that help reduce drag, make the vehicle quieter and just looks cool.\\nFairings have been a key feature for the Fringe team in recent years, which is celebrating its 50th anniversary of Buggy.\\n\"We have a reputation of being the quietest on the course,\" said Diya Nuxoll, who wrapped up her bachelor\\'s degree in mechanical engineering in December and is working on an advanced studies master\\'s degree in design and manufacturing. Nuxoll leads the Fringe team as one of two head mechanics.\\nAn mural celebrating Buggy can be found in\\xa0CMU\\'s\\xa0Stever House. The artwork is part of a larger installation showcasing Sweepstakes and its traditions.\\nSweepstakes Slang\\nBuggy: Vehicle being raced and also a nickname for the competition. Chute: A section of the freeroll portion of the buggy course (near the southwestern end of Frew Street at its intersection with Schenley Drive) where buggies make the sharp righthand turn from Schenley Drive onto Frew Street. Chute Flagger: Team member who provides a signal for buggy drivers to know when to start the right-hand turn from Schenley Drive onto Frew Street. Driver: Person who travels with a buggy and controls the vehicles via steering and braking systems. Pushbar: Structure attached to a buggy that a person pushes to propel that buggy forward. Pusher: Person who propels a buggy via a pushbar along one of the five hills of the buggy course. Shell: Entire outer structure or covering of a buggy that determines that buggy\\'s aerodynamic characteristics. Transition: Procedure whereby one pusher finishes pushing a buggy and the next pusher in sequence starts to push that same buggy.\\n            \\n Members of Fringe, celebrating their 50th year of Buggy racing,\\xa0allowed a rare visit into its shop for a behind-the-scenes tour. \\n \\n This year Fringe is planning to roll four different vehicles, built and maintained in the Fringe workshop in the basement of the East Campus Garage, known as the \"Froom.\"\\n\"We\\'re used to saying everything with \\'fr\\' in front of it but when we say something in front of other people, it gets them confused,\" said Fringe head mechanic Dave Singh, who will graduate in May with a bachelor\\'s degree in mechanical engineering and biomedical engineering.\\nFringe vehicles often are named with the letter \"B,\" like Boson, Blueshift, Bissa and Bumper. Other teams, such as Apex, often use names that connotate fire, while the SDC (Student Dormitory Committee) team, uses names such as Vice, Bane, Avarice and Malice.\\nThis year\\'s buggy names will be under wraps until Thursday, when the Buggy Showcase will take place from noon to 2 p.m. in Weigand Gymnasium in the Cohon University Center. It\\'s one of the few times spectators can see buggies up close.\\n \\n Dave Singh explains how Fringe builds its buggies. Each team keeps its actual processes a secret, but each buggy has certain features such as a body, pushbar, wheels and driving and braking mechanisms. Behind Singh are other buggies built by Fringe.\\n \\n Behind the Wheel\\nDrivers, have the closest connection to the vehicles, aside from the mechanics who spent countless hours building and maintaining individual buggies. While races are brief, each driver must log a number of practice runs to qualify to race. Teams practice nights and weekends throughout the fall and spring semesters as weather permits. Buggies are often built around drivers, so the fit can be snug, and drivers are often less than 5 feet, 3 inches in height. Most — but not all — are women.\\nTishya Girdhar, a junior in neuroscience, is social chair for Fringe. She came to CMU wanting to drive, but first she had to get over being claustrophobic. She started as a mechanic and got behind the wheel for last year\\'s Sweepstakes.\\n\"Our team\\'s philosophy is to teach everyone how to do everything,\" Girdhar said. \"But I came to CMU wanting to drive. I wanted to drive so badly.\"\\n \\n Amy Chen demonstrates how a driver is positioned inside of Boson, a Fringe buggy built in 2016.\\n \\n Safety is a top priority for teams. All drivers are required to have five pieces of safety gear: mouth guard, goggles, a harness that includes three points of contact to the body of the buggy, gloves and a helmet. And during practice runs and races, flaggers man the course to let drivers know if it\\'s safe to proceed.\\nThe streets of the race course haven\\'t changed, but the condition of the course varies due to potholes, which can make or break a fast run. The last course records were set in 2017, but this year potholes are making for some challenging conditions, said Amy Chen, a senior in psychology who has been driving for four years.\\nWith the vehicles just inches from the ground, \"even poorly filled potholes makes it dangerous to drive,\" she added.\\nDepending on Friday and Saturday\\'s conditions, most heats may run just two lanes instead of three. But still, despite some of the challenges, Chen said she wouldn\\'t miss it.\\n\"Being a driver is really fun,\" Chen said. \"I love going fast and going around the course.\"\\n \\n \\n — #C'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = text[6: 6970]\n",
    "text = text.split('\\n')\n",
    "text = [t for t in text if t != '']\n",
    "text = '\\n'.join(text)\n",
    "text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "with open('/zfsauton2/home/yifuc/11711-RAG/data/cmu/buggy_history.json', 'w') as f:\n",
    "    json.dump([text], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tartan, Scotty, and Kiltie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#it's really just easier to copy the whole page \n",
    "text = '''Tartan Facts: Who founded Carnegie Mellon University? Carnegie Technical Schools was founded in 1900 by Andrew Carnegie. Twelve years later it became known as the Carnegie Institute of Technology. In 1967, the school merged with Mellon Institute and became what is known today as Carnegie Mellon University. What is a Tartan? The Carnegie Mellon athletic teams are nicknamed the \"Tartans\" as a nod to Andrew Carnegie's Scottish heritage. A tartan is often misrepresented as a fierce warrior from either the Asian tundra or Scottish highlands. In actuality, a Tartan is a twilled woolen fabric with a plaid design. It is of Scottish origin and consists of stripes of various colors and widths against a solid ground, denoting a particular family lineage. The school's founder, Andrew Carnegie, was born in Dunfermline, Scotland, in 1835. Carnegie came to the United States in 1848 and founded Carnegie Technical Schools in Pittsburgh in 1900.\n",
    "\n",
    "The Scottish terrier mascot performer sports Carnegie tartan attire, while the graphic mascot is wearing a plaid scarf around its neck. So what's the difference between tartan and plaid?\n",
    "\n",
    "You'll know it's a tartan if...• \"It's a check or pattern in a variety of colours in woven fabric in which bands of colour are repeated in equal proportion in warp (running lengthwise) and weft (running across).\"\n",
    "• \"Each stripe of the warp crosses every stripe of the weft, so when vertical and horizontal stripes of the same color cross, the result is solid color at the point of intersection.\"\n",
    "• \"The arrangement of colored threads is the same in the warp as in the weft.\"\n",
    "\n",
    "You can find our official tartan on various items in the University Store.\n",
    "\n",
    "Source: \"Tartan: Romancing the Plaid,\" by Jeffrey Banks and Doria De La Chapelle \n",
    "\n",
    " \n",
    "\n",
    "Official Mascot?\n",
    "More than a century after Carnegie Mellon University opened its doors, an official mascot finally made its mark. Although students have dressed as a Scottish terrier — typically referred to as Scotty — for 50 years, it wasn't until 2007 that Carnegie Mellon officially welcomed the Scottish terrier as the university's first mascot.\n",
    "\n",
    "In keeping with tradition, students, faculty, staff and alumni voted to name the new official mascot Scotty. But it's not just the costumed mascot that voters named. The live dog is also known as Scotty to the Carnegie Mellon community.\n",
    "\n",
    "About Scotty | Celebrating Scotty | Performer Requests\n",
    "\n",
    " \n",
    "\n",
    "Department Quick Facts\n",
    "\n",
    "Name of School: Carnegie Mellon University\n",
    "\n",
    "City/Zip: Pittsburgh, PA 15213\n",
    "\n",
    "Founded: 1900\n",
    "\n",
    "Enrollment (undergrad): 6,982- Fall 2021\n",
    "\n",
    "Enrollment (graduate and doctoral): 7,062 - Fall 2021\n",
    "\n",
    "Alumni: 102,577\n",
    "\n",
    "Nickname: Tartans\n",
    "\n",
    "Mascot: Scottie Dog\n",
    "\n",
    "School Colors: Cardinal and Gray\n",
    "\n",
    "Football Stadium and Track and Field: Gesling Stadium\n",
    "\n",
    "Capacity: 3,500\n",
    "\n",
    "Surface: FieldTurf\n",
    "\n",
    "Basketball/Volleyball Gym: Wiegand Gymnasium\n",
    "\n",
    "Capacity: 500\n",
    "\n",
    "Soccer Stadium: CMU Soccer Field\n",
    "\n",
    "Capacity: 250 Surface: FieldTurf Affiliation: NCAA Division III Conference: University Athletic Association | Presidents' Athletic Conference (Football Only) President: Dr. Farnam Jahanian Alma Mater, Year: University of Texas at Austin Athletic Director: Dr. Josh Centor Alma Mater, Year: Brandeis, 2004 Athletic Department Phone: 412-268-8054 Mailing Address:  5000 Forbes Avenue / Pittsburgh, PA 15213\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "with open('/zfsauton2/home/yifuc/11711-RAG/data/cmu/tartan_fact.json', 'w') as f:\n",
    "    json.dump([text], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''About Scotty\n",
    "\n",
    "The Scottish terrier has long been a familiar figure around Carnegie Mellon's campus. For years students have suited up in an unofficial Scottish terrier costume to excite the fans at athletic events. But the relationship between the Scottish terrier breed and Carnegie Mellon far precedes anybody doing somersaults in a dog costume. Andrew Carnegie, founder of the university, kept a Scottish terrier as his pet.\n",
    "\n",
    "Scotty's road from popular icon to official mascot of the university began in 2006. Carnegie Mellon formed a Mascot Identity Task Force in November 2006, which consisted of students, faculty, staff and alumni. The Task Force was co-chaired by Director of Athletics Susan Bassett and Dean of Student Affairs Jennifer Church.\n",
    "\n",
    "The mascot selection process included a series of surveys and a university Town Hall meeting. Nearly 78 percent of 2,370 students surveyed in February 2007 voted for the Scottish terrier, and approximately 25 percent of 400 alumni surveyed thought the Scottish terrier was already the mascot.\n",
    "\n",
    "In the spring, the Task Force partnered with SME Branding — a firm with more than 17 years of experience creating mascots for professional sports teams and universities — to develop the graphics for the mascot. During October, students and alumni reviewed potential mascot images in focus groups.\n",
    "\n",
    "Carnegie Mellon's official mascot debuted at the Nov. 10, 2007 home football game. The graphic features a profile of a distinguished, bold Scottish terrier sporting a plaid scarf around his neck. The dog is contained in a shield, representing Carnegie Mellon's Scottish heritage.\n",
    "\n",
    "The Task Force then partnered with a mascot costume company to design our Scottish terrier in the winter of 2007. The official Scotty costume was unveiled at the 2008 Spring Carnival.\n",
    "\n",
    "Fun Fact: The Scottish terrier breed is known for its keen, alert and intelligent expression. Its temperament is described as determined and thoughtful while its physical aspects exemplify strength, power and agility in a small package. Many of these traits are also apparent throughout the university, making the Scottish terrier a natural choice for Carnegie Mellon's mascot.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "with open('/zfsauton2/home/yifuc/11711-RAG/data/cmu/scotty_fact.json', 'w') as f:\n",
    "    json.dump([text], f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '''The Kiltie Band\n",
    "\n",
    "The Kiltie Band began in 1908 with a group of just seven students dedicated to supporting Carnegie Tech football, and today's Kiltie Band continues a tradition of excellence originated over a century ago. The Band took the field for its first official performance on November 25th, 1922, on what would have been Andrew Carnegie's 87th birthday.\n",
    "\n",
    "The Kiltie Marching Band consists of musicians and colorguard and plays at all Carnegie Mellon home football games. There is a new show every week with music ranging from traditional marches and oldies to current pop tunes and jazz standards.\n",
    "\n",
    "After football season, the Kiltie Concert Band plays a holiday concert and two spring concerts, including a performance at Carnegie Mellon's Spring Carnival. Additionally, a part of the band gets together and acts as a pep band for the basketball teams.\n",
    "\n",
    "The Kiltie Band is a campus organization recognized by Student Senate and administered by the Department of Athletics, Physical Education and Recreation. Membership is open to all members of the campus/community without audition. Participants must do their best to promote the success of the ensemble through attendance, attitude, and dependability.\n",
    "FAQs\n",
    "\n",
    "Q: When are rehearsals?\n",
    "A: Football season: Mondays and Thursdays from 5:30 p.m.-7:30 p.m. and Game Day 9:30-11:00a.m. Concert Season: Mondays and Thursdays from 5:30-6:30 p.m.\n",
    "Location is the CUC Studio Theater.\n",
    "\n",
    "Q: When are performances?\n",
    "A: The Kiltie Band performs at all home football games, a holiday concert, and two spring concerts. Times and dates for all performances are announced at the first practice.\n",
    "\n",
    "Q: Are there auditions?\n",
    "A: No, any member of the campus community with music experience is able to join the Kiltie Band!\n",
    "\n",
    "Q: Do I have to memorize music?\n",
    "A: No, the music is changed for every show. You should invest in a good and trusty lyre.\n",
    "\n",
    "Q: When is the first rehearsal?\n",
    "A: On the first day of class (Monday) at 5:30 p.m. in the CUC Studio Theater. If you have questions, please email the Direcator at jolisar@andrew.cmu.edu.\n",
    "\n",
    "Q: Can I borrow a school instrument?\n",
    "A: There are several Kiltie-owned instruments available for your use. Loans begin at 4:30 p.m. before the first rehearsal. \n",
    "\n",
    "Q: What do they wear under those kilts?\n",
    "A: Join and you'll find out!\n",
    "\n",
    "For more information, visit the Kiltie Band website.\n",
    "\n",
    "Interested in Joining?\n",
    "\n",
    "Please email the following information to Kiltie Band Director Jeremy Olisar.\n",
    "\n",
    "    Name\n",
    "    High School\n",
    "    Address at Carnegie Mellon (if known)\n",
    "    Home address\n",
    "    Cell number\n",
    "    Home number\n",
    "    Whether you plan on being in the band or colorguard\n",
    "    If in the band what instrument(s) you play\n",
    "    Whether you need to borrow an instrument or equipment\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "with open('/zfsauton2/home/yifuc/11711-RAG/data/cmu/kiltie_band_fact.json', 'w') as f:\n",
    "    json.dump([text], f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carnival"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepared locally using selenium"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Commencement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\n",
    "''' \n",
    "Commencement Schedule\n",
    "\n",
    "We are proud to celebrate our newest graduates during this year’s commencement exercises!\n",
    "\n",
    "Main Commencement Ceremony\n",
    "\n",
    "    Bachelor’s, master’s and doctoral degree candidates and their guests are invited to join the main commencement ceremony on Sunday, May 12, for the conferral of all degrees.\n",
    "    The main ceremony will include remarks from the president, keynote speaker, student speaker and academic deans, in addition to recognition of the honorary degree recipients.\n",
    "    There is no limit on number of guests who can attend the main commencement ceremony and tickets are not needed.\n",
    "    The ceremony will take place on CMU’s campus beginning at 10 a.m. and will be approximately 1.5 hours long. All guests must be seated by 9:15 a.m. for the start of teh student procession. Access to guest seating will be restricted once the student procession begins.\n",
    "''',\n",
    "''' \n",
    "Diploma Ceremonies\n",
    "\n",
    "    In addition to the main commencement ceremony, each college/school/department will host a diploma ceremony to recognize all graduating students. Diploma ceremonies will be held over the course of the weekend (Friday, May 10–Sunday, May 12).\n",
    "    Diploma ceremonies will include the presentation of diplomas to graduates, hooding of doctoral candidates and remarks from their college/school/department leadership. Each ceremony is organized and customized by their college/school/department.\n",
    "    Diploma ceremonies will take place both on and off campus. Locations and times for diploma ceremonies will be provided soon.\n",
    "    Diploma ceremonies typically include a reception and the length of time for each ceremony varies based on the number of graduates.\n",
    "\n",
    "More details on the weekend schedule, including a diploma ceremony schedule, will be provided in the coming weeks.  \n",
    "''',\n",
    "''' \n",
    "\n",
    "Thursday, May 9\n",
    "\n",
    "Phi Beta Kappa Initiation Ceremony\n",
    "Ceremony: 2–3 p.m.\n",
    "McConomy Auditorium, Cohon University Center\n",
    "Reception: 3–4 p.m.\n",
    "Connan Room, Cohon University Center\n",
    "\n",
    "Contact:\n",
    "Joseph Devine\n",
    "jd0x@andrew.cmu.edu \n",
    "\n",
    "Joanne Ursenbach\n",
    "joanneu@andrew.cmu.edu \n",
    "\n",
    "President's Graduates Toast (bachelor's students)\n",
    "3:30–4:30 p.m.\n",
    "Location TBD\n",
    "Registration required. Invitation, along with registration details, will be sent in late April.\n",
    "\n",
    "First Gen Graduation Recognition\n",
    "Reception: 5-5:30 p.m.\n",
    "Alumni Concert Hall, College of Fine Arts\n",
    "Ceremony: 5:30-6:30p.m.\n",
    "Kresge Theater, College of Fine Arts\n",
    "\n",
    "Contact:\n",
    "M. Shernell Smith\n",
    "mssmith@andrew.cmu.edu \n",
    "\n",
    "Sam Colavecchio\n",
    "scolavec@andrew.cmu.edu \n",
    "412-268-7733\n",
    "\n",
    "''',\n",
    "''' \n",
    "Friday, May 10\n",
    "\n",
    "Diploma Ceremonies\n",
    "Various times\n",
    "\n",
    "Senior Leadership Recognition Ceremony\n",
    "4–5:30 p.m.\n",
    "Wiegand Gym, Cohon University Center\n",
    "Undergraduate students and their guests by invitation only. This ceremony recognizes nominated seniors who have reflected upon their specific leadership contributions during their time at CMU.\n",
    "''',\n",
    "''' \n",
    "\n",
    "Saturday, May 11\n",
    "\n",
    "Diploma Ceremonies\n",
    "Various times\n",
    "\n",
    "Center for Student Diversity and Inclusion Ceremony\n",
    "Noon–2:30 p.m.\n",
    "Simmons Auditorium, Tepper Building\n",
    "\n",
    "Contact:\n",
    "M. Shernell Smith\n",
    "mssmith@andrew.cmu.edu \n",
    "\n",
    "Sam Colavecchio\n",
    "scolavec@andrew.cmu.edu \n",
    "\n",
    "Naval ROTC Commissioning\n",
    "Ceremony: 1:30-2:30 p.m.\n",
    "Auditorium, Soldiers & Sailors Memorial Hall & Museum *\n",
    "4141 Fifth Avenue, Pittsburgh, PA 15213\n",
    "\n",
    "Contact:\n",
    "Mike Danko\n",
    "mdanko@andrew.cmu.edu \n",
    "\n",
    "The President’s Reception in honor of CMU’s Doctoral Candidates\n",
    "4–6 p.m.\n",
    "Tepper Building Atrium\n",
    "\n",
    "''',\n",
    "''' \n",
    "Sunday, May 12\n",
    "\n",
    "Gesling Stadium opens to guests\n",
    "8 a.m.\n",
    "\n",
    "Robing and procession for graduates\n",
    "9–10 a.m.\n",
    "Various locations across campus\n",
    "\n",
    "Student procession begins\n",
    "9:15 a.m.\n",
    "All guests in stadium must be seated. Access to guest seating will be restricted once the procession begins.\n",
    "\n",
    "Commencement Ceremony\n",
    "10–11:30 a.m.\n",
    "Gesling Stadium, CMU’s campus\n",
    "\n",
    "Diploma Ceremonies\n",
    "Various times\n",
    "''',\n",
    "''' \n",
    "What is the difference between a diploma ceremony and the main commencement ceremony?\n",
    "\n",
    "All graduates (bachelor’s, master’s and doctoral degree candidates) will be celebrated at the main commencement ceremony on Sunday, May 12. The ceremony will include a procession of graduates, conferral of degrees, remarks from the president, keynote speaker, student speaker and academic deans, in addition to recognition of the honorary degree recipients.\n",
    "\n",
    "In addition to the main commencement ceremony, each college/school/department will host a diploma ceremony to recognize their graduating students. Diploma ceremonies will be held over the course of the weekend (Friday, May 10–Sunday, May 12) and include the presentation of diplomas to graduates, hooding of doctoral candidates and remarks from their college/school/department leadership.\n",
    "\n",
    "''',\n",
    "''' \n",
    "Are there any restrictions for guests?\n",
    "\n",
    "Guests are welcome to attend commencement exercises. There is no limit to the number of guests for the main commencement ceremony and tickets are not needed. \n",
    "\n",
    "There is no limit to the number of guests for the President’s Reception for Doctoral Candidates and tickets are not needed. However, we ask that all doctoral graduates register the number of guests they are planning to bring to the reception so that we can prepare accordingly. \n",
    "\n",
    "Please note that some diploma ceremonies will limit guests based on venue capacity. \n",
    "''',\n",
    "''' \n",
    "My family and friends aren’t able to travel to Pittsburgh for commencement weekend; will the events be livestreamed?\n",
    "\n",
    "We know how important it is to share this milestone with families near and far. Sunday’s Commencement Ceremony will be livestreamed. Diploma ceremonies will be also be live-streamed to the extent possible.\n",
    "\n",
    "''',\n",
    "''' \n",
    "Are drones or unmanned aircraft systems permitted to be operated as part of commencement?\n",
    "The operation of drones is prohibited prior to, during and after the commencement ceremony on May 12. For more information about the Drone Safety Program, click here. \n",
    "''',\n",
    "''' \n",
    "Graduates\n",
    "\n",
    "Congratulations on your upcoming graduation! We can't wait to celebrate with you during Commencement, May 10 – 12, 2024.\n",
    "\n",
    "The following groups are eligible to participate in commencement exercises:\n",
    "\n",
    "    Bachelor’s degree students who completed their degree in August 2023, December 2023 and expected May and August 2024 graduates.\n",
    "    Master’s degree students who completed their degree in August 2023, December 2023 and expected May and August 2024 graduates.\n",
    "    Doctoral degree students who have defended their dissertation and completed all degree requirements in August 2023, December 2023 and expected May 2024 graduates.\n",
    "\n",
    "As you plan for commencement weekend, refer to this page for information about deadlines, cap and gown, robing, processional and more. We will update the site with new information frequently so check back often!\n",
    "''',\n",
    "''' \n",
    "\n",
    "Diplomas\n",
    "\n",
    "Graduating students will receive diplomas at their diploma ceremonies, which colleges/schools/departments will host throughout the weekend. We will share a final schedule in the coming weeks.  \n",
    "\n",
    "If you are unable to attend your diploma ceremony, we will mail you your diploma. You should enter your diploma delivery addresses in SIO under Academic Info > Graduation and Diploma. Because students often move shortly after graduation, we use this address instead of your permanent address.\n",
    "\n",
    "If you are eligible to receive honors cords and medallions, you will receive them either prior to commencement or at your diploma ceremony. We will share more details in the spring.   \n",
    "\n",
    "If you have questions regarding diploma distribution, please contact Enrollment Services at uro-diplomas@andrew.cmu.edu.\n",
    "\n",
    "''',\n",
    "''' \n",
    "\n",
    "Cap & Gown\n",
    "\n",
    "If you plan to attend the main commencement ceremony or your diploma ceremony, you should plan to purchase a cap and gown.\n",
    "\n",
    "To purchase cap, gown and in-stock diploma frames, be sure to stop by Rangos Ballroom on March 13-14 from 10 a.m.-5 p.m. Check out graduation swag, class rings and announcements, and enter to win door prizes.\n",
    "\n",
    "If you are unable to attend, you can purchase your regalia at the University Store or order online by April 12 to have it shipped to you in time. Regalia fees can be found here.\n",
    "\n",
    "''',\n",
    "\n",
    "'''\n",
    "\n",
    "Diploma Frames, Class Rings and other Graduation Merchandise\n",
    "\n",
    "You can purchase items to help commemorate your graduation at the University Store.\n",
    "\n",
    "Visit the University Store website and choose the Graduation tab for more information on ordering CMU personalized class rings, personalized announcements, gifts, diploma frames and more to celebrate your graduation.\n",
    "\n",
    "''',\n",
    "''' \n",
    "\n",
    "Career and Professional Development Center\n",
    "\n",
    "The Career and Professional Development Center (CPDC) offers a variety of career services for students and alumni. In addition to the services described on the CPDC website, alumni can use all of the resources available to current students, including job postings in Handshake.\n",
    "\n",
    "''',\n",
    "''' \n",
    "\n",
    "CMU Alumni Association\n",
    "\n",
    "Congratulations graduating students and welcome to the Carnegie Mellon University Alumni Association! Learn how to connect with other alumni and engage with the university through events and other alumni programming by joining the CMU Alumni Community.\n",
    "\n",
    "Following your graduation, you will begin receiving information about upcoming events, available resources, campus news and more to help introduce you to the Alumni Association. To ensure that you receive these messages, please update your contact information. And as your city, job and contact information change over time, please keep your information current so that we can stay in touch!\n",
    "\n",
    "''',\n",
    "\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "with open('/zfsauton2/home/yifuc/11711-RAG/data/cmu/commencement_schedule.json', 'w') as f:\n",
    "    json.dump(text, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Handbooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 49/49 [00:00<00:00, 58.67it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 65.29it/s]\n",
      "100%|██████████| 48/48 [00:00<00:00, 67.74it/s]\n",
      "100%|██████████| 56/56 [00:01<00:00, 42.20it/s]\n",
      "100%|██████████| 53/53 [00:00<00:00, 162.72it/s]\n"
     ]
    }
   ],
   "source": [
    "#use pypdf to parse an online pdf\n",
    "urls = []\n",
    "import io\n",
    "import requests\n",
    "from PyPDF2 import PdfReader\n",
    "from tqdm import tqdm\n",
    "\n",
    "urls = [r'https://lti.cs.cmu.edu/sites/default/files/PhD_Student_Handbook_2023-2024.pdf', \n",
    "         r'https://lti.cs.cmu.edu/sites/default/files/MLT%20Student%20Handbook%202023%20-%202024.pdf',\n",
    "         r'https://lti.cs.cmu.edu/sites/default/files/MIIS%20Handbook_2023%20-%202024.pdf',\n",
    "         r'https://lti.cs.cmu.edu/sites/default/files/MCDS%20Handbook%2023-24%20AY.pdf',\n",
    "         r'https://msaii.cs.cmu.edu/sites/default/files/Handbook-MSAII-2022-2023.pdf']\n",
    "texts = []\n",
    "for url in urls:\n",
    "    headers = {'User-Agent': 'Mozilla/5.0 (X11; Windows; Windows x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.5060.114 Safari/537.36'}\n",
    "\n",
    "    response = requests.get(url=url, headers=headers, timeout=120)\n",
    "    on_fly_mem_obj = io.BytesIO(response.content)\n",
    "    pdf_file = PdfReader(on_fly_mem_obj)\n",
    "    page_text = []\n",
    "    for p in tqdm(pdf_file.pages):\n",
    "        page_text.append( p.extract_text() )\n",
    "    texts.append(page_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#texts is a list of list, make it a list of strings, by removing inner list, keep strings as what they are\n",
    "texxts = []\n",
    "for text in texts:\n",
    "    texxts.extend(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json \n",
    "with open('/zfsauton2/home/yifuc/11711-RAG/data/cmu/handbook_texts.json', 'w') as f:\n",
    "    json.dump(texxts, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def faculty_publication(data, num_samples=50, verbose_p=0.5):\n",
    "    '''\n",
    "    Author: Yonatan Bisk \n",
    "    Title: HomeRobot: An Open Source Software Stack for Mobile Manipulation Research \n",
    "    Publication year: 2024 \n",
    "    '''\n",
    "\n",
    "    template = {\n",
    "        \"Which LTI faculty published the paper [title] in [year]?\":\"author\",\n",
    "        \"Who is the author of the LTI paper [title] published in [year]?\":\"author\",\n",
    "        \"What is the title of the paper published by LTI faculty [author] in [year]?\":\"title\",\n",
    "        \"What is the abstract of the paper [title] published by LTI faculty [author] in [year]?\":\"abstract\",\n",
    "        \"Who are the coauthors of the paper [title] published by LTI faculty [author] in [year]?\":\"coauthor\",\n",
    "    }\n",
    "\n",
    "    result = []\n",
    "\n",
    "    for _ in range(num_samples):\n",
    "        data_dict = dict()\n",
    "        data_string = random.choice(data)\n",
    "        author = data_string.split(\"Author: \")[1].split(\"Title\")[0].strip()\n",
    "        title = data_string.split(\"Title: \")[1].split(\"Publication\")[0].strip()\n",
    "        year = data_string.split(\"Publication year: \")[1].split(\"Coauthors\")[0].strip()\n",
    "        coauthors = data_string.split(\"Coauthors: \")[1].split(\"Abstract\")[0].strip()\n",
    "        try:\n",
    "            abstract = data_string.split(\"Abstract: \")[1].strip()\n",
    "        except:\n",
    "            abstract = None\n",
    "\n",
    "        question = random.choice(list(template.keys()))\n",
    "        question_type = template[question]\n",
    "\n",
    "        if question_type == 'author':\n",
    "            answer = author\n",
    "            question = question.replace('[title]', title)\n",
    "            context = data_string\n",
    "   \n",
    "        if question_type == 'title':\n",
    "            answer = title\n",
    "            question = question.replace('[author]', author)\n",
    "            context = data_string\n",
    "        \n",
    "        if question_type == 'abstract':\n",
    "            answer = abstract\n",
    "            if abstract is None:\n",
    "                continue \n",
    "            question = question.replace('[title]', title)\n",
    "            question = question.replace('[author]', author)\n",
    "            context = data_string\n",
    "        \n",
    "        if question_type == 'coauthor':\n",
    "            answer = coauthors\n",
    "            question = question.replace('[title]', title)\n",
    "            question = question.replace('[author]', author)\n",
    "            context = data_string\n",
    "        \n",
    "        question = question.replace('[year]', year)\n",
    "        data_dict['question'] = question\n",
    "        data_dict['answer'] = answer.strip()\n",
    "        data_dict['context'] = context\n",
    "        result.append(data_dict)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'question': 'Which LTI faculty published the paper Joint Modelling of Spoken Language Understanding Tasks with Integrated Dialog History in 2023?',\n",
       "  'answer': 'Shinji Watanabe',\n",
       "  'context': 'Author: Shinji Watanabe Title: Joint Modelling of Spoken Language Understanding Tasks with Integrated Dialog History Publication year: 2023 Coauthors: Siddhant Arora, Hayato Futami, E. Tsunoo, Brian Yan, Shinji Watanabe Abstract: Most human interactions occur in the form of spoken conversations where the semantic meaning of a given utterance depends on the context. Each utterance in spoken conversation can be represented by many semantic and speaker attributes, and there has been an interest in building Spoken Language Understanding (SLU) systems for automatically predicting these attributes. Recent work has shown that incorporating dialogue history can help advance SLU performance. However, separate models are used for each SLU task, leading to an increase in inference time and computation cost. Motivated by this, we aim to ask: can we jointly model all the SLU tasks while incorporating context to facilitate low-latency and lightweight inference? To answer this, we propose a novel model architecture that learns dialog context to jointly predict the intent, dialog act, speaker role, and emotion for the spoken utterance. Note that our joint prediction is based on an autoregressive model and we need to decide the prediction order of dialog attributes, which is not trivial. To mitigate the issue, we also propose an order agnostic training method. Our experiments show that our joint model achieves similar results to task-specific classifiers and can effectively integrate dialog context to further improve the SLU performance.1'},\n",
       " {'question': 'Who are the coauthors of the paper Cappy: Outperforming and Boosting Large Multi-Task LMs with a Small Scorer published by LTI faculty Eric P. Xing in 2023?',\n",
       "  'answer': 'Bowen Tan, Yun Zhu, Lijuan Liu, Eric P. Xing, Zhiting Hu, Jindong Chen',\n",
       "  'context': 'Author: Eric P. Xing Title: Cappy: Outperforming and Boosting Large Multi-Task LMs with a Small Scorer Publication year: 2023 Coauthors: Bowen Tan, Yun Zhu, Lijuan Liu, Eric P. Xing, Zhiting Hu, Jindong Chen Abstract: Large language models (LLMs) such as T0, FLAN, and OPT-IML, excel in multi-tasking under a unified instruction-following paradigm, where they also exhibit remarkable generalization abilities to unseen tasks. Despite their impressive performance, these LLMs, with sizes ranging from several billion to hundreds of billions of parameters, demand substantial computational resources, making their training and inference expensive and inefficient. Furthermore, adapting these models to downstream applications, particularly complex tasks, is often unfeasible due to the extensive hardware requirements for finetuning, even when utilizing parameter-efficient approaches such as prompt tuning. Additionally, the most powerful multi-task LLMs, such as OPT-IML-175B and FLAN-PaLM-540B, are not publicly accessible, severely limiting their customization potential. To address these challenges, we introduce a pretrained small scorer, Cappy, designed to enhance the performance and efficiency of multi-task LLMs. With merely 360 million parameters, Cappy functions either independently on classification tasks or serve as an auxiliary component for LLMs, boosting their performance. Moreover, Cappy enables efficiently integrating downstream supervision without requiring LLM finetuning nor the access to their parameters. Our experiments demonstrate that, when working independently on 11 language understanding tasks from PromptSource, Cappy outperforms LLMs that are several orders of magnitude larger. Besides, on 45 complex tasks from BIG-Bench, Cappy boosts the performance of the advanced multi-task LLM, FLAN-T5, by a large margin. Furthermore, Cappy is flexible to cooperate with other LLM adaptations, including finetuning and in-context learning, offering additional performance enhancement.'},\n",
       " {'question': 'Who are the coauthors of the paper Learning to Filter Context for Retrieval-Augmented Generation published by LTI faculty Graham Neubig in 2023?',\n",
       "  'answer': 'Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md. Rizwan Parvez, Graham Neubig',\n",
       "  'context': 'Author: Graham Neubig Title: Learning to Filter Context for Retrieval-Augmented Generation Publication year: 2023 Coauthors: Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md. Rizwan Parvez, Graham Neubig Abstract: On-the-fly retrieval of relevant knowledge has proven an essential element of reliable systems for tasks such as open-domain question answering and fact verification. However, because retrieval systems are not perfect, generation models are required to generate outputs given partially or entirely irrelevant passages. This can cause over- or under-reliance on context, and result in problems in the generated output such as hallucinations. To alleviate these problems, we propose FILCO, a method that improves the quality of the context provided to the generator by (1) identifying useful context based on lexical and information-theoretic approaches, and (2) training context filtering models that can filter retrieved contexts at test time. We experiment on six knowledge-intensive tasks with FLAN-T5 and LLaMa2, and demonstrate that our method outperforms existing approaches on extractive question answering (QA), complex multi-hop and long-form QA, fact verification, and dialog generation tasks. FILCO effectively improves the quality of context, whether or not it supports the canonical output.'},\n",
       " {'question': 'Who are the coauthors of the paper End-to-End Evaluation for Low-Latency Simultaneous Speech Translation published by LTI faculty A. Waibel in 2023?',\n",
       "  'answer': 'Christian Huber, Tu Anh Dinh, Carlos Mullov, Ngoc-Quan Pham, T. Nguyen, Fabian Retkowski, Stefan Constantin, Enes Yavuz Ugan, Danni Liu, Zhaolin Li, Sai Koneru, J. Niehues, A. Waibel',\n",
       "  'context': 'Author: A. Waibel Title: End-to-End Evaluation for Low-Latency Simultaneous Speech Translation Publication year: 2023 Coauthors: Christian Huber, Tu Anh Dinh, Carlos Mullov, Ngoc-Quan Pham, T. Nguyen, Fabian Retkowski, Stefan Constantin, Enes Yavuz Ugan, Danni Liu, Zhaolin Li, Sai Koneru, J. Niehues, A. Waibel Abstract: The challenge of low-latency speech translation has recently draw significant interest in the research community as shown by several publications and shared tasks. Therefore, it is essential to evaluate these different approaches in realistic scenarios. However, currently only specific aspects of the systems are evaluated and often it is not possible to compare different approaches. In this work, we propose the first framework to perform and evaluate the various aspects of low-latency speech translation under realistic conditions. The evaluation is carried out in an end-to-end fashion. This includes the segmentation of the audio as well as the run-time of the different components. Secondly, we compare different approaches to low-latency speech translation using this framework. We evaluate models with the option to revise the output as well as methods with fixed output. Furthermore, we directly compare state-of-the-art cascaded as well as end-to-end systems. Finally, the framework allows to automatically evaluate the translation quality as well as latency and also provides a web interface to show the low-latency model outputs to the user.'},\n",
       " {'question': 'Who is the author of the LTI paper PWESuite: Phonetic Word Embeddings and Tasks They Facilitate published in 2023?',\n",
       "  'answer': 'David R. Mortensen',\n",
       "  'context': 'Author: David R. Mortensen Title: PWESuite: Phonetic Word Embeddings and Tasks They Facilitate Publication year: 2023 Coauthors: Vilém Zouhar, Kalvin Chang, Chenxuan Cui, Nathaniel Carlson, Nathaniel R. Robinson, Mrinmaya Sachan, David R. Mortensen Abstract: Mapping words into a fixed-dimensional vector space is the backbone of modern NLP. While most word embedding methods successfully encode semantic information, they overlook phonetic information that is crucial for many tasks. We develop three methods that use articulatory features to build phonetically informed word embeddings. To address the inconsistent evaluation of existing phonetic word embedding methods, we also contribute a task suite to fairly evaluate past, current, and future methods. We evaluate both (1) intrinsic aspects of phonetic word embeddings, such as word retrieval and correlation with sound similarity, and (2) extrinsic performance on tasks such as rhyme and cognate detection and sound analogies. We hope our task suite will promote reproducibility and inspire future phonetic embedding research.'},\n",
       " {'question': 'What is the title of the paper published by LTI faculty Chenyan Xiong in 2023?',\n",
       "  'answer': 'Structure-Aware Language Model Pretraining Improves Dense Retrieval on Structured Data',\n",
       "  'context': 'Author: Chenyan Xiong Title: Structure-Aware Language Model Pretraining Improves Dense Retrieval on Structured Data Publication year: 2023 Coauthors: Xinze Li, Zhenghao Liu, Chenyan Xiong, Shi Yu, Yu Gu, Zhiyuan Liu, Ge Yu Abstract: This paper presents Structure Aware Dense Retrieval (SANTA) model, which encodes user queries and structured data in one universal embedding space for retrieving structured data. SANTA proposes two pretraining methods to make language models structure-aware and learn effective representations for structured data: 1) Structured Data Alignment, which utilizes the natural alignment relations between structured data and unstructured data for structure-aware pretraining. It contrastively trains language models to represent multi-modal text data and teaches models to distinguish matched structured data for unstructured texts. 2) Masked Entity Prediction, which designs an entity-oriented mask strategy and asks language models to fill in the masked entities. Our experiments show that SANTA achieves state-of-the-art on code search and product search and conducts convincing results in the zero-shot setting. SANTA learns tailored representations for multi-modal text data by aligning structured and unstructured data pairs and capturing structural semantics by masking and predicting entities in the structured data. All codes are available at https://github.com/OpenMatch/OpenMatch.'},\n",
       " {'question': 'Which LTI faculty published the paper A Study on the Calibration of In-context Learning in 2023?',\n",
       "  'answer': 'Eric Xing',\n",
       "  'context': 'Author: Eric Xing Title: A Study on the Calibration of In-context Learning Publication year: 2023 Coauthors: Hanlin Zhang, Yi-Fan Zhang, Yaodong Yu, Dhruv Madeka, Dean Foster, Eric Xing, Hima Lakkaraju, S. Kakade Abstract: Accurate uncertainty quantification is crucial for the safe deployment of language models (LMs), and prior research has demonstrated improvements in the calibration of modern LMs. Our study focuses on in-context learning (ICL), a prevalent method for adapting static LMs through tailored prompts, and examines the balance between performance and calibration across a broad spectrum of natural language understanding and reasoning tasks. Through comprehensive experiments, we observe that, with an increasing number of ICL examples, models initially exhibit increased miscalibration before achieving better calibration and miscalibration tends to arise in low-shot settings. Moreover, we find that methods aimed at improving usability, such as fine-tuning and chain-of-thought (CoT) prompting, can lead to miscalibration and unreliable natural language explanations, suggesting that new methods may be required for scenarios where models are expected to be reliable.'},\n",
       " {'question': 'What is the abstract of the paper Making Scalable Meta Learning Practical published by LTI faculty Eric P. Xing in 2023?',\n",
       "  'answer': 'Despite its flexibility to learn diverse inductive biases in machine learning programs, meta learning (i.e., learning to learn) has long been recognized to suffer from poor scalability due to its tremendous compute/memory costs, training instability, and a lack of efficient distributed training support. In this work, we focus on making scalable meta learning practical by introducing SAMA, which combines advances in both implicit differentiation algorithms and systems. Specifically, SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients. Evaluated on multiple large-scale meta learning benchmarks, SAMA showcases up to 1.7/4.8x increase in throughput and 2.0/3.8x decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms. Furthermore, we show that SAMA-based data optimization leads to consistent improvements in text classification accuracy with BERT and RoBERTa large language models, and achieves state-of-the-art results in both small- and large-scale data pruning on image classification tasks, demonstrating the practical applicability of scalable meta learning across language and vision domains.',\n",
       "  'context': 'Author: Eric P. Xing Title: Making Scalable Meta Learning Practical Publication year: 2023 Coauthors: Sang Keun Choe, Sanket Vaibhav Mehta, Hwijeen Ahn, W. Neiswanger, Pengtao Xie, Emma Strubell, Eric P. Xing Abstract: Despite its flexibility to learn diverse inductive biases in machine learning programs, meta learning (i.e., learning to learn) has long been recognized to suffer from poor scalability due to its tremendous compute/memory costs, training instability, and a lack of efficient distributed training support. In this work, we focus on making scalable meta learning practical by introducing SAMA, which combines advances in both implicit differentiation algorithms and systems. Specifically, SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients. Evaluated on multiple large-scale meta learning benchmarks, SAMA showcases up to 1.7/4.8x increase in throughput and 2.0/3.8x decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms. Furthermore, we show that SAMA-based data optimization leads to consistent improvements in text classification accuracy with BERT and RoBERTa large language models, and achieves state-of-the-art results in both small- and large-scale data pruning on image classification tasks, demonstrating the practical applicability of scalable meta learning across language and vision domains.'},\n",
       " {'question': 'What is the title of the paper published by LTI faculty Yiming Yang in 2023?',\n",
       "  'answer': 'MRI Features for Predicting Microvascular Invasion and Postoperative Recurrence in Hepatocellular Carcinoma Without Peritumoral Hypointensity',\n",
       "  'context': 'Author: Yiming Yang Title: MRI Features for Predicting Microvascular Invasion and Postoperative Recurrence in Hepatocellular Carcinoma Without Peritumoral Hypointensity Publication year: 2023 Coauthors: Zhiyuan Chen, Xiaohuan Li, Yu Zhang, Yiming Yang, Yan Zhang, Dongjing Zhou, Yu Yang, Shuping Zhang, Yupin Liu Abstract: Purpose To identify MRI features of hepatocellular carcinoma (HCC) that predict microvascular invasion (MVI) and postoperative intrahepatic recurrence in patients without peritumoral hepatobiliary phase (HBP) hypointensity. Patients and Methods One hundred and thirty patients with HCC who underwent preoperative gadoxetate-enhanced MRI and curative hepatic resection were retrospectively reviewed. Two radiologists reviewed all preoperative MR images and assessed the radiological features of HCCs. The ability of peritumoral HBP hypointensity to identify MVI and intrahepatic recurrence was analyzed. We then assessed the MRI features of HCC that predicted the MVI and intrahepatic recurrence-free survival (RFS) in the subgroup without peritumoral HBP hypointensity. Finally, a two-step flowchart was constructed to assist in clinical decision-making. Results Peritumoral HBP hypointensity (odds ratio, 3.019; 95% confidence interval: 1.071–8.512; P=0.037) was an independent predictor of MVI. The sensitivity, specificity, positive predictive value, negative predictive value, and AUROC of peritumoral HBP hypointensity in predicting MVI were 23.80%, 91.04%, 71.23%, 55.96%, and 0.574, respectively. Intrahepatic RFS was significantly shorter in patients with peritumoral HBP hypointensity (P<0.001). In patients without peritumoral HBP hypointensity, the only significant difference between MVI-positive and MVI-negative HCCs was the presence of a radiological capsule (P=0.038). Satellite nodule was an independent risk factor for intrahepatic RFS (hazard ratio,3.324; 95% CI: 1.733–6.378; P<0.001). The high-risk HCC detection rate was significantly higher when using the two-step flowchart that incorporated peritumoral HBP hypointensity and satellite nodule than when using peritumoral HBP hypointensity alone (P<0.001). Conclusion In patients without peritumoral HBP hypointensity, a radiological capsule is useful for identifying MVI and satellite nodule is an independent risk factor for intrahepatic RFS.'},\n",
       " {'question': 'Which LTI faculty published the paper Conversational Search with Random Walks over Entity Graphs in 2023?',\n",
       "  'answer': 'Jamie Callan',\n",
       "  'context': \"Author: Jamie Callan Title: Conversational Search with Random Walks over Entity Graphs Publication year: 2023 Coauthors: Gustavo Gonçalves, João Magalhães, Jamie Callan Abstract: The entities that emerge during a conversation can be used to model topics, but not all entities are equally useful for this task. Modeling the conversation with entity graphs and predicting each entity's centrality in the conversation provides additional information that improves the retrieval of answer passages for the current question. Experiments show that using random walks to estimate entity centrality on conversation entity graphs improves top precision answer passage ranking over competitive transformer-based baselines.\"},\n",
       " {'question': 'What is the title of the paper published by LTI faculty Louis-Philippe Morency in 2023?',\n",
       "  'answer': 'Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions',\n",
       "  'context': 'Author: Louis-Philippe Morency Title: Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions Publication year: 2023 Coauthors: Himanshu Thakur, Atishay Jain, Praneetha Vaddamanu, P. Liang, Louis-Philippe Morency Abstract: Societal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people. Since large-scale retraining of these models from scratch is both time and compute-expensive, a variety of approaches have been previously proposed that de-bias a pre-trained model. While the majority of current state-of-the-art debiasing methods focus on changes to the training regime, in this paper, we propose data intervention strategies as a powerful yet simple technique to reduce gender bias in pre-trained models. Specifically, we empirically show that by fine-tuning a pre-trained model on only 10 debiased (intervened) training examples, the tendency to favor any gender is significantly reduced. Since our proposed method only needs a few training examples, we argue that our few-shot de-biasing approach is highly feasible and practical. Through extensive experimentation, we show that our de-biasing technique performs better than competitive state-of-the-art baselines with minimal loss in language modeling ability.'},\n",
       " {'question': 'Who are the coauthors of the paper FNeural Speech Enhancement with Very Low Algorithmic Latency and Complexity via Integrated full- and sub-band Modeling published by LTI faculty Shinji Watanabe in 2023?',\n",
       "  'answer': 'Zhongqiu Wang, Samuele Cornell, Shukjae Choi, Younglo Lee, Byeonghak Kim, Shinji Watanabe',\n",
       "  'context': 'Author: Shinji Watanabe Title: FNeural Speech Enhancement with Very Low Algorithmic Latency and Complexity via Integrated full- and sub-band Modeling Publication year: 2023 Coauthors: Zhongqiu Wang, Samuele Cornell, Shukjae Choi, Younglo Lee, Byeonghak Kim, Shinji Watanabe Abstract: We propose FSB-LSTM, a novel long short-term memory (LSTM) based architecture that integrates full- and sub-band (FSB) modeling, for single- and multi-channel speech enhancement in the short-time Fourier transform (STFT) domain. The model maintains an information highway to flow an over-complete input representation through multiple FSB-LSTM modules. Each FSB-LSTM module consists of a full-band block to model spectro-temporal patterns at all frequencies and a sub-band block to model patterns within each sub-band, where each of the two blocks takes a down-sampled representation as input and returns an up-sampled discriminative representation to be added to the block input via a residual connection. The model is designed to have a low algorithmic complexity, a small run-time buffer and a very low algorithmic latency, at the same time producing a strong enhancement performance on a noisy-reverberant speech enhancement task even if the hop size is as low as 2 ms.'},\n",
       " {'question': 'Which LTI faculty published the paper A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech in 2023?',\n",
       "  'answer': 'Alexander I. Rudnicky',\n",
       "  'context': 'Author: Alexander I. Rudnicky Title: A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech Publication year: 2023 Coauthors: Li-Wei Chen, Shinji Watanabe, Alexander I. Rudnicky Abstract: Recent Text-to-Speech (TTS) systems trained on reading or acted corpora have achieved near human-level naturalness. The diversity of human speech, however, often goes beyond the coverage of these corpora. We believe the ability to handle such diversity is crucial for AI systems to achieve human-level communication. Our work explores the use of more abundant real-world data for building speech synthesizers. We train TTS systems using real-world speech from YouTube and podcasts. We observe the mismatch between training and inference alignments in mel-spectrogram based autoregressive models, leading to unintelligible synthesis, and demonstrate that learned discrete codes within multiple code groups effectively resolves this issue. We introduce our MQTTS system whose architecture is designed for multiple code generation and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality. We conduct ablation analyses to identify the efficacy of our methods. We show that MQTTS outperforms existing TTS systems in several objective and subjective measures.'},\n",
       " {'question': 'What is the title of the paper published by LTI faculty E. Xing in 2023?',\n",
       "  'answer': 'Understanding Masked Autoencoders via Hierarchical Latent Variable Models',\n",
       "  'context': 'Author: E. Xing Title: Understanding Masked Autoencoders via Hierarchical Latent Variable Models Publication year: 2023 Coauthors: Lingjing Kong, Martin Q. Ma, Guan-Hong Chen, E. Xing, Yuejie Chi, Louis-Philippe Morency, Kun Zhang Abstract: Masked autoencoder (MAE), a simple and effective self-supervised learning framework based on the reconstruction of masked image regions, has recently achieved prominent success in a variety of vision tasks. Despite the emergence of intriguing empirical observations on MAE, a theoretically principled understanding is still lacking. In this work, we formally characterize and justify existing empirical in-sights and provide theoretical guarantees of MAE. We formulate the underlying data-generating process as a hierarchical latent variable model, and show that under reasonable assumptions, MAE provably identifies a set of latent variables in the hierarchical model, explaining why MAE can extract high-level information from pixels. Further, we show how key hyperparameters in MAE (the masking ratio and the patch size) determine which true latent variables to be recovered, therefore influencing the level of semantic information in the representation. Specifically, extremely large or small masking ratios inevitably lead to low-level representations. Our theory offers coherent explanations of existing empirical observations and provides insights for potential empirical improvements and fundamental limitations of the masked-reconstruction paradigm. We conduct extensive experiments to validate our theoretical insights.'},\n",
       " {'question': 'Which LTI faculty published the paper Using counterfactual contrast to improve compositional generalization for multi-step quantitative reasoning in 2023?',\n",
       "  'answer': 'C. Rosé',\n",
       "  'context': 'Author: C. Rosé Title: Using counterfactual contrast to improve compositional generalization for multi-step quantitative reasoning Publication year: 2023 Coauthors: Armineh Nourbakhsh, Sameena Shah, C. Rosé Abstract: In quantitative question answering, compositional generalization is one of the main challenges of state of the art models, especially when longer sequences of reasoning steps are required. In this paper we propose CounterComp, a method that uses counterfactual scenarios to generate samples with compositional contrast. Instead of a data augmentation approach, CounterComp is based on metric learning, which allows for direct sampling from the training set and circumvents the need for additional human labels. Our proposed auxiliary metric learning loss improves the performance of three state of the art models on four recently released datasets. We also show how the approach can improve OOD performance on unseen domains, as well as unseen compositions. Lastly, we demonstrate how the method can lead to better compositional attention patterns during training.'},\n",
       " {'question': 'What is the abstract of the paper ESPnet-ST-v2: Multipurpose Spoken Language Translation Toolkit published by LTI faculty Shinji Watanabe in 2023?',\n",
       "  'answer': 'ESPnet-ST-v2 is a revamp of the open-source ESPnet-ST toolkit necessitated by the broadening interests of the spoken language translation community. ESPnet-ST-v2 supports 1) offline speech-to-text translation (ST), 2) simultaneous speech-to-text translation (SST), and 3) offline speech-to-speech translation (S2ST) – each task is supported with a wide variety of approaches, differentiating ESPnet-ST-v2 from other open source spoken language translation toolkits. This toolkit offers state-of-the-art architectures such as transducers, hybrid CTC/attention, multi-decoders with searchable intermediates, time-synchronous blockwise CTC/attention, Translatotron models, and direct discrete unit models. In this paper, we describe the overall design, example models for each task, and performance benchmarking behind ESPnet-ST-v2, which is publicly available at https://github.com/espnet/espnet.',\n",
       "  'context': \"Author: Shinji Watanabe Title: ESPnet-ST-v2: Multipurpose Spoken Language Translation Toolkit Publication year: 2023 Coauthors: Brian Yan, Jiatong Shi, Yun Tang, H. Inaguma, Yifan Peng, Siddharth Dalmia, Peter Pol'ak, Patrick Fernandes, Dan Berrebbi, Tomoki Hayashi, Xiaohui Zhang, Zhaoheng Ni, Moto Hira, Soumi Maiti, J. Pino, Shinji Watanabe Abstract: ESPnet-ST-v2 is a revamp of the open-source ESPnet-ST toolkit necessitated by the broadening interests of the spoken language translation community. ESPnet-ST-v2 supports 1) offline speech-to-text translation (ST), 2) simultaneous speech-to-text translation (SST), and 3) offline speech-to-speech translation (S2ST) – each task is supported with a wide variety of approaches, differentiating ESPnet-ST-v2 from other open source spoken language translation toolkits. This toolkit offers state-of-the-art architectures such as transducers, hybrid CTC/attention, multi-decoders with searchable intermediates, time-synchronous blockwise CTC/attention, Translatotron models, and direct discrete unit models. In this paper, we describe the overall design, example models for each task, and performance benchmarking behind ESPnet-ST-v2, which is publicly available at https://github.com/espnet/espnet.\"},\n",
       " {'question': 'What is the title of the paper published by LTI faculty Louis-Philippe Morency in 2023?',\n",
       "  'answer': 'Factorized Contrastive Learning: Going Beyond Multi-view Redundancy',\n",
       "  'context': 'Author: Louis-Philippe Morency Title: Factorized Contrastive Learning: Going Beyond Multi-view Redundancy Publication year: 2023 Coauthors: P. Liang, Zihao Deng, Martin Q. Ma, James Y. Zou, Louis-Philippe Morency, R. Salakhutdinov Abstract: In a wide range of multimodal tasks, contrastive learning has become a particularly appealing approach since it can successfully learn representations from abundant unlabeled data with only pairing information (e.g., image-caption or video-audio pairs). Underpinning these approaches is the assumption of multi-view redundancy - that shared information between modalities is necessary and sufficient for downstream tasks. However, in many real-world settings, task-relevant information is also contained in modality-unique regions: information that is only present in one modality but still relevant to the task. How can we learn self-supervised multimodal representations to capture both shared and unique information relevant to downstream tasks? This paper proposes FactorCL, a new multimodal representation learning method to go beyond multi-view redundancy. FactorCL is built from three new contributions: (1) factorizing task-relevant information into shared and unique representations, (2) capturing task-relevant information via maximizing MI lower bounds and removing task-irrelevant information via minimizing MI upper bounds, and (3) multimodal data augmentations to approximate task relevance without labels. On large-scale real-world datasets, FactorCL captures both shared and unique information and achieves state-of-the-art results on six benchmarks'},\n",
       " {'question': 'Which LTI faculty published the paper A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks in 2023?',\n",
       "  'answer': 'Shinji Watanabe',\n",
       "  'context': 'Author: Shinji Watanabe Title: A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks Publication year: 2023 Coauthors: Yifan Peng, Kwangyoun Kim, Felix Wu, Brian Yan, Siddhant Arora, William Chen, Jiyang Tang, Suwon Shon, Prashant Sridhar, Shinji Watanabe Abstract: Conformer, a convolution-augmented Transformer variant, has become the de facto encoder architecture for speech processing due to its superior performance in various tasks, including automatic speech recognition (ASR), speech translation (ST) and spoken language understanding (SLU). Recently, a new encoder called E-Branchformer has outperformed Conformer in the LibriSpeech ASR benchmark, making it promising for more general speech applications. This work compares E-Branchformer and Conformer through extensive experiments using different types of end-to-end sequence-to-sequence models. Results demonstrate that E-Branchformer achieves comparable or better performance than Conformer in almost all evaluation sets across 15 ASR, 2 ST, and 3 SLU benchmarks, while being more stable during training. We will release our training configurations and pre-trained models for reproducibility, which can benefit the speech community.'},\n",
       " {'question': 'Who are the coauthors of the paper Learning to Ask Questions for Zero-shot Dialogue State Tracking published by LTI faculty Alexander I. Rudnicky in 2023?',\n",
       "  'answer': 'Diogo Tavares, David Semedo, Alexander I. Rudnicky, João Magalhães',\n",
       "  'context': 'Author: Alexander I. Rudnicky Title: Learning to Ask Questions for Zero-shot Dialogue State Tracking Publication year: 2023 Coauthors: Diogo Tavares, David Semedo, Alexander I. Rudnicky, João Magalhães Abstract: We present a method for performing zero-shot Dialogue State Tracking (DST) by casting the task as a learning-to-ask-questions framework. The framework learns to pair the best question generation (QG) strategy with in-domain question answering (QA) methods to extract slot values from a dialogue without any human intervention. A novel self-supervised QA pretraining step using in-domain data is essential to learn the structure without requiring any slot-filling annotations. Moreover, we show that QG methods need to be aligned with the same grammatical person used in the dialogue. Empirical evaluation on the MultiWOZ 2.1 dataset demonstrates that our approach, when used alongside robust QA models, outperforms existing zero-shot methods in the challenging task of zero-shot cross domain adaptation-given a comparable amount of domain knowledge during data creation. Finally, we analyze the impact of the types of questions used, and demonstrate that the algorithmic approach outperforms template-based question generation.'},\n",
       " {'question': 'Which LTI faculty published the paper Fine-grained Hallucination Detection and Editing for Language Models in 2024?',\n",
       "  'answer': 'Graham Neubig',\n",
       "  'context': 'Author: Graham Neubig Title: Fine-grained Hallucination Detection and Editing for Language Models Publication year: 2024 Coauthors: Abhika Mishra, Akari Asai, Vidhisha Balachandran, Yizhong Wang, Graham Neubig, Yulia Tsvetkov, Hannaneh Hajishirzi Abstract: Large language models (LMs) are prone to generate factual errors, which are often called hallucinations. In this paper, we introduce a comprehensive taxonomy of hallucinations and argue that hallucinations manifest in diverse forms, each requiring varying degrees of careful assessments to verify factuality. We propose a novel task of automatic fine-grained hallucination detection and construct a new evaluation benchmark, FavaBench, that includes about one thousand fine-grained human judgments on three LM outputs across various domains. Our analysis reveals that ChatGPT and Llama2-Chat (70B, 7B) exhibit diverse types of hallucinations in the majority of their outputs in information-seeking scenarios. We train FAVA, a retrieval-augmented LM by carefully creating synthetic data to detect and correct fine-grained hallucinations. On our benchmark, our automatic and human evaluations show that FAVA significantly outperforms ChatGPT and GPT-4 on fine-grained hallucination detection, and edits suggested by FAVA improve the factuality of LM-generated text.'},\n",
       " {'question': 'What is the title of the paper published by LTI faculty Shinji Watanabe in 2023?',\n",
       "  'answer': 'Voxtlm: unified decoder-only models for consolidating speech recognition/synthesis and speech/text continuation tasks',\n",
       "  'context': 'Author: Shinji Watanabe Title: Voxtlm: unified decoder-only models for consolidating speech recognition/synthesis and speech/text continuation tasks Publication year: 2023 Coauthors: Soumi Maiti, Yifan Peng, Shukjae Choi, Jee-weon Jung, Xuankai Chang, Shinji Watanabe Abstract: We propose a decoder-only language model, VoxtLM, that can perform four tasks: speech recognition, speech synthesis, text generation, and speech continuation. VoxtLM integrates text vocabulary with discrete speech tokens from self-supervised speech features and uses special tokens to enable multitask learning. Compared to a single-task model, VoxtLM exhibits a significant improvement in speech synthesis, with improvements in both speech intelligibility from 28.9 to 5.6 and objective quality from 2.68 to 3.90. VoxtLM also improves speech generation and speech recognition performance over the single-task counterpart. Further, VoxtLM is trained with publicly available data and training recipes and model checkpoints are open-sourced to make fully reproducible work.'},\n",
       " {'question': 'What is the title of the paper published by LTI faculty Fernando Diaz in 2023?',\n",
       "  'answer': 'Overview of the TREC 2021 Fair Ranking Track',\n",
       "  'context': \"Author: Fernando Diaz Title: Overview of the TREC 2021 Fair Ranking Track Publication year: 2023 Coauthors: Asia J. Biega, Fernando Diaz, Michael D. Ekstrand, Sebastian Kohlmeier Abstract: The TREC Fair Ranking Track aims to provide a platform for participants to develop and evaluate novel retrieval algorithms that can provide a fair exposure to a mixture of demographics or attributes, such as ethnicity, that are represented by relevant documents in response to a search query. For example, particular demographics or attributes can be represented by the documents' topical content or authors. The 2021 Fair Ranking Track adopted a resource allocation task. The task focused on supporting Wikipedia editors who are looking to improve the encyclopedia's coverage of topics under the purview of a WikiProject. WikiProject coordinators and/or Wikipedia editors search for Wikipedia documents that are in need of editing to improve the quality of the article. The 2021 Fair Ranking track aimed to ensure that documents that are about, or somehow represent, certain protected characteristics receive a fair exposure to the Wikipedia editors, so that the documents have an fair opportunity of being improved and, therefore, be well-represented in Wikipedia. The under-representation of particular protected characteristics in Wikipedia can result in systematic biases that can have a negative human, social, and economic impact, particularly for disadvantaged or protected societal groups.\"},\n",
       " {'question': 'What is the abstract of the paper One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning published by LTI faculty Eric P. Xing in 2023?',\n",
       "  'answer': 'We present Generalized LoRA (GLoRA), an advanced approach for universal parameter-efficient fine-tuning tasks. Enhancing Low-Rank Adaptation (LoRA), GLoRA employs a generalized prompt module to optimize pre-trained model weights and adjust intermediate activations, providing more flexibility and capability across diverse tasks and datasets. Moreover, GLoRA facilitates efficient parameter adaptation by employing a scalable, modular, layer-wise structure search that learns individual adapter of each layer. Originating from a unified mathematical formulation, GLoRA exhibits strong transfer learning, few-shot learning and domain generalization abilities, as it adapts to new tasks through not only weights but also additional dimensions like activations. Comprehensive experiments demonstrate that GLoRA outperforms all previous methods in natural, specialized, and structured vision benchmarks, achieving superior accuracy with fewer parameters and computations. The proposed method on LLaMA-1 and LLaMA-2 also show considerable enhancements compared to the original LoRA in the language domain. Furthermore, our structural re-parameterization design ensures that GLoRA incurs no extra inference cost, rendering it a practical solution for resource-limited applications. Code and models are available at: https://github.com/Arnav0400/ViT-Slim/tree/master/GLoRA.',\n",
       "  'context': 'Author: Eric P. Xing Title: One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning Publication year: 2023 Coauthors: Arnav Chavan, Zhuang Liu, D. Gupta, Eric P. Xing, Zhiqiang Shen Abstract: We present Generalized LoRA (GLoRA), an advanced approach for universal parameter-efficient fine-tuning tasks. Enhancing Low-Rank Adaptation (LoRA), GLoRA employs a generalized prompt module to optimize pre-trained model weights and adjust intermediate activations, providing more flexibility and capability across diverse tasks and datasets. Moreover, GLoRA facilitates efficient parameter adaptation by employing a scalable, modular, layer-wise structure search that learns individual adapter of each layer. Originating from a unified mathematical formulation, GLoRA exhibits strong transfer learning, few-shot learning and domain generalization abilities, as it adapts to new tasks through not only weights but also additional dimensions like activations. Comprehensive experiments demonstrate that GLoRA outperforms all previous methods in natural, specialized, and structured vision benchmarks, achieving superior accuracy with fewer parameters and computations. The proposed method on LLaMA-1 and LLaMA-2 also show considerable enhancements compared to the original LoRA in the language domain. Furthermore, our structural re-parameterization design ensures that GLoRA incurs no extra inference cost, rendering it a practical solution for resource-limited applications. Code and models are available at: https://github.com/Arnav0400/ViT-Slim/tree/master/GLoRA.'},\n",
       " {'question': 'Which LTI faculty published the paper MultiZoo & MultiBench: A Standardized Toolkit for Multimodal Deep Learning in 2023?',\n",
       "  'answer': 'Louis-Philippe Morency',\n",
       "  'context': 'Author: Louis-Philippe Morency Title: MultiZoo & MultiBench: A Standardized Toolkit for Multimodal Deep Learning Publication year: 2023 Coauthors: P. Liang, Yiwei Lyu, Xiang Fan, Arav Agarwal, Yun Cheng, Louis-Philippe Morency, R. Salakhutdinov Abstract: Learning multimodal representations involves integrating information from multiple heterogeneous sources of data. In order to accelerate progress towards understudied modalities and tasks while ensuring real-world robustness, we release MultiZoo, a public toolkit consisting of standardized implementations of>20 core multimodal algorithms and MultiBench, a large-scale benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas. Together, these provide an automated end-to-end machine learning pipeline that simplifies and standardizes data loading, experimental setup, and model evaluation. To enable holistic evaluation, we offer a comprehensive methodology to assess (1) generalization, (2) time and space complexity, and (3) modality robustness. MultiBench paves the way towards a better understanding of the capabilities and limitations of multimodal models, while ensuring ease of use, accessibility, and reproducibility. Our toolkits are publicly available, will be regularly updated, and welcome inputs from the community.'},\n",
       " {'question': 'Which LTI faculty published the paper Neural-Symbolic Interaction and Co-Evolving in 2023?',\n",
       "  'answer': 'Eric P. Xing',\n",
       "  'context': 'Author: Eric P. Xing Title: Neural-Symbolic Interaction and Co-Evolving Publication year: 2023 Coauthors: Bowen Tan, Shibo Hao, Eric P. Xing, Zhiting Hu '},\n",
       " {'question': \"Who are the coauthors of the paper Multimodal Feature Selection for Detecting Mothers' Depression in Dyadic Interactions with their Adolescent Offspring published by LTI faculty Louis-Philippe Morency in 2023?\",\n",
       "  'answer': 'Maneesh Bilalpur, Saurabh Hinduja, Laura A. Cariola, Lisa B. Sheeber, Nick Alien, László A. Jeni, Louis-Philippe Morency, J. Cohn',\n",
       "  'context': \"Author: Louis-Philippe Morency Title: Multimodal Feature Selection for Detecting Mothers' Depression in Dyadic Interactions with their Adolescent Offspring Publication year: 2023 Coauthors: Maneesh Bilalpur, Saurabh Hinduja, Laura A. Cariola, Lisa B. Sheeber, Nick Alien, László A. Jeni, Louis-Philippe Morency, J. Cohn Abstract: Depression is the most common psychological disorder, a leading cause of disability world-wide, and a major contributor to inter-generational transmission of psychopathol-ogy within families. To contribute to our understanding of depression within families and to inform modality selection and feature reduction, it is critical to identify interpretable features in developmentally appropriate contexts. Mothers with and without depression were studied. Depression was defined as history of treatment for depression and elevations in current or recent symptoms. We explored two multimodal feature selection strategies in dyadic interaction tasks of mothers with their adolescent children for depression detection. Modalities included face and head dynamics, facial action units, speech-related behavior, and verbal features. The initial feature space was vast and inter-correlated (collinear). To reduce dimension-ality and gain insight into the relative contribution of each modality and feature, we explored feature selection strategies using Variance Inflation Factor (VIF) and Shapley values. On an average collinearity correction through VIF resulted in about 4 times feature reduction across unimodal and multimodal features. Collinearity correction was also found to be an optimal intermediate step prior to Shapley analysis. Shapley feature selection following VIF yielded best performance. The top 15 features obtained through Shapley achieved 78 % accuracy. The most informative features came from all four modalities sampled, which supports the importance of multimodal feature selection.\"},\n",
       " {'question': 'What is the abstract of the paper Symbolic Planning and Code Generation for Grounded Dialogue published by LTI faculty Daniel Fried in 2023?',\n",
       "  'answer': 'Large language models (LLMs) excel at processing and generating both text and code. However, LLMs have had limited applicability in grounded task-oriented dialogue as they are difficult to steer toward task objectives and fail to handle novel grounding. We present a modular and interpretable grounded dialogue system that addresses these shortcomings by composing LLMs with a symbolic planner and grounded code execution. Our system consists of a reader and planner: the reader leverages an LLM to convert partner utterances into executable code, calling functions that perform grounding. The translated code’s output is stored to track dialogue state, while a symbolic planner determines the next appropriate response. We evaluate our system’s performance on the demanding OneCommon dialogue task, involving collaborative reference resolution on abstract images of scattered dots. Our system substantially outperforms the previous state-of-the-art, including improving task success in human evaluations from 56% to 69% in the most challenging setting.',\n",
       "  'context': 'Author: Daniel Fried Title: Symbolic Planning and Code Generation for Grounded Dialogue Publication year: 2023 Coauthors: Justin T Chiu, Wenting Zhao, Derek Chen, Saujas Vaduguru, Alexander M. Rush, Daniel Fried Abstract: Large language models (LLMs) excel at processing and generating both text and code. However, LLMs have had limited applicability in grounded task-oriented dialogue as they are difficult to steer toward task objectives and fail to handle novel grounding. We present a modular and interpretable grounded dialogue system that addresses these shortcomings by composing LLMs with a symbolic planner and grounded code execution. Our system consists of a reader and planner: the reader leverages an LLM to convert partner utterances into executable code, calling functions that perform grounding. The translated code’s output is stored to track dialogue state, while a symbolic planner determines the next appropriate response. We evaluate our system’s performance on the demanding OneCommon dialogue task, involving collaborative reference resolution on abstract images of scattered dots. Our system substantially outperforms the previous state-of-the-art, including improving task success in human evaluations from 56% to 69% in the most challenging setting.'},\n",
       " {'question': 'Who are the coauthors of the paper PAM: Prompting Audio-Language Models for Audio Quality Assessment published by LTI faculty Rita Singh in 2024?',\n",
       "  'answer': 'Soham Deshmukh, Dareen Alharthi, Benjamin Elizalde, Hannes Gamper, Mahmoud Al Ismail, Rita Singh, Bhiksha Raj, Huaming Wang',\n",
       "  'context': 'Author: Rita Singh Title: PAM: Prompting Audio-Language Models for Audio Quality Assessment Publication year: 2024 Coauthors: Soham Deshmukh, Dareen Alharthi, Benjamin Elizalde, Hannes Gamper, Mahmoud Al Ismail, Rita Singh, Bhiksha Raj, Huaming Wang Abstract: While audio quality is a key performance metric for various audio processing tasks, including generative modeling, its objective measurement remains a challenge. Audio-Language Models (ALMs) are pre-trained on audio-text pairs that may contain information about audio quality, the presence of artifacts, or noise. Given an audio input and a text prompt related to quality, an ALM can be used to calculate a similarity score between the two. Here, we exploit this capability and introduce PAM, a no-reference metric for assessing audio quality for different audio processing tasks. Contrary to other\"reference-free\"metrics, PAM does not require computing embeddings on a reference dataset nor training a task-specific model on a costly set of human listening scores. We extensively evaluate the reliability of PAM against established metrics and human listening scores on four tasks: text-to-audio (TTA), text-to-music generation (TTM), text-to-speech (TTS), and deep noise suppression (DNS). We perform multiple ablation studies with controlled distortions, in-the-wild setups, and prompt choices. Our evaluation shows that PAM correlates well with existing metrics and human listening scores. These results demonstrate the potential of ALMs for computing a general-purpose audio quality metric.'},\n",
       " {'question': 'Who are the coauthors of the paper Convoifilter: A case study of doing cocktail party speech recognition published by LTI faculty A. Waibel in 2023?',\n",
       "  'answer': 'T. Nguyen, A. Waibel',\n",
       "  'context': \"Author: A. Waibel Title: Convoifilter: A case study of doing cocktail party speech recognition Publication year: 2023 Coauthors: T. Nguyen, A. Waibel Abstract: This paper presents an end-to-end model designed to improve automatic speech recognition (ASR) for a particular speaker in a crowded, noisy environment. The model utilizes a single-channel speech enhancement module that isolates the speaker's voice from background noise (ConVoiFilter) and an ASR module. The model can decrease ASR's word error rate (WER) from 80% to 26.4% through this approach. Typically, these two components are adjusted independently due to variations in data requirements. However, speech enhancement can create anomalies that decrease ASR efficiency. By implementing a joint fine-tuning strategy, the model can reduce the WER from 26.4% in separate tuning to 14.5% in joint tuning. We openly share our pre-trained model to foster further research hf.co/nguyenvulebinh/voice-filter.\"},\n",
       " {'question': 'Who are the coauthors of the paper SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs published by LTI faculty Yonatan Bisk in 2023?',\n",
       "  'answer': 'Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey, Yanping Huang, David A. Ross, Irfan Essa, Yonatan Bisk, Ming Yang, K. Murphy, A. Hauptmann, Lu Jiang',\n",
       "  'context': \"Author: Yonatan Bisk Title: SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs Publication year: 2023 Coauthors: Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey, Yanping Huang, David A. Ross, Irfan Essa, Yonatan Bisk, Ming Yang, K. Murphy, A. Hauptmann, Lu Jiang Abstract: In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos. SPAE converts between raw pixels and interpretable lexical tokens (or words) extracted from the LLM's vocabulary. The resulting tokens capture both the semantic meaning and the fine-grained details needed for visual reconstruction, effectively translating the visual content into a language comprehensible to the LLM, and empowering it to perform a wide array of multimodal tasks. Our approach is validated through in-context learning experiments with frozen PaLM 2 and GPT 3.5 on a diverse set of image understanding and generation tasks. Our method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%.\"},\n",
       " {'question': 'Who are the coauthors of the paper Toward General-Purpose Robots via Foundation Models: A Survey and Meta-Analysis published by LTI faculty Yonatan Bisk in 2023?',\n",
       "  'answer': 'Yafei Hu, Quanting Xie, Vidhi Jain, Jonathan Francis, Jay Patrikar, Nikhil Varma Keetha, Seungchan Kim, Yaqi Xie, Tianyi Zhang, Shibo Zhao, Yu Quan Chong, Chen Wang, Katia P. Sycara, Matthew Johnson-Roberson, Dhruv Batra, Xiaolong Wang, Sebastian Scherer, Z. Kira, Fei Xia, Yonatan Bisk',\n",
       "  'context': 'Author: Yonatan Bisk Title: Toward General-Purpose Robots via Foundation Models: A Survey and Meta-Analysis Publication year: 2023 Coauthors: Yafei Hu, Quanting Xie, Vidhi Jain, Jonathan Francis, Jay Patrikar, Nikhil Varma Keetha, Seungchan Kim, Yaqi Xie, Tianyi Zhang, Shibo Zhao, Yu Quan Chong, Chen Wang, Katia P. Sycara, Matthew Johnson-Roberson, Dhruv Batra, Xiaolong Wang, Sebastian Scherer, Z. Kira, Fei Xia, Yonatan Bisk Abstract: Building general-purpose robots that can operate seamlessly, in any environment, with any object, and utilizing various skills to complete diverse tasks has been a long-standing goal in Artificial Intelligence. Unfortunately, however, most existing robotic systems have been constrained - having been designed for specific tasks, trained on specific datasets, and deployed within specific environments. These systems usually require extensively-labeled data, rely on task-specific models, have numerous generalization issues when deployed in real-world scenarios, and struggle to remain robust to distribution shifts. Motivated by the impressive open-set performance and content generation capabilities of web-scale, large-capacity pre-trained models (i.e., foundation models) in research fields such as Natural Language Processing (NLP) and Computer Vision (CV), we devote this survey to exploring (i) how these existing foundation models from NLP and CV can be applied to the field of robotics, and also exploring (ii) what a robotics-specific foundation model would look like. We begin by providing an overview of what constitutes a conventional robotic system and the fundamental barriers to making it universally applicable. Next, we establish a taxonomy to discuss current work exploring ways to leverage existing foundation models for robotics and develop ones catered to robotics. Finally, we discuss key challenges and promising future directions in using foundation models for enabling general-purpose robotic systems. We encourage readers to view our living GitHub repository of resources, including papers reviewed in this survey as well as related projects and repositories for developing foundation models for robotics.'},\n",
       " {'question': 'Which LTI faculty published the paper Effective Prompt Extraction from Language Models in 2023?',\n",
       "  'answer': 'Daphne Ippolito',\n",
       "  'context': \"Author: Daphne Ippolito Title: Effective Prompt Extraction from Language Models Publication year: 2023 Coauthors: Yiming Zhang, Daphne Ippolito Abstract: The text generated by large language models is commonly controlled by prompting, where a prompt prepended to a user's query guides the model's output. The prompts used by companies to guide their models are often treated as secrets, to be hidden from the user making the query. They have even been treated as commodities to be bought and sold. However, anecdotal reports have shown adversarial users employing prompt extraction attacks to recover these prompts. In this paper, we present a framework for systematically measuring the effectiveness of these attacks. In experiments with 3 different sources of prompts and 11 underlying large language models, we find that simple text-based attacks can in fact reveal prompts with high probability. Our framework determines with high precision whether an extracted prompt is the actual secret prompt, rather than a model hallucination. Prompt extraction experiments on real systems such as Bing Chat and ChatGPT suggest that system prompts can be revealed by an adversary despite existing defenses in place.\"},\n",
       " {'question': 'Who is the author of the LTI paper Score: A Rule Engine for the Scone Knowledge Base System published in 2023?',\n",
       "  'answer': 'S. Fahlman',\n",
       "  'context': 'Author: S. Fahlman Title: Score: A Rule Engine for the Scone Knowledge Base System Publication year: 2023 Coauthors: Jeffrey Chen, S. Fahlman Abstract: We present Score, a rule engine designed and implemented for the Scone knowledge base system. Scone is a knowledge base system designed for storing and manipulating rich representations of general knowledge in symbolic form. It represents knowledge in the form of nodes and links in a network structure, and it can perform basic inference about the relationships between different elements efficiently. On its own, Scone acts as a sort of\"smart memory\"that can interface with other software systems. One area of improvement for Scone is how useful it can be in supplying knowledge to an intelligent agent that can use the knowledge to perform actions and update the knowledge base with its observations. We augment the Scone system with a production rule engine that automatically performs simple inference based on existing and newly-added structures in Scone\\'s knowledge base, potentially improving the capabilities of any planning systems built on top of Scone. Production rule systems consist of\"if-then\"production rules that try to match their predicates to existing knowledge and fire their actions when their predicates are satisfied. We propose two kinds of production rules, if-added and if-needed rules, that differ in how they are checked and fired to cover multiple use cases. We then implement methods to efficiently check and fire these rules in a large knowledge base. The new rule engine is not meant to be a complex stand-alone planner, so we discuss how it fits into the context of Scone and future work on planning systems.'},\n",
       " {'question': 'Who are the coauthors of the paper End-to-End Evaluation for Low-Latency Simultaneous Speech Translation published by LTI faculty A. Waibel in 2023?',\n",
       "  'answer': 'Christian Huber, Tu Anh Dinh, Carlos Mullov, Ngoc-Quan Pham, T. Nguyen, Fabian Retkowski, Stefan Constantin, Enes Yavuz Ugan, Danni Liu, Zhaolin Li, Sai Koneru, J. Niehues, A. Waibel',\n",
       "  'context': 'Author: A. Waibel Title: End-to-End Evaluation for Low-Latency Simultaneous Speech Translation Publication year: 2023 Coauthors: Christian Huber, Tu Anh Dinh, Carlos Mullov, Ngoc-Quan Pham, T. Nguyen, Fabian Retkowski, Stefan Constantin, Enes Yavuz Ugan, Danni Liu, Zhaolin Li, Sai Koneru, J. Niehues, A. Waibel Abstract: The challenge of low-latency speech translation has recently draw significant interest in the research community as shown by several publications and shared tasks. Therefore, it is essential to evaluate these different approaches in realistic scenarios. However, currently only specific aspects of the systems are evaluated and often it is not possible to compare different approaches. In this work, we propose the first framework to perform and evaluate the various aspects of low-latency speech translation under realistic conditions. The evaluation is carried out in an end-to-end fashion. This includes the segmentation of the audio as well as the run-time of the different components. Secondly, we compare different approaches to low-latency speech translation using this framework. We evaluate models with the option to revise the output as well as methods with fixed output. Furthermore, we directly compare state-of-the-art cascaded as well as end-to-end systems. Finally, the framework allows to automatically evaluate the translation quality as well as latency and also provides a web interface to show the low-latency model outputs to the user.'},\n",
       " {'question': 'Who are the coauthors of the paper Exploring Artificial Intelligence in English Language Arts with StoryQ published by LTI faculty C. Rosé in 2023?',\n",
       "  'answer': 'J. Chao, Rebecca Ellis, Shiyan Jiang, C. Rosé, W. Finzer, Can Tatar, James Fiacco, Kenia Wiedemann',\n",
       "  'context': 'Author: C. Rosé Title: Exploring Artificial Intelligence in English Language Arts with StoryQ Publication year: 2023 Coauthors: J. Chao, Rebecca Ellis, Shiyan Jiang, C. Rosé, W. Finzer, Can Tatar, James Fiacco, Kenia Wiedemann Abstract: Exploring Artificial Intelligence (AI) in English Language Arts (ELA) with StoryQ is a 10-hour curriculum module designed for high school ELA classes. The module introduces students to fundamental AI concepts and essential machine learning workflow using StoryQ, a web-based GUI environment for Grades 6-12 learners. In this module, students work with unstructured text data and learn to train, test, and improve text classification models such as intent recognition, clickbait filter, and sentiment analysis. As they interact with machine-learning language models deeply, students also gain a nuanced understanding of language and how to wield it, not just as a data structure, but as a tool in our human-human encounters as well. The current version contains eight lessons, all delivered through a full-featured online learning and teaching platform. Computers and Internet access are required to implement the module. The module was piloted in an ELA class in the Spring of 2022, and the student learning outcomes were positive. The module is currently undergoing revision and will be further tested and improved in Fall 2022.'},\n",
       " {'question': 'Who are the coauthors of the paper Power Hungry Processing: Watts Driving the Cost of AI Deployment? published by LTI faculty Emma Strubell in 2023?',\n",
       "  'answer': 'A. Luccioni, Yacine Jernite, Emma Strubell',\n",
       "  'context': 'Author: Emma Strubell Title: Power Hungry Processing: Watts Driving the Cost of AI Deployment? Publication year: 2023 Coauthors: A. Luccioni, Yacine Jernite, Emma Strubell Abstract: Recent years have seen a surge in the popularity of commercial AI products based on generative, multi-purpose AI systems promising a unified approach to building machine learning (ML) models into technology. However, this ambition of\"generality\"comes at a steep cost to the environment, given the amount of energy these systems require and the amount of carbon that they emit. In this work, we propose the first systematic comparison of the ongoing inference cost of various categories of ML systems, covering both task-specific (i.e. finetuned models that carry out a single task) and `general-purpose\\' models, (i.e. those trained for multiple tasks). We measure deployment cost as the amount of energy and carbon required to perform 1,000 inferences on representative benchmark dataset using these models. We find that multi-purpose, generative architectures are orders of magnitude more expensive than task-specific systems for a variety of tasks, even when controlling for the number of model parameters. We conclude with a discussion around the current trend of deploying multi-purpose generative ML systems, and caution that their utility should be more intentionally weighed against increased costs in terms of energy and emissions. All the data from our study can be accessed via an interactive demo to carry out further exploration and analysis.'},\n",
       " {'question': 'What is the title of the paper published by LTI faculty Jamie Callan in 2023?',\n",
       "  'answer': 'Multi-Objective Improvement of Android Applications',\n",
       "  'context': 'Author: Jamie Callan Title: Multi-Objective Improvement of Android Applications Publication year: 2023 Coauthors: Jamie Callan, J. Petke Abstract: Non-functional properties, such as runtime or memory use, are important to mobile app users and developers, as they affect user experience. Previous work on automated improvement of non-functional properties in mobile apps failed to address the inherent trade-offs between such properties. We propose a practical approach and the first open-source tool, GIDroid (2023), for multi-objective automated improvement of Android apps. In particular, we use Genetic improvement, a search-based technique that navigates the space of software variants to find improved software. We use a simulation-based testing framework to greatly improve the speed of search. GIDroid contains three state-of-the-art multi-objective algorithms, and two new mutation operators, which cache the results of method calls. Genetic improvement relies on testing to validate patches. Previous work showed that tests in open-source Android applications are scarce. We thus wrote tests for 21 versions of 7 Android apps, creating a new benchmark for performance improvements. We used GIDroid to improve versions of mobile apps where developers had previously found improvements to runtime, memory, and bandwidth use. Our technique automatically re-discovers 64% of existing improvements. We then applied our approach to current versions of software in which there were no known improvements. We were able to improve execution time by up to 35%, and memory use by up to 33% in these apps.'},\n",
       " {'question': 'Who are the coauthors of the paper Transformed Protoform Reconstruction published by LTI faculty David R. Mortensen in 2023?',\n",
       "  'answer': 'Young Min Kim, Kalvin Chang, Chenxuan Cui, David R. Mortensen',\n",
       "  'context': 'Author: David R. Mortensen Title: Transformed Protoform Reconstruction Publication year: 2023 Coauthors: Young Min Kim, Kalvin Chang, Chenxuan Cui, David R. Mortensen Abstract: Protoform reconstruction is the task of inferring what morphemes or words appeared like in the ancestral languages of a set of daughter languages. Meloni et al (2021) achieved the state-of-the-art on Latin protoform reconstruction with an RNN-based encoder-decoder with attention model. We update their model with the state-of-the-art seq2seq model: the Transformer. Our model outperforms their model on a suite of different metrics on two different datasets: their Romance data of 8,000 cognates spanning 5 languages and a Chinese dataset (Hou 2004) of 800+ cognates spanning 39 varieties. We also probe our model for potential phylogenetic signal contained in the model. Our code is publicly available at https://github.com/cmu-llab/acl-2023.'},\n",
       " {'question': 'Who are the coauthors of the paper The Framework Tax: Disparities Between Inference Efficiency in Research and Deployment published by LTI faculty Emma Strubell in 2023?',\n",
       "  'answer': 'Jared Fernandez, Jacob Kahn, Clara Na, Yonatan Bisk, Emma Strubell',\n",
       "  'context': 'Author: Emma Strubell Title: The Framework Tax: Disparities Between Inference Efficiency in Research and Deployment Publication year: 2023 Coauthors: Jared Fernandez, Jacob Kahn, Clara Na, Yonatan Bisk, Emma Strubell Abstract: Increased focus on the computational efficiency of NLP systems has motivated the design of efficient model architectures and improvements to underlying hardware accelerators. However, the resulting increases in computational throughput and reductions in floating point operations have not directly translated to improvements in wall-clock inference latency. We demonstrate that these discrepancies can be largely attributed to bottlenecks introduced by deep learning frameworks. We denote this phenomenon as the \\\\textit{framework tax}, and observe that the disparity is growing as hardware speed increases over time. In this work, we examine this phenomenon through a series of case studies analyzing the effects of model design decisions, framework paradigms, and hardware platforms on total model latency. Code is available at https://github.com/JaredFern/Framework-Tax.'},\n",
       " {'question': 'Who is the author of the LTI paper CompleQA: Benchmarking the Impacts of Knowledge Graph Completion Methods on Question Answering published in 2023?',\n",
       "  'answer': 'Chenyan Xiong',\n",
       "  'context': 'Author: Chenyan Xiong Title: CompleQA: Benchmarking the Impacts of Knowledge Graph Completion Methods on Question Answering Publication year: 2023 Coauthors: Donghan Yu, Yu Gu, Chenyan Xiong, Yiming Yang '},\n",
       " {'question': 'What is the title of the paper published by LTI faculty Shinji Watanabe in 2023?',\n",
       "  'answer': 'Intrusion of Coastal Oyashio water to Funka Bay and Tsugaru Strait occasionally disturbed by Kuroshio-originating warm core ring',\n",
       "  'context': 'Author: Shinji Watanabe Title: Intrusion of Coastal Oyashio water to Funka Bay and Tsugaru Strait occasionally disturbed by Kuroshio-originating warm core ring Publication year: 2023 Coauthors: H. Abe, Y. Yahiro, T. Hasegawa, T. Hirawake, H. Onishi, A. Ooki, T. Takatsu, K. Sasaki, M. Wakita, H. Kaneko, Shinji Watanabe, T. Tanaka, T. Okunishi, S. Ohno, S. Hashizume '},\n",
       " {'question': 'What is the abstract of the paper DeMuX: Data-efficient Multilingual Learning published by LTI faculty Graham Neubig in 2023?',\n",
       "  'answer': 'We consider the task of optimally fine-tuning pre-trained multilingual models, given small amounts of unlabelled target data and an annotation budget. In this paper, we introduce DEMUX, a framework that prescribes the exact data-points to label from vast amounts of unlabelled multilingual data, having unknown degrees of overlap with the target set. Unlike most prior works, our end-to-end framework is language-agnostic, accounts for model representations, and supports multilingual target configurations. Our active learning strategies rely upon distance and uncertainty measures to select task-specific neighbors that are most informative to label, given a model. DeMuX outperforms strong baselines in 84% of the test cases, in the zero-shot setting of disjoint source and target language sets (including multilingual target pools), across three models and four tasks. Notably, in low-budget settings (5-100 examples), we observe gains of up to 8-11 F1 points for token-level tasks, and 2-5 F1 for complex tasks. Our code is released here: https://github.com/simran-khanuja/demux.',\n",
       "  'context': 'Author: Graham Neubig Title: DeMuX: Data-efficient Multilingual Learning Publication year: 2023 Coauthors: Simran Khanuja, Srinivas Gowriraj, L. Dery, Graham Neubig Abstract: We consider the task of optimally fine-tuning pre-trained multilingual models, given small amounts of unlabelled target data and an annotation budget. In this paper, we introduce DEMUX, a framework that prescribes the exact data-points to label from vast amounts of unlabelled multilingual data, having unknown degrees of overlap with the target set. Unlike most prior works, our end-to-end framework is language-agnostic, accounts for model representations, and supports multilingual target configurations. Our active learning strategies rely upon distance and uncertainty measures to select task-specific neighbors that are most informative to label, given a model. DeMuX outperforms strong baselines in 84% of the test cases, in the zero-shot setting of disjoint source and target language sets (including multilingual target pools), across three models and four tasks. Notably, in low-budget settings (5-100 examples), we observe gains of up to 8-11 F1 points for token-level tasks, and 2-5 F1 for complex tasks. Our code is released here: https://github.com/simran-khanuja/demux.'},\n",
       " {'question': 'Which LTI faculty published the paper The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features in 2023?',\n",
       "  'answer': 'Rita Singh',\n",
       "  'context': 'Author: Rita Singh Title: The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features Publication year: 2023 Coauthors: Liao Qu, X. Zou, Xiang Li, Yandong Wen, Rita Singh, B. Raj Abstract: This work unveils the enigmatic link between phonemes and facial features. Traditional studies on voice-face correlations typically involve using a long period of voice input, including generating face images from voices and reconstructing 3D face meshes from voices. However, in situations like voice-based crimes, the available voice evidence may be short and limited. Additionally, from a physiological perspective, each segment of speech -- phoneme -- corresponds to different types of airflow and movements in the face. Therefore, it is advantageous to discover the hidden link between phonemes and face attributes. In this paper, we propose an analysis pipeline to help us explore the voice-face relationship in a fine-grained manner, i.e., phonemes v.s. facial anthropometric measurements (AM). We build an estimator for each phoneme-AM pair and evaluate the correlation through hypothesis testing. Our results indicate that AMs are more predictable from vowels compared to consonants, particularly with plosives. Additionally, we observe that if a specific AM exhibits more movement during phoneme pronunciation, it is more predictable. Our findings support those in physiology regarding correlation and lay the groundwork for future research on speech-face multimodal learning.'},\n",
       " {'question': 'Who is the author of the LTI paper Towards Open-Domain Twitter User Profile Inference published in 2023?',\n",
       "  'answer': 'Alexander Hauptmann',\n",
       "  'context': 'Author: Alexander Hauptmann Title: Towards Open-Domain Twitter User Profile Inference Publication year: 2023 Coauthors: Haoyang Wen, Zhenxin Xiao, E. Hovy, Alexander Hauptmann Abstract: ,'},\n",
       " {'question': 'What is the abstract of the paper Automating Sound Change Prediction for Phylogenetic Inference: A Tukanoan Case Study published by LTI faculty David R. Mortensen in 2024?',\n",
       "  'answer': 'We describe a set of new methods to partially automate linguistic phylogenetic inference given (1) cognate sets with their respective protoforms and sound laws, (2) a mapping from phones to their articulatory features and (3) a typological database of sound changes.We train a neural network on these sound change data to weight articulatory distances between phones and predict intermediate sound change steps between historical protoforms and their modern descendants, replacing a linguistic expert in part of a parsimony-based phylogenetic inference algorithm. In our best experiments on Tukanoan languages, this method produces trees with a Generalized Quartet Distance of 0.12 from a tree that used expert annotations, a significant improvement over other semi-automated baselines. We discuss potential benefits and drawbacks to our neural approach and parsimony-based tree prediction. We also experiment with a minimal generalization learner for automatic sound law induction, finding it less effective than sound laws from expert annotation. Our code is publicly available.',\n",
       "  'context': 'Author: David R. Mortensen Title: Automating Sound Change Prediction for Phylogenetic Inference: A Tukanoan Case Study Publication year: 2024 Coauthors: Kalvin Chang, Nathaniel Robinson, Anna Cai, Ting Chen, Annie Zhang, David R. Mortensen Abstract: We describe a set of new methods to partially automate linguistic phylogenetic inference given (1) cognate sets with their respective protoforms and sound laws, (2) a mapping from phones to their articulatory features and (3) a typological database of sound changes.We train a neural network on these sound change data to weight articulatory distances between phones and predict intermediate sound change steps between historical protoforms and their modern descendants, replacing a linguistic expert in part of a parsimony-based phylogenetic inference algorithm. In our best experiments on Tukanoan languages, this method produces trees with a Generalized Quartet Distance of 0.12 from a tree that used expert annotations, a significant improvement over other semi-automated baselines. We discuss potential benefits and drawbacks to our neural approach and parsimony-based tree prediction. We also experiment with a minimal generalization learner for automatic sound law induction, finding it less effective than sound laws from expert annotation. Our code is publicly available.'},\n",
       " {'question': 'What is the abstract of the paper Text-Transport: Toward Learning Causal Effects of Natural Language published by LTI faculty Louis-Philippe Morency in 2023?',\n",
       "  'answer': \"As language technologies gain prominence in real-world settings, it is important to understand how changes to language affect reader perceptions. This can be formalized as the causal effect of varying a linguistic attribute (e.g., sentiment) on a reader's response to the text. In this paper, we introduce Text-Transport, a method for estimation of causal effects from natural language under any text distribution. Current approaches for valid causal effect estimation require strong assumptions about the data, meaning the data from which one can estimate valid causal effects often is not representative of the actual target domain of interest. To address this issue, we leverage the notion of distribution shift to describe an estimator that transports causal effects between domains, bypassing the need for strong assumptions in the target domain. We derive statistical guarantees on the uncertainty of this estimator, and we report empirical results and analyses that support the validity of Text-Transport across data settings. Finally, we use Text-Transport to study a realistic setting--hate speech on social media--in which causal effects do shift significantly between text domains, demonstrating the necessity of transport when conducting causal inference on natural language.\",\n",
       "  'context': \"Author: Louis-Philippe Morency Title: Text-Transport: Toward Learning Causal Effects of Natural Language Publication year: 2023 Coauthors: Victoria Lin, Louis-Philippe Morency, Eli Ben-Michael Abstract: As language technologies gain prominence in real-world settings, it is important to understand how changes to language affect reader perceptions. This can be formalized as the causal effect of varying a linguistic attribute (e.g., sentiment) on a reader's response to the text. In this paper, we introduce Text-Transport, a method for estimation of causal effects from natural language under any text distribution. Current approaches for valid causal effect estimation require strong assumptions about the data, meaning the data from which one can estimate valid causal effects often is not representative of the actual target domain of interest. To address this issue, we leverage the notion of distribution shift to describe an estimator that transports causal effects between domains, bypassing the need for strong assumptions in the target domain. We derive statistical guarantees on the uncertainty of this estimator, and we report empirical results and analyses that support the validity of Text-Transport across data settings. Finally, we use Text-Transport to study a realistic setting--hate speech on social media--in which causal effects do shift significantly between text domains, demonstrating the necessity of transport when conducting causal inference on natural language.\"},\n",
       " {'question': 'What is the title of the paper published by LTI faculty Shinji Watanabe in 2023?',\n",
       "  'answer': 'Enhancing End-to-End Conversational Speech Translation Through Target Language Context Utilization',\n",
       "  'context': 'Author: Shinji Watanabe Title: Enhancing End-to-End Conversational Speech Translation Through Target Language Context Utilization Publication year: 2023 Coauthors: A. Hussein, Brian Yan, Antonios Anastasopoulos, Shinji Watanabe, S. Khudanpur Abstract: Incorporating longer context has been shown to benefit machine translation, but the inclusion of context in end-to-end speech translation (E2E-ST) remains under-studied. To bridge this gap, we introduce target language context in E2E-ST, enhancing coherence and overcoming memory constraints of extended audio segments. Additionally, we propose context dropout to ensure robustness to the absence of context, and further improve performance by adding speaker information. Our proposed contextual E2E-ST outperforms the isolated utterance-based E2E-ST approach. Lastly, we demonstrate that in conversational speech, contextual information primarily contributes to capturing context style, as well as resolving anaphora and named entities.'},\n",
       " {'question': 'Who are the coauthors of the paper PWESuite: Phonetic Word Embeddings and Tasks They Facilitate published by LTI faculty David R. Mortensen in 2023?',\n",
       "  'answer': 'Vilém Zouhar, Kalvin Chang, Chenxuan Cui, Nathaniel Carlson, Nathaniel R. Robinson, Mrinmaya Sachan, David R. Mortensen',\n",
       "  'context': 'Author: David R. Mortensen Title: PWESuite: Phonetic Word Embeddings and Tasks They Facilitate Publication year: 2023 Coauthors: Vilém Zouhar, Kalvin Chang, Chenxuan Cui, Nathaniel Carlson, Nathaniel R. Robinson, Mrinmaya Sachan, David R. Mortensen Abstract: Mapping words into a fixed-dimensional vector space is the backbone of modern NLP. While most word embedding methods successfully encode semantic information, they overlook phonetic information that is crucial for many tasks. We develop three methods that use articulatory features to build phonetically informed word embeddings. To address the inconsistent evaluation of existing phonetic word embedding methods, we also contribute a task suite to fairly evaluate past, current, and future methods. We evaluate both (1) intrinsic aspects of phonetic word embeddings, such as word retrieval and correlation with sound similarity, and (2) extrinsic performance on tasks such as rhyme and cognate detection and sound analogies. We hope our task suite will promote reproducibility and inspire future phonetic embedding research.'},\n",
       " {'question': 'Who are the coauthors of the paper SPEERLoom: An Open-Source Loom Kit for Interdisciplinary Engagement in Math, Engineering, and Textiles published by LTI faculty Carolyn Rosé in 2023?',\n",
       "  'answer': 'Samantha Speer, Ana P Garcia-Alonzo, Joey Huang, N. Yankova, Carolyn Rosé, Kylie A Peppler, James Mccann, Melisa Orta Martinez',\n",
       "  'context': 'Author: Carolyn Rosé Title: SPEERLoom: An Open-Source Loom Kit for Interdisciplinary Engagement in Math, Engineering, and Textiles Publication year: 2023 Coauthors: Samantha Speer, Ana P Garcia-Alonzo, Joey Huang, N. Yankova, Carolyn Rosé, Kylie A Peppler, James Mccann, Melisa Orta Martinez Abstract: Weaving is a fabrication process that is grounded in mathematics and engineering: from the binary, matrix-like nature of the pattern drafts weavers have used for centuries, to the punch card programming of the first Jacquard looms. This intersection of disciplines provides an opportunity to ground abstract mathematical concepts in a concrete and embodied art, viewing this textile art through the lens of engineering. Currently, available looms are not optimized to take advantage of this opportunity to increase mathematics learning by providing hands-on interdisciplinary learning in collegiate classrooms. In this work, we present SPEERLoom: an open-source, robotic Jacquard loom kit designed to be a tool for interweaving cloth fabrication, mathematics, and engineering to support interdisciplinary learning in the classroom. We discuss the design requirements and subsequent design of SPEERLoom. We also present the results of a pilot study in a post-secondary class finding that SPEERLoom supports hands-on, interdisciplinary learning of math, engineering, and textiles.'}]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json \n",
    "data = json.load(open('/zfsauton2/home/yifuc/11711-RAG/data/cmu/faculty_publication.json'))\n",
    "faculty_publication(data, 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.docstore.document import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "a = Document(page_content = 'sb')\n",
    "\n",
    "splitted = RecursiveCharacterTextSplitter(chunk_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='asoijdoiajwdoiawd'),\n",
       " Document(page_content='asoijdoiajwdoiawd')]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = Document(page_content ='asoijdoiajwdoiawd')\n",
    "\n",
    "splitted.split_documents([a, a])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_w_io_base(f, mode: str):\n",
    "    if not isinstance(f, io.IOBase):\n",
    "        f_dirname = os.path.dirname(f)\n",
    "        if f_dirname != \"\":\n",
    "            os.makedirs(f_dirname, exist_ok=True)\n",
    "        f = open(f, mode=mode)\n",
    "    return f\n",
    "\n",
    "\n",
    "def _make_r_io_base(f, mode: str):\n",
    "    if not isinstance(f, io.IOBase):\n",
    "        f = open(f, mode=mode)\n",
    "    return f\n",
    "\n",
    "def jdump(obj, f, mode=\"w\", indent=4, default=str):\n",
    "    \"\"\"Dump a str or dictionary to a file in json format.\n",
    "\n",
    "    Args:\n",
    "        obj: An object to be written.\n",
    "        f: A string path to the location on disk.\n",
    "        mode: Mode for opening the file.\n",
    "        indent: Indent for storing json dictionaries.\n",
    "        default: A function to handle non-serializable entries; defaults to `str`.\n",
    "    \"\"\"\n",
    "    f = _make_w_io_base(f, mode)\n",
    "    if isinstance(obj, (dict, list)):\n",
    "        json.dump(obj, f, indent=indent, default=default)\n",
    "    elif isinstance(obj, str):\n",
    "        f.write(obj)\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected type: {type(obj)}\")\n",
    "    f.close()\n",
    "\n",
    "def jload(f, mode=\"r\"):\n",
    "    \"\"\"Load a .json file into a dictionary.\"\"\"\n",
    "    f = _make_r_io_base(f, mode)\n",
    "    jdict = json.load(f)\n",
    "    f.close()\n",
    "    return jdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "import io \n",
    "import json \n",
    "\n",
    "def load_documents(text_path, chunk_size=1000, chunk_overlap=150):\n",
    "    '''\n",
    "    text_path: str, a directory of .txt files\n",
    "    return: \n",
    "    langchain document format (a list of trucated document)\n",
    "    '''\n",
    "    if 'databrick' in text_path:\n",
    "        loader = DirectoryLoader(text_path, glob=\"**/*.txt\")\n",
    "        data = loader.load()\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        docs = text_splitter.split_documents(data)\n",
    "        return docs\n",
    "    elif 'cmu' in text_path:\n",
    "        docs = []\n",
    "        #files that need to be truncated\n",
    "        truncate_files = ['buggy_history.json', 'cmu_history.json', 'handbook_text.json', 'kiltie_band_fact.json',\n",
    "                          'scotty_fact.json', 'tartan_fact.json']\n",
    "        files = listdir(text_path)\n",
    "        docs = []\n",
    "        for file in files:\n",
    "            doc = jload(text_path + '/' + file)\n",
    "            doc = [Document(page_content = s) for s in doc]\n",
    "            if file not in truncate_files:\n",
    "                docs.extend(doc)\n",
    "            else:\n",
    "                text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "                doc = text_splitter.split_documents(doc)\n",
    "                docs.extend(doc)\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Tartan Facts: Who founded Carnegie Mellon University? Carnegie Technical Schools was founded in 1900 by Andrew Carnegie. Twelve years later it became known as the Carnegie Institute of Technology. In 1967, the school merged with Mellon Institute and became what is known today as Carnegie Mellon University. What is a Tartan? The Carnegie Mellon athletic teams are nicknamed the \"Tartans\" as a nod to Andrew Carnegie\\'s Scottish heritage. A tartan is often misrepresented as a fierce warrior from either the Asian tundra or Scottish highlands. In actuality, a Tartan is a twilled woolen fabric with a plaid design. It is of Scottish origin and consists of stripes of various colors and widths against a solid ground, denoting a particular family lineage. The school\\'s founder, Andrew Carnegie, was born in Dunfermline, Scotland, in 1835. Carnegie came to the United States in 1848 and founded Carnegie Technical Schools in Pittsburgh in 1900.'),\n",
       " Document(page_content='The Scottish terrier mascot performer sports Carnegie tartan attire, while the graphic mascot is wearing a plaid scarf around its neck. So what\\'s the difference between tartan and plaid?\\n\\nYou\\'ll know it\\'s a tartan if...• \"It\\'s a check or pattern in a variety of colours in woven fabric in which bands of colour are repeated in equal proportion in warp (running lengthwise) and weft (running across).\"\\n• \"Each stripe of the warp crosses every stripe of the weft, so when vertical and horizontal stripes of the same color cross, the result is solid color at the point of intersection.\"\\n• \"The arrangement of colored threads is the same in the warp as in the weft.\"\\n\\nYou can find our official tartan on various items in the University Store.\\n\\nSource: \"Tartan: Romancing the Plaid,\" by Jeffrey Banks and Doria De La Chapelle'),\n",
       " Document(page_content='Source: \"Tartan: Romancing the Plaid,\" by Jeffrey Banks and Doria De La Chapelle \\n\\n \\n\\nOfficial Mascot?\\nMore than a century after Carnegie Mellon University opened its doors, an official mascot finally made its mark. Although students have dressed as a Scottish terrier — typically referred to as Scotty — for 50 years, it wasn\\'t until 2007 that Carnegie Mellon officially welcomed the Scottish terrier as the university\\'s first mascot.\\n\\nIn keeping with tradition, students, faculty, staff and alumni voted to name the new official mascot Scotty. But it\\'s not just the costumed mascot that voters named. The live dog is also known as Scotty to the Carnegie Mellon community.\\n\\nAbout Scotty | Celebrating Scotty | Performer Requests\\n\\n \\n\\nDepartment Quick Facts\\n\\nName of School: Carnegie Mellon University\\n\\nCity/Zip: Pittsburgh, PA 15213\\n\\nFounded: 1900\\n\\nEnrollment (undergrad): 6,982- Fall 2021\\n\\nEnrollment (graduate and doctoral): 7,062 - Fall 2021\\n\\nAlumni: 102,577\\n\\nNickname: Tartans'),\n",
       " Document(page_content=\"Founded: 1900\\n\\nEnrollment (undergrad): 6,982- Fall 2021\\n\\nEnrollment (graduate and doctoral): 7,062 - Fall 2021\\n\\nAlumni: 102,577\\n\\nNickname: Tartans\\n\\nMascot: Scottie Dog\\n\\nSchool Colors: Cardinal and Gray\\n\\nFootball Stadium and Track and Field: Gesling Stadium\\n\\nCapacity: 3,500\\n\\nSurface: FieldTurf\\n\\nBasketball/Volleyball Gym: Wiegand Gymnasium\\n\\nCapacity: 500\\n\\nSoccer Stadium: CMU Soccer Field\\n\\nCapacity: 250 Surface: FieldTurf Affiliation: NCAA Division III Conference: University Athletic Association | Presidents' Athletic Conference (Football Only) President: Dr. Farnam Jahanian Alma Mater, Year: University of Texas at Austin Athletic Director: Dr. Josh Centor Alma Mater, Year: Brandeis, 2004 Athletic Department Phone: 412-268-8054 Mailing Address:  5000 Forbes Avenue / Pittsburgh, PA 15213\"),\n",
       " Document(page_content=' \\nPh.D. in Language and Information Technology: The Ph.D. in LTI focuses on developing the next generation of scientific and entrepreneurial leaders. The first two years of the Ph.D. program are similar to our MLT program. After the second year, you will spend most of your time working closely with your faculty advisor on research that advances the state-of-the-art in computer science.\\n\\nPh.D. students are expected to publish papers about original research in the most competitive scientific journals and international conference proceedings, and to present their research at conferences and workshops. Most of our Ph.D. graduates become professors and research scientists, while a few have started their own companies.\\n'),\n",
       " Document(page_content=' \\nPh.D. in Language and Information Technology Requirements: In general, students pursuing a Ph.D. in Language and Information Technologies must\\n\\n    Pass at least 96 units of graduate-level courses.\\n    Satisfy proficiencies in writing, presentation, programming and teaching; and\\n    Propose, write and defend a Ph.D. dissertation (thesis).\\n\\nStudents must also attend the LTI Colloquium each semester and satisfy our Research Speaking Requirement.\\n\\nFor a detailed breakdown of the above requirements, download and read the PhD Handbook.\\n'),\n",
       " Document(page_content=' \\nPh.D. in Language and Information Technology Curriculum: In order to obtain your Ph.D. in Language and Information Technologies, you need to pass 96 units (generally, eight courses) of graduate courses that fulfill these requirements:\\n\\n    At least 72 units of LTI courses: Must include one class in each LTI focus area.\\n    At least 24 units of SCS courses.\\n    At least two lab courses in two different research areas.\\n'),\n",
       " Document(page_content=' \\nPh.D. in Language and Information Technology Admission: \\n\\nCarnegie Mellon\\'s School of Computer Science has a centralized online application process. Applications and all supporting documentation for fall admission to any of the LTI\\'s graduate programs must be received by the application deadline. Incomplete applications will not be considered. The application period for Fall 2024 will open on September 6, 2023.\\n\\nFinal Application Deadline\\n\\nDecember 13, 2023 at 3 p.m. EST.\\n\\nCost\\n\\n    $100 per program and $80 if the applicant applies before November 29, 2023 at 3 p.m. EST (early deadline).\\n\\nFee Waivers\\n\\nFee waivers may be available in cases of financial hardship. For more information, please refer to the School of Computer Science Fee Waiver page.\\n\\nRequirements\\n\\nThe School of Computer Science requires the following for all Ph.D. applications.\\n\\n    GRE scores: GREs are now optional, but if you want to submit GRE scores:\\n    These must be less than five years old. The GRE Subject Test is not required, but is recommended. Our Institution Code is 2074; Department Code is 0402.\\n    English Proficiency Requirement: If you will be studying on an F-1 or J-1 visa, and English is not a native language for you (native language…meaning spoken at home and from birth), an official copy of an English proficiency score report is required. The English proficiency requirement cannot be waived for any reason. Find more information under \"Test Scores\" on our FAQ page.\\n    Successful applicants will have a minimum TOEFL score of 100. Our Institution Code is 4256; the Department Code is 78.\\n    Official transcripts from each university you have attended, regardless of whether you received your degree there.\\n    Current resume.\\n    Statement of Purpose.\\n    Three letters of recommendation.\\n\\nFor more details on these requirements, please see the SCS Doctoral Admissions page.\\n\\nIn addition to the SCS guidelines, the LTI requires:\\n\\n    A short (1-3 minute) video of yourself. There will be a prompt question that you will respond to. You will have three attempts. This is not a required part of the application process, but it\\'s strongly suggested.\\n    Any outside funding you are receiving must be accompanied by an official award letter.\\n\\nNo incomplete applications will be eligible for consideration.\\n\\nFor specific application/admissions questions, please contact Stacey Young.\\n'),\n",
       " Document(page_content=' \\nDual-Degree Ph.D. in Language and Information Technologies (Portugal Partnership): The LTI offers a dual-degree Ph.D. in Language and Information Technologies in cooperation with:\\n\\n    Universidade de Aveiro (Ph.D. in Computer Engineering), Universidade do Minho (Ph.D. in Informatics)  and the Universidade do Porto (FCUP, Ph.D. in Computer Science and FEUP, Ph.D. in Computer Science) as part of MAPi;\\n    Universidade de Lisboa, Faculdade de Ciências – FCUL (Ph.D. in Informatics) \\n\\n    Universidade de Lisboa, Instituto Superior Técnico – IST  (Ph.D. in Computer Science and Engineering, Ph.D. in Electrical and Computer Engineering, Ph.D. in Information Security)\\n\\n    Universidade Nova de Lisboa, Faculdade de Ciências e Tecnologia – FCTUNL (Ph.D. in Computer Science)\\n\\n    Universidade de Coimbra, Faculdade de Ciências e Tecnologia – FCTUC (Ph.D. in Information Science and Technology)\\n\\nStudents jointly enrolled in the LTI Ph.D program spend a year in Portugal, then two years at Carnegie Mellon taking classes in linguistics, computer science, statistical learning and task orientation.\\n\\nAfter completing the majority of their academic requirements, students return to Portugal for the next two years to conduct extensive research, ultimately leading to a dissertation topic that will be publicly defended. One adviser from each institution co-supervises their student’s progress and helps to define their final thesis topic.\\n'),\n",
       " Document(page_content=' \\nDual-Degree Ph.D. in Language and Information Technologies (Portugal Partnership) Requirments: Students participating in the dual-degree program will spend their first year in Portugal, followed by two years in Pittsburgh to complete their coursework. They will complete a maximum of eight courses with a proper balance of focus areas (linguistics, computer science, statistical/learning and task orientation). After that, they will return to Portugal for their last two years, pursuing research and completing their dissertation. For more, see the Carnegie Mellon | Portugal page.\\n'),\n",
       " Document(page_content=' \\nDual-Degree Ph.D. in Language and Information Technologies (Portugal Partnership) Admission: \\n\\nStudents applying to the dual degree program must apply through Carnegie Mellon\\'s online application. In addition to the requirements listed below, prospective students must also contact Stacey Young when applying.\\n\\nCarnegie Mellon\\'s School of Computer Science has a centralized online application process. Applications and all supporting documentation for fall admission to any of the LTI\\'s graduate programs must be received by the application deadline. Incomplete applications will not be considered. The application period for Fall 2024 will open on September 6, 2023.\\n\\nFinal Application Deadline\\n\\nDecember 13, 2023 3:00 p.m. EST.\\n\\nCost\\n\\n    $100 per program, $80 for applications submitted by November 29, 2023 at 3:00 p.m. EST (early deadline).\\n\\nFee Waivers\\n\\nFee waivers may be available in cases of financial hardship, or for participants in select programs. For more information, please refer to the School of Computer Science Fee Waiver page.\\n\\nRequirements\\n\\nThe School of Computer Science requires the following for all Ph.D. applications.\\n\\n    GRE scores: These must be less than five years old. The GRE Subject Test is not required, but is recommended. Our Institution Code is 2074; Department Code is 0402.\\n    English Proficiency Requirement: If you will be studying on an F-1 or J-1 visa, and English is not a native language for you (native language…meaning spoken at home and from birth), an official copy of an English proficiency score report is required. The English proficiency requirement cannot be waived for any reason. Find more information under \"Test Scores\" on our FAQ page.\\n    Successful applicants will have a minimum TOEFL score of 100. Our Institution Code is 4256; the Department Code is 78.\\n    Official transcripts from each university you have attended, regardless of whether you received your degree there.\\n    Current resume.\\n    Statement of Purpose.\\n    Three letters of recommendation.\\n\\nFor more details on these requirements, please see the SCS Doctoral Admissions page.\\n\\nIn addition to the SCS guidelines, the LTI requires:\\n\\n    A short (1-3 minute) video of yourself. There will be a prompt question that you will respond to. You will have three attempts. This is not a required part of the application process, but it\\'s strongly suggested.\\n\\n    Any outside funding you are receiving must be accompanied by an official award letter.\\n\\nNo incomplete applications will be eligible for consideration.\\n\\nFor specific application/admissions questions, please contact Stacey Young.\\n\\n'),\n",
       " Document(page_content=' \\nMaster of Language Technologies: The MLT program prepares students for a research career in academia or industry. In this program, you’ll be immersed in research for two full years. During the academic year, your time will be evenly split between taking courses and doing research with your faculty advisor. Your summer will be devoted entirely to research. Many MLT grads continue on to Ph.D. programs at CMU and other top institutions, while others pursue careers at companies emphasizing research and rapid innovation.\\n'),\n",
       " Document(page_content=' \\nMaster of Language Technologies Requirements: \\n\\nThe MLT program lasts two years (24 months), and students must complete two summers of research. Students should usually expect to graduate in August of their second year.\\n\\nMLT students take 120 or more course units (about 10 courses), at least 72 of which are LTI courses, and 24 of which are School of Computer Science (SCS) courses. Most of these are 12-unit courses, although lab courses are typically 6 units. Our courses generally assume knowledge of programming and data structures. The remaining units may also be taken from the LTI, or with approval from the faculty advisor, any other senior- or graduate-level course offered at CMU or Pitt.\\n\\nDirected research is another integral part of the MLT program; MLT students carry out directed research during their studies, with guidance from their faculty advisors.\\n\\nStudents may also choose to complete an optional MLT thesis. Guidelines can be found in the PDF iconMLT Handbook.\\n\\n'),\n",
       " Document(page_content=' \\nMaster of Language Technologies Admission: \\n\\nCarnegie Mellon\\'s School of Computer Science has a centralized online application process. Applications and all supporting documentation for fall admission to any of the LTI\\'s graduate programs must be received by the application deadline. Incomplete applications will not be considered. The application period for Fall 2024 will open on September 6, 2023.\\n\\n*Please note, we no longer require mailed, hard versions of transcripts or test scores at the time of application. Do not mail anything to the admissions office. If you are accepted to a program, you will be given instruction to then mail your materials.\\n\\nFinal Application Deadline\\n\\nDecember 13, 2023 at 3 p.m. EST.\\n\\nCost\\n\\n    $100 per program, $80 for applications submitted before November 29, 2023 at 3PM EST (early deadline).\\n\\nFee Waivers\\n\\nFee waivers may be available in cases of financial hardship, or for participants in select \"pipeline\" programs. For more information, please refer to the School of Computer Science Fee Waiver page.\\n\\nRequirements\\n\\nThe School of Computer Science requires the following for all graduate program applications:\\n\\n    GRE scores: GREs are now optional, but if you want to submit GRE scores:\\n    These must be less than five years old. A GRE subject test in science, engineering, computer science, math, etc. is not required, but you may complete one and submit the scores if you wish. Our Institution Code is 2074; Department Code is 0402.\\n    If you will be studying on an F-1 or J-1 visa, and English is not a native language for you (native language…meaning spoken at home and from birth), an official copy of an English proficiency score report is required. The English proficiency requirement cannot be waived for any reason. Find more information under \"Test Scores\" on our FAQ page.\\n    Successful applicants will have a minimum TOEFL score of 100. Our Institution Code is 4256; the Department Code is 78.\\n    Official transcripts from each university you have attended, regardless of whether you received your degree there.\\n    Current resume.\\n    Statement of Purpose.\\n    Three letters of recommendation.\\n\\nFor more details on these requirements, please see the SCS Master\\'s Admissions page.\\n\\nIn addition to the SCS guidelines, the LTI requires:\\n\\n    A short (1-3 minute) video of yourself. There will be a prompt question that you will respond to. You will have three attempts. This is not a required part of the application process, but it\\'s strongly suggested.\\n\\n    Any outside funding you are receiving must be accompanied by an official award letter.\\n\\nNo incomplete applications will be eligible for consideration.\\n\\nFor specific application/admissions questions, please contact Kate Schaich.\\n\\nTuition Rates\\n\\nTuition is set by the School of Computer Science and can vary by year. Current tuition rates can be found on the Graduate Tuition section of the Student Financial Services website.\\n\\nFinancial Aid Resources\\n\\nResearch Assistant-ships are occasionally offered by research advisors to current MLT students. These are not guaranteed and vary from semester-to-semester. They fluctuate and are dependent on the funding source, research advisor and MLT student.\\n\\nStudent Financial Services has additional information on financial aid and billing / payments. They have a detailed outline of how to apply for financial aid on the Graduate Financial Aid Process page.\\n\\nEnrollment & Finances has additional resource links to assist with financial aid and tuition payments.\\n\\nGraduate Education – Financial Assistance provides resources for current students regarding emergency loans and conference travel grants.\\n\\n'),\n",
       " Document(page_content=\" \\nMaster of Science in Intelligent Information Systems (MIIS): The Master's in Intelligent Information Systems degree focuses on recognizing and extracting meaning from text, spoken language and video. As an MIIS student, you’ll receive the department’s deepest exposure to content analysis and machine learning. In addition to completing the program’s coursework, you’ll work on directed study projects with your faculty advisor for two semesters; participate in a summer internship; and collaborate with your peers on a semester-long, group-oriented capstone project. This combination of classroom instruction, professional experience, and using new skills in significant projects with world-class colleagues will help prepare you for a successful career in industry or government. Our alumni have gone on to exciting careers at places like Apple, IBM and Google, and most have job offers within six weeks of graduation.\\n\"),\n",
       " Document(page_content=' \\nMaster of Science in Intelligent Information Systems (MIIS) Requirements: \\n\\nThe Intelligent Information Systems degree offers students the flexibility to create their own course of study in consultation with their advisor. \\n\\nMIIS students gain three types of practical experience: software development supervised by their advisor (24 units equivalent to two courses); a summer internship (which can be waived for students that have sufficient prior professional experience); and a capstone project executed in a group of peers (42 units equivalent to three 12-unit courses and one 6-unit course). This combination is proven to help IIS students to broaden their skills quickly. The MIIS degree is offered in two options:\\n\\nOption 1. Standard MIIS degree (MIIS-16) - A 16-month track that is completed in three academic semesters (fall, spring, fall) and a summer internship.  \\n\\nOption 2. MIIS: Advanced Study degree (MIIS-21) - A 21-month track that is completed in four academic semesters (fall, spring, fall, spring) and a summer internship.\\n\\nMIIS: Advanced Study track offers indepth degree in one of the following areas of concentration:\\n\\n    Human Language for Language Technologies\\n    Language Technology Application\\n    Machine Learning for Language Technologies\\n\\nPart-time education option is available in some cases. \\n\\nMIIS-16 students must take at least 84 units (typically 7 courses) of qualifying and elective courses that satisfy human language, machine learning, and language technology applications breadth requirements. MIIS-21 students have to take at least two more courses from the selected concentration area to satisfy their degree requirements, making it total of 108 units (typically 9 courses) of qualifying and elective courses, that also satisfy breadth requirements.\\n\\nFor a full list of requirements, read the MIIS Handbook.\\n\\n'),\n",
       " Document(page_content=' \\nMaster of Science in Intelligent Information Systems (MIIS) Admission: \\n\\nCarnegie Mellon\\'s School of Computer Science has a centralized online application process. Applications and all supporting documentation for fall admission to any of the LTI\\'s graduate programs must be received by the application deadline. Incomplete applications will not be considered. The application period for Fall 2024 will open on September 6, 2023.\\n\\n*Please note, we no longer require mailed, hard versions of transcripts or test scores at the time of application. Do not mail anything to the admissions office. If you are accepted to a program, you will be given instruction to then mail your materials.\\n\\nFinal Application Deadline\\n\\nDecember 13, 2023 at 3:00 p.m. EST.\\n\\nCost\\n\\n    $100 per program, $80 for applications submitted before November 29, 2023 at 3:00 p.m. EST (early deadline).\\n\\nFee Waivers\\n\\nFee waivers may be available in cases of financial hardship, or for participants in select \"pipeline\" programs. For more information, please refer to the School of Computer Science Fee Waiver page.\\n\\nRequirements\\n\\nThe School of Computer Science requires the following for all Master\\'s applications.\\n\\n    GRE scores: MIIS applicants must submit their GRE scores. The scores must be less than five years old. The GRE Subject Test is not required, but is recommended. Our Institution Code is 2074; Department Code is 0402.\\n\\n    Proof of English Language Proficiency:\\n\\nIf you will be studying on an F-1 or J-1 visa, and English is not a native language for you (native language…meaning spoken at home and from birth), we are required to formally evaluate your English proficiency. We require applicants who will be studying on an F-1 or J-1 visa, and for whom English is not a native language, to demonstrate English proficiency via one of these standardized tests: TOEFL (preferred), IELTS, or Duolingo.  We discourage the use of the \"TOEFL ITP Plus for China,\" since speaking is not scored.\\n\\nWe do not issue waivers for non-native speakers of English.  In particular, we do not issue waivers based on previous study at a U.S. high school, college, or university.  We also do not issue waivers based on previous study at an English-language high school, college, or university outside of the United States.  No amount of educational experience in English, regardless of which country it occurred in, will result in a test waiver. Submit valid, recent scores: If as described above you are required to submit proof of English proficiency, your TOEFL, IELTS or Duolingo test scores will be considered valid as follows: If you have not received a bachelor’s degree in the U.S., you will need to submit an English proficiency score no older than two years. (scores from exams taken before Sept. 1, 2021, will not be accepted.)\\n\\nIf you are currently working on or have received a bachelor\\'s and/or a master\\'s degree in the U.S., you may submit an expired test score up to five years old. (scores from exams taken before Sept. 1, 2018, will not be accepted.)Additional details about English proficiency requirements are provided on the FAQ page.\\n\\n    Official transcripts from each university you have attended, regardless of whether you received your degree there.\\n    Current resume.\\n    Statement of Purpose.\\n    Three letters of recommendation.\\n\\nFor more details on these requirements, please see the SCS Master\\'s Admissions page.\\n\\nIn addition to the SCS guidelines, the MIIS requires:\\n\\n    A short (1-3 minute) video of yourself. There will be a prompt question that you will respond to. You will have three attempts.\\n\\nNo incomplete applications will be eligible for consideration.\\n\\nFor specific application/admissions questions, please contact Brianna Eriksen.\\n\\n'),\n",
       " Document(page_content=' \\nMaster of Computational Data Science (MCDS): The MCDS degree focuses on engineering and deploying large-scale information systems. Our comprehensive curriculum equips you with the skills and knowledge to develop the layers of technology involved in the next generation of massive information system deployments and analyze the data these systems generate. When you graduate, you’ll have a unified vision of these systems from your core courses; internship experience; and semester-long, group-oriented capstone project. MCDS graduates are sought-after software engineers, data scientists and project managers at leading information technology, software services and social media companies.\\n'),\n",
       " Document(page_content=' \\n\\nMaster of Computational Data Science (MCDS) Requirements:\\n\\nThe MCDS program offers three majors: Systems, Analytics, and Human-Centered Data Science. All three require the same total number of course credits, split among required core courses, electives, data science seminar and capstone courses specifically defined for each major. The degree can also be earned in two different ways, depending on the length of time you spend working on it. Regardless of the timing option, all MCDS students must complete a minimum of 144 units to graduate.\\n\\nHere are the options:\\n\\n    Standard Timing — a 16-month degree consisting of study for fall and spring semesters, a summer internship, and fall semester of study. Each semester comprises a minimum of 48 units. This timing is typical for most students. Students graduate in December.\\n    Extended Timing — a 20-month degree consisting of study for fall and spring semesters, a summer internship, and a second year of fall and spring study. Each semester comprises a minimum of 36 units. Students graduate in May.\\n\\nFor a complete overview of the MCDS requirements, visit the MCDS website or read the MCDS Handbook.\\n\\n'),\n",
       " Document(page_content=' \\nMaster of Computational Data Science (MCDS) Admission:\\n\\n\\nCarnegie Mellon\\'s School of Computer Science has a centralized online application process. Applications and all supporting documentation for fall admission to any of the LTI\\'s graduate programs must be received by the application deadline. Incomplete applications will not be considered. The application period for Fall 2024 will open on September 6, 2023.\\n\\n*Please note, we no longer require mailed, hard versions of transcripts or test scores at the time of application. Do not mail anything to the admissions office. If you are accepted to a program, you will be given instruction to then mail your materials.\\n\\nFinal Application Deadline\\n\\nDecember 13, 2023 at 3:00 p.m. EST.\\n\\nCost\\n\\n    $100 per program, $80 for applications submitted before November 29,2023 at 3:00 p.m. EST (early deadline).\\n\\nFee Waivers\\n\\nFee waivers may be available in cases of financial hardship, or for participants in select \"pipeline\" programs. For more information, please refer to the School of Computer Science Fee Waiver page.\\n\\nRequirements\\n\\nThe School of Computer Science requires the following for all Ph.D. applications.\\n\\n    GRE scores: These must be less than five years old. The GRE Subject Test is not required, but is recommended. Our Institution Code is 2074; Department Code is 0402.\\n\\n    Proof of English Language Proficiency:\\n    If you will be studying on an F-1 or J-1 visa, and English is not a native language for you (native language…meaning spoken at home and from birth), we are required to formally evaluate your English proficiency. We require applicants who will be studying on an F-1 or J-1 visa, and for whom English is not a native language, to demonstrate English proficiency via one of these standardized tests: TOEFL (preferred), IELTS, or Duolingo. We discourage the use of the \"TOEFL ITP Plus for China,\" since speaking is not scored.\\n\\n    We do not issue waivers for non-native speakers of English. In particular, we do not issue waivers based on previous study at a U.S. high school, college, or university. We also do not issue waivers based on previous study at an English-language high school, college, or university outside of the United States. No amount of educational experience in English, regardless of which country it occurred in, will result in a test waiver.\\n\\n    Applicants applying to MCDS are required to submit scores from an English proficiency exam taken within the last two years. Scores taken before Sept. 1, 2021, will not be accepted regardless of whether you have previously studied in the U.S. For more information about their English proficiency score policies, visit the MCDS admission website. \\n\\n    Successful applicants will have a minimum TOEFL score of 100 (Reading, Listening, Speaking, Writing scores all 25 or above), IELTS score of 7.5 (Reading 7 or above, Listening 7 or above, Speaking 7.5 or above, Writing 6.5 or above), or DuoLingo score of 120 (Literacy 115 or above, Comprehension 125 or above, Production 100 or above, Conversation 105 or above). Our Institution Code is 4256; the Department Code is 78. Additional details about English proficiency requirements are provided on the FAQ page. \\n    Official transcripts from each university you have attended, regardless of whether you received your degree there.\\n    A short (1-3 minutes) video of yourself. Tell us about you and why you are interested in the MCDS program. This is not a required part of the application process, but it is STRONGLY suggested.  \\n    Current resume.\\n    Statement of Purpose.\\n    Three letters of recommendation.\\n\\nFor more details on these requirements, please see the SCS Master\\'s Admissions page.\\n\\nIn addition to the SCS guidelines, the LTI requires:\\n\\n    Any outside funding you are receiving must be accompanied by an official award letter.\\n\\nNo incomplete applications will be eligible for consideration.\\n\\nFor specific application/admissions questions, please contact Jennifer Lucas or Caitlin Korpus.\\n\\n'),\n",
       " Document(page_content=' \\nMaster of Computational Data Science (MCDS) Curriculum:\\n\\nTo earn an MCDS degree, student must pass courses in the core curriculum, the MCDS seminar, a concentration area and electives. Students must also complete a capstone project in which they work on a research project at CMU or on an industry-sponsored project.\\n\\nIn total, students must complete 144 eligible units of study, including eight 12-unit courses, two 12-unit seminar courses and one 24-unit capstone course. Students must choose at minimum five core courses. The remainder of the 12-unit courses with course numbers 600 or greater can be electives chosen from the SCS course catalog. Any additional non-prerequisite units taken beyond the 144 units are also considered electives.\\n\\nMCDS students must also pass the undergraduate course 15-513 Introduction to Computer Systems (6 units), typically in the summer before their program commences. The student must pass with a grade of B- or better. Failure to pass the course means that the student takes 15-213 during either the fall or spring semester. Note that in both cases the units do not count toward the 144 eligible units of study.\\n'),\n",
       " Document(page_content=' \\nMaster of Science in Artificial Intelligence and Innovation (MSAII):\\nThe Master of Science in Artificial Intelligence and Innovation (MSAII) program is a successor to the M.S. in Biotechnology, Innovation and Computing (MSBIC). It combines a rigorous AI and machine learning curriculum with real-world team experience in identifying an AI market niche and developing a responsive product in cooperation with external stakeholders. The core program, which lasts four semesters and leads to a capstone project, focuses on both intrapreneurship and entrepreneurship, equipping graduates to either begin a startup or develop a new organization within an existing company. Students will also gain critical practical skills, such as making persuasive technical presentations, assembling development teams, and evaluating the potential of new market ideas.\\n'),\n",
       " Document(page_content=' \\nMaster of Science in Artificial Intelligence and Innovation (MSAII) Requirements:\\n\\n\\nIncoming students generally hold undergraduate degrees in computer science, software engineering, bioinformatics or bioengineering. To earn the MSAII degree, you must pass courses in the Core Curriculum, the Knowledge Requirements and Electives. You must also complete a capstone project in which you work on a development project as part of the Core Curriculum. In total, you will complete 192 eligible units of study, including 84 units of Core Curriculum (including the 36-unit Capstone), 72 units of Knowledge Requirements and at least 36 units of approved Electives.\\n\\nFor full requirements and program details, read the MSAII Handbook.\\n\\n'),\n",
       " Document(page_content=' \\n\\nMaster of Science in Artificial Intelligence and Innovation (MSAII) Admission: \\n\\nCarnegie Mellon\\'s School of Computer Science has a centralized online application process. Applications and all supporting documentation for fall admission to any of the LTI\\'s graduate programs must be received by the application deadline. Incomplete applications will not be considered. The application period for Fall 2024 will open on September 6, 2023.\\n\\nFinal Application Deadline\\n\\nDecember 13, 2023 at 3:00 p.m. EST.\\n\\nCost\\n\\n    $100 for one program, $80 if application is submitted before November 29, 2023 at 3PM EST (early deadline).\\n\\nFee Waivers\\n\\nFee waivers may be available in cases of financial hardship, or for participants in select \"pipeline\" programs. For more information, please refer to the School of Computer Science Fee Waiver page.\\n\\nRequirements\\n\\nThe School of Computer Science requires the following for all applications:\\n\\n    A GPA of 3.0 or higher. (Students should report raw university GPA scores and NOT converted scores. Please DO NOT convert your international score to a US GPA or weighted GPA or other system).\\n\\n    GRE scores: GRE is required. Our Institution Code is 2074; Department Code is 0402. \\n\\n    English Language Proficiency: If you will be studying on an F-1 or J-1 visa, and English is not a native language for you (native language…meaning spoken at home and from birth), an official copy of an English proficiency score report is required. The English proficiency requirement cannot be waived for any reason. Find more information under \"Test Scores\" on our FAQ page.Unofficial transcripts from each university you have attended, regardless of whether you received a degree.\\n\\n    Current resume.\\n\\n    Statement of Purpose. A Statement of Purpose is not a resume. It should discuss your reasons for choosing the MSAII program and indicate your intended career path. \\n\\n    Three letters of recommendation.\\n\\n    A short (1-3 minutes) video of yourself. Tell us about you and why you are interested in the MSAII program. This is not a required part of the application process, but it is STRONGLY suggested.  \\n\\nFor specific application/admissions questions, please contact Amber Vivis. \\n'),\n",
       " Document(page_content=' \\nUndergraduate LT Concentration: Human language technologies have become an increasingly central component of computer science. Information retrieval, machine translation and speech technology are used daily by the general public, while text mining, natural language processing and language-based tutoring are common within more specialized professional or educational environments. The LTI prepares students for this world by offering a minor that gives you the opportunity to not only learn about language technologies, but to also apply that knowledge through a directed project.\\n'),\n",
       " Document(page_content=' \\nUndergraduate LT Concentration Requirements: \\n\\n\\nStudents interested in the language technologies minor must complete our prerequisite courses with an average grade of B (3.0) or better before applying to the program. (Students who do not meet this average must submit a letter of explanation along with their application.) Prerequisites include:\\n\\n    Principles of Imperative Computation (15-122)\\n    Principles of Functional Programming (15-150)\\n\\nWe also strongly encourage candidates to take\\n\\n    Differential and Integral Calculus (21-120) and Integration and Approximation (21-122)\\n    Matrices and Linear Transformations (21-241) or Matrix Theory (21-242)\\n    Probability and Computing (15-259) or Probability (21-325) or Probability Theory for Computer Scientists (36-218) or Introduction to Probability Theory (36-225)\\n\\n\\n'),\n",
       " Document(page_content=\"\\nUndergraduate LT Concentration Admission:\\nStudents interested in earning a minor in language technologies must apply for admission no later than September 30 of their senior year. An admission decision will usually be made within one month. Students may petition the LTI undergraduate program director to be admitted to the minor earlier or later in their undergraduate careers. To apply, contact the program's director, Carolyn Rosé.\\n\"),\n",
       " Document(page_content=' \\nUndergraduate LT Concentration Curriculum:\\nThe Language Technologies Concentration requires that SCS students complete one core course and their choice of three elective courses of at least 9 units each. The electives can be chosen from a specific set of stand-alone courses. In addition to the four courses, students are required to do an undergraduate research project for at least 9 units to complete their concentration.\\n'),\n",
       " Document(page_content='Author: Yonatan Bisk Title: HomeRobot: An Open Source Software Stack for Mobile Manipulation Research Publication year: 2024 Coauthors: Chris Paxton, Austin Wang, Binit Shah, Blaine Matulevich, Dhruv Shah, Karmesh Yadav, Santhosh Ramakrishnan, Sriram Yenamandra, Yonatan Bisk Abstract: Reproducibility in robotics research requires capable, shared hardware platforms which can be used for a wide variety of research. We’ve seen the power of these sorts of shared platforms in more general machine learning research, where there is constant iteration on shared AI platforms like PyTorch. To be able to make rapid progress in robotics in the same way, we propose that we need: (1) shared real-world platforms which allow different teams to test and compare methods at low cost; (2) challenging simulations that reflect real-world environments and especially can drive perception and planning research; and (3) low-cost platforms with enough software to get started addressing all of these problems. To this end, we propose HomeRobot, a mobile manipulator software stack with associated benchmark in simulation, which is initially based on the low-cost, human-safe Hello Robot Stretch.'),\n",
       " Document(page_content=\"Author: Yonatan Bisk Title: SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs Publication year: 2023 Coauthors: Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey, Yanping Huang, David A. Ross, Irfan Essa, Yonatan Bisk, Ming Yang, K. Murphy, A. Hauptmann, Lu Jiang Abstract: In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos. SPAE converts between raw pixels and interpretable lexical tokens (or words) extracted from the LLM's vocabulary. The resulting tokens capture both the semantic meaning and the fine-grained details needed for visual reconstruction, effectively translating the visual content into a language comprehensible to the LLM, and empowering it to perform a wide array of multimodal tasks. Our approach is validated through in-context learning experiments with frozen PaLM 2 and GPT 3.5 on a diverse set of image understanding and generation tasks. Our method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%.\"),\n",
       " Document(page_content='Author: Yonatan Bisk Title: HomeRobot: Open-Vocabulary Mobile Manipulation Publication year: 2023 Coauthors: Sriram Yenamandra, A. Ramachandran, Karmesh Yadav, Austin S. Wang, Mukul Khanna, Théophile Gervet, Tsung-Yen Yang, Vidhi Jain, Alexander Clegg, John Turner, Z. Kira, M. Savva, Angel X. Chang, Devendra Singh Chaplot, Dhruv Batra, Roozbeh Mottaghi, Yonatan Bisk, Chris Paxton Abstract: HomeRobot (noun): An affordable compliant robot that navigates homes and manipulates a wide range of objects in order to complete everyday tasks. Open-Vocabulary Mobile Manipulation (OVMM) is the problem of picking any object in any unseen environment, and placing it in a commanded location. This is a foundational challenge for robots to be useful assistants in human environments, because it involves tackling sub-problems from across robotics: perception, language understanding, navigation, and manipulation are all essential to OVMM. In addition, integration of the solutions to these sub-problems poses its own substantial challenges. To drive research in this area, we introduce the HomeRobot OVMM benchmark, where an agent navigates household environments to grasp novel objects and place them on target receptacles. HomeRobot has two components: a simulation component, which uses a large and diverse curated object set in new, high-quality multi-room home environments; and a real-world component, providing a software stack for the low-cost Hello Robot Stretch to encourage replication of real-world experiments across labs. We implement both reinforcement learning and heuristic (model-based) baselines and show evidence of sim-to-real transfer. Our baselines achieve a 20% success rate in the real world; our experiments identify ways future research work improve performance. See videos on our website: https://ovmm.github.io/.'),\n",
       " Document(page_content='Author: Yonatan Bisk Title: EXCALIBUR: Encouraging and Evaluating Embodied Exploration Publication year: 2023 Coauthors: Hao Zhu, Raghav Kapoor, So Yeon Min, Winson Han, Jiatai Li, Kaiwen Geng, Graham Neubig, Yonatan Bisk, Aniruddha Kembhavi, Luca Weihs Abstract: Experience precedes understanding. Humans constantly explore and learn about their environment out of curiosity, gather information, and update their models of the world. On the other hand, machines are either trained to learn passively from static and fixed datasets, or taught to complete specific goal-conditioned tasks. To encourage the development of exploratory interactive agents, we present the EXCALIBUR benchmark. EXCALIBUR allows agents to explore their environment for long durations and then query their understanding of the physical world via inquiries like: “is the small heavy red bowl made from glass?” or “is there a silver spoon heavier than the egg?”. This design encourages agents to perform free-form home exploration without myopia induced by goal conditioning. Once the agents have answered a series of questions, they can renter the scene to refine their knowledge, update their beliefs, and improve their performance on the questions. Our experiments demonstrate the challenges posed by this dataset for the present-day state-of-the-art embodied systems and the headroom afforded to develop new innovative methods. Finally, we present a virtual reality interface that enables humans to seamlessly interact within the simulated world and use it to gather human performance measures. EXCALIBUR affords unique challenges in comparison to presentday benchmarks and represents the next frontier for embodied AI research.'),\n",
       " Document(page_content=\"Author: Yonatan Bisk Title: Plan, Eliminate, and Track - Language Models are Good Teachers for Embodied Agents Publication year: 2023 Coauthors: Yue Wu, So Yeon Min, Yonatan Bisk, R. Salakhutdinov, A. Azaria, Yuan-Fang Li, Tom M. Mitchell, Shrimai Prabhumoye Abstract: Pre-trained large language models (LLMs) capture procedural knowledge about the world. Recent work has leveraged LLM's ability to generate abstract plans to simplify challenging control tasks, either by action scoring, or action modeling (fine-tuning). However, the transformer architecture inherits several constraints that make it difficult for the LLM to directly serve as the agent: e.g. limited input lengths, fine-tuning inefficiency, bias from pre-training, and incompatibility with non-text environments. To maintain compatibility with a low-level trainable actor, we propose to instead use the knowledge in LLMs to simplify the control problem, rather than solving it. We propose the Plan, Eliminate, and Track (PET) framework. The Plan module translates a task description into a list of high-level sub-tasks. The Eliminate module masks out irrelevant objects and receptacles from the observation for the current sub-task. Finally, the Track module determines whether the agent has accomplished each sub-task. On the AlfWorld instruction following benchmark, the PET framework leads to a significant 15% improvement over SOTA for generalization to human goal specifications.\"),\n",
       " Document(page_content='Author: Yonatan Bisk Title: Open X-Embodiment: Robotic Learning Datasets and RT-X Models Publication year: 2023 Coauthors: A. Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, A. Irpan, Alexander Khazatsky, Anant Rai, Anika Singh, Anthony Brohan, A. Raffin, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Schölkopf, Brian Ichter, Cewu Lu, Charles Xu, Chelsea Finn, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Chuer Pan, Chuyuan Fu, Coline Devin, Danny Driess, Deepak Pathak, Dhruv Shah, Dieter Büchler, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Federico Ceola, Fei Xia, F. Stulp, Gaoyue Zhou, G. Sukhatme, G. Salhotra, Ge Yan, Giulio Schiavi, G. Kahn, Hao Su, Haoshu Fang, Haochen Shi, H. B. Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jan Peters, Jan Schneider, Jasmine Hsu, J. Bohg, Jeff Bingham, Jiajun Wu, Jialin Wu, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jitendra Malik, Jonathan Tompson, Jonathan Yang, Joseph J. Lim, João Silvério, Junhyek Han, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, K. Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Zhang, Krishan Rana, K. Srinivasan, Lawrence Yunliang Chen, Lerrel Pinto, Liam Tan, Lionel Ott, Lisa Lee, Masayoshi Tomizuka, Maximilian Du, Michael Ahn, Mingtong Zhang, Mingyu Ding, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, N. Heess, Nikhil Joshi, Niko Suenderhauf, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, P. Sanketi, Paul Wohlhart, Peng Xu, P. Sermanet, Priya Sundaresan, Q. Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan C. Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham D. Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, S. Adebola, Simon Guist, Soroush Nasiriany, S. Schaal, Stefan Welker, Stephen Tian, Sudeep Dasari, Suneel Belkhale, Takayuki Osa, Tatsuya Harada, T. Matsushima, Ted Xiao, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Zhao, Travis Armstrong, Trevor Darrell, Vidhi Jain, Vincent Vanhoucke, W. Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xuanlin Li, Yao Lu, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yueh-hua Wu, Yujin Tang, Yuke Zhu, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zhuo Xu, Zichen Jeff Cui Abstract: —Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer'),\n",
       " Document(page_content='Author: Yonatan Bisk Title: Toward General-Purpose Robots via Foundation Models: A Survey and Meta-Analysis Publication year: 2023 Coauthors: Yafei Hu, Quanting Xie, Vidhi Jain, Jonathan Francis, Jay Patrikar, Nikhil Varma Keetha, Seungchan Kim, Yaqi Xie, Tianyi Zhang, Shibo Zhao, Yu Quan Chong, Chen Wang, Katia P. Sycara, Matthew Johnson-Roberson, Dhruv Batra, Xiaolong Wang, Sebastian Scherer, Z. Kira, Fei Xia, Yonatan Bisk Abstract: Building general-purpose robots that can operate seamlessly, in any environment, with any object, and utilizing various skills to complete diverse tasks has been a long-standing goal in Artificial Intelligence. Unfortunately, however, most existing robotic systems have been constrained - having been designed for specific tasks, trained on specific datasets, and deployed within specific environments. These systems usually require extensively-labeled data, rely on task-specific models, have numerous generalization issues when deployed in real-world scenarios, and struggle to remain robust to distribution shifts. Motivated by the impressive open-set performance and content generation capabilities of web-scale, large-capacity pre-trained models (i.e., foundation models) in research fields such as Natural Language Processing (NLP) and Computer Vision (CV), we devote this survey to exploring (i) how these existing foundation models from NLP and CV can be applied to the field of robotics, and also exploring (ii) what a robotics-specific foundation model would look like. We begin by providing an overview of what constitutes a conventional robotic system and the fundamental barriers to making it universally applicable. Next, we establish a taxonomy to discuss current work exploring ways to leverage existing foundation models for robotics and develop ones catered to robotics. Finally, we discuss key challenges and promising future directions in using foundation models for enabling general-purpose robotic systems. We encourage readers to view our living GitHub repository of resources, including papers reviewed in this survey as well as related projects and repositories for developing foundation models for robotics.'),\n",
       " Document(page_content=\"Author: Yonatan Bisk Title: Open X-Embodiment: Robotic Learning Datasets and RT-X Models Publication year: 2023 Coauthors: Open X-Embodiment Collaboration, A. Padalkar, Acorn Pooley, Ajay Mandlekar, Ajinkya Jain, Albert Tung, Alex Bewley, Alex Herzog, A. Irpan, Alexander Khazatsky, Anant Rai, Anika Singh, Animesh Garg, Anthony Brohan, A. Raffin, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, B. Scholkopf, Brian Ichter, Cewu Lu, Charles Xu, Chelsea Finn, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Chuer Pan, Chuyuan Fu, Coline Devin, Danny Driess, Deepak Pathak, Dhruv Shah, Dieter Buchler, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Federico Ceola, Fei Xia, F. Stulp, Gaoyue Zhou, G. Sukhatme, G. Salhotra, Ge Yan, Giulio Schiavi, G. Kahn, Hao Su, Haoshu Fang, Haochen Shi, H. B. Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jan Peters, Jan Schneider, Jasmine Hsu, J. Bohg, Jeff Bingham, Jiajun Wu, Jialin Wu, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jitendra Malik, Jonathan Booher, Jonathan Tompson, Jonathan Yang, Joseph J. Lim, Joao Silv'erio, Junhyek Han, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, K. Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Zhang, Krishan Rana, K. Srinivasan, Lawrence Yunliang Chen, Lerrel Pinto, Fei-Fei Li, Liam Tan, Lionel Ott, Lisa Lee, Masayoshi Tomizuka, Max Spero, Maximilian Du, Michael Ahn, Mingtong Zhang, Mingyu Ding, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, N. Heess, Nikhil Joshi, Niko Suenderhauf, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, P. Sanketi, Paul Wohlhart, Peng Xu, P. Sermanet, Priya Sundaresan, Q. Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Roberto Mart'in-Mart'in, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan C. Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham D. Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, S. Adebola, Simon Guist, Soroush Nasiriany, Stefan Schaal, Stefan Welker, Stephen Tian, Sudeep Dasari, Suneel Belkhale, Takayuki Osa, Tatsuya Harada, T. Matsushima, Ted Xiao, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Zhao, Travis Armstrong, Trevor Darrell, Vidhi Jain, Vincent Vanhoucke, W. Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xuanlin Li, Yao Lu, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yueh-hua Wu, Yujin Tang, Yuke Zhu, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zhuo Xu, Zichen Jeff Cui Abstract: Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer Vision, this has led to a consolidation of pretrained models, with general pretrained backbones serving as a starting point for many applications. Can such a consolidation happen in robotics? Conventionally, robotic learning methods train a separate model for every application, every robot, and even every environment. Can we instead train generalist X-robot policy that can be adapted efficiently to new robots, tasks, and environments? In this paper, we provide datasets in standardized data formats and models to make it possible to explore this possibility in the context of robotic manipulation, alongside experimental results that provide an example of effective X-robot policies. We assemble a dataset from 22 different robots collected through a collaboration between 21 institutions, demonstrating 527 skills (160266 tasks). We show that a high-capacity model trained on this data, which we call RT-X, exhibits positive transfer and improves the capabilities of multiple robots by leveraging experience from other platforms. More details can be found on the project website $\\\\href{https://robotics-transformer-x.github.io}{\\\\text{robotics-transformer-x.github.io}}$.\"),\n",
       " Document(page_content=\"Author: Yonatan Bisk Title: MOSAIC: Learning Unified Multi-Sensory Object Property Representations for Robot Perception Publication year: 2023 Coauthors: Gyan Tatiya, Jonathan M Francis, Ho-Hsiang Wu, Yonatan Bisk, J. Sinapov Abstract: A holistic understanding of object properties across diverse sensory modalities (e.g., visual, audio, and haptic) is essential for tasks ranging from object categorization to complex manipulation. Drawing inspiration from cognitive science studies that emphasize the significance of multi-sensory integration in human perception, we introduce MOSAIC (Multi-modal Object property learning with Self-Attention and Integrated Comprehension), a novel framework designed to facilitate the learning of unified multi-sensory object property representations. While it is undeniable that visual information plays a prominent role, we acknowledge that many fundamental object properties extend beyond the visual domain to encompass attributes like texture, mass distribution, or sounds, which significantly influence how we interact with objects. In MOSAIC, we leverage this profound insight by distilling knowledge from the extensive pre-trained Contrastive Language-Image Pre-training (CLIP) model, aligning these representations not only across vision but also haptic and auditory sensory modalities. Through extensive experiments on a dataset where a humanoid robot interacts with 100 objects across 10 exploratory behaviors, we demonstrate the versatility of MOSAIC in two task families: object categorization and object-fetching tasks. Our results underscore the efficacy of MOSAIC's unified representations, showing competitive performance in category recognition through a simple linear probe setup and excelling in the fetch object task under zero-shot transfer conditions. This work pioneers the application of CLIP-based sensory grounding in robotics, promising a significant leap in multi-sensory perception capabilities for autonomous systems. We have released the code, datasets, and additional results: https://github.com/gtatiya/MOSAIC.\"),\n",
       " Document(page_content='Author: Yonatan Bisk Title: Reasoning about the Unseen for Efficient Outdoor Object Navigation Publication year: 2023 Coauthors: Quanting Xie, Tianyi Zhang, Kedi Xu, M. Johnson-Roberson, Yonatan Bisk Abstract: Robots should exist anywhere humans do: indoors, outdoors, and even unmapped environments. In contrast, the focus of recent advancements in Object Goal Navigation(OGN) has targeted navigating in indoor environments by leveraging spatial and semantic cues that do not generalize outdoors. While these contributions provide valuable insights into indoor scenarios, the broader spectrum of real-world robotic applications often extends to outdoor settings. As we transition to the vast and complex terrains of outdoor environments, new challenges emerge. Unlike the structured layouts found indoors, outdoor environments lack clear spatial delineations and are riddled with inherent semantic ambiguities. Despite this, humans navigate with ease because we can reason about the unseen. We introduce a new task OUTDOOR, a new mechanism for Large Language Models (LLMs) to accurately hallucinate possible futures, and a new computationally aware success metric for pushing research forward in this more complex domain. Additionally, we show impressive results on both a simulated drone and physical quadruped in outdoor environments. Our agent has no premapping and our formalism outperforms naive LLM-based approaches'),\n",
       " Document(page_content='Author: Yonatan Bisk Title: Open X-Embodiment: Robotic Learning Datasets and RT-X Models Publication year: 2023 Coauthors: A. Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, A. Irpan, Alexander Khazatsky, Anant Rai, Anika Singh, Anthony Brohan, A. Raffin, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Schölkopf, Brian Ichter, Cewu Lu, Charles Xu, Chelsea Finn, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Chuer Pan, Chuyuan Fu, Coline Devin, Danny Driess, Deepak Pathak, Dhruv Shah, Dieter Büchler, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Federico Ceola, Fei Xia, F. Stulp, Gaoyue Zhou, G. Sukhatme, G. Salhotra, Ge Yan, Giulio Schiavi, G. Kahn, Hao Su, Haoshu Fang, Haochen Shi, H. B. Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jan Peters, Jan Schneider, Jasmine Hsu, J. Bohg, Jeff Bingham, Jiajun Wu, Jialin Wu, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jitendra Malik, Jonathan Tompson, Jonathan Yang, Joseph J. Lim, João Silvério, Junhyek Han, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, K. Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Zhang, Krishan Rana, K. Srinivasan, Lawrence Yunliang Chen, Lerrel Pinto, Liam Tan, Lionel Ott, Lisa Lee, Masayoshi Tomizuka, Maximilian Du, Michael Ahn, Mingtong Zhang, Mingyu Ding, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, N. Heess, Nikhil Joshi, Niko Suenderhauf, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, P. Sanketi, Paul Wohlhart, Peng Xu, P. Sermanet, Priya Sundaresan, Q. Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan C. Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham D. Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, S. Adebola, Simon Guist, Soroush Nasiriany, S. Schaal, Stefan Welker, Stephen Tian, Sudeep Dasari, Suneel Belkhale, Takayuki Osa, Tatsuya Harada, T. Matsushima, Ted Xiao, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Zhao, Travis Armstrong, Trevor Darrell, Vidhi Jain, Vincent Vanhoucke, W. Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xuanlin Li, Yao Lu, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yueh-hua Wu, Yujin Tang, Yuke Zhu, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zhuo Xu, Zichen Jeff Cui Abstract: —Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer'),\n",
       " Document(page_content='Author: Yonatan Bisk Title: SLAP: Spatial-Language Attention Policies Publication year: 2023 Coauthors: Priyam Parashar, Vidhi Jain, Xiaohan Zhang, Jay Vakil, Sam Powers, Yonatan Bisk, Chris Paxton Abstract: Despite great strides in language-guided manipulation, existing work has been constrained to table-top settings. Table-tops allow for perfect and consistent camera angles, properties are that do not hold in mobile manipulation. Task plans that involve moving around the environment must be robust to egocentric views and changes in the plane and angle of grasp. A further challenge is ensuring this is all true while still being able to learn skills efficiently from limited data. We propose Spatial-Language Attention Policies (SLAP) as a solution. SLAP uses three-dimensional tokens as the input representation to train a single multi-task, language-conditioned action prediction policy. Our method shows an 80% success rate in the real world across eight tasks with a single model, and a 47.5% success rate when unseen clutter and unseen object configurations are introduced, even with only a handful of examples per task. This represents an improvement of 30% over prior work (20% given unseen distractors and configurations). We see a 4x improvement over baseline in mobile manipulation setting. In addition, we show how SLAPs robustness allows us to execute Task Plans from open-vocabulary instructions using a large language model for multi-step mobile manipulation. For videos, see the website: https://robotslap.github.io'),\n",
       " Document(page_content='Author: Yonatan Bisk Title: The Framework Tax: Disparities Between Inference Efficiency in Research and Deployment Publication year: 2023 Coauthors: Jared Fernandez, Jacob Kahn, Clara Na, Yonatan Bisk, Emma Strubell Abstract: Increased focus on the computational efficiency of NLP systems has motivated the design of efficient model architectures and improvements to underlying hardware accelerators. However, the resulting increases in computational throughput and reductions in floating point operations have not directly translated to improvements in wall-clock inference latency. We demonstrate that these discrepancies can be largely attributed to bottlenecks introduced by deep learning frameworks. We denote this phenomenon as the \\\\textit{framework tax}, and observe that the disparity is growing as hardware speed increases over time. In this work, we examine this phenomenon through a series of case studies analyzing the effects of model design decisions, framework paradigms, and hardware platforms on total model latency. Code is available at https://github.com/JaredFern/Framework-Tax.'),\n",
       " Document(page_content='Author: Yonatan Bisk Title: SPRING: Studying the Paper and Reasoning to Play Games Publication year: 2023 Coauthors: Yue Wu, So Yeon Min, Shrimai Prabhumoye, Yonatan Bisk, R. Salakhutdinov, A. Azaria, Tom M. Mitchell, Yuan-Fang Li Abstract: Open-world survival games pose significant challenges for AI algorithms due to their multi-tasking, deep exploration, and goal prioritization requirements. Despite reinforcement learning (RL) being popular for solving games, its high sample complexity limits its effectiveness in complex open-world games like Crafter or Minecraft. We propose a novel approach, SPRING, to read the game\\'s original academic paper and use the knowledge learned to reason and play the game through a large language model (LLM). Prompted with the LaTeX source as game context and a description of the agent\\'s current observation, our SPRING framework employs a directed acyclic graph (DAG) with game-related questions as nodes and dependencies as edges. We identify the optimal action to take in the environment by traversing the DAG and calculating LLM responses for each node in topological order, with the LLM\\'s answer to final node directly translating to environment actions. In our experiments, we study the quality of in-context\"reasoning\"induced by different forms of prompts under the setting of the Crafter open-world environment. Our experiments suggest that LLMs, when prompted with consistent chain-of-thought, have great potential in completing sophisticated high-level trajectories. Quantitatively, SPRING with GPT-4 outperforms all state-of-the-art RL baselines, trained for 1M steps, without any training. Finally, we show the potential of games as a test bed for LLMs.'),\n",
       " Document(page_content='Author: Yonatan Bisk Title: WebArena: A Realistic Web Environment for Building Autonomous Agents Publication year: 2023 Coauthors: Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig Abstract: With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.'),\n",
       " Document(page_content='Author: Yonatan Bisk Title: Computational Language Acquisition with Theory of Mind Publication year: 2023 Coauthors: Andy T. Liu, Hao Zhu, Emmy Liu, Yonatan Bisk, Graham Neubig Abstract: Unlike current state-of-the-art language models, young children actively acquire language through interactions with their surrounding environment and caretakers. One mechanism that has been argued to be critical to language learning is the ability to infer the mental states of other agents in social environments, coined Theory of Mind (ToM) by Premack&Woodruff (1978). Drawing inspiration from the modern operationalized versions of ToM implemented in Rabinowitz et al. (2018) and Zhu et al. (2021), we build language-learning agents equipped with ToM, and measure its effects on the learning process. We model ToM by giving the speaker agent an internal listener model that is trained alongside the speaker and used to rerank potential utterances. We experiment with varying task difficulty, hypothesizing that models will acquire more complex language to adapt to stronger environmental pressures. We find that training speakers with a highly weighted ToM listener component leads to performance gains in our image referential game setting. We also find some evidence that increasing task difficulty in the training process results in more fluent and precise utterances in evaluation. This suggests the potential utility of further incorporating ToM, as well as other insights from child language acquisition, into computational models of language acquisition.'),\n",
       " Document(page_content='Author: Yonatan Bisk Title: Open X-Embodiment: Robotic Learning Datasets and RT-X Models Publication year: 2023 Coauthors: A. Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, A. Irpan, Alexander Khazatsky, Anant Rai, Anika Singh, Anthony Brohan, A. Raffin, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Schölkopf, Brian Ichter, Cewu Lu, Charles Xu, Chelsea Finn, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Chuer Pan, Chuyuan Fu, Coline Devin, Danny Driess, Deepak Pathak, Dhruv Shah, Dieter Büchler, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Federico Ceola, Fei Xia, F. Stulp, Gaoyue Zhou, G. Sukhatme, G. Salhotra, Ge Yan, Giulio Schiavi, Hao Su, Haoshu Fang, Haochen Shi, H. B. Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jaehyung Kim, Jan Schneider, Jasmine Hsu, J. Bohg, Jeff Bingham, Jiajun Wu, Jialin Wu, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jitendra Malik, Jonathan Tompson, Jonathan Yang, Joseph J. Lim, João Silvério, Junhyek Han, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, K. Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Zhang, K. Majd, Krishan Rana, K. Srinivasan, Lawrence Yunliang Chen, Lerrel Pinto, Liam Tan, Lionel Ott, Lisa Lee, Masayoshi Tomizuka, Maximilian Du, Michael Ahn, Mingtong Zhang, Mingyu Ding, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, N. Heess, Nikhil Joshi, Niko Suenderhauf, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, P. Sanketi, Paul Wohlhart, Peng Xu, P. Sermanet, Priya Sundaresan, Q. Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan C. Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Sherry Moore, Shikhar Bahl, Shivin Dass, Shuran Song, Sichun Xu, Siddhant Haldar, S. Adebola, Simon Guist, Soroush Nasiriany, S. Schaal, Stefan Welker, Stephen Tian, Sudeep Dasari, Suneel Belkhale, Takayuki Osa, Tatsuya Harada, T. Matsushima, Ted Xiao, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Zhao, Travis Armstrong, T. Darrell, Vidhi Jain, Vincent Vanhoucke, W. Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xuanlin Li, Yao Lu, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yueh-hua Wu, Yujin Tang, Yuke Zhu, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zhuo Xu, Zichen Jeff Cui Abstract: —Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer Vision, this has led to a consolidation of pretrained models,'),\n",
       " Document(page_content=\"Author: Yonatan Bisk Title: SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents Publication year: 2023 Coauthors: Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, Maarten Sap Abstract: Humans are social beings; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and interact under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills. These findings demonstrate SOTOPIA's promise as a general platform for research on evaluating and improving social intelligence in artificial agents.\"),\n",
       " Document(page_content=\"Author: Jamie Callan Title: Conversational Search with Random Walks over Entity Graphs Publication year: 2023 Coauthors: Gustavo Gonçalves, João Magalhães, Jamie Callan Abstract: The entities that emerge during a conversation can be used to model topics, but not all entities are equally useful for this task. Modeling the conversation with entity graphs and predicting each entity's centrality in the conversation provides additional information that improves the retrieval of answer passages for the current question. Experiments show that using random walks to estimate entity centrality on conversation entity graphs improves top precision answer passage ranking over competitive transformer-based baselines.\"),\n",
       " Document(page_content='Author: Jamie Callan Title: KALE: Using a K-Sparse Projector for Lexical Expansion Publication year: 2023 Coauthors: Luís Borges, Bruno Martins, Jamie Callan Abstract: Recent research has proposed retrieval approaches based on sparse representations and inverted indexes, with terms produced by neural language models and leveraging the advantages from both neural retrieval and lexical matching. This paper proposes KALE, a new lightweight method of this family that uses a small model with a k-sparse projector to convert dense representations into a sparse set of entries from a latent vocabulary. The KALE vocabulary captures semantic concepts than perform well when used in isolation, and perform better when extending the original lexical vocabulary, this way improving first-stage retrieval accuracy. Experiments with the MSMARCOv1 passage retrieval dataset, the TREC Deep Learning dataset, and BEIR datasets, examined the effectiveness of KALE under varying conditions. Results show that the KALE terms can replace the original lexical vocabulary, with gains in accuracy and efficiency. Combining KALE with the original lexical vocabulary, or with other learned terms, can further improve retrieval accuracy with only a modest increase in computational cost.'),\n",
       " Document(page_content='Author: Jamie Callan Title: CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms Publication year: 2023 Coauthors: Zhen Fan, Luyu Gao, Jamie Callan Abstract: Lexical exact-match systems perform text retrieval efficiently with sparse matching signals and fast retrieval through inverted lists, but naturally suffer from the mismatch between lexical surface form and implicit term semantics. This paper proposes to directly bridge the surface form space and the term semantics space in lexical exact-match retrieval via contextualized surface forms (CSF). Each CSF pairs a lexical surface form with a context source, and is represented by a lexical form weight and a contextualized semantic vector representation. This framework is able to perform sparse lexicon-based retrieval by learning to represent each query and document as a \"bag-of-CSFs\", simultaneously addressing two key factors in sparse retrieval: vocabulary expansion of surface form and semantic representation of term meaning. At retrieval time, it efficiently matches CSFs through exact-match of learned surface forms, and effectively scores each CSF pair via contextual semantic representations, leading to joint improvement in both term match and term scoring. Multiple experiments show that this approach successfully resolves the main mismatch issues in lexical exact-match retrieval and outperforms state-of-the-art lexical exact-match systems, reaching comparable accuracy as lexical all-to-all soft match systems as an efficient exact-match-based system.'),\n",
       " Document(page_content='Author: Jamie Callan Title: COILcr: Efficient Semantic Matching in Contextualized Exact Match Retrieval Publication year: 2023 Coauthors: Zhen Fan, Luyu Gao, Rohan Jha, Jamie Callan '),\n",
       " Document(page_content='Author: Jamie Callan Title: Active Retrieval Augmented Generation Publication year: 2023 Coauthors: Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig Abstract: Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.'),\n",
       " Document(page_content='Author: Jamie Callan Title: Multi-Objective Improvement of Android Applications Publication year: 2023 Coauthors: Jamie Callan, J. Petke Abstract: Non-functional properties, such as runtime or memory use, are important to mobile app users and developers, as they affect user experience. Previous work on automated improvement of non-functional properties in mobile apps failed to address the inherent trade-offs between such properties. We propose a practical approach and the first open-source tool, GIDroid (2023), for multi-objective automated improvement of Android apps. In particular, we use Genetic improvement, a search-based technique that navigates the space of software variants to find improved software. We use a simulation-based testing framework to greatly improve the speed of search. GIDroid contains three state-of-the-art multi-objective algorithms, and two new mutation operators, which cache the results of method calls. Genetic improvement relies on testing to validate patches. Previous work showed that tests in open-source Android applications are scarce. We thus wrote tests for 21 versions of 7 Android apps, creating a new benchmark for performance improvements. We used GIDroid to improve versions of mobile apps where developers had previously found improvements to runtime, memory, and bandwidth use. Our technique automatically re-discovers 64% of existing improvements. We then applied our approach to current versions of software in which there were no known improvements. We were able to improve execution time by up to 35%, and memory use by up to 33% in these apps.'),\n",
       " Document(page_content='Author: Jamie Callan Title: Building Retrieval Systems for the ClueWeb22-B Corpus Publication year: 2024 Coauthors: Harshit Mehrotra, Jamie Callan, Zhen Fan Abstract: The ClueWeb22 dataset containing nearly 10 billion documents was released in 2022 to support academic and industry research. The goal of this project was to build retrieval baselines for the English section of the\"super head\"part (category B) of this dataset. These baselines can then be used by the research community to compare their systems and also to generate data to train/evaluate new retrieval and ranking algorithms. The report covers sparse and dense first stage retrievals as well as neural rerankers that were implemented for this dataset. These systems are available as a service on a Carnegie Mellon University cluster.'),\n",
       " Document(page_content=\"Author: Mona T. Diab Title: Can Large Language Models Infer Causation from Correlation? Publication year: 2023 Coauthors: Zhijing Jin, Jiarui Liu, Zhiheng Lyu, Spencer Poff, Mrinmaya Sachan, Rada Mihalcea, Mona T. Diab, B. Scholkopf Abstract: Causal inference is one of the hallmarks of human intelligence. While the field of CausalNLP has attracted much interest in the recent years, existing causal inference datasets in NLP primarily rely on discovering causality from empirical knowledge (e.g., commonsense knowledge). In this work, we propose the first benchmark dataset to test the pure causal inference skills of large language models (LLMs). Specifically, we formulate a novel task Corr2Cause, which takes a set of correlational statements and determines the causal relationship between the variables. We curate a large-scale dataset of more than 200K samples, on which we evaluate seventeen existing LLMs. Through our experiments, we identify a key shortcoming of LLMs in terms of their causal inference skills, and show that these models achieve almost close to random performance on the task. This shortcoming is somewhat mitigated when we try to re-purpose LLMs for this skill via finetuning, but we find that these models still fail to generalize -- they can only perform causal inference in in-distribution settings when variable names and textual expressions used in the queries are similar to those in the training set, but fail in out-of-distribution settings generated by perturbing these queries. Corr2Cause is a challenging task for LLMs, and would be helpful in guiding future research on improving LLMs' pure reasoning skills and generalizability. Our data is at https://huggingface.co/datasets/causalnlp/corr2cause. Our code is at https://github.com/causalNLP/corr2cause.\"),\n",
       " Document(page_content='Author: Mona T. Diab Title: ALERT: Adapt Language Models to Reasoning Tasks Publication year: 2023 Coauthors: Ping Yu, Tianlu Wang, O. Yu. Golovneva, Badr AlKhamissi, Siddharth Verma, Zhijing Jin, Gargi Ghosh, Mona T. Diab, Asli Celikyilmaz '),\n",
       " Document(page_content='Author: Mona T. Diab Title: Care4Lang at MEDIQA-Chat 2023: Fine-tuning Language Models for Classifying and Summarizing Clinical Dialogues Publication year: 2023 Coauthors: Amal AlQahtani, R. Salama, Mona T. Diab, Abdou Youssef Abstract: Summarizing medical conversations is one of the tasks proposed by MEDIQA-Chat to promote research on automatic clinical note generation from doctor-patient conversations. In this paper, we present our submission to this task using fine-tuned language models, including T5, BART and BioGPT models. The fine-tuned models are evaluated using ensemble metrics including ROUGE, BERTScore andBLEURT. Among the fine-tuned models, Flan-T5 achieved the highest aggregated score for dialogue summarization.'),\n",
       " Document(page_content='Author: Mona T. Diab Title: The Troubling Emergence of Hallucination in Large Language Models - An Extensive Definition, Quantification, and Prescriptive Remediations Publication year: 2023 Coauthors: Vipula Rawte, Swagata Chakraborty, Agnibh Pathak, Anubhav Sarkar, S.M. Towhidul Islam Tonmoy, Islam Tonmoy, Aman Chadha, Amit P. Sheth, Amitava Das, Paris, A. Sridhar, Erik Visser, Improved, Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, Yunfeng Liu, Roformer, Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori Hashimoto, Stanford, Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtow-icz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, Susan Zhang, Stephen Roller, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher De-wan, Mona T. Diab, Xi Xian Li, Todor Victoria Lin, Myle Ott, Kurt Shuster, Punit Daniel Simig, Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer. 2022, Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom Brown, Alec Radford, Dario Amodei, Paul F. Chris-tiano Abstract: The recent advancements in Large Language Models (LLMs) have garnered widespread acclaim for their remarkable emerging capabilities. However, the issue of hallucination has parallelly emerged as a by-product, posing significant concerns. While some recent endeavors have been made to identify and mitigate different types of hallucination, there has been a limited emphasis on the nuanced categorization of hallucination and associated mitigation methods. To address this gap, we offer a fine-grained discourse on profiling hallucination based on its degree, orientation, and category, along with offering strategies for alleviation. As such, we define two overarching orientations of hallucination: (i) factual mirage (FM) and (ii) silver lining (SL). To provide a more comprehensive understanding, both orientations are further sub-categorized into intrinsic and extrinsic, with three degrees of severity - (i) mild, (ii) moderate, and (iii) alarming. We also meticulously categorize hallucination into six types: (i) acronym ambiguity, (ii) numeric nuisance, (iii) generated golem, (iv) virtual voice, (v) geographic erratum, and (vi) time wrap. Furthermore, we curate HallucInation eLiciTation (HILT), a publicly available dataset comprising of 75,000 samples generated using 15 contemporary LLMs along with human annotations for the aforementioned categories. Finally, to establish a method for quantifying and to offer a comparative spectrum that allows us to evaluate and rank LLMs based on their vulnerability to producing hallucinations, we propose Hallucination Vulnerability Index (HVI). We firmly believe that HVI holds significant value as a tool for the wider NLP community, with the potential to serve as a rubric in AI-related policy-making. In conclusion, we propose two solution strategies for mitigating hallucinations.'),\n",
       " Document(page_content='Author: Mona T. Diab Title: OPT-R: Exploring the Role of Explanations in Finetuning and Prompting for Reasoning Skills of Large Language Models Publication year: 2023 Coauthors: Badr AlKhamissi, Siddharth Verma, Ping Yu, Zhijing Jin, Asli Celikyilmaz, Mona T. Diab Abstract: We conduct a thorough investigation into the reasoning capabilities of Large Language Models (LLMs), focusing specifically on the Open Pretrained Transformers (OPT) models as a representative of such models. Our study entails finetuning three different sizes of OPT on a carefully curated reasoning corpus, resulting in two sets of finetuned models: OPT-R, finetuned without explanations, and OPT-RE, finetuned with explanations. We then evaluate all models on 57 out-of-domain tasks drawn from the Super-NaturalInstructions benchmark, covering 26 distinct reasoning skills, utilizing three prompting techniques. Through a comprehensive grid of 27 configurations and 6,156 test evaluations, we investigate the dimensions of finetuning, prompting, and scale to understand the role of explanations on different reasoning skills. Our findings reveal that having explanations in the fewshot exemplar has no significant impact on the model’s performance when the model is finetuned, while positively affecting the non-finetuned counterpart. Moreover, we observe a slight yet consistent increase in classification accuracy as we incorporate explanations during prompting and finetuning, respectively. Finally, we offer insights on which reasoning skills benefit the most from incorporating explanations during finetuning and prompting, such as Numerical (+20.4%) and Analogical (+13.9%) reasoning, as well as skills that exhibit negligible or negative effects.'),\n",
       " Document(page_content='Author: Mona T. Diab Title: Methods for Measuring, Updating, and Visualizing Factual Beliefs in Language Models Publication year: 2023 Coauthors: Peter Hase, Mona T. Diab, Asli Celikyilmaz, Xian Li, Zornitsa Kozareva, Veselin Stoyanov, Mohit Bansal, Srini Iyer Abstract: Language models can memorize a considerable amount of factual information during pretraining that can be elicited through prompting or finetuning models on tasks like question answering. In this paper, we discuss approaches to measuring model factual beliefs, updating incorrect factual beliefs in models, and visualizing graphical relationships between factual beliefs. Our main contributions include: (1) new metrics for evaluating belief-updating methods focusing on the logical consistency of beliefs, (2) a training objective for Sequential, Local, and Generalizing updates (SLAG) that improves the performance of existing hypernetwork approaches, and (3) the introduction of the belief graph, a new form of visualization for language models that shows relationships between stored model beliefs. Our experiments suggest that models show only limited consistency between factual beliefs, but update methods can both fix incorrect model beliefs and greatly improve their consistency. Although off-the-shelf optimizers are surprisingly strong belief-updating baselines, our learned optimizers can outperform them in more difficult settings than have been considered in past work.'),\n",
       " Document(page_content='Author: Mona T. Diab Title: Author Correction: Arabic natural language processing for Qur’anic research: a systematic review Publication year: 2023 Coauthors: M. Bashir, Aqil M. Azmi, H. Nawaz, W. Zaghouani, Mona T. Diab, Ala I. Al-Fuqaha, Junaid Qadir '),\n",
       " Document(page_content='Author: Mona T. Diab Title: Evaluating Multilingual Speech Translation under Realistic Conditions with Resegmentation and Terminology Publication year: 2023 Coauthors: Elizabeth Salesky, Kareem Darwish, Mohamed Al-Badrashiny, Mona T. Diab, J. Niehues Abstract: We present the ACL 60/60 evaluation sets for multilingual translation of ACL 2022 technical presentations into 10 target languages. This dataset enables further research into multilingual speech translation under realistic recording conditions with unsegmented audio and domain-specific terminology, applying NLP tools to text and speech in the technical domain, and evaluating and improving model robustness to diverse speaker demographics.'),\n",
       " Document(page_content='Author: Fernando Diaz Title: Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision Publication year: 2023 Coauthors: Fernando Diaz Abstract: Across a variety of ranking tasks, researchers use reciprocal rank to measure the effectiveness for users interested in exactly one relevant item. Despite its widespread use, evidence suggests that reciprocal rank is brittle when discriminating between systems. This brittleness, in turn, is compounded in modern evaluation settings where current, high-precision systems may be difficult to distinguish. We address the lack of sensitivity of reciprocal rank by introducing and connecting it to the concept of best-case retrieval, an evaluation method focusing on assessing the quality of a ranking for the most satisfied possible user across possible recall requirements. This perspective allows us to generalize reciprocal rank and define a new preference-based evaluation we call lexicographic precision or lexiprecision. By mathematical construction, we ensure that lexiprecision preserves differences detected by reciprocal rank, while empirically improving sensitivity and robustness across a broad set of retrieval and recommendation tasks.'),\n",
       " Document(page_content='Author: Fernando Diaz Title: Fairness Through Domain Awareness: Mitigating Popularity Bias For Music Discovery Publication year: 2023 Coauthors: Rebecca Salganik, Fernando Diaz, G. Farnadi Abstract: As online music platforms grow, music recommender systems play a vital role in helping users navigate and discover content within their vast musical databases. At odds with this larger goal, is the presence of popularity bias, which causes algorithmic systems to favor mainstream content over, potentially more relevant, but niche items. In this work we explore the intrinsic relationship between music discovery and popularity bias. To mitigate this issue we propose a domain-aware, individual fairness-based approach which addresses popularity bias in graph neural network (GNNs) based recommender systems. Our approach uses individual fairness to reflect a ground truth listening experience, i.e., if two songs sound similar, this similarity should be reflected in their representations. In doing so, we facilitate meaningful music discovery that is robust to popularity bias and grounded in the music domain. We apply our BOOST methodology to two discovery based tasks, performing recommendations at both the playlist level and user level. Then, we ground our evaluation in the cold start setting, showing that our approach outperforms existing fairness benchmarks in both performance and recommendation of lesser-known content. Finally, our analysis explains why our proposed methodology is a novel and promising approach to mitigating popularity bias and improving the discovery of new and niche content in music recommender systems.'),\n",
       " Document(page_content=\"Author: Fernando Diaz Title: Overview of the TREC 2021 Fair Ranking Track Publication year: 2023 Coauthors: Asia J. Biega, Fernando Diaz, Michael D. Ekstrand, Sebastian Kohlmeier Abstract: The TREC Fair Ranking Track aims to provide a platform for participants to develop and evaluate novel retrieval algorithms that can provide a fair exposure to a mixture of demographics or attributes, such as ethnicity, that are represented by relevant documents in response to a search query. For example, particular demographics or attributes can be represented by the documents' topical content or authors. The 2021 Fair Ranking Track adopted a resource allocation task. The task focused on supporting Wikipedia editors who are looking to improve the encyclopedia's coverage of topics under the purview of a WikiProject. WikiProject coordinators and/or Wikipedia editors search for Wikipedia documents that are in need of editing to improve the quality of the article. The 2021 Fair Ranking track aimed to ensure that documents that are about, or somehow represent, certain protected characteristics receive a fair exposure to the Wikipedia editors, so that the documents have an fair opportunity of being improved and, therefore, be well-represented in Wikipedia. The under-representation of particular protected characteristics in Wikipedia can result in systematic biases that can have a negative human, social, and economic impact, particularly for disadvantaged or protected societal groups.\"),\n",
       " Document(page_content=\"Author: Fernando Diaz Title: Recall, Robustness, and Lexicographic Evaluation Publication year: 2023 Coauthors: Fernando Diaz, Bhaskar Mitra Abstract: Researchers use recall to evaluate rankings across a variety of retrieval, recommendation, and machine learning tasks. While there is a colloquial interpretation of recall in set-based evaluation, the research community is far from a principled understanding of recall metrics for rankings. The lack of principled understanding of or motivation for recall has resulted in criticism amongst the retrieval community that recall is useful as a measure at all. In this light, we reflect on the measurement of recall in rankings from a formal perspective. Our analysis is composed of three tenets: recall, robustness, and lexicographic evaluation. First, we formally define `recall-orientation' as sensitivity to movement of the bottom-ranked relevant item. Second, we analyze our concept of recall orientation from the perspective of robustness with respect to possible searchers and content providers. Finally, we extend this conceptual and theoretical treatment of recall by developing a practical preference-based evaluation method based on lexicographic comparison. Through extensive empirical analysis across 17 TREC tracks, we establish that our new evaluation method, lexirecall, is correlated with existing recall metrics and exhibits substantially higher discriminative power and stability in the presence of missing labels. Our conceptual, theoretical, and empirical analysis substantially deepens our understanding of recall and motivates its adoption through connections to robustness and fairness.\"),\n",
       " Document(page_content='Author: S. Fahlman Title: Score: A Rule Engine for the Scone Knowledge Base System Publication year: 2023 Coauthors: Jeffrey Chen, S. Fahlman Abstract: We present Score, a rule engine designed and implemented for the Scone knowledge base system. Scone is a knowledge base system designed for storing and manipulating rich representations of general knowledge in symbolic form. It represents knowledge in the form of nodes and links in a network structure, and it can perform basic inference about the relationships between different elements efficiently. On its own, Scone acts as a sort of\"smart memory\"that can interface with other software systems. One area of improvement for Scone is how useful it can be in supplying knowledge to an intelligent agent that can use the knowledge to perform actions and update the knowledge base with its observations. We augment the Scone system with a production rule engine that automatically performs simple inference based on existing and newly-added structures in Scone\\'s knowledge base, potentially improving the capabilities of any planning systems built on top of Scone. Production rule systems consist of\"if-then\"production rules that try to match their predicates to existing knowledge and fire their actions when their predicates are satisfied. We propose two kinds of production rules, if-added and if-needed rules, that differ in how they are checked and fired to cover multiple use cases. We then implement methods to efficiently check and fire these rules in a large knowledge base. The new rule engine is not meant to be a complex stand-alone planner, so we discuss how it fits into the context of Scone and future work on planning systems.'),\n",
       " Document(page_content='Author: Daniel Fried Title: Pragmatic Inference with a CLIP Listener for Contrastive Captioning Publication year: 2023 Coauthors: Jiefu Ou, Benno Krojer, Daniel Fried Abstract: We propose a simple yet effective and robust method for contrastive captioning: generating discriminative captions that distinguish target images from very similar alternative distractor images. Our approach is built on a pragmatic inference procedure that formulates captioning as a reference game between a speaker, which produces possible captions describing the target, and a listener, which selects the target given the caption. Unlike previous methods that derive both speaker and listener distributions from a single captioning model, we leverage an off-the-shelf CLIP model to parameterize the listener. Compared with captioner-only pragmatic models, our method benefits from rich vision language alignment representations from CLIP when reasoning over distractors. Like previous methods for discriminative captioning, our method uses a hyperparameter to control the tradeoff between the informativity (how likely captions are to allow a human listener to discriminate the target image) and the fluency of the captions. However, we find that our method is substantially more robust to the value of this hyperparameter than past methods, which allows us to automatically optimize the captions for informativity - outperforming past methods for discriminative captioning by 11% to 15% accuracy in human evaluations'),\n",
       " Document(page_content=\"Author: Daniel Fried Title: SantaCoder: don't reach for the stars! Publication year: 2023 Coauthors: Loubna Ben Allal, Raymond Li, Denis Kocetkov, Chenghao Mou, Christopher Akiki, Carlos Muñoz Ferrandis, Niklas Muennighoff, Mayank Mishra, A. Gu, Manan Dey, Logesh Kumar Umapathi, Carolyn Jane Anderson, Yangtian Zi, J. Poirier, Hailey Schoelkopf, S. Troshin, Dmitry Abulkhanov, M. Romero, M. Lappert, F. Toni, Bernardo Garc'ia del R'io, Qian Liu, Shamik Bose, Urvashi Bhattacharyya, Terry Yue Zhuo, I. Yu, Paulo Villegas, Marco Zocca, Sourab Mangrulkar, D. Lansky, Huu Nguyen, Danish Contractor, Luisa Villa, Jia Li, Dzmitry Bahdanau, Yacine Jernite, S. Hughes, Daniel Fried, Arjun Guha, Harm de Vries, Leandro von Werra Abstract: The BigCode project is an open-scientific collaboration working on the responsible development of large language models for code. This tech report describes the progress of the collaboration until December 2022, outlining the current state of the Personally Identifiable Information (PII) redaction pipeline, the experiments conducted to de-risk the model architecture, and the experiments investigating better preprocessing methods for the training data. We train 1.1B parameter models on the Java, JavaScript, and Python subsets of The Stack and evaluate them on the MultiPL-E text-to-code benchmark. We find that more aggressive filtering of near-duplicates can further boost performance and, surprisingly, that selecting files from repositories with 5+ GitHub stars deteriorates performance significantly. Our best model outperforms previous open-source multilingual code generation models (InCoder-6.7B and CodeGen-Multi-2.7B) in both left-to-right generation and infilling on the Java, JavaScript, and Python portions of MultiPL-E, despite being a substantially smaller model. All models are released under an OpenRAIL license at https://hf.co/bigcode.\"),\n",
       " Document(page_content='Author: Daniel Fried Title: Grounding Language Models to Images for Multimodal Generation Publication year: 2023 Coauthors: Jing Yu Koh, R. Salakhutdinov, Daniel Fried Abstract: We propose an efﬁcient method to ground pre-trained text-only language models to the visual domain, enabling them to process and generate arbitrarily interleaved image-and-text data. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and ﬁnetune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text interleaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contextual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an ef-fective, general solution for leveraging pretrained language models in visually grounded settings.'),\n",
       " Document(page_content='Author: Daniel Fried Title: StarCoder: may the source be with you! Publication year: 2023 Coauthors: Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, J. Lamy-Poirier, João Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, J. Stillerman, S. Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, N. Fahmy, Urvashi Bhattacharyya, W. Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, M. Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jana Ebert, Tri Dao, Mayank Mishra, A. Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Muñoz Ferrandis, Sean M. Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, Harm de Vries Abstract: The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process. We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python, can be prompted to achieve 40\\\\% pass@1 on HumanEval, and still retains its performance on other programming languages. We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license.'),\n",
       " Document(page_content='Author: Daniel Fried Title: Grounding Language Models to Images for Multimodal Inputs and Outputs Publication year: 2023 Coauthors: Jing Yu Koh, R. Salakhutdinov, Daniel Fried Abstract: We propose an efficient method to ground pretrained text-only language models to the visual domain, enabling them to process arbitrarily interleaved image-and-text data, and generate text interleaved with retrieved images. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and finetune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text interleaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contextual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an effective, general solution for leveraging pretrained language models in visually grounded settings.'),\n",
       " Document(page_content='Author: Daniel Fried Title: Generating Images with Multimodal Language Models Publication year: 2023 Coauthors: Jing Yu Koh, Daniel Fried, R. Salakhutdinov Abstract: We propose a method to fuse frozen text-only large language models (LLMs) with pre-trained image encoder and decoder models, by mapping between their embedding spaces. Our model demonstrates a wide suite of multimodal capabilities: image retrieval, novel image generation, and multimodal dialogue. Ours is the first approach capable of conditioning on arbitrarily interleaved image and text inputs to generate coherent image (and text) outputs. To achieve strong performance on image generation, we propose an efficient mapping network to ground the LLM to an off-the-shelf text-to-image generation model. This mapping network translates hidden representations of text into the embedding space of the visual models, enabling us to leverage the strong text representations of the LLM for visual outputs. Our approach outperforms baseline generation models on tasks with longer and more complex language. In addition to novel image generation, our model is also capable of image retrieval from a prespecified dataset, and decides whether to retrieve or generate at inference time. This is done with a learnt decision module which conditions on the hidden representations of the LLM. Our model exhibits a wider range of capabilities compared to prior multimodal language models. It can process image-and-text inputs, and produce retrieved images, generated images, and generated text -- outperforming non-LLM based generation models across several text-to-image tasks that measure context dependence.'),\n",
       " Document(page_content='Author: Daniel Fried Title: WebArena: A Realistic Web Environment for Building Autonomous Agents Publication year: 2023 Coauthors: Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig Abstract: With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.'),\n",
       " Document(page_content=\"Author: Daniel Fried Title: Amortizing Pragmatic Program Synthesis with Rankings Publication year: 2023 Coauthors: Yewen Pu, Saujas Vaduguru, Priyan Vaithilingam, Elena L. Glassman, Daniel Fried Abstract: In program synthesis, an intelligent system takes in a set of user-generated examples and returns a program that is logically consistent with these examples. The usage of Rational Speech Acts (RSA) framework has been successful in building \\\\emph{pragmatic} program synthesizers that return programs which -- in addition to being logically consistent -- account for the fact that a user chooses their examples informatively. However, the computational burden of running the RSA algorithm has restricted the application of pragmatic program synthesis to domains with a small number of possible programs. This work presents a novel method of amortizing the RSA algorithm by leveraging a \\\\emph{global pragmatic ranking} -- a single, total ordering of all the hypotheses. We prove that for a pragmatic synthesizer that uses a single demonstration, our global ranking method exactly replicates RSA's ranked responses. We further empirically show that global rankings effectively approximate the full pragmatic synthesizer in an online, multi-demonstration setting. Experiments on two program synthesis domains using our pragmatic ranking method resulted in orders of magnitudes of speed ups compared to the RSA synthesizer, while outperforming the standard, non-pragmatic synthesizer.\"),\n",
       " Document(page_content='Author: Daniel Fried Title: Symbolic Planning and Code Generation for Grounded Dialogue Publication year: 2023 Coauthors: Justin T Chiu, Wenting Zhao, Derek Chen, Saujas Vaduguru, Alexander M. Rush, Daniel Fried Abstract: Large language models (LLMs) excel at processing and generating both text and code. However, LLMs have had limited applicability in grounded task-oriented dialogue as they are difficult to steer toward task objectives and fail to handle novel grounding. We present a modular and interpretable grounded dialogue system that addresses these shortcomings by composing LLMs with a symbolic planner and grounded code execution. Our system consists of a reader and planner: the reader leverages an LLM to convert partner utterances into executable code, calling functions that perform grounding. The translated code’s output is stored to track dialogue state, while a symbolic planner determines the next appropriate response. We evaluate our system’s performance on the demanding OneCommon dialogue task, involving collaborative reference resolution on abstract images of scattered dots. Our system substantially outperforms the previous state-of-the-art, including improving task success in human evaluations from 56% to 69% in the most challenging setting.'),\n",
       " Document(page_content=\"Author: Daniel Fried Title: Generating Pragmatic Examples to Train Neural Program Synthesizers Publication year: 2023 Coauthors: Saujas Vaduguru, Daniel Fried, Yewen Pu Abstract: Programming-by-example is the task of synthesizing a program that is consistent with a set of user-provided input-output examples. As examples are often an under-specification of one's intent, a good synthesizer must choose the intended program from the many that are consistent with the given set of examples. Prior work frames program synthesis as a cooperative game between a listener (that synthesizes programs) and a speaker (a user choosing examples), and shows that models of computational pragmatic inference are effective in choosing the user intended programs. However, these models require counterfactual reasoning over a large set of programs and examples, which is infeasible in realistic program spaces. In this paper, we propose a novel way to amortize this search with neural networks. We sample pairs of programs and examples via self-play between listener and speaker models, and use pragmatic inference to choose informative training examples from this sample.We then use the informative dataset to train models to improve the synthesizer's ability to disambiguate user-provided examples without human supervision. We validate our method on the challenging task of synthesizing regular expressions from example strings, and find that our method (1) outperforms models trained without choosing pragmatic examples by 23% (a 51% relative increase) (2) matches the performance of supervised learning on a dataset of pragmatic examples provided by humans, despite using no human data in training.\"),\n",
       " Document(page_content='Author: Daniel Fried Title: Asking More Informative Questions for Grounded Retrieval Publication year: 2023 Coauthors: Sedrick Scott Keh, Justin T. Chiu, Daniel Fried Abstract: When a model is trying to gather information in an interactive setting, it benefits from asking informative questions. However, in the case of a grounded multi-turn image identification task, previous studies have been constrained to polar yes/no questions, limiting how much information the model can gain in a single turn. We present an approach that formulates more informative, open-ended questions. In doing so, we discover that off-the-shelf visual question answering (VQA) models often make presupposition errors, which standard information gain question selection methods fail to account for. To address this issue, we propose a method that can incorporate presupposition handling into both question selection and belief updates. Specifically, we use a two-stage process, where the model first filters out images which are irrelevant to a given question, then updates its beliefs about which image the user intends. Through self-play and human evaluations, we show that our method is successful in asking informative open-ended questions, increasing accuracy over the past state-of-the-art by 14%, while resulting in 48% more efficient games in human evaluations.'),\n",
       " Document(page_content='Author: Daniel Fried Title: VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks Publication year: 2024 Coauthors: Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, Daniel Fried Abstract: Autonomous agents capable of planning, reasoning, and executing actions on the web offer a promising avenue for automating computer tasks. However, the majority of existing benchmarks primarily focus on text-based agents, neglecting many natural tasks that require visual information to effectively solve. Given that most computer interfaces cater to human perception, visual information often augments textual data in ways that text-only models struggle to harness effectively. To bridge this gap, we introduce VisualWebArena, a benchmark designed to assess the performance of multimodal web agents on realistic \\\\textit{visually grounded tasks}. VisualWebArena comprises of a set of diverse and complex web-based tasks that evaluate various capabilities of autonomous multimodal agents. To perform on this benchmark, agents need to accurately process image-text inputs, interpret natural language instructions, and execute actions on websites to accomplish user-defined objectives. We conduct an extensive evaluation of state-of-the-art LLM-based autonomous agents, including several multimodal models. Through extensive quantitative and qualitative analysis, we identify several limitations of text-only LLM agents, and reveal gaps in the capabilities of state-of-the-art multimodal language agents. VisualWebArena provides a framework for evaluating multimodal autonomous language agents, and offers insights towards building stronger autonomous agents for the web. Our code, baseline models, and data is publicly available at https://jykoh.com/vwa.'),\n",
       " Document(page_content=\"Author: Daniel Fried Title: Comparative Knowledge Distillation Publication year: 2023 Coauthors: Alex Wilf, Alex Tianyi Xu, P. Liang, A. Obolenskiy, Daniel Fried, Louis-Philippe Morency Abstract: In the era of large scale pretrained models, Knowledge Distillation (KD) serves an important role in transferring the wisdom of computationally heavy teacher models to lightweight, efficient student models while preserving performance. Traditional KD paradigms, however, assume readily available access to teacher models for frequent inference -- a notion increasingly at odds with the realities of costly, often proprietary, large scale models. Addressing this gap, our paper considers how to minimize the dependency on teacher model inferences in KD in a setting we term Few Teacher Inference Knowledge Distillation (FTI KD). We observe that prevalent KD techniques and state of the art data augmentation strategies fall short in this constrained setting. Drawing inspiration from educational principles that emphasize learning through comparison, we propose Comparative Knowledge Distillation (CKD), which encourages student models to understand the nuanced differences in a teacher model's interpretations of samples. Critically, CKD provides additional learning signals to the student without making additional teacher calls. We also extend the principle of CKD to groups of samples, enabling even more efficient learning from limited teacher calls. Empirical evaluation across varied experimental settings indicates that CKD consistently outperforms state of the art data augmentation and KD techniques.\"),\n",
       " Document(page_content=\"Author: Daniel Fried Title: SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents Publication year: 2023 Coauthors: Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, Maarten Sap Abstract: Humans are social beings; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and interact under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills. These findings demonstrate SOTOPIA's promise as a general platform for research on evaluating and improving social intelligence in artificial agents.\"),\n",
       " Document(page_content='Author: Daniel Fried Title: TroVE: Inducing Verifiable and Efficient Toolboxes for Solving Programmatic Tasks Publication year: 2024 Coauthors: Zhiruo Wang, Daniel Fried, Graham Neubig Abstract: Language models (LMs) can solve tasks such as answering questions about tables or images by writing programs. However, using primitive functions often leads to verbose and error-prone programs, and higher-level functions require expert design. To enable better solutions without human labor, we ask code LMs to curate reusable high-level functions, and use them to write solutions. We present TROVE, a training-free method of inducing a verifiable and efficient toolbox of functions, by generating via using, growing, and periodically trimming the toolbox. On 11 datasets from math, table question answering, and image reasoning tasks, TROVE consistently yields simpler solutions with higher accuracy than baselines using CODELLAMA and previous methods using GPT, while using 79-98% smaller toolboxes. TROVE further enables 31% faster and 13% more accurate human verification than baselines. With the same pipeline, it creates diverse functions for varied tasks and datasets, providing insights into their individual characteristics.'),\n",
       " Document(page_content='Author: Daniel Fried Title: API-Assisted Code Generation for Question Answering on Varied Table Structures Publication year: 2023 Coauthors: Yihan Cao, Shuyi Chen, Ryan Liu, Zhiruo Wang, Daniel Fried Abstract: A persistent challenge to table question answering (TableQA) by generating executable programs has been adapting to varied table structures, typically requiring domain-specific logical forms. In response, this paper introduces a unified TableQA framework that: (1) provides a unified representation for structured tables as multi-index Pandas data frames, (2) uses Python as a powerful querying language, and (3) uses few-shot prompting to translate NL questions into Python programs, which are executable on Pandas data frames. Furthermore, to answer complex relational questions with extended program functionality and external knowledge, our framework allows customized APIs that Python programs can call. We experiment with four TableQA datasets that involve tables of different structures -- relational, multi-table, and hierarchical matrix shapes -- and achieve prominent improvements over past state-of-the-art systems. In ablation studies, we (1) show benefits from our multi-index representation and APIs over baselines that use only an LLM, and (2) demonstrate that our approach is modular and can incorporate additional APIs.'),\n",
       " Document(page_content='Author: Daniel Fried Title: AutoReply: Detecting Nonsense in Dialogue with Discriminative Replies Publication year: 2023 Coauthors: Weiyan Shi, Emily Dinan, Adi Renduchintala, Daniel Fried, Athul Paul Jacob, Zhou Yu, Mike Lewis '),\n",
       " Document(page_content='Author: Daniel Fried Title: Data Augmentation for Code Translation with Comparable Corpora and Multiple References Publication year: 2023 Coauthors: Yiqing Xie, Atharva Naik, Daniel Fried, Carolyn Rose Abstract: One major challenge of translating code between programming languages is that parallel training data is often limited. To overcome this challenge, we present two data augmentation techniques, one that builds comparable corpora (i.e., code pairs with similar functionality), and another that augments existing parallel data with multiple reference translations. Specifically, we build and analyze multiple types of comparable corpora, including programs generated from natural language documentation using a code generation model. Furthermore, to reduce overfitting to a single reference translation, we automatically generate additional translation references for available parallel data and filter the translations by unit tests, which increases variation in target translations. Experiments show that our data augmentation techniques significantly improve CodeT5 for translation between Java, Python, and C++ by an average of 7.5% Computational Accuracy (CA@1), which verifies the correctness of translations by execution. The code is available at https://github.com/Veronicium/CMTrans.'),\n",
       " Document(page_content='Author: A. Gershman Title: The DARPA Wikidata Overlay: Wikidata as an ontology for natural language processing Publication year: 2023 Coauthors: Elizabeth Spaulding, Kathryn Conger, A. Gershman, Rosario Uceda-Sosa, S. Brown, James Pustejovsky, Peter Anick, Martha Palmer Abstract: With 102,530,067 items currently in its crowd-sourced knowledge base, Wikidata provides NLP practitioners a unique and powerful resource for inference and reasoning over real-world entities. However, because Wikidata is very entity focused, events and actions are often labeled with eventive nouns (e.g., the process of diagnosing a person’s illness is labeled “diagnosis”), and the typical participants in an event are not described or linked to that event concept (e.g., the medical professional or patient). Motivated by a need for an adaptable, comprehensive, domain-flexible ontology for information extraction, including identifying the roles entities are playing in an event, we present a curated subset of Wikidata in which events have been enriched with PropBank roles. To enable richer narrative understanding between events from Wikidata concepts, we have also provided a comprehensive mapping from temporal Qnodes and Pnodes to the Allen Interval Temporal Logic relations.'),\n",
       " Document(page_content='Author: Alexander Hauptmann Title: Leveraging body pose estimation for gesture recognition in human-robot interaction using synthetic data Publication year: 2023 Coauthors: Xiaoyu Zhu, Celso de Melo, Alexander Hauptmann Abstract: Effectively recognizing human gestures from variant viewpoints plays a fundamental role in the successful collaboration between humans and robots. Deep learning approaches have achieved promising performance in gesture recognition. However, they are usually data-hungry and require large-scale labeled data, which are not usually accessible in a practical setting. Synthetic data, on the other hand, can be easily obtained from simulators with fine-grained annotations and variant modalities. Existing state-of-the-art approaches have shown promising results using synthetic data, but there is still a large performance gap between the models trained on synthetic data and real data. To learn domain-invariant feature representations, we propose a novel approach which jointly takes RGB videos and 3D meshes as inputs to perform robust action recognition. We empirically validate our model on the RoCoG-v2 dataset, which consists of a variety of real and synthetic videos of gestures from the ground and air perspectives. We show that our model trained on synthetic data can outperform state-of-the-art models under the same training setting and models trained on real data.'),\n",
       " Document(page_content='Author: Alexander Hauptmann Title: Towards Open-Domain Twitter User Profile Inference Publication year: 2023 Coauthors: Haoyang Wen, Zhenxin Xiao, E. Hovy, Alexander Hauptmann Abstract: ,'),\n",
       " Document(page_content='Author: A. Hauptmann Title: Zero-Shot and Few-Shot Stance Detection on Varied Topics via Conditional Generation Publication year: 2023 Coauthors: Haoyang Wen, A. Hauptmann Abstract: Zero-shot and few-shot stance detection identify the polarity of text with regard to a certain target when we have only limited or no training resources for the target. Previous work generally formulates the problem into a classification setting, ignoring the potential use of label text. In this paper, we instead utilize a conditional generation framework and formulate the problem as denoising from partially-filled templates, which can better utilize the semantics among input, label, and target texts. We further propose to jointly train an auxiliary task, target prediction, and to incorporate manually constructed incorrect samples with unlikelihood training to improve the representations for both target and label texts. We also verify the effectiveness of target-related Wikipedia knowledge with the generation framework. Experiments show that our proposed method significantly outperforms several strong baselines on VAST, and achieves new state-of-the-art performance.'),\n",
       " Document(page_content=\"Author: A. Hauptmann Title: SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs Publication year: 2023 Coauthors: Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey, Yanping Huang, David A. Ross, Irfan Essa, Yonatan Bisk, Ming Yang, K. Murphy, A. Hauptmann, Lu Jiang Abstract: In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos. SPAE converts between raw pixels and interpretable lexical tokens (or words) extracted from the LLM's vocabulary. The resulting tokens capture both the semantic meaning and the fine-grained details needed for visual reconstruction, effectively translating the visual content into a language comprehensible to the LLM, and empowering it to perform a wide array of multimodal tasks. Our approach is validated through in-context learning experiments with frozen PaLM 2 and GPT 3.5 on a diverse set of image understanding and generation tasks. Our method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%.\"),\n",
       " Document(page_content='Author: A. Hauptmann Title: STMT: A Spatial-Temporal Mesh Transformer for MoCap-Based Action Recognition Publication year: 2023 Coauthors: Xiaoyu Zhu, Po-Yao (Bernie) Huang, Junwei Liang, Celso M. de Melo, A. Hauptmann Abstract: We study the problem of human action recognition using motion capture (MoCap) sequences. Unlike existing techniques that take multiple manual steps to derive standard-ized skeleton representations as model input, we propose a novel Spatial-Temporal Mesh Transformer (STMT) to directly model the mesh sequences. The model uses a hierarchical transformer with intra-frame off-set attention and inter-frame self-attention. The attention mechanism allows the model to freely attend between any two vertex patches to learn nonlocal relationships in the spatial-temporal domain. Masked vertex modeling and future frame prediction are used as two self-supervised tasks to fully activate the bi-directional and auto-regressive attention in our hierarchical transformer. The proposed method achieves state-of-the-art performance compared to skeleton-based and point-cloud-based models on common MoCap benchmarks. Code is available at https://github.com/zgzxy001/STMT.'),\n",
       " Document(page_content='Author: A. Hauptmann Title: ChartReader: A Unified Framework for Chart Derendering and Comprehension without Heuristic Rules Publication year: 2023 Coauthors: Zhi-Qi Cheng, Qianwen Dai, Siyao Li, Jingdong Sun, T. Mitamura, A. Hauptmann Abstract: Charts are a powerful tool for visually conveying complex data, but their comprehension poses a challenge due to the diverse chart types and intricate components. Existing chart comprehension methods suffer from either heuristic rules or an over-reliance on OCR systems, resulting in suboptimal performance. To address these issues, we present ChartReader, a unified framework that seamlessly integrates chart derendering and comprehension tasks. Our approach includes a transformer-based chart component detection module and an extended pre-trained vision-language model for chart-to-X tasks. By learning the rules of charts automatically from annotated datasets, our approach eliminates the need for manual rule-making, reducing effort and enhancing accuracy. We also introduce a data variable replacement technique and extend the input and position embeddings of the pre-trained model for cross-task training. We evaluate ChartReader on Chart-to-Table, ChartQA, and Chart-to-Text tasks, demonstrating its superiority over existing methods. Our proposed framework can significantly reduce the manual effort involved in chart analysis, providing a step towards a universal chart understanding model. Moreover, our approach offers opportunities for plug-and-play integration with mainstream LLMs such as T5 and TaPas, extending their capability to chart comprehension tasks.1'),\n",
       " Document(page_content='Author: A. Hauptmann Title: DocumentNet: Bridging the Data Gap in Document Pre-training Publication year: 2023 Coauthors: Lijun Yu, Jin Miao, Xiaoyu Sun, Jiayi Chen, A. Hauptmann, H. Dai, Wei Wei Abstract: Document understanding tasks, in particular, Visually-rich Document Entity Retrieval (VDER), have gained significant attention in recent years thanks to their broad applications in enterprise AI. However, publicly available data have been scarce for these tasks due to strict privacy constraints and high annotation costs. To make things worse, the non-overlapping entity spaces from different datasets hinder the knowledge transfer between document types. In this paper, we propose a method to collect massive-scale and weakly labeled data from the web to benefit the training of VDER models. The collected dataset, named DocumentNet, does not depend on specific document types or entity sets, making it universally applicable to all VDER tasks. The current DocumentNet consists of 30M documents spanning nearly 400 document types organized in a four-level ontology. Experiments on a set of broadly adopted VDER tasks show significant improvements when DocumentNet is incorporated into the pre-training for both classic and few-shot learning settings. With the recent emergence of large language models (LLMs), DocumentNet provides a large data source to extend their multi-modal capabilities for VDER.'),\n",
       " Document(page_content=\"Author: Alexander Hauptmann Title: Hyperbolic vs Euclidean Embeddings in Few-Shot Learning: Two Sides of the Same Coin Publication year: 2023 Coauthors: Gabriel Moreira, Manuel Marques, J. Costeira, Alexander Hauptmann Abstract: Recent research in representation learning has shown that hierarchical data lends itself to low-dimensional and highly informative representations in hyperbolic space. However, even if hyperbolic embeddings have gathered attention in image recognition, their optimization is prone to numerical hurdles. Further, it remains unclear which applications stand to benefit the most from the implicit bias imposed by hyperbolicity, when compared to traditional Euclidean features. In this paper, we focus on prototypical hyperbolic neural networks. In particular, the tendency of hyperbolic embeddings to converge to the boundary of the Poincar\\\\'e ball in high dimensions and the effect this has on few-shot classification. We show that the best few-shot results are attained for hyperbolic embeddings at a common hyperbolic radius. In contrast to prior benchmark results, we demonstrate that better performance can be achieved by a fixed-radius encoder equipped with the Euclidean metric, regardless of the embedding dimension.\"),\n",
       " Document(page_content='Author: Alexander G. Hauptmann Title: Breaking The Limits of Text-conditioned 3D Motion Synthesis with Elaborative Descriptions Publication year: 2023 Coauthors: Yijun Qian, Jack Urbanek, Alexander G. Hauptmann, Jungdam Won Abstract: Given its wide applications, there is increasing focus on generating 3D human motions from textual descriptions. Differing from the majority of previous works, which regard actions as single entities and can only generate short sequences for simple motions, we propose EMS, an elaborative motion synthesis model conditioned on detailed natural language descriptions. It generates natural and smooth motion sequences for long and complicated actions by factorizing them into groups of atomic actions. Meanwhile, it understands atomic-action level attributes (e.g., motion direction, speed, and body parts) and enables users to generate sequences of unseen complex actions from unique sequences of known atomic actions with independent attribute settings and timings applied. We evaluate our method on the KIT Motion-Language and BABEL benchmarks, where it outperforms all previous state-of-the-art with noticeable margins.'),\n",
       " Document(page_content='Author: Daphne Ippolito Title: Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System Publication year: 2023 Coauthors: Daphne Ippolito, Nicholas Carlini, Katherine Lee, Milad Nasr, Yun William Yu Abstract: Neural language models are increasingly deployed into APIs and websites that allow a user to pass in a prompt and receive generated text. Many of these systems do not reveal generation parameters. In this paper, we present methods to reverse-engineer the decoding method used to generate text (i.e., top-_k_ or nucleus sampling). Our ability to discover which decoding strategy was used has implications for detecting generated text. Additionally, the process of discovering the decoding strategy can reveal biases caused by selecting decoding settings which severely truncate a model’s predicted distributions. We perform our attack on several families of open-source language models, as well as on production systems (e.g., ChatGPT).'),\n",
       " Document(page_content=\"Author: Daphne Ippolito Title: A Pretrainer's Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity Publication year: 2023 Coauthors: S. Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph, Denny Zhou, Jason Wei, Kevin Robinson, David M. Mimno, Daphne Ippolito Abstract: Pretraining is the preliminary and fundamental step in developing capable language models (LM). Despite this, pretraining data design is critically under-documented and often guided by empirically unsupported intuitions. To address this, we pretrain 28 1.5B parameter decoder-only models, training on data curated (1) at different times, (2) with varying toxicity and quality filters, and (3) with different domain compositions. First, we quantify the effect of pretraining data age. A temporal shift between evaluation data and pretraining data leads to performance degradation, which is not overcome by finetuning. Second, we explore the effect of quality and toxicity filters, showing a trade-off between performance on standard benchmarks and risk of toxic generations. Our findings indicate there does not exist a one-size-fits-all solution to filtering training data. We also find that the effects of different types of filtering are not predictable from text domain characteristics. Lastly, we empirically validate that the inclusion of heterogeneous data sources, like books and web, is broadly beneficial and warrants greater prioritization. These findings constitute the largest set of experiments to validate, quantify, and expose many undocumented intuitions about text pretraining, which we hope will help support more informed data-centric decisions in LM development.\"),\n",
       " Document(page_content='Author: Daphne Ippolito Title: Extracting Training Data from Diffusion Models Publication year: 2023 Coauthors: Nicholas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramèr, B. Balle, Daphne Ippolito, Eric Wallace Abstract: Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training.'),\n",
       " Document(page_content='Author: Daphne Ippolito Title: Report of the 1st Workshop on Generative AI and Law Publication year: 2023 Coauthors: A. F. Cooper, Katherine Lee, James Grimmelmann, Daphne Ippolito, Christopher Callison-Burch, Christopher A. Choquette-Choo, Niloofar Mireshghallah, Miles Brundage, David Mimno, M. Choksi, J. Balkin, Nicholas Carlini, Christopher De Sa, Jonathan Frankle, Deep Ganguli, Bryant Gipson, A. Guadamuz, Swee Leng Harris, Abigail Z. Jacobs, Elizabeth Joh, Gautam Kamath, M. Lemley, Cass Matthews, C. McLeavey, Corynne Mcsherry, Milad Nasr, Paul Ohm, Adam Roberts, Tom Rubin, Pamela Samuelson, Ludwig Schubert, Kristen Vaccaro, Luis Villa, Felix Wu, Elana Zeide Abstract: This report presents the takeaways of the inaugural Workshop on Generative AI and Law (GenLaw), held in July 2023. A cross-disciplinary group of practitioners and scholars from computer science and law convened to discuss the technical, doctrinal, and policy challenges presented by law for Generative AI, and by Generative AI for law, with an emphasis on U.S. law in particular. We begin the report with a high-level statement about why Generative AI is both immensely significant and immensely challenging for law. To meet these challenges, we conclude that there is an essential need for 1) a shared knowledge base that provides a common conceptual language for experts across disciplines; 2) clarification of the distinctive technical capabilities of generative-AI systems, as compared and contrasted to other computer and AI systems; 3) a logical taxonomy of the legal issues these systems raise; and, 4) a concrete research agenda to promote collaboration and knowledge-sharing on emerging issues at the intersection of Generative AI and law. In this report, we synthesize the key takeaways from the GenLaw workshop that begin to address these needs. All of the listed authors contributed to the workshop upon which this report is based, but they and their organizations do not necessarily endorse all of the specific claims in this report.'),\n",
       " Document(page_content='Author: Daphne Ippolito Title: Are aligned neural networks adversarially aligned? Publication year: 2023 Coauthors: Nicholas Carlini, Milad Nasr, Christopher A. Choquette-Choo, Matthew Jagielski, Irena Gao, Anas Awadalla, Pang Wei Koh, Daphne Ippolito, Katherine Lee, Florian Tramèr, Ludwig Schmidt Abstract: Large language models are now tuned to align with the goals of their creators, namely to be\"helpful and harmless.\"These models should respond helpfully to user questions, but refuse to answer requests that could cause harm. However, adversarial users can construct inputs which circumvent attempts at alignment. In this work, we study to what extent these models remain aligned, even when interacting with an adversarial user who constructs worst-case inputs (adversarial examples). These inputs are designed to cause the model to emit harmful content that would otherwise be prohibited. We show that existing NLP-based optimization attacks are insufficiently powerful to reliably attack aligned text models: even when current NLP-based attacks fail, we can find adversarial inputs with brute force. As a result, the failure of current attacks should not be seen as proof that aligned text models remain aligned under adversarial inputs. However the recent trend in large-scale ML models is multimodal models that allow users to provide images that influence the text that is generated. We show these models can be easily attacked, i.e., induced to perform arbitrary un-aligned behavior through adversarial perturbation of the input image. We conjecture that improved NLP attacks may demonstrate this same level of adversarial control over text-only models.'),\n",
       " Document(page_content=\"Author: Daphne Ippolito Title: Effective Prompt Extraction from Language Models Publication year: 2023 Coauthors: Yiming Zhang, Daphne Ippolito Abstract: The text generated by large language models is commonly controlled by prompting, where a prompt prepended to a user's query guides the model's output. The prompts used by companies to guide their models are often treated as secrets, to be hidden from the user making the query. They have even been treated as commodities to be bought and sold. However, anecdotal reports have shown adversarial users employing prompt extraction attacks to recover these prompts. In this paper, we present a framework for systematically measuring the effectiveness of these attacks. In experiments with 3 different sources of prompts and 11 underlying large language models, we find that simple text-based attacks can in fact reveal prompts with high probability. Our framework determines with high precision whether an extracted prompt is the actual secret prompt, rather than a model hallucination. Prompt extraction experiments on real systems such as Bing Chat and ChatGPT suggest that system prompts can be revealed by an adversary despite existing defenses in place.\"),\n",
       " Document(page_content='Author: Daphne Ippolito Title: Scalable Extraction of Training Data from (Production) Language Models Publication year: 2023 Coauthors: Milad Nasr, Nicholas Carlini, Jonathan Hayase, Matthew Jagielski, A. F. Cooper, Daphne Ippolito, Christopher A. Choquette-Choo, Eric Wallace, Florian Tramèr, Katherine Lee Abstract: This paper studies extractable memorization: training data that an adversary can efficiently extract by querying a machine learning model without prior knowledge of the training dataset. We show an adversary can extract gigabytes of training data from open-source language models like Pythia or GPT-Neo, semi-open models like LLaMA or Falcon, and closed models like ChatGPT. Existing techniques from the literature suffice to attack unaligned models; in order to attack the aligned ChatGPT, we develop a new divergence attack that causes the model to diverge from its chatbot-style generations and emit training data at a rate 150x higher than when behaving properly. Our methods show practical attacks can recover far more data than previously thought, and reveal that current alignment techniques do not eliminate memorization.'),\n",
       " Document(page_content='Author: Li Lei Title: Long-tailed Extreme Multi-label Text Classification by the Retrieval of Generated Pseudo Label Descriptions Publication year: 2023 Coauthors: Ruohong Zhang, Yau-Shian Wang, Yiming Yang, Donghan Yu, Tom Vu, Li Lei Abstract: Extreme Multi-label Text Classification (XMTC) has been a tough challenge in machine learning research and applications due to the sheer sizes of the label spaces and the severe data scarcity problem associated with the long tail of rare labels in highly skewed distributions. This paper addresses the challenge of tail label prediction by leveraging the power of dense neural retrieval model in mapping input documents (as queries) to relevant label descriptions. To further enhance the quality of label descriptions, we propose to generate pseudo label descriptions from a trained bag-of-words (BoW) classifier, which demonstrates better classification performance under severe scarce data conditions.The proposed approach achieves the state-of-the-art (SOTA) performance of overall label prediction on XMTC benchmark datasets and especially outperforms the SOTA models in the tail label prediction. We also provide a theoretical analysis for relating the BoW and neural models w.r.t. performance lower bound.'),\n",
       " Document(page_content='Author: T. Mitamura Title: Language-Agnostic Transformers and Assessing ChatGPT-Based Query Rewriting for Multilingual Document-Grounded QA Publication year: 2023 Coauthors: Srinivas Gowriraj, Soham Dinesh Tiwari, Mitali Potnis, Srijan Bansal, T. Mitamura, Eric Nyberg Abstract: The DialDoc 2023 shared task has expanded the document-grounded dialogue task to encompass multiple languages, despite having limited annotated data. This paper assesses the effectiveness of both language-agnostic and language-aware paradigms for multilingual pre-trained transformer models in a bi-encoder-based dense passage retriever (DPR), concluding that the language-agnostic approach is superior. Additionally, the study investigates the impact of query rewriting techniques using large language models, such as ChatGPT, on multilingual, document-grounded question-answering systems. The experiments conducted demonstrate that, for the examples examined, query rewriting does not enhance performance compared to the original queries. This failure is due to topic switching in final dialogue turns and irrelevant topics being considered for query rewriting.'),\n",
       " Document(page_content='Author: T. Mitamura Title: ChartReader: A Unified Framework for Chart Derendering and Comprehension without Heuristic Rules Publication year: 2023 Coauthors: Zhi-Qi Cheng, Qianwen Dai, Siyao Li, Jingdong Sun, T. Mitamura, A. Hauptmann Abstract: Charts are a powerful tool for visually conveying complex data, but their comprehension poses a challenge due to the diverse chart types and intricate components. Existing chart comprehension methods suffer from either heuristic rules or an over-reliance on OCR systems, resulting in suboptimal performance. To address these issues, we present ChartReader, a unified framework that seamlessly integrates chart derendering and comprehension tasks. Our approach includes a transformer-based chart component detection module and an extended pre-trained vision-language model for chart-to-X tasks. By learning the rules of charts automatically from annotated datasets, our approach eliminates the need for manual rule-making, reducing effort and enhancing accuracy. We also introduce a data variable replacement technique and extend the input and position embeddings of the pre-trained model for cross-task training. We evaluate ChartReader on Chart-to-Table, ChartQA, and Chart-to-Text tasks, demonstrating its superiority over existing methods. Our proposed framework can significantly reduce the manual effort involved in chart analysis, providing a step towards a universal chart understanding model. Moreover, our approach offers opportunities for plug-and-play integration with mainstream LLMs such as T5 and TaPas, extending their capability to chart comprehension tasks.1'),\n",
       " Document(page_content=\"Author: T. Mitamura Title: Hierarchical Event Grounding Publication year: 2023 Coauthors: Jiefu Ou, Adithya Pratapa, Rishubh Gupta, T. Mitamura Abstract: Event grounding aims at linking mention references in text corpora to events from a knowledge base (KB). Previous work on this task focused primarily on linking to a single KB event, thereby overlooking the hierarchical aspects of events. Events in documents are typically described at various levels of spatio-temporal granularity. These hierarchical relations are utilized in downstream tasks of narrative understanding and schema construction. In this work, we present an extension to the event grounding task that requires tackling hierarchical event structures from the KB. Our proposed task involves linking a mention reference to a set of event labels from a subevent hierarchy in the KB. We propose a retrieval methodology that leverages event hierarchy through an auxiliary hierarchical loss. On an automatically created multilingual dataset from Wikipedia and Wikidata, our experiments demonstrate the effectiveness of the hierarchical loss against retrieve and re-rank baselines. Furthermore, we demonstrate the systems' ability to aid hierarchical discovery among unseen events. Code is available at https://github.com/JefferyO/Hierarchical-Event-Grounding\"),\n",
       " Document(page_content='Author: Louis-Philippe Morency Title: Quantifying&Modeling Multimodal Interactions: An Information Decomposition Framework Publication year: 2023 Coauthors: P. Liang, Yun Cheng, Xiang Fan, Chun Kai Ling, Suzanne Nie, Richard J. Chen, Zihao Deng, Nicholas Allen, R. Auerbach, Faisal Mahmood, R. Salakhutdinov, Louis-Philippe Morency Abstract: The recent explosion of interest in multimodal applications has resulted in a wide selection of datasets and methods for representing and integrating information from different modalities. Despite these empirical advances, there remain fundamental research questions: How can we quantify the interactions that are necessary to solve a multimodal task? Subsequently, what are the most suitable multimodal models to capture these interactions? To answer these questions, we propose an information-theoretic approach to quantify the degree of redundancy, uniqueness, and synergy relating input modalities with an output task. We term these three measures as the PID statistics of a multimodal distribution (or PID for short), and introduce two new estimators for these PID statistics that scale to high-dimensional distributions. To validate PID estimation, we conduct extensive experiments on both synthetic datasets where the PID is known and on large-scale multimodal benchmarks where PID estimations are compared with human annotations. Finally, we demonstrate their usefulness in (1) quantifying interactions within multimodal datasets, (2) quantifying interactions captured by multimodal models, (3) principled approaches for model selection, and (4) three real-world case studies engaging with domain experts in pathology, mood prediction, and robotic perception where our framework helps to recommend strong multimodal models for each application.'),\n",
       " Document(page_content='Author: Louis-Philippe Morency Title: Quantifying & Modeling Feature Interactions: An Information Decomposition Framework Publication year: 2023 Coauthors: P. Liang, Yun Cheng, Xiang Fan, Chun Kai Ling, Suzanne Nie, Richard J. Chen, Zihao Deng, Faisal Mahmood, R. Salakhutdinov, Louis-Philippe Morency Abstract: The recent explosion of interest in multimodal applications has resulted in a wide selection of datasets and methods for representing and inte-grating information from different signals. Despite these empirical advances, there remain fundamental research questions: how can we quantify the nature of interactions that exist among input features? Subsequently, how can we capture these interactions using suitable data-driven methods? To answer this question, we propose an information-theoretic approach to quantify the degree of redundancy , uniqueness , and synergy across input features, which we term the PID statistics of a multimodal distribution. Using 2 newly proposed estimators that scale to high-dimensional distributions, we demonstrate their usefulness in quantifying the interactions within multimodal datasets, the nature of interactions captured by multimodal models, and principled approaches for model selection. We conduct extensive experiments on both synthetic datasets where the PID statistics are known and on large-scale multimodal benchmarks where PID estimation was previously impossible. Finally, to demonstrate the real-world applicability of our approach, we present three case studies in pathology, mood prediction, and robotic perception where our framework accurately recommends strong multimodal models for each application.'),\n",
       " Document(page_content='Author: Louis-Philippe Morency Title: Think Twice: Perspective-Taking Improves Large Language Models\\' Theory-of-Mind Capabilities Publication year: 2023 Coauthors: Alex Wilf, Sihyun Shawn Lee, P. Liang, Louis-Philippe Morency Abstract: Human interactions are deeply rooted in the interplay of thoughts, beliefs, and desires made possible by Theory of Mind (ToM): our cognitive ability to understand the mental states of ourselves and others. Although ToM may come naturally to us, emulating it presents a challenge to even the most advanced Large Language Models (LLMs). Recent improvements to LLMs\\' reasoning capabilities from simple yet effective prompting techniques such as Chain-of-Thought have seen limited applicability to ToM. In this paper, we turn to the prominent cognitive science theory\"Simulation Theory\"to bridge this gap. We introduce SimToM, a novel two-stage prompting framework inspired by Simulation Theory\\'s notion of perspective-taking. To implement this idea on current ToM benchmarks, SimToM first filters context based on what the character in question knows before answering a question about their mental state. Our approach, which requires no additional training and minimal prompt-tuning, shows substantial improvement over existing methods, and our analysis reveals the importance of perspective-taking to Theory-of-Mind capabilities. Our findings suggest perspective-taking as a promising direction for future research into improving LLMs\\' ToM capabilities.'),\n",
       " Document(page_content='Author: Louis-Philippe Morency Title: Reconstructing the neurodynamics of face perception during real world vision in humans using intracranial EEG recordings Publication year: 2023 Coauthors: Arish Alreja, Michael J. Ward, J. A. Colan, Qianli Ma, R. M. Richardson, Louis-Philippe Morency, A. Ghuman '),\n",
       " Document(page_content='Author: Louis-Philippe Morency Title: Tutorial on Multimodal Machine Learning: Principles, Challenges, and Open Questions Publication year: 2023 Coauthors: P. Liang, Louis-Philippe Morency Abstract: Multimodal machine learning is a vibrant multi-disciplinary research field that aims to design computer agents capable of understanding, reasoning, and learning through integrating multiple communicative modalities, including linguistic, acoustic, visual, tactile, and physiological messages. With the recent interest in video understanding, embodied autonomous agents, text-to-image generation, and multisensor fusion in healthcare and robotics, multimodality has brought unique computational and theoretical challenges to the machine learning community given the heterogeneity of data sources and the interconnections often found between modalities. However, the breadth of progress in multimodal research has made it difficult to identify the common themes and open questions in the field. By synthesizing a broad range of application domains and theoretical frameworks from both historical and recent perspectives, this tutorial is designed to provide an overview of the computational and theoretical foundations of multimodal machine learning. Building upon a new edition of our survey paper on multimodal ML and academic courses at CMU, this tutorial will cover three topics: (1) what is multimodal: the principles in learning from heterogeneous, connected, and interacting data, (2) why is it hard: a taxonomy of six core technical challenges faced in multimodal ML but understudied in unimodal ML, and (3) what is next: major directions for future research as identified by our taxonomy.'),\n",
       " Document(page_content='Author: Louis-Philippe Morency Title: Continual Learning for Personalized Co-Speech Gesture Generation Publication year: 2023 Coauthors: Chaitanya Ahuja, Pratik Joshi, Ryo Ishii, Louis-Philippe Morency Abstract: Co-speech gestures are a key channel of human communication, making them important for personalized chat agents to generate. In the past, gesture generation models assumed that data for each speaker is available all at once, and in large amounts. However in practical scenarios, speaker data comes sequentially and in small amounts as the agent personalizes with more speakers, akin to a continual learning paradigm. While more recent works have shown progress in adapting to low-resource data, they catastrophically forget the gesture styles of initial speakers they were trained on. Also, prior generative continual learning works are not multimodal, making this space less studied. In this paper, we explore this new paradigm and propose C-DiffGAN: an approach that continually learns new speaker gesture styles with only a few minutes of per-speaker data, while retaining previously learnt styles. Inspired by prior continual learning works, C-DiffGAN encourages knowledge retention by 1) generating reminiscences of previous low-resource speaker data, then 2) crossmodally aligning to them to mitigate catastrophic forgetting. We quantitatively demonstrate improved performance and reduced forgetting over strong baselines through standard continual learning measures, reinforced by a qualitative user study that shows that our method produces more natural, style-preserving gestures. Code and videos can be found at https://chahuja.com/cdiffgan'),\n",
       " Document(page_content='Author: Louis-Philippe Morency Title: MultiZoo & MultiBench: A Standardized Toolkit for Multimodal Deep Learning Publication year: 2023 Coauthors: P. Liang, Yiwei Lyu, Xiang Fan, Arav Agarwal, Yun Cheng, Louis-Philippe Morency, R. Salakhutdinov Abstract: Learning multimodal representations involves integrating information from multiple heterogeneous sources of data. In order to accelerate progress towards understudied modalities and tasks while ensuring real-world robustness, we release MultiZoo, a public toolkit consisting of standardized implementations of>20 core multimodal algorithms and MultiBench, a large-scale benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas. Together, these provide an automated end-to-end machine learning pipeline that simplifies and standardizes data loading, experimental setup, and model evaluation. To enable holistic evaluation, we offer a comprehensive methodology to assess (1) generalization, (2) time and space complexity, and (3) modality robustness. MultiBench paves the way towards a better understanding of the capabilities and limitations of multimodal models, while ensuring ease of use, accessibility, and reproducibility. Our toolkits are publicly available, will be regularly updated, and welcome inputs from the community.'),\n",
       " Document(page_content='Author: Louis-Philippe Morency Title: MMOE: Mixture of Multimodal Interaction Experts Publication year: 2023 Coauthors: Haofei Yu, P. Liang, R. Salakhutdinov, Louis-Philippe Morency Abstract: Multimodal machine learning, which studies the information and interactions across various input modalities, has made significant advancements in understanding the relationship between images and descriptive text. However, this is just a portion of the potential multimodal interactions seen in the real world and does not include new interactions between conflicting utterances and gestures in predicting sarcasm, for example. Notably, the current methods for capturing shared information often do not extend well to these more nuanced interactions, sometimes performing as low as 50% in binary classification. In this paper, we address this problem via a new approach called MMOE, which stands for a mixture of multimodal interaction experts. Our method automatically classifies data points from unlabeled multimodal datasets by their interaction type and employs specialized models for each specific interaction. Based on our experiments, this approach improves performance on these challenging interactions by more than 10%, leading to an overall increase of 2% for tasks like sarcasm prediction. As a result, interaction quantification provides new insights for dataset analysis and yields simple approaches that obtain state-of-the-art performance.'),\n",
       " Document(page_content=\"Author: Louis-Philippe Morency Title: SenteCon: Leveraging Lexicons to Learn Human-Interpretable Language Representations Publication year: 2023 Coauthors: Victoria Lin, Louis-Philippe Morency Abstract: Although deep language representations have become the dominant form of language featurization in recent years, in many settings it is important to understand a model's decision-making process. This necessitates not only an interpretable model but also interpretable features. In particular, language must be featurized in a way that is interpretable while still characterizing the original text well. We present SenteCon, a method for introducing human interpretability in deep language representations. Given a passage of text, SenteCon encodes the text as a layer of interpretable categories in which each dimension corresponds to the relevance of a specific category. Our empirical evaluations indicate that encoding language with SenteCon provides high-level interpretability at little to no cost to predictive performance on downstream tasks. Moreover, we find that SenteCon outperforms existing interpretable language representations with respect to both its downstream performance and its agreement with human characterizations of the text.\"),\n",
       " Document(page_content='Author: Louis-Philippe Morency Title: Randomized trial of brief interpersonal psychotherapy and cognitive behavioral therapy for depression delivered both in-person and by telehealth. Publication year: 2023 Coauthors: H. Swartz, Lauren M. Bylsma, Jay Fournier, J. Girard, C. Spotts, J. Cohn, Louis-Philippe Morency '),\n",
       " Document(page_content='Author: Louis-Philippe Morency Title: MultiIoT: Towards Large-scale Multisensory Learning for the Internet of Things Publication year: 2023 Coauthors: Shentong Mo, P. Liang, Russ Salakhutdinov, Louis-Philippe Morency Abstract: The Internet of Things (IoT), the network integrating billions of smart physical devices embedded with sensors, software, and communication technologies for the purpose of connecting and exchanging data with other devices and systems, is a critical and rapidly expanding component of our modern world. The IoT ecosystem provides a rich source of real-world modalities such as motion, thermal, geolocation, imaging, depth, sensors, video, and audio for prediction tasks involving the pose, gaze, activities, and gestures of humans as well as the touch, contact, pose, 3D of physical objects. Machine learning presents a rich opportunity to automatically process IoT data at scale, enabling efficient inference for impact in understanding human wellbeing, controlling physical devices, and interconnecting smart cities. To develop machine learning technologies for IoT, this paper proposes MultiIoT, the most expansive IoT benchmark to date, encompassing over 1.15 million samples from 12 modalities and 8 tasks. MultiIoT introduces unique challenges involving (1) learning from many sensory modalities, (2) fine-grained interactions across long temporal ranges, and (3) extreme heterogeneity due to unique structure and noise topologies in real-world sensors. We also release a set of strong modeling baselines, spanning modality and task-specific methods to multisensory and multitask models to encourage future research in multisensory representation learning for IoT.'),\n",
       " Document(page_content='Author: Louis-Philippe Morency Title: Neural Mixed Effects for Nonlinear Personalized Predictions Publication year: 2023 Coauthors: T. Wörtwein, Nicholas Allen, Lisa B. Sheeber, R. Auerbach, J. Cohn, Louis-Philippe Morency Abstract: Personalized prediction is a machine learning approach that predicts a person’s future observations based on their past labeled observations and is typically used for sequential tasks, e.g., to predict daily mood ratings. When making personalized predictions, a model can combine two types of trends: (a) trends shared across people, i.e., person-generic trends, such as being happier on weekends, and (b) unique trends for each person, i.e., person-specific trends, such as a stressful weekly meeting. Mixed effect models are popular statistical models to study both trends by combining person-generic and person-specific parameters. Though linear mixed effect models are gaining popularity in machine learning by integrating them with neural networks, these integrations are currently limited to linear person-specific parameters: ruling out nonlinear person-specific trends. In this paper, we propose Neural Mixed Effect (NME) models to optimize nonlinear person-specific parameters anywhere in a neural network in a scalable manner1. NME combines the efficiency of neural network optimization with nonlinear mixed effects modeling. Empirically, we observe that NME improves performance across six unimodal and multimodal datasets, including a smartphone dataset to predict daily mood and a mother-adolescent dataset to predict affective state sequences where half the mothers experience symptoms of depression. Furthermore, we evaluate NME for two model architectures, including for neural conditional random fields (CRF) to predict affective state sequences where the CRF learns nonlinear person-specific temporal transitions between affective states. Analysis of these person-specific transitions on the mother-adolescent dataset shows interpretable trends related to the mother’s depression symptoms.'),\n",
       " Document(page_content='Author: Louis-Philippe Morency Title: MultiZoo and MultiBench: A Standardized Toolkit for Multimodal Deep Learning Publication year: 2023 Coauthors: P. Liang, Yiwei Lyu, Xiang Fan, Arav Agarwal, Yun Cheng, Louis-Philippe Morency, R. Salakhutdinov '),\n",
       " Document(page_content=\"Author: Louis-Philippe Morency Title: SHAP-based Prediction of Mother's History of Depression to Understand the Influence on Child Behavior Publication year: 2023 Coauthors: Maneesh Bilalpur, Saurabh Hinduja, Laura Cariola, Lisa B. Sheeber, Nicholas B Allen, Louis-Philippe Morency, Jeffrey F. Cohn Abstract: Depression strongly impacts parents’ behavior. Does parents’ depression strongly affect the behavior of their children as well? To investigate this question, we compared dyadic interactions between 73 depressed and 75 non-depressed mothers and their adolescent child. Families were of low income and 84% were white. Child behavior was measured from audio-video recordings using manual annotation of verbal and nonverbal behavior by expert coders and by multimodal computational measures of facial expression, face and head dynamics, prosody, speech behavior, and linguistics. For both sets of measures, we used Support Vector Machines. For computational measures, we investigated the relative contribution of single versus multiple modalities using a novel approach to SHapley Additive exPlanations (SHAP). Computational measures outperformed manual ratings by human experts. Among individual computational measures, prosody was the most informative. SHAP reduction resulted in a four-fold decrease in the number of features and highest performance (77% accuracy; positive and negative agreements at 75% and 76%, respectively). These findings suggest that maternal depression strongly impacts the behavior of adolescent children; differences are most revealed in prosody; multimodal features together with SHAP reduction are most powerful.\"),\n",
       " Document(page_content=\"Author: Louis-Philippe Morency Title: Text-Transport: Toward Learning Causal Effects of Natural Language Publication year: 2023 Coauthors: Victoria Lin, Louis-Philippe Morency, Eli Ben-Michael Abstract: As language technologies gain prominence in real-world settings, it is important to understand how changes to language affect reader perceptions. This can be formalized as the causal effect of varying a linguistic attribute (e.g., sentiment) on a reader's response to the text. In this paper, we introduce Text-Transport, a method for estimation of causal effects from natural language under any text distribution. Current approaches for valid causal effect estimation require strong assumptions about the data, meaning the data from which one can estimate valid causal effects often is not representative of the actual target domain of interest. To address this issue, we leverage the notion of distribution shift to describe an estimator that transports causal effects between domains, bypassing the need for strong assumptions in the target domain. We derive statistical guarantees on the uncertainty of this estimator, and we report empirical results and analyses that support the validity of Text-Transport across data settings. Finally, we use Text-Transport to study a realistic setting--hate speech on social media--in which causal effects do shift significantly between text domains, demonstrating the necessity of transport when conducting causal inference on natural language.\"),\n",
       " Document(page_content='Author: Louis-Philippe Morency Title: Lecture Presentations Multimodal Dataset: Towards Understanding Multimodality in Educational Videos Publication year: 2023 Coauthors: Dong Won Lee, Chaitanya Ahuja, P. Liang, Sanika Natu, Louis-Philippe Morency Abstract: Many educational videos use slide presentations, a sequence of visual pages that contain text and figures accompanied by spoken language, which are constructed and presented carefully in order to optimally transfer knowledge to students. Previous studies in multimedia and psychology attribute the effectiveness of lecture presentations to their multimodal nature. As a step toward developing vision-language models to aid in student learning as intelligent teacher assistants, we introduce the Lecture Presentations Multimodal (LPM) Dataset as a large-scale benchmark testing the capabilities of vision-and-language models in multimodal understanding of educational videos. Our dataset contains aligned slides and spoken language, for 180+ hours of video and 9000+ slides, with 10 lecturers from various subjects (e.g., computer science, dentistry, biology). We introduce three research tasks, (1) figure-to-text retrieval, (2) text-to-figure retrieval, and (3) generation of slide explanations, which are grounded in multimedia learning and psychology principles to test a vision-language model’s understanding of multimodal content. We provide manual annotations to help implement these tasks and establish baselines on them. Comparing baselines and human student performances, we find that state-of-the-art vision-language models (zero-shot and fine-tuned) struggle in (1) weak crossmodal alignment between slides and spoken text, (2) learning novel visual mediums, (3) technical language, and (4) long-range sequences. We introduce PolyViLT, a novel multimodal transformer trained with a multi-instance learning loss that is more effective than current approaches for retrieval. We conclude by shedding light on the challenges and opportunities in multimodal understanding of educational presentation videos.'),\n",
       " Document(page_content='Author: Louis-Philippe Morency Title: Counterfactual Augmentation for Multimodal Learning Under Presentation Bias Publication year: 2023 Coauthors: Victoria Lin, Louis-Philippe Morency, D. Dimitriadis, Srinagesh Sharma Abstract: In real-world machine learning systems, labels are often derived from user behaviors that the system wishes to encourage. Over time, new models must be trained as new training examples and features become available. However, feedback loops between users and models can bias future user behavior, inducing a presentation bias in the labels that compromises the ability to train new models. In this paper, we propose counterfactual augmentation, a novel causal method for correcting presentation bias using generated counterfactual labels. Our empirical evaluations demonstrate that counterfactual augmentation yields better downstream performance compared to both uncorrected models and existing bias-correction methods. Model analyses further indicate that the generated counterfactuals align closely with true counterfactuals in an oracle setting.'),\n",
       " Document(page_content=\"Author: Louis-Philippe Morency Title: Multimodal Feature Selection for Detecting Mothers' Depression in Dyadic Interactions with their Adolescent Offspring Publication year: 2023 Coauthors: Maneesh Bilalpur, Saurabh Hinduja, Laura A. Cariola, Lisa B. Sheeber, Nick Alien, László A. Jeni, Louis-Philippe Morency, J. Cohn Abstract: Depression is the most common psychological disorder, a leading cause of disability world-wide, and a major contributor to inter-generational transmission of psychopathol-ogy within families. To contribute to our understanding of depression within families and to inform modality selection and feature reduction, it is critical to identify interpretable features in developmentally appropriate contexts. Mothers with and without depression were studied. Depression was defined as history of treatment for depression and elevations in current or recent symptoms. We explored two multimodal feature selection strategies in dyadic interaction tasks of mothers with their adolescent children for depression detection. Modalities included face and head dynamics, facial action units, speech-related behavior, and verbal features. The initial feature space was vast and inter-correlated (collinear). To reduce dimension-ality and gain insight into the relative contribution of each modality and feature, we explored feature selection strategies using Variance Inflation Factor (VIF) and Shapley values. On an average collinearity correction through VIF resulted in about 4 times feature reduction across unimodal and multimodal features. Collinearity correction was also found to be an optimal intermediate step prior to Shapley analysis. Shapley feature selection following VIF yielded best performance. The top 15 features obtained through Shapley achieved 78 % accuracy. The most informative features came from all four modalities sampled, which supports the importance of multimodal feature selection.\"),\n",
       " Document(page_content='Author: Louis-Philippe Morency Title: Difference-Masking: Choosing What to Mask in Continued Pretraining Publication year: 2023 Coauthors: Alex Wilf, Syeda Nahida Akter, Leena Mathur, P. Liang, Sheryl Mathew, Mengrou Shou, Eric Nyberg, Louis-Philippe Morency Abstract: The self-supervised objective of masking-and-predicting has led to promising performance gains on a variety of downstream tasks. However, while most approaches randomly mask tokens, there is strong intuition that deciding what to mask can substantially improve learning outcomes. We investigate this in continued pretraining setting in which pretrained models continue to pretrain on domain-specific data before performing some downstream task. We introduce Difference-Masking, a masking strategy that automatically chooses what to mask during continued pretraining by considering what makes a task domain different from the pretraining domain. Empirically, we find that Difference-Masking outperforms baselines on continued pretraining settings across four diverse language-only and multimodal video tasks.'),\n",
       " Document(page_content=\"Author: Louis-Philippe Morency Title: Expanding the Role of Affective Phenomena in Multimodal Interaction Research Publication year: 2023 Coauthors: Leena Mathur, Maja J Matari'c, Louis-Philippe Morency Abstract: In recent decades, the field of affective computing has made substantial progress in advancing the ability of AI systems to recognize and express affective phenomena, such as affect and emotions, during human-human and human-machine interactions. This paper describes our examination of research at the intersection of multimodal interaction and affective computing, with the objective of observing trends and identifying understudied areas. We examined over 16,000 papers from selected conferences in multimodal interaction, affective computing, and natural language processing: ACM International Conference on Multimodal Interaction, AAAC International Conference on Affective Computing and Intelligent Interaction, Annual Meeting of the Association for Computational Linguistics, and Conference on Empirical Methods in Natural Language Processing. We identified 910 affect-related papers and present our analysis of the role of affective phenomena in these papers. We find that this body of research has primarily focused on enabling machines to recognize or express affect and emotion; there has been limited research on how affect and emotion predictions might, in turn, be used by AI systems to enhance machine understanding of human social behaviors and cognitive states. Based on our analysis, we discuss directions to expand the role of affective phenomena in multimodal interaction research.\"),\n",
       " Document(page_content='Author: Louis-Philippe Morency Title: Multimodal Fusion Interactions: A Study of Human and Automatic Quantification Publication year: 2023 Coauthors: P. Liang, Yun Cheng, R. Salakhutdinov, Louis-Philippe Morency Abstract: In order to perform multimodal fusion of heterogeneous signals, we need to understand their interactions: how each modality individually provides information useful for a task and how this information changes in the presence of other modalities. In this paper, we perform a comparative study of how humans annotate two categorizations of multimodal interactions: (1) partial labels, where different annotators annotate the label given the first, second, and both modalities, and (2) counterfactual labels, where the same annotator annotates the label given the first modality before asking them to explicitly reason about how their answer changes when given the second. We further propose an alternative taxonomy based on (3) information decomposition, where annotators annotate the degrees of redundancy: the extent to which modalities individually and together give the same predictions, uniqueness: the extent to which one modality enables a prediction that the other does not, and synergy: the extent to which both modalities enable one to make a prediction that one would not otherwise make using individual modalities. Through experiments and annotations, we highlight several opportunities and limitations of each approach and propose a method to automatically convert annotations of partial and counterfactual labels to information decomposition, yielding an accurate and efficient method for quantifying multimodal interactions.'),\n",
       " Document(page_content='Author: Louis-Philippe Morency Title: Intensive Longitudinal Assessment of Adolescents to Predict Suicidal Thoughts and Behaviors. Publication year: 2023 Coauthors: R. Auerbach, Ranqing Lan, H. Galfalvy, Kira L. Alqueza, J. Cohn, Ryann Crowley, Katherine Durham, Karla Joyce, Lauren E. Kahn, Rahil A. Kamath, Louis-Philippe Morency, G. Porta, A. Srinivasan, Jamie Zelazny, D. Brent, Nicholas Allen '),\n",
       " Document(page_content='Author: Louis-Philippe Morency Title: Multimodal Learning Without Labeled Multimodal Data: Guarantees and Applications Publication year: 2023 Coauthors: P. Liang, Chun Kai Ling, Yun Cheng, A. Obolenskiy, Yudong Liu, Rohan Pandey, Alex Wilf, Louis-Philippe Morency, R. Salakhutdinov Abstract: In many machine learning systems that jointly learn from multiple modalities, a core research question is to understand the nature of multimodal interactions: the emergence of new task-relevant information during learning from both modalities that was not present in either alone. We study this challenge of interaction quantification in a semi-supervised setting with only labeled unimodal data and naturally co-occurring multimodal data (e.g., unlabeled images and captions, video and corresponding audio) but when labeling them is time-consuming. Using a precise information-theoretic definition of interactions, our key contributions are the derivations of lower and upper bounds to quantify the amount of multimodal interactions in this semi-supervised setting. We propose two lower bounds based on the amount of shared information between modalities and the disagreement between separately trained unimodal classifiers, and derive an upper bound through connections to approximate algorithms for min-entropy couplings. We validate these estimated bounds and show how they accurately track true interactions. Finally, two semi-supervised multimodal applications are explored based on these theoretical results: (1) analyzing the relationship between multimodal performance and estimated interactions, and (2) self-supervised learning that embraces disagreement between modalities beyond agreement as is typically done.'),\n",
       " Document(page_content='Author: Louis-Philippe Morency Title: Representation Learning for Interpersonal and Multimodal Behavior Dynamics: A Multiview Extension of Latent Change Score Models Publication year: 2023 Coauthors: A. Vail, J. Girard, Lauren M. Bylsma, Jay Fournier, Holly A. Swartz, Jeffrey F. Cohn, Louis-Philippe Morency Abstract: Characterizing the dynamics of behavior across multiple modalities and individuals is a vital component of computational behavior analysis. This is especially important in certain applications, such as psychotherapy, where individualized tracking of behavior patterns can provide valuable information about the patient’s mental state. Conventional methods that rely on aggregate statistics and correlational metrics may not always suffice, as they are often unable to capture causal relationships or evaluate the true probability of identified patterns. To address these challenges, we present a novel approach to learning multimodal and interpersonal representations of behavior dynamics during one-on-one interaction. Our approach is enabled by the introduction of a multiview extension of latent change score models, which facilitates the concurrent capture of both inter-modal and interpersonal behavior dynamics and the identification of directional relationships between them. A core advantage of our approach is its high level of interpretability while simultaneously achieving strong predictive performance. We evaluate our approach within the domain of therapist-client interactions, with the objective of gaining a deeper understanding about the collaborative relationship between the two, a crucial element of the therapeutic process. Our results demonstrate improved performance over conventional approaches that rely upon summary statistics or correlational metrics. Furthermore, since our multiview approach includes the explicit modeling of uncertainty, it naturally lends itself to integration with probabilistic classifiers, such as Gaussian process models. We demonstrate that this integration leads to even further improved performance, all the while maintaining highly interpretable qualities. Our analysis provides compelling motivation for further exploration of stochastic systems within computational models of behavior.'),\n",
       " Document(page_content=\"Author: Louis-Philippe Morency Title: Comparative Knowledge Distillation Publication year: 2023 Coauthors: Alex Wilf, Alex Tianyi Xu, P. Liang, A. Obolenskiy, Daniel Fried, Louis-Philippe Morency Abstract: In the era of large scale pretrained models, Knowledge Distillation (KD) serves an important role in transferring the wisdom of computationally heavy teacher models to lightweight, efficient student models while preserving performance. Traditional KD paradigms, however, assume readily available access to teacher models for frequent inference -- a notion increasingly at odds with the realities of costly, often proprietary, large scale models. Addressing this gap, our paper considers how to minimize the dependency on teacher model inferences in KD in a setting we term Few Teacher Inference Knowledge Distillation (FTI KD). We observe that prevalent KD techniques and state of the art data augmentation strategies fall short in this constrained setting. Drawing inspiration from educational principles that emphasize learning through comparison, we propose Comparative Knowledge Distillation (CKD), which encourages student models to understand the nuanced differences in a teacher model's interpretations of samples. Critically, CKD provides additional learning signals to the student without making additional teacher calls. We also extend the principle of CKD to groups of samples, enabling even more efficient learning from limited teacher calls. Empirical evaluation across varied experimental settings indicates that CKD consistently outperforms state of the art data augmentation and KD techniques.\"),\n",
       " Document(page_content='Author: Louis-Philippe Morency Title: MultiViz: Towards User-Centric Visualizations and Interpretations of Multimodal Models Publication year: 2023 Coauthors: P. Liang, Yiwei Lyu, Gunjan Chhablani, Nihal Jain, Zihao Deng, Xingbo Wang, Louis-Philippe Morency, R. Salakhutdinov Abstract: The nature of human and computer interactions are inherently multimodal, which has led to substantial interest in building interpretable, interactive, and reliable multimodal interfaces. However, modern multimodal models and interfaces are typically black-box neural networks, which makes it challenging to understand their internal mechanics. How can we visualize their internal workings in order to empower stakeholders to visualize model behavior, perform model debugging, and promote trust in these models? Our paper proposes MultiViz, a method for analyzing the behavior of multimodal models via 4 stages: (1) unimodal importance, (2) cross-modal interactions, (3) multimodal representations and (4) multimodal prediction. MultiViz includes modular visualization tools for each stage before combining outputs from all stages through an interactive and human-in-the-loop API. Through user studies with 21 participants on 8 trained models across 6 real-world tasks, we show that the complementary stages in MultiViz together enable users to (1) simulate model predictions, (2) assign interpretable concepts to features, (3) perform error analysis on model misclassifications, and (4) use insights from error analysis to debug models. MultiViz is publicly available at https://github.com/pliang279/MultiViz, will be regularly updated with new visualization tools and metrics, and welcomes input from the community1.'),\n",
       " Document(page_content='Author: Louis-Philippe Morency Title: Understanding Masked Autoencoders via Hierarchical Latent Variable Models Publication year: 2023 Coauthors: Lingjing Kong, Martin Q. Ma, Guan-Hong Chen, E. Xing, Yuejie Chi, Louis-Philippe Morency, Kun Zhang Abstract: Masked autoencoder (MAE), a simple and effective self-supervised learning framework based on the reconstruction of masked image regions, has recently achieved prominent success in a variety of vision tasks. Despite the emergence of intriguing empirical observations on MAE, a theoretically principled understanding is still lacking. In this work, we formally characterize and justify existing empirical in-sights and provide theoretical guarantees of MAE. We formulate the underlying data-generating process as a hierarchical latent variable model, and show that under reasonable assumptions, MAE provably identifies a set of latent variables in the hierarchical model, explaining why MAE can extract high-level information from pixels. Further, we show how key hyperparameters in MAE (the masking ratio and the patch size) determine which true latent variables to be recovered, therefore influencing the level of semantic information in the representation. Specifically, extremely large or small masking ratios inevitably lead to low-level representations. Our theory offers coherent explanations of existing empirical observations and provides insights for potential empirical improvements and fundamental limitations of the masked-reconstruction paradigm. We conduct extensive experiments to validate our theoretical insights.'),\n",
       " Document(page_content='Author: Louis-Philippe Morency Title: Factorized Contrastive Learning: Going Beyond Multi-view Redundancy Publication year: 2023 Coauthors: P. Liang, Zihao Deng, Martin Q. Ma, James Y. Zou, Louis-Philippe Morency, R. Salakhutdinov Abstract: In a wide range of multimodal tasks, contrastive learning has become a particularly appealing approach since it can successfully learn representations from abundant unlabeled data with only pairing information (e.g., image-caption or video-audio pairs). Underpinning these approaches is the assumption of multi-view redundancy - that shared information between modalities is necessary and sufficient for downstream tasks. However, in many real-world settings, task-relevant information is also contained in modality-unique regions: information that is only present in one modality but still relevant to the task. How can we learn self-supervised multimodal representations to capture both shared and unique information relevant to downstream tasks? This paper proposes FactorCL, a new multimodal representation learning method to go beyond multi-view redundancy. FactorCL is built from three new contributions: (1) factorizing task-relevant information into shared and unique representations, (2) capturing task-relevant information via maximizing MI lower bounds and removing task-irrelevant information via minimizing MI upper bounds, and (3) multimodal data augmentations to approximate task relevance without labels. On large-scale real-world datasets, FactorCL captures both shared and unique information and achieves state-of-the-art results on six benchmarks'),\n",
       " Document(page_content=\"Author: Louis-Philippe Morency Title: SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents Publication year: 2023 Coauthors: Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, Maarten Sap Abstract: Humans are social beings; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and interact under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills. These findings demonstrate SOTOPIA's promise as a general platform for research on evaluating and improving social intelligence in artificial agents.\"),\n",
       " Document(page_content='Author: Louis-Philippe Morency Title: Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions Publication year: 2023 Coauthors: Himanshu Thakur, Atishay Jain, Praneetha Vaddamanu, P. Liang, Louis-Philippe Morency Abstract: Societal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people. Since large-scale retraining of these models from scratch is both time and compute-expensive, a variety of approaches have been previously proposed that de-bias a pre-trained model. While the majority of current state-of-the-art debiasing methods focus on changes to the training regime, in this paper, we propose data intervention strategies as a powerful yet simple technique to reduce gender bias in pre-trained models. Specifically, we empirically show that by fine-tuning a pre-trained model on only 10 debiased (intervened) training examples, the tendency to favor any gender is significantly reduced. Since our proposed method only needs a few training examples, we argue that our few-shot de-biasing approach is highly feasible and practical. Through extensive experimentation, we show that our de-biasing technique performs better than competitive state-of-the-art baselines with minimal loss in language modeling ability.'),\n",
       " Document(page_content='Author: David R. Mortensen Title: ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages Publication year: 2023 Coauthors: Nathaniel R. Robinson, Perez Ogayo, David R. Mortensen, Graham Neubig Abstract: Large language models (LLMs) implicitly learn to perform a range of language tasks, including machine translation (MT). Previous studies explore aspects of LLMs’ MT capabilities. However, there exist a wide variety of languages for which recent LLM MT performance has never before been evaluated. Without published experimental evidence on the matter, it is difficult for speakers of the world’s diverse languages to know how and whether they can use LLMs for their languages. We present the first experimental evidence for an expansive set of 204 languages, along with MT cost analysis, using the FLORES-200 benchmark. Trends reveal that GPT models approach or exceed traditional MT model performance for some high-resource languages (HRLs) but consistently lag for low-resource languages (LRLs), under-performing traditional MT for 84.1% of languages we covered. Our analysis reveals that a language’s resource level is the most important feature in determining ChatGPT’s relative ability to translate it, and suggests that ChatGPT is especially disadvantaged for LRLs and African languages.'),\n",
       " Document(page_content=\"Author: David R. Mortensen Title: Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models Publication year: 2023 Coauthors: Orevaoghene Ahia, Sachin Kumar, Hila Gonen, Jungo Kasai, David R. Mortensen, Noah A. Smith, Yulia Tsvetkov Abstract: Language models have graduated from being research prototypes to commercialized products offered as web APIs, and recent works have highlighted the multilingual capabilities of these products. The API vendors charge their users based on usage, more specifically on the number of ``tokens'' processed or generated by the underlying language models. What constitutes a token, however, is training data and model dependent with a large variance in the number of tokens required to convey the same information in different languages. In this work, we analyze the effect of this non-uniformity on the fairness of an API's pricing policy across languages. We conduct a systematic analysis of the cost and utility of OpenAI's language model API on multilingual benchmarks in 22 typologically diverse languages. We show evidence that speakers of a large number of the supported languages are overcharged while obtaining poorer results. These speakers tend to also come from regions where the APIs are less affordable to begin with. Through these analyses, we aim to increase transparency around language model APIs' pricing policies and encourage the vendors to make them more equitable.\"),\n",
       " Document(page_content='Author: David R. Mortensen Title: African Substrates Rather Than European Lexifiers to Augment African-diaspora Creole Translation Publication year: 2023 Coauthors: David R. Mortensen Abstract: Machine translation (MT) model training is difficult for low-resource languages. This is especially true for African-diaspora Creole languages because of data scarcity. Cross-lingual data augmentation methods with knowledge transfer from related high-resource languages are a common technique to overcome this disadvantage. For instance, practitioners may transfer knowledge from a language in the same language family as the low-resource language of interest. Africandiaspora Creole languages are low-resource and simultaneously have relationships with multiple language groups. These languages, such as Haitian Creole and Jamaican Patois, are typically lexified by colonial European languages, but they are structurally similar to African languages. We explore the advantages of transferring knowledge from the European lexifier language versus the phylogenetic and typological relatives of the African substrate languages. We analysed Haitian and Jamaican MT: both controlling tightly for data properties across compared transfer languages and later allowing use of all data we collected. Our inquiry demonstrates a significant advantage in using African transfer languages in some settings.'),\n",
       " Document(page_content='Author: David R. Mortensen Title: SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing Publication year: 2023 Coauthors: Taiqi He, Lindia Tjuatja, Nathaniel R. Robinson, Shinji Watanabe, David R. Mortensen, Graham Neubig, L. Levin Abstract: In our submission to the SIGMORPHON 2023 Shared Task on interlinear glossing (IGT), we explore approaches to data augmentation and modeling across seven low-resource languages. For data augmentation, we explore two approaches: creating artificial data from the provided training data and utilizing existing IGT resources in other languages. On the modeling side, we test an enhanced version of the provided token classification baseline as well as a pretrained multilingual seq2seq model. Additionally, we apply post-correction using a dictionary for Gitksan, the language with the smallest amount of data. We find that our token classification models are the best performing, with the highest word-level accuracy for Arapaho and highest morpheme-level accuracy for Gitksan out of all submissions. We also show that data augmentation is an effective strategy, though applying artificial data pretraining has very different effects across both models tested.'),\n",
       " Document(page_content='Author: David R. Mortensen Title: Multilingual TTS Accent Impressions for Accented ASR Publication year: 2023 Coauthors: G. Karakasidis, Nathaniel R. Robinson, Yaroslav Getman, Atieno Ogayo, Ragheb Al-Ghezi, Ananya Ayasi, Shinji Watanabe, David R. Mortensen, M. Kurimo '),\n",
       " Document(page_content='Author: David R. Mortensen Title: Construction Grammar Provides Unique Insight into Neural Language Models Publication year: 2023 Coauthors: Leonie Weissweiler, Taiqi He, Naoki Otani, David R. Mortensen, L. Levin, Hinrich Schütze Abstract: Construction Grammar (CxG) has recently been used as the basis for probing studies that have investigated the performance of large pretrained language models (PLMs) with respect to the structure and meaning of constructions. In this position paper, we make suggestions for the continuation and augmentation of this line of research. We look at probing methodology that was not designed with CxG in mind, as well as probing methodology that was designed for specific constructions. We analyse selected previous work in detail, and provide our view of the most important challenges and research questions that this promising new field faces.'),\n",
       " Document(page_content='Author: David R. Mortensen Title: Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation Publication year: 2023 Coauthors: David R. Mortensen, Ela Gulsen, Taiqi He, Nathaniel R. Robinson, Jonathan D. Amith, Lindia Tjuatja, L. Levin Abstract: Interlinear glossing provides a vital type of morphosyntactic annotation, both for linguists and language revitalists, and numerous conventions exist for representing it formally and computationally. Some of these formats are human readable; others are machine readable. Some are easy to edit with general-purpose tools. Few represent non-concatentative processes like infixation, reduplication, mutation, truncation, and tonal overwriting in a consistent and formally rigorous way (on par with affixation). We propose an annotation conventionâ€”Generalized Glossing Guidelines (GGG) that combines all of these positive properties using an Item-and-Process (IP) framework. We describe the format, demonstrate its linguistic adequacy, and compare it with two other interlinear glossed text annotation schemes.'),\n",
       " Document(page_content='Author: David R. Mortensen Title: Transformed Protoform Reconstruction Publication year: 2023 Coauthors: Young Min Kim, Kalvin Chang, Chenxuan Cui, David R. Mortensen Abstract: Protoform reconstruction is the task of inferring what morphemes or words appeared like in the ancestral languages of a set of daughter languages. Meloni et al (2021) achieved the state-of-the-art on Latin protoform reconstruction with an RNN-based encoder-decoder with attention model. We update their model with the state-of-the-art seq2seq model: the Transformer. Our model outperforms their model on a suite of different metrics on two different datasets: their Romance data of 8,000 cognates spanning 5 languages and a Chinese dataset (Hou 2004) of 800+ cognates spanning 39 varieties. We also probe our model for potential phylogenetic signal contained in the model. Our code is publicly available at https://github.com/cmu-llab/acl-2023.'),\n",
       " Document(page_content='Author: David R. Mortensen Title: PWESuite: Phonetic Word Embeddings and Tasks They Facilitate Publication year: 2023 Coauthors: Vilém Zouhar, Kalvin Chang, Chenxuan Cui, Nathaniel Carlson, Nathaniel R. Robinson, Mrinmaya Sachan, David R. Mortensen Abstract: Mapping words into a fixed-dimensional vector space is the backbone of modern NLP. While most word embedding methods successfully encode semantic information, they overlook phonetic information that is crucial for many tasks. We develop three methods that use articulatory features to build phonetically informed word embeddings. To address the inconsistent evaluation of existing phonetic word embedding methods, we also contribute a task suite to fairly evaluate past, current, and future methods. We evaluate both (1) intrinsic aspects of phonetic word embeddings, such as word retrieval and correlation with sound similarity, and (2) extrinsic performance on tasks such as rhyme and cognate detection and sound analogies. We hope our task suite will promote reproducibility and inspire future phonetic embedding research.'),\n",
       " Document(page_content='Author: David R. Mortensen Title: Phonotactic Complexity across Dialects Publication year: 2024 Coauthors: Ryan Soh-Eun Shim, Kalvin Chang, David R. Mortensen Abstract: Received wisdom in linguistic typology holds that if the structure of a language becomes more complex in one dimension, it will simplify in another, building on the assumption that all languages are equally complex (Joseph and Newmeyer, 2012). We study this claim on a micro-level, using a tightly-controlled sample of Dutch dialects (across 366 collection sites) and Min dialects (across 60 sites), which enables a more fair comparison across varieties. Even at the dialect level, we find empirical evidence for a tradeoff between word length and a computational measure of phonotactic complexity from a LSTM-based phone-level language model-a result previously documented only at the language level. A generalized additive model (GAM) shows that dialects with low phonotactic complexity concentrate around the capital regions, which we hypothesize to correspond to prior hypotheses that language varieties of greater or more diverse populations show reduced phonotactic complexity. We also experiment with incorporating the auxiliary task of predicting syllable constituency, but do not find an increase in the negative correlation observed.'),\n",
       " Document(page_content='Author: David R. Mortensen Title: Automating Sound Change Prediction for Phylogenetic Inference: A Tukanoan Case Study Publication year: 2024 Coauthors: Kalvin Chang, Nathaniel Robinson, Anna Cai, Ting Chen, Annie Zhang, David R. Mortensen Abstract: We describe a set of new methods to partially automate linguistic phylogenetic inference given (1) cognate sets with their respective protoforms and sound laws, (2) a mapping from phones to their articulatory features and (3) a typological database of sound changes.We train a neural network on these sound change data to weight articulatory distances between phones and predict intermediate sound change steps between historical protoforms and their modern descendants, replacing a linguistic expert in part of a parsimony-based phylogenetic inference algorithm. In our best experiments on Tukanoan languages, this method produces trees with a Generalized Quartet Distance of 0.12 from a tree that used expert annotations, a significant improvement over other semi-automated baselines. We discuss potential benefits and drawbacks to our neural approach and parsimony-based tree prediction. We also experiment with a minimal generalization learner for automatic sound law induction, finding it less effective than sound laws from expert annotation. Our code is publicly available.'),\n",
       " Document(page_content='Author: David R Mortensen Title: Calibrated Seq2seq Models for Efficient and Generalizable Ultra-fine Entity Typing Publication year: 2023 Coauthors: Yanlin Feng, Adithya Pratapa, David R Mortensen Abstract: Ultra-fine entity typing plays a crucial role in information extraction by predicting fine-grained semantic types for entity mentions in text. However, this task poses significant challenges due to the massive number of entity types in the output space. The current state-of-the-art approaches, based on standard multi-label classifiers or cross-encoder models, suffer from poor generalization performance or inefficient inference. In this paper, we present CASENT, a seq2seq model designed for ultra-fine entity typing that predicts ultra-fine types with calibrated confidence scores. Our model takes an entity mention as input and employs constrained beam search to generate multiple types autoregressively. The raw sequence probabilities associated with the predicted types are then transformed into confidence scores using a novel calibration method. We conduct extensive experiments on the UFET dataset which contains over 10k types. Our method outperforms the previous state-of-the-art in terms of F1 score and calibration error, while achieving an inference speedup of over 50 times. Additionally, we demonstrate the generalization capabilities of our model by evaluating it in zero-shot and few-shot settings on five specialized domain entity typing datasets that are unseen during training. Remarkably, our model outperforms large language models with 10 times more parameters in the zero-shot setting, and when fine-tuned on 50 examples, it significantly outperforms ChatGPT on all datasets. Our code, models and demo are available at https://github.com/yanlinf/CASENT.'),\n",
       " Document(page_content=\"Author: David R. Mortensen Title: Counting the Bugs in ChatGPT's Wugs: A Multilingual Investigation into the Morphological Capabilities of a Large Language Model Publication year: 2023 Coauthors: Leonie Weissweiler, Valentin Hofmann, Anjali Kantharuban, Anna Cai, Ritam Dutt, Amey Hengle, Anubha Kabra, Atharva Kulkarni, Abhishek Vijayakumar, Haofei Yu, Hinrich Schütze, Kemal Oflazer, David R. Mortensen Abstract: Large language models (LLMs) have recently reached an impressive level of linguistic capability, prompting comparisons with human language skills. However, there have been relatively few systematic inquiries into the linguistic capabilities of the latest generation of LLMs, and those studies that do exist (i) ignore the remarkable ability of humans to generalize, (ii) focus only on English, and (iii) investigate syntax or semantics and overlook other capabilities that lie at the heart of human language, like morphology. Here, we close these gaps by conducting the first rigorous analysis of the morphological capabilities of ChatGPT in four typologically varied languages (specifically, English, German, Tamil, and Turkish). We apply a version of Berko's (1958) wug test to ChatGPT, using novel, uncontaminated datasets for the four examined languages. We find that ChatGPT massively underperforms purpose-built systems, particularly in English. Overall, our results -- through the lens of morphology -- cast a new light on the linguistic capabilities of ChatGPT, suggesting that claims of human-like language skills are premature and misleading.\"),\n",
       " Document(page_content='Author: Graham Neubig Title: Fine-grained Hallucination Detection and Editing for Language Models Publication year: 2024 Coauthors: Abhika Mishra, Akari Asai, Vidhisha Balachandran, Yizhong Wang, Graham Neubig, Yulia Tsvetkov, Hannaneh Hajishirzi Abstract: Large language models (LMs) are prone to generate factual errors, which are often called hallucinations. In this paper, we introduce a comprehensive taxonomy of hallucinations and argue that hallucinations manifest in diverse forms, each requiring varying degrees of careful assessments to verify factuality. We propose a novel task of automatic fine-grained hallucination detection and construct a new evaluation benchmark, FavaBench, that includes about one thousand fine-grained human judgments on three LM outputs across various domains. Our analysis reveals that ChatGPT and Llama2-Chat (70B, 7B) exhibit diverse types of hallucinations in the majority of their outputs in information-seeking scenarios. We train FAVA, a retrieval-augmented LM by carefully creating synthetic data to detect and correct fine-grained hallucinations. On our benchmark, our automatic and human evaluations show that FAVA significantly outperforms ChatGPT and GPT-4 on fine-grained hallucination detection, and edits suggested by FAVA improve the factuality of LM-generated text.'),\n",
       " Document(page_content='Author: Graham Neubig Title: Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate Publication year: 2024 Coauthors: Steffi Chern, Ethan Chern, Graham Neubig, Pengfei Liu Abstract: Despite the utility of Large Language Models (LLMs) across a wide range of tasks and scenarios, developing a method for reliably evaluating LLMs across varied contexts continues to be challenging. Modern evaluation approaches often use LLMs to assess responses generated by LLMs. However, the meta-evaluation conducted to assess the effectiveness of these LLMs as evaluators is typically constrained by the coverage of existing benchmarks or requires extensive human annotation. This underscores the urgency of methods for scalable meta-evaluation that can effectively, reliably, and efficiently evaluate the performance of LLMs as evaluators across diverse tasks and scenarios, particularly in potentially new, user-defined scenarios. To fill this gap, we propose ScaleEval, an agent-debate-assisted meta-evaluation framework that leverages the capabilities of multiple communicative LLM agents. This framework supports multi-round discussions to assist human annotators in discerning the most capable LLMs as evaluators, which significantly eases their workload in cases that used to require large-scale annotations during meta-evaluation. We release the code for our framework, which is publicly available at: \\\\url{https://github.com/GAIR-NLP/scaleeval}.'),\n",
       " Document(page_content='Author: Graham Neubig Title: TroVE: Inducing Verifiable and Efficient Toolboxes for Solving Programmatic Tasks Publication year: 2024 Coauthors: Zhiruo Wang, Daniel Fried, Graham Neubig Abstract: Language models (LMs) can solve tasks such as answering questions about tables or images by writing programs. However, using primitive functions often leads to verbose and error-prone programs, and higher-level functions require expert design. To enable better solutions without human labor, we ask code LMs to curate reusable high-level functions, and use them to write solutions. We present TROVE, a training-free method of inducing a verifiable and efficient toolbox of functions, by generating via using, growing, and periodically trimming the toolbox. On 11 datasets from math, table question answering, and image reasoning tasks, TROVE consistently yields simpler solutions with higher accuracy than baselines using CODELLAMA and previous methods using GPT, while using 79-98% smaller toolboxes. TROVE further enables 31% faster and 13% more accurate human verification than baselines. With the same pipeline, it creates diverse functions for varied tasks and datasets, providing insights into their individual characteristics.'),\n",
       " Document(page_content='Author: Graham Neubig Title: VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks Publication year: 2024 Coauthors: Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, Daniel Fried Abstract: Autonomous agents capable of planning, reasoning, and executing actions on the web offer a promising avenue for automating computer tasks. However, the majority of existing benchmarks primarily focus on text-based agents, neglecting many natural tasks that require visual information to effectively solve. Given that most computer interfaces cater to human perception, visual information often augments textual data in ways that text-only models struggle to harness effectively. To bridge this gap, we introduce VisualWebArena, a benchmark designed to assess the performance of multimodal web agents on realistic \\\\textit{visually grounded tasks}. VisualWebArena comprises of a set of diverse and complex web-based tasks that evaluate various capabilities of autonomous multimodal agents. To perform on this benchmark, agents need to accurately process image-text inputs, interpret natural language instructions, and execute actions on websites to accomplish user-defined objectives. We conduct an extensive evaluation of state-of-the-art LLM-based autonomous agents, including several multimodal models. Through extensive quantitative and qualitative analysis, we identify several limitations of text-only LLM agents, and reveal gaps in the capabilities of state-of-the-art multimodal language agents. VisualWebArena provides a framework for evaluating multimodal autonomous language agents, and offers insights towards building stronger autonomous agents for the web. Our code, baseline models, and data is publicly available at https://jykoh.com/vwa.'),\n",
       " Document(page_content='Author: Graham Neubig Title: DiffusER: Diffusion via Edit-based Reconstruction Publication year: 2023 Coauthors: Machel Reid, V. Hellendoorn, Graham Neubig '),\n",
       " Document(page_content=\"Author: Graham Neubig Title: Cross-Modal Fine-Tuning: Align then Refine Publication year: 2023 Coauthors: Junhong Shen, Liam Li, L. Dery, Corey Staten, M. Khodak, Graham Neubig, Ameet Talwalkar Abstract: Fine-tuning large-scale pretrained models has led to tremendous progress in well-studied modalities such as vision and NLP. However, similar gains have not been observed in many other modalities due to a lack of relevant pretrained models. In this work, we propose ORCA, a general cross-modal fine-tuning framework that extends the applicability of a single large-scale pretrained model to diverse modalities. ORCA adapts to a target task via an align-then-refine workflow: given the target input, ORCA first learns an embedding network that aligns the embedded feature distribution with the pretraining modality. The pretrained model is then fine-tuned on the embedded data to exploit the knowledge shared across modalities. Through extensive experiments, we show that ORCA obtains state-of-the-art results on 3 benchmarks containing over 60 datasets from 12 modalities, outperforming a wide range of hand-designed, AutoML, general-purpose, and task-specific methods. We highlight the importance of data alignment via a series of ablation studies and demonstrate ORCA's utility in data-limited regimes.\"),\n",
       " Document(page_content='Author: Graham Neubig Title: ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages Publication year: 2023 Coauthors: Nathaniel R. Robinson, Perez Ogayo, David R. Mortensen, Graham Neubig Abstract: Large language models (LLMs) implicitly learn to perform a range of language tasks, including machine translation (MT). Previous studies explore aspects of LLMs’ MT capabilities. However, there exist a wide variety of languages for which recent LLM MT performance has never before been evaluated. Without published experimental evidence on the matter, it is difficult for speakers of the world’s diverse languages to know how and whether they can use LLMs for their languages. We present the first experimental evidence for an expansive set of 204 languages, along with MT cost analysis, using the FLORES-200 benchmark. Trends reveal that GPT models approach or exceed traditional MT model performance for some high-resource languages (HRLs) but consistently lag for low-resource languages (LRLs), under-performing traditional MT for 84.1% of languages we covered. Our analysis reveals that a language’s resource level is the most important feature in determining ChatGPT’s relative ability to translate it, and suggests that ChatGPT is especially disadvantaged for LRLs and African languages.'),\n",
       " Document(page_content='Author: Graham Neubig Title: GlobalBench: A Benchmark for Global Progress in Natural Language Processing Publication year: 2023 Coauthors: Yueqi Song, Catherine Cui, Simran Khanuja, Pengfei Liu, FAHIM FAISAL, Alissa Ostapenko, Genta Indra Winata, Alham Fikri Aji, Samuel Cahyawijaya, Yulia Tsvetkov, Antonios Anastasopoulos, Graham Neubig Abstract: Despite the major advances in NLP, significant disparities in NLP system performance across languages still exist. Arguably, these are due to uneven resource allocation and sub-optimal incentives to work on less resourced languages. To track and further incentivize the global development of equitable language technology, we introduce GlobalBench. Prior multilingual benchmarks are static and have focused on a limited number of tasks and languages. In contrast, GlobalBench is an ever-expanding collection that aims to dynamically track progress on all NLP datasets in all languages. Rather than solely measuring accuracy, GlobalBench also tracks the estimated per-speaker utility and equity of technology across all languages, providing a multi-faceted view of how language technology is serving people of the world. Furthermore, GlobalBench is designed to identify the most under-served languages, and rewards research efforts directed towards those languages. At present, the most under-served languages are the ones with a relatively high population, but nonetheless overlooked by composite multilingual benchmarks (like Punjabi, Portuguese, and Wu Chinese). Currently, GlobalBench covers 966 datasets in 190 languages, and has 1,128 system submissions spanning 62 languages.'),\n",
       " Document(page_content=\"Author: Graham Neubig Title: Learning Performance-Improving Code Edits Publication year: 2023 Coauthors: Aman Madaan, Alex Shypula, Uri Alon, Milad Hashemi, Parthasarathy Ranganathan, Yiming Yang, Graham Neubig, A. Yazdanbakhsh Abstract: The waning of Moore's Law has shifted the focus of the tech industry towards alternative methods for continued performance gains. While optimizing compilers are a standard tool to help increase program efficiency, programmers continue to shoulder much responsibility in crafting and refactoring code with better performance characteristics. In this paper, we investigate the ability of large language models (LLMs) to suggest functionally correct, performance improving code edits. We hypothesize that language models can suggest such edits in ways that would be impractical for static analysis alone. We investigate these questions by curating a large-scale dataset of Performance-Improving Edits, PIE. PIE contains trajectories of programs, where a programmer begins with an initial, slower version and iteratively makes changes to improve the program's performance. We use PIE to evaluate and improve the capacity of large language models. Specifically, use examples from PIE to fine-tune multiple variants of CODEGEN, a billion-scale Transformer-decoder model. Additionally, we use examples from PIE to prompt OpenAI's CODEX using a few-shot prompting. By leveraging PIE, we find that both CODEX and CODEGEN can generate performance-improving edits, with speedups of more than 2.5x for over 25% of the programs, for C++ and Python, even after the C++ programs were compiled using the O3 optimization level. Crucially, we show that PIE allows CODEGEN, an open-sourced and 10x smaller model than CODEX, to match the performance of CODEX on this challenging task. Overall, this work opens new doors for creating systems and methods that can help programmers write efficient code.\"),\n",
       " Document(page_content='Author: Graham Neubig Title: CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code Publication year: 2023 Coauthors: Shuyan Zhou, Uri Alon, Sumit Agarwal, Graham Neubig Abstract: Since the rise of neural natural-language-to-code models (NL->Code) that can generate long expressions and statements rather than a single next-token, one of the major problems has been reliably evaluating their generated output. In this paper, we propose CodeBERTScore: an evaluation metric for code generation, which builds on BERTScore (Zhang et al., 2020). Instead of encoding only the generated tokens as in BERTScore, CodeBERTScore also encodes the natural language input preceding the generated code, thus modeling the consistency between the generated code and its given natural language context as well. We perform an extensive evaluation of CodeBERTScore across four programming languages. We find that CodeBERTScore achieves a higher correlation with human preference and with functional correctness than all existing metrics. That is, generated code that receives a higher score by CodeBERTScore is more likely to be preferred by humans, as well as to function correctly when executed. We release five language-specific pretrained models to use with our publicly available code. Our language-specific models have been downloaded more than 1,000,000 times from the Huggingface Hub. Our code and data are available at https://github.com/neulab/code-bert-score'),\n",
       " Document(page_content='Author: Graham Neubig Title: Divergences between Language Models and Human Brains Publication year: 2023 Coauthors: Yuchen Zhou, Emmy Liu, Graham Neubig, Leila Wehbe Abstract: Do machines and humans process language in similar ways? Recent research has hinted in the affirmative, finding that brain signals can be effectively predicted using the internal representations of language models (LMs). Although such results are thought to reflect shared computational principles between LMs and human brains, there are also clear differences in how LMs and humans represent and use language. In this work, we systematically explore the divergences between human and machine language processing by examining the differences between LM representations and human brain responses to language as measured by Magnetoencephalography (MEG) across two datasets in which subjects read and listened to narrative stories. Using a data-driven approach, we identify two domains that are not captured well by LMs: social/emotional intelligence and physical commonsense. We then validate these domains with human behavioral experiments and show that fine-tuning LMs on these domains can improve their alignment with human brain responses.'),\n",
       " Document(page_content='Author: Graham Neubig Title: Neural Machine Translation for the Indigenous Languages of the Americas: An Introduction Publication year: 2023 Coauthors: Manuel Mager, R. Bhatnagar, Graham Neubig, Ngoc Thang Vu, Katharina Kann Abstract: Neural models have drastically advanced state of the art for machine translation (MT) between high-resource languages. Traditionally, these models rely on large amounts of training data, but many language pairs lack these resources. However, an important part of the languages in the world do not have this amount of data. Most languages from the Americas are among them, having a limited amount of parallel and monolingual data, if any. Here, we present an introduction to the interested reader to the basic challenges, concepts, and techniques that involve the creation of MT systems for these languages. Finally, we discuss the recent advances and findings and open questions, product of an increased interest of the NLP community in these languages.'),\n",
       " Document(page_content=\"Author: Graham Neubig Title: User-Centric Evaluation of OCR Systems for Kwak’wala Publication year: 2023 Coauthors: Shruti Rijhwani, Daisy Rosenblum, Michayla King, Antonios Anastasopoulos, Graham Neubig Abstract: There has been recent interest in improving optical character recognition (OCR) for endangered languages, particularly because a large number of documents and books in these languages are not in machine-readable formats. The performance of OCR systems is typically evaluated using automatic metrics such as character and word error rates. While error rates are useful for the comparison of different models and systems, they do not measure whether and how the transcriptions produced from OCR tools are useful to downstream users. In this paper, we present a human-centric evaluation of OCR systems, focusing on the Kwak'wala language as a case study. With a user study, we show that utilizing OCR reduces the time spent in the manual transcription of culturally valuable documents -- a task that is often undertaken by endangered language community members and researchers -- by over 50%. Our results demonstrate the potential benefits that OCR tools can have on downstream language documentation and revitalization efforts.\"),\n",
       " Document(page_content='Author: Graham Neubig Title: EXCALIBUR: Encouraging and Evaluating Embodied Exploration Publication year: 2023 Coauthors: Hao Zhu, Raghav Kapoor, So Yeon Min, Winson Han, Jiatai Li, Kaiwen Geng, Graham Neubig, Yonatan Bisk, Aniruddha Kembhavi, Luca Weihs Abstract: Experience precedes understanding. Humans constantly explore and learn about their environment out of curiosity, gather information, and update their models of the world. On the other hand, machines are either trained to learn passively from static and fixed datasets, or taught to complete specific goal-conditioned tasks. To encourage the development of exploratory interactive agents, we present the EXCALIBUR benchmark. EXCALIBUR allows agents to explore their environment for long durations and then query their understanding of the physical world via inquiries like: “is the small heavy red bowl made from glass?” or “is there a silver spoon heavier than the egg?”. This design encourages agents to perform free-form home exploration without myopia induced by goal conditioning. Once the agents have answered a series of questions, they can renter the scene to refine their knowledge, update their beliefs, and improve their performance on the questions. Our experiments demonstrate the challenges posed by this dataset for the present-day state-of-the-art embodied systems and the headroom afforded to develop new innovative methods. Finally, we present a virtual reality interface that enables humans to seamlessly interact within the simulated world and use it to gather human performance measures. EXCALIBUR affords unique challenges in comparison to presentday benchmarks and represents the next frontier for embodied AI research.'),\n",
       " Document(page_content='Author: Graham Neubig Title: Multi-Dimensional Evaluation of Text Summarization with In-Context Learning Publication year: 2023 Coauthors: Sameer Jain, Vaishakh Keshava, Swarnashree Mysore Sathyendra, Patrick Fernandes, Pengfei Liu, Graham Neubig, Chunting Zhou Abstract: Evaluation of natural language generation (NLG) is complex and multi-dimensional. Generated text can be evaluated for fluency, coherence, factuality, or any other dimensions of interest. Most frameworks that perform such multi-dimensional evaluation require training on large manually or synthetically generated datasets. In this paper, we study the efficacy of large language models as multi-dimensional evaluators using in-context learning, obviating the need for large training datasets. Our experiments show that in-context learning-based evaluators are competitive with learned evaluation frameworks for the task of text summarization, establishing state-of-the-art on dimensions such as relevance and factual consistency. We then analyze the effects of factors such as the selection and number of in-context examples on performance. Finally, we study the efficacy of in-context learning based evaluators in evaluating zero-shot summaries written by large language models such as GPT-3.'),\n",
       " Document(page_content='Author: Graham Neubig Title: A Gold Standard Dataset for the Reviewer Assignment Problem Publication year: 2023 Coauthors: Ivan Stelmakh, J. Wieting, Graham Neubig, Nihar B. Shah Abstract: Many peer-review venues are either using or looking to use algorithms to assign submissions to reviewers. The crux of such automated approaches is the notion of the\"similarity score\"--a numerical estimate of the expertise of a reviewer in reviewing a paper--and many algorithms have been proposed to compute these scores. However, these algorithms have not been subjected to a principled comparison, making it difficult for stakeholders to choose the algorithm in an evidence-based manner. The key challenge in comparing existing algorithms and developing better algorithms is the lack of the publicly available gold-standard data that would be needed to perform reproducible research. We address this challenge by collecting a novel dataset of similarity scores that we release to the research community. Our dataset consists of 477 self-reported expertise scores provided by 58 researchers who evaluated their expertise in reviewing papers they have read previously. We use this data to compare several popular algorithms employed in computer science conferences and come up with recommendations for stakeholders. Our main findings are as follows. First, all algorithms make a non-trivial amount of error. For the task of ordering two papers in terms of their relevance for a reviewer, the error rates range from 12%-30% in easy cases to 36%-43% in hard cases, highlighting the vital need for more research on the similarity-computation problem. Second, most existing algorithms are designed to work with titles and abstracts of papers, and in this regime the Specter+MFR algorithm performs best. Third, to improve performance, it may be important to develop modern deep-learning based algorithms that can make use of the full texts of papers: the classical TD-IDF algorithm enhanced with full texts of papers is on par with the deep-learning based Specter+MFR that cannot make use of this information.'),\n",
       " Document(page_content='Author: Graham Neubig Title: SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing Publication year: 2023 Coauthors: Taiqi He, Lindia Tjuatja, Nathaniel R. Robinson, Shinji Watanabe, David R. Mortensen, Graham Neubig, L. Levin Abstract: In our submission to the SIGMORPHON 2023 Shared Task on interlinear glossing (IGT), we explore approaches to data augmentation and modeling across seven low-resource languages. For data augmentation, we explore two approaches: creating artificial data from the provided training data and utilizing existing IGT resources in other languages. On the modeling side, we test an enhanced version of the provided token classification baseline as well as a pretrained multilingual seq2seq model. Additionally, we apply post-correction using a dictionary for Gitksan, the language with the smallest amount of data. We find that our token classification models are the best performing, with the highest word-level accuracy for Arapaho and highest morpheme-level accuracy for Gitksan out of all submissions. We also show that data augmentation is an effective strategy, though applying artificial data pretraining has very different effects across both models tested.'),\n",
       " Document(page_content=\"Author: Graham Neubig Title: Alignment for Honesty Publication year: 2023 Coauthors: Yuqing Yang, Ethan Chern, Xipeng Qiu, Graham Neubig, Pengfei Liu Abstract: Recent research has made significant strides in applying alignment techniques to enhance the helpfulness and harmlessness of large language models (LLMs) in accordance with human intentions. In this paper, we argue for the importance of alignment for honesty, ensuring that LLMs proactively refuse to answer questions when they lack knowledge, while still not being overly conservative. However, a pivotal aspect of alignment for honesty involves discerning the limits of an LLM's knowledge, which is far from straightforward. This challenge demands comprehensive solutions in terms of metric development, benchmark creation, and training methodologies. In this paper, we address these challenges by first establishing a precise problem definition and defining ``honesty'' inspired by the Analects of Confucius. This serves as a cornerstone for developing metrics that effectively measure an LLM's honesty by quantifying its progress post-alignment. Furthermore, we introduce a flexible training framework which is further instantiated by several efficient fine-tuning techniques that emphasize honesty without sacrificing performance on other tasks. Our extensive experiments reveal that these aligned models show a marked increase in honesty, as indicated by our proposed metrics. We open-source a wealth of resources to facilitate future research at https://github.com/GAIR-NLP/alignment-for-honesty, including honesty-aligned models, training and evaluation datasets for honesty alignment, concept glossary, as well as all relevant source code.\"),\n",
       " Document(page_content='Author: Graham Neubig Title: Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation Publication year: 2023 Coauthors: Patrick Fernandes, Aman Madaan, Emmy Liu, António Farinhas, Pedro Henrique Martins, Amanda Bertsch, José G. C. de Souza, Shuyan Zhou, Tongshuang Sherry Wu, Graham Neubig, André F. T. Martins Abstract: Many recent advances in natural language generation have been fueled by training large language models on internet-scale data. However, this paradigm can lead to models that generate toxic, inaccurate, and unhelpful content, and automatic evaluation metrics often fail to identify these behaviors. As models become more capable, human feedback is an invaluable signal for evaluating and improving models. This survey aims to provide an overview of the recent research that has leveraged human feedback to improve natural language generation. First, we introduce an encompassing formalization of feedback, and identify and organize existing research into a taxonomy following this formalization. Next, we discuss how feedback can be described by its format and objective, and cover the two approaches proposed to use feedback (either for training or decoding): directly using the feedback or training feedback models. We also discuss existing datasets for human-feedback data collection, and concerns surrounding feedback collection. Finally, we provide an overview of the nascent field of AI feedback, which exploits large language models to make judgments based on a set of principles and minimize the need for human intervention.'),\n",
       " Document(page_content='Author: Graham Neubig Title: Program-Aided Reasoners (better) Know What They Know Publication year: 2023 Coauthors: Anubha Kabra, Sanketh Rangreji, Yash Mathur, Aman Madaan, Emmy Liu, Graham Neubig Abstract: Prior work shows that program-aided reasoning, in which large language models (LLMs) are combined with programs written in programming languages such as Python, can significantly improve accuracy on various reasoning tasks. However, while accuracy is essential, it is also important for such reasoners to\"know what they know\", which can be quantified through the calibration of the model. In this paper, we compare the calibration of Program Aided Language Models (PAL) and text-based Chain-of-thought (COT) prompting techniques over 5 datasets and 2 model types: LLaMA models and OpenAI models. Our results indicate that PAL leads to improved calibration in 75% of the instances. Our analysis uncovers that prompting styles that produce lesser diversity in generations also have more calibrated results, and thus we also experiment with inducing lower generation diversity using temperature scaling and find that for certain temperatures, PAL is not only more accurate but is also more calibrated than COT. Overall, we demonstrate that, in the majority of cases, program-aided reasoners better know what they know than text-based counterparts.'),\n",
       " Document(page_content='Author: Graham Neubig Title: Learning to Filter Context for Retrieval-Augmented Generation Publication year: 2023 Coauthors: Zhiruo Wang, Jun Araki, Zhengbao Jiang, Md. Rizwan Parvez, Graham Neubig Abstract: On-the-fly retrieval of relevant knowledge has proven an essential element of reliable systems for tasks such as open-domain question answering and fact verification. However, because retrieval systems are not perfect, generation models are required to generate outputs given partially or entirely irrelevant passages. This can cause over- or under-reliance on context, and result in problems in the generated output such as hallucinations. To alleviate these problems, we propose FILCO, a method that improves the quality of the context provided to the generator by (1) identifying useful context based on lexical and information-theoretic approaches, and (2) training context filtering models that can filter retrieved contexts at test time. We experiment on six knowledge-intensive tasks with FLAN-T5 and LLaMa2, and demonstrate that our method outperforms existing approaches on extractive question answering (QA), complex multi-hop and long-form QA, fact verification, and dialog generation tasks. FILCO effectively improves the quality of context, whether or not it supports the canonical output.'),\n",
       " Document(page_content='Author: Graham Neubig Title: FacTool: Factuality Detection in Generative AI - A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios Publication year: 2023 Coauthors: Ethan Chern, Steffi Chern, Shiqi Chen, Weizhe Yuan, Kehua Feng, Chunting Zhou, Junxian He, Graham Neubig, Pengfei Liu Abstract: The emergence of generative pre-trained models has facilitated the synthesis of high-quality text, but it has also posed challenges in identifying factual errors in the generated text. In particular: (1) A wider range of tasks now face an increasing risk of containing factual errors when handled by generative models. (2) Generated texts tend to be lengthy and lack a clearly defined granularity for individual facts. (3) There is a scarcity of explicit evidence available during the process of fact checking. With the above challenges in mind, in this paper, we propose FacTool, a task and domain agnostic framework for detecting factual errors of texts generated by large language models (e.g., ChatGPT). Experiments on four different tasks (knowledge-based QA, code generation, mathematical reasoning, and scientific literature review) show the efficacy of the proposed method. We release the code of FacTool associated with ChatGPT plugin interface at https://github.com/GAIR-NLP/factool .'),\n",
       " Document(page_content='Author: Graham Neubig Title: Active Retrieval Augmented Generation Publication year: 2023 Coauthors: Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig Abstract: Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.'),\n",
       " Document(page_content=\"Author: Graham Neubig Title: Large Language Models Enable Few-Shot Clustering Publication year: 2023 Coauthors: Vijay Viswanathan, Kiril Gashteovski, Carolin (Haas) Lawrence, Tongshuang Sherry Wu, Graham Neubig Abstract: Unlike traditional unsupervised clustering, semi-supervised clustering allows users to provide meaningful structure to the data, which helps the clustering algorithm to match the user's intent. Existing approaches to semi-supervised clustering require a significant amount of feedback from an expert to improve the clusters. In this paper, we ask whether a large language model can amplify an expert's guidance to enable query-efficient, few-shot semi-supervised text clustering. We show that LLMs are surprisingly effective at improving clustering. We explore three stages where LLMs can be incorporated into clustering: before clustering (improving input features), during clustering (by providing constraints to the clusterer), and after clustering (using LLMs post-correction). We find incorporating LLMs in the first two stages can routinely provide significant improvements in cluster quality, and that LLMs enable a user to make trade-offs between cost and accuracy to produce desired clusters. We release our code and LLM prompts for the public to use.\"),\n",
       " Document(page_content=\"Author: Graham Neubig Title: Solving NLP Problems through Human-System Collaboration: A Discussion-based Approach Publication year: 2023 Coauthors: Masahiro Kaneko, Graham Neubig, Naoaki Okazaki Abstract: Humans work together to solve common problems by having discussions, explaining, and agreeing or disagreeing with each other. Similarly, if a system can have discussions with humans when solving tasks, it can improve the system's performance and reliability. In previous research on explainability, it has only been possible for the system to make predictions and for humans to ask questions about them rather than having a mutual exchange of opinions. This research aims to create a dataset and computational framework for systems that discuss and refine their predictions through dialogue. Through experiments, we show that the proposed system can have beneficial discussions with humans improving the accuracy by up to 25 points in the natural language inference task.\"),\n",
       " Document(page_content='Author: Graham Neubig Title: Crossing the Threshold: Idiomatic Machine Translation through Retrieval Augmentation and Loss Weighting Publication year: 2023 Coauthors: Emmy Liu, Aditi Chaudhary, Graham Neubig Abstract: Idioms are common in everyday language, but often pose a challenge to translators because their meanings do not follow from the meanings of their parts. Despite significant advances, machine translation systems still struggle to translate idiomatic expressions. We provide a simple characterization of idiomatic translation and related issues. This allows us to conduct a synthetic experiment revealing a tipping point at which transformer-based machine translation models correctly default to idiomatic translations. To expand multilingual resources, we compile a dataset of ~4k natural sentences containing idiomatic expressions in French, Finnish, and Japanese. To improve translation of natural idioms, we introduce two straightforward yet effective techniques: the strategic upweighting of training loss on potentially idiomatic sentences, and using retrieval-augmented models. This not only improves the accuracy of a strong pretrained MT model on idiomatic sentences by up to 13% in absolute accuracy, but also holds potential benefits for non-idiomatic sentences.'),\n",
       " Document(page_content='Author: Graham Neubig Title: Why do Nearest Neighbor Language Models Work? Publication year: 2023 Coauthors: Frank F. Xu, Uri Alon, Graham Neubig Abstract: Language models (LMs) compute the probability of a text by sequentially computing a representation of an already-seen context and using this representation to predict the next word. Currently, most LMs calculate these representations through a neural network consuming the immediate previous context. However recently, retrieval-augmented LMs have shown to improve over standard neural LMs, by accessing information retrieved from a large datastore, in addition to their standard, parametric, next-word prediction. In this paper, we set out to understand why retrieval-augmented language models, and specifically why k-nearest neighbor language models (kNN-LMs) perform better than standard parametric LMs, even when the k-nearest neighbor component retrieves examples from the same training set that the LM was originally trained on. To this end, we perform a careful analysis of the various dimensions over which kNN-LM diverges from standard LMs, and investigate these dimensions one by one. Empirically, we identify three main reasons why kNN-LM performs better than standard LMs: using a different input representation for predicting the next tokens, approximate kNN search, and the importance of softmax temperature for the kNN distribution. Further, we incorporate these insights into the model architecture or the training procedure of the standard parametric LM, improving its results without the need for an explicit retrieval component. The code is available at https://github.com/frankxu2004/knnlm-why.'),\n",
       " Document(page_content='Author: Graham Neubig Title: Syntax and Semantics Meet in the “Middle”: Probing the Syntax-Semantics Interface of LMs Through Agentivity Publication year: 2023 Coauthors: Lindia Tjuatja, Emmy Liu, L. Levin, Graham Neubig Abstract: Recent advances in large language models have prompted researchers to examine their abilities across a variety of linguistic tasks, but little has been done to investigate how models handle the interactions in meaning across words and larger syntactic forms—i.e. phenomena at the intersection of syntax and semantics. We present the semantic notion of agentivity as a case study for probing such interactions. We created a novel evaluation dataset by utilitizing the unique linguistic properties of a subset of optionally transitive English verbs. This dataset was used to prompt varying sizes of three model classes to see if they are sensitive to agentivity at the lexical level, and if they can appropriately employ these word-level priors given a specific syntactic context. Overall, GPT-3 text-davinci-003 performs extremely well across all experiments, outperforming all other models tested by far. In fact, the results are even better correlated with human judgements than both syntactic and semantic corpus statistics. This suggests that LMs may potentially serve as more useful tools for linguistic annotation, theory testing, and discovery than select corpora for certain tasks.'),\n",
       " Document(page_content='Author: Graham Neubig Title: DataFinder: Scientific Dataset Recommendation from Natural Language Descriptions Publication year: 2023 Coauthors: Vijay Viswanathan, Luyu Gao, Tongshuang Sherry Wu, Pengfei Liu, Graham Neubig Abstract: Modern machine learning relies on datasets to develop and validate research ideas. Given the growth of publicly available data, finding the right dataset to use is increasingly difficult. Any research question imposes explicit and implicit constraints on how well a given dataset will enable researchers to answer this question, such as dataset size, modality, and domain. We operationalize the task of recommending datasets given a short natural language description of a research idea, to help people find relevant datasets for their needs. Dataset recommendation poses unique challenges as an information retrieval problem; datasets are hard to directly index for search and there are no corpora readily available for this task. To facilitate this task, we build the DataFinder Dataset which consists of a larger automatically-constructed training set (17.5K queries) and a smaller expert-annotated evaluation set (392 queries). Using this data, we compare various information retrieval algorithms on our test set and present a superior bi-encoder retriever for text-based dataset recommendation. This system, trained on the DataFinder Dataset, finds more relevant search results than existing third-party dataset search engines. To encourage progress on dataset recommendation, we release our dataset and models to the public.'),\n",
       " Document(page_content=\"Author: Graham Neubig Title: Multi-lingual and Multi-cultural Figurative Language Understanding Publication year: 2023 Coauthors: Anubha Kabra, Emmy Liu, Simran Khanuja, Alham Fikri Aji, Genta Indra Winata, Samuel Cahyawijaya, Anuoluwapo Aremu, Perez Ogayo, Graham Neubig Abstract: Figurative language permeates human communication, but at the same time is relatively understudied in NLP. Datasets have been created in English to accelerate progress towards measuring and improving figurative language processing in language models (LMs). However, the use of figurative language is an expression of our cultural and societal experiences, making it difficult for these phrases to be universally applicable. In this work, we create a figurative language inference dataset, \\\\datasetname, for seven diverse languages associated with a variety of cultures: Hindi, Indonesian, Javanese, Kannada, Sundanese, Swahili and Yoruba. Our dataset reveals that each language relies on cultural and regional concepts for figurative expressions, with the highest overlap between languages originating from the same region. We assess multilingual LMs' abilities to interpret figurative language in zero-shot and few-shot settings. All languages exhibit a significant deficiency compared to English, with variations in performance reflecting the availability of pre-training and fine-tuning data, emphasizing the need for LMs to be exposed to a broader range of linguistic and cultural variation during training.\"),\n",
       " Document(page_content='Author: Graham Neubig Title: It’s MBR All the Way Down: Modern Generation Techniques Through the Lens of Minimum Bayes Risk Publication year: 2023 Coauthors: Amanda Bertsch, Alex Xie, Graham Neubig, Matthew R. Gormley Abstract: Minimum Bayes Risk (MBR) decoding is a method for choosing the outputs of a machine learning system based not on the output with the highest probability, but the output with the lowest risk (expected error) among multiple candidates. It is a simple but powerful method: for an additional cost at inference time, MBR provides reliable several-point improvements across metrics for a wide variety of tasks without any additional data or training. Despite this, MBR is not frequently applied in NLP works, and knowledge of the method itself is limited. We first provide an introduction to the method and the recent literature. We show that several recent methods that do not reference MBR can be written as special cases of MBR; this reformulation provides additional theoretical justification for the performance of these methods, explaining some results that were previously only empirical. We provide theoretical and empirical results about the effectiveness of various MBR variants and make concrete recommendations for the application of MBR in NLP models, including future directions in this area.'),\n",
       " Document(page_content='Author: Graham Neubig Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input Publication year: 2023 Coauthors: Amanda Bertsch, Uri Alon, Graham Neubig, Matthew R. Gormley Abstract: Since the proposal of transformers, these models have been limited to bounded input lengths, because of their need to attend to every token in the input. In this work, we propose Unlimiformer: a general approach that wraps any existing pretrained encoder-decoder transformer, and offloads the cross-attention computation to a single k-nearest-neighbor (kNN) index, while the returned kNN distances are the attention dot-product scores. This kNN index can be kept on either the GPU or CPU memory and queried in sub-linear time; this way, we can index practically unlimited input sequences, while every attention head in every decoder layer retrieves its top-k keys, instead of attending to every key. We evaluate Unlimiformer on several long-document and book-summarization benchmarks, showing that it can process even 500k token-long inputs from the BookSum dataset, without any input truncation at test time. We demonstrate that Unlimiformer improves pretrained models such as BART and Longformer by extending them to unlimited inputs without additional learned weights and without modifying their code. We make our code and models publicly available at https://github.com/abertsch72/unlimiformer .'),\n",
       " Document(page_content='Author: Graham Neubig Title: WebArena: A Realistic Web Environment for Building Autonomous Agents Publication year: 2023 Coauthors: Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig Abstract: With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.'),\n",
       " Document(page_content='Author: Graham Neubig Title: Prompt2Model: Generating Deployable Models from Natural Language Instructions Publication year: 2023 Coauthors: Vijay Viswanathan, Chenyang Zhao, Amanda Bertsch, Tongshuang Sherry Wu, Graham Neubig Abstract: Large language models (LLMs) enable system builders today to create competent NLP systems through prompting, where they only need to describe the task in natural language and provide a few examples. However, in other ways, LLMs are a step backward from traditional special-purpose NLP models; they require extensive computational resources for deployment and can be gated behind APIs. In this paper, we propose Prompt2Model, a general-purpose method that takes a natural language task description like the prompts provided to LLMs, and uses it to train a special-purpose model that is conducive to deployment. This is done through a multi-step process of retrieval of existing datasets and pretrained models, dataset generation using LLMs, and supervised fine-tuning on these retrieved and generated datasets. Over three tasks, we demonstrate that given the same few-shot prompt as input, Prompt2Model trains models that outperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20% while being up to 700 times smaller. We also show that this data can be used to obtain reliable performance estimates of model performance, enabling model developers to assess model reliability before deployment. Prompt2Model is available open-source at https://github.com/neulab/prompt2model.'),\n",
       " Document(page_content='Author: Graham Neubig Title: Computational Language Acquisition with Theory of Mind Publication year: 2023 Coauthors: Andy T. Liu, Hao Zhu, Emmy Liu, Yonatan Bisk, Graham Neubig Abstract: Unlike current state-of-the-art language models, young children actively acquire language through interactions with their surrounding environment and caretakers. One mechanism that has been argued to be critical to language learning is the ability to infer the mental states of other agents in social environments, coined Theory of Mind (ToM) by Premack&Woodruff (1978). Drawing inspiration from the modern operationalized versions of ToM implemented in Rabinowitz et al. (2018) and Zhu et al. (2021), we build language-learning agents equipped with ToM, and measure its effects on the learning process. We model ToM by giving the speaker agent an internal listener model that is trained alongside the speaker and used to rerank potential utterances. We experiment with varying task difficulty, hypothesizing that models will acquire more complex language to adapt to stronger environmental pressures. We find that training speakers with a highly weighted ToM listener component leads to performance gains in our image referential game setting. We also find some evidence that increasing task difficulty in the training process results in more fluent and precise utterances in evaluation. This suggests the potential utility of further incorporating ToM, as well as other insights from child language acquisition, into computational models of language acquisition.'),\n",
       " Document(page_content='Author: Graham Neubig Title: DeMuX: Data-efficient Multilingual Learning Publication year: 2023 Coauthors: Simran Khanuja, Srinivas Gowriraj, L. Dery, Graham Neubig Abstract: We consider the task of optimally fine-tuning pre-trained multilingual models, given small amounts of unlabelled target data and an annotation budget. In this paper, we introduce DEMUX, a framework that prescribes the exact data-points to label from vast amounts of unlabelled multilingual data, having unknown degrees of overlap with the target set. Unlike most prior works, our end-to-end framework is language-agnostic, accounts for model representations, and supports multilingual target configurations. Our active learning strategies rely upon distance and uncertainty measures to select task-specific neighbors that are most informative to label, given a model. DeMuX outperforms strong baselines in 84% of the test cases, in the zero-shot setting of disjoint source and target language sets (including multilingual target pools), across three models and four tasks. Notably, in low-budget settings (5-100 examples), we observe gains of up to 8-11 F1 points for token-level tasks, and 2-5 F1 for complex tasks. Our code is released here: https://github.com/simran-khanuja/demux.'),\n",
       " Document(page_content='Author: Graham Neubig Title: Improving Factuality of Abstractive Summarization via Contrastive Reward Learning Publication year: 2023 Coauthors: Ethan Chern, Zhiruo Wang, Sanjan Das, Bhavuk Sharma, Pengfei Liu, Graham Neubig Abstract: Modern abstractive summarization models often generate summaries that contain hallucinated or contradictory information. In this paper, we propose a simple but effective contrastive learning framework that incorporates recent developments in reward learning and factuality metrics. Empirical studies demonstrate that the proposed framework enables summarization models to learn from feedback of factuality metrics using contrastive reward learning, leading to more factual summaries by human evaluations. This suggests that further advances in learning and evaluation algorithms can feed directly into providing more factual summaries. Code and human evaluation results will be publicly available at \\\\url{https://github.com/EthanC111/factuality_summarization}.'),\n",
       " Document(page_content=\"Author: Graham Neubig Title: SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents Publication year: 2023 Coauthors: Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, Maarten Sap Abstract: Humans are social beings; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and interact under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills. These findings demonstrate SOTOPIA's promise as a general platform for research on evaluating and improving social intelligence in artificial agents.\"),\n",
       " Document(page_content='Author: Graham Neubig Title: The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation Publication year: 2023 Coauthors: Patrick Fernandes, Daniel Deutsch, M. Finkelstein, Parker Riley, André F. T. Martins, Graham Neubig, Ankush Garg, J. Clark, Markus Freitag, Orhan Firat Abstract: Automatic evaluation of machine translation (MT) is a critical tool driving the rapid iterative development of MT systems. While considerable progress has been made on estimating a single scalar quality score, current metrics lack the informativeness of more detailed schemes that annotate individual errors, such as Multidimensional Quality Metrics (MQM). In this paper, we help fill this gap by proposing AutoMQM, a prompting technique which leverages the reasoning and in-context learning capabilities of large language models (LLMs) and asks them to identify and categorize errors in translations. We start by evaluating recent LLMs, such as PaLM and PaLM-2, through simple score prediction prompting, and we study the impact of labeled data through in-context learning and finetuning. We then evaluate AutoMQM with PaLM-2 models, and we find that it improves performance compared to just prompting for scores (with particularly large gains for larger models) while providing interpretability through error spans that align with human annotations.'),\n",
       " Document(page_content='Author: Graham Neubig Title: Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes Publication year: 2024 Coauthors: L. Dery, Steven Kolawole, Jean-Francois Kagey, Virginia Smith, Graham Neubig, Ameet Talwalkar Abstract: Given the generational gap in available hardware between lay practitioners and the most endowed institutions, LLMs are becoming increasingly inaccessible as they grow in size. Whilst many approaches have been proposed to compress LLMs to make their resource consumption manageable, these methods themselves tend to be resource intensive, putting them out of the reach of the very user groups they target. In this work, we explore the problem of structured pruning of LLMs using only forward passes. We seek to empower practitioners to prune models so large that their available hardware has just enough memory to run inference. We develop Bonsai, a gradient-free, perturbative pruning method capable of delivering small, fast, and accurate pruned models. We observe that Bonsai outputs pruned models that (i) outperform those generated by more expensive gradient-based structured pruning methods, and (ii) are twice as fast (with comparable accuracy) as those generated by semi-structured pruning methods requiring comparable resources as Bonsai. We also leverage Bonsai to produce a new sub-2B model using a single A6000 that yields state-of-the-art performance on 4/6 tasks on the Huggingface Open LLM leaderboard.'),\n",
       " Document(page_content=\"Author: Graham Neubig Title: Multitask Learning Can Improve Worst-Group Outcomes Publication year: 2023 Coauthors: Atharva Kulkarni, L. Dery, Amrith Rajagopal Setlur, Aditi Raghunathan, Ameet Talwalkar, Graham Neubig Abstract: In order to create machine learning systems that serve a variety of users well, it is vital to not only achieve high average performance but also ensure equitable outcomes across diverse groups. However, most machine learning methods are designed to improve a model's average performance on a chosen end task without consideration for their impact on worst group error. Multitask learning (MTL) is one such widely used technique. In this paper, we seek not only to understand the impact of MTL on worst-group accuracy but also to explore its potential as a tool to address the challenge of group-wise fairness. We primarily consider the common setting of fine-tuning a pre-trained model, where, following recent work (Gururangan et al., 2020; Dery et al., 2023), we multitask the end task with the pre-training objective constructed from the end task data itself. In settings with few or no group annotations, we find that multitasking often, but not always, achieves better worst-group accuracy than Just-Train-Twice (JTT; Liu et al. (2021)) -- a representative distributionally robust optimization (DRO) method. Leveraging insights from synthetic data experiments, we propose to modify standard MTL by regularizing the joint multitask representation space. We run a large number of fine-tuning experiments across computer vision and natural language and find that our regularized MTL approach consistently outperforms JTT on both worst and average group outcomes. Our official code can be found here: https://github.com/atharvajk98/MTL-group-robustness.\"),\n",
       " Document(page_content='Author: Graham Neubig Title: Do LLMs exhibit human-like response biases? A case study in survey design Publication year: 2023 Coauthors: Lindia Tjuatja, Valerie Chen, Sherry Tongshuang Wu, Ameet Talwalkar, Graham Neubig Abstract: As large language models (LLMs) become more capable, there is growing excitement about the possibility of using LLMs as proxies for humans in real-world tasks where subjective labels are desired, such as in surveys and opinion polling. One widely-cited barrier to the adoption of LLMs as proxies for humans in subjective tasks is their sensitivity to prompt wording - but interestingly, humans also display sensitivities to instruction changes in the form of response biases. We investigate the extent to which LLMs reflect human response biases, if at all. We look to survey design, where human response biases caused by changes in the wordings of\"prompts\"have been extensively explored in social psychology literature. Drawing from these works, we design a dataset and framework to evaluate whether LLMs exhibit human-like response biases in survey questionnaires. Our comprehensive evaluation of nine models shows that popular open and commercial LLMs generally fail to reflect human-like behavior, particularly in models that have undergone RLHF. Furthermore, even if a model shows a significant change in the same direction as humans, we find that they are sensitive to perturbations that do not elicit significant changes in humans. These results highlight the pitfalls of using LLMs as human proxies, and underscore the need for finer-grained characterizations of model behavior. Our code, dataset, and collected samples are available at https://github.com/lindiatjuatja/BiasMonkey'),\n",
       " Document(page_content=\"Author: Graham Neubig Title: An In-depth Look at Gemini's Language Abilities Publication year: 2023 Coauthors: Syeda Nahida Akter, Zichun Yu, Aashiq Muhamed, Tianyue Ou, Alex Bauerle, Ángel Alexander Cabrera, Krish Dholakia, Chenyan Xiong, Graham Neubig Abstract: The recently released Google Gemini class of models are the first to comprehensively report results that rival the OpenAI GPT series across a wide variety of tasks. In this paper, we do an in-depth exploration of Gemini's language abilities, making two contributions. First, we provide a third-party, objective comparison of the abilities of the OpenAI GPT and Google Gemini models with reproducible code and fully transparent results. Second, we take a closer look at the results, identifying areas where one of the two model classes excels. We perform this analysis over 10 datasets testing a variety of language abilities, including reasoning, answering knowledge-based questions, solving math problems, translating between languages, generating code, and acting as instruction-following agents. From this analysis, we find that Gemini Pro achieves accuracy that is close but slightly inferior to the corresponding GPT 3.5 Turbo on all tasks that we benchmarked. We further provide explanations for some of this under-performance, including failures in mathematical reasoning with many digits, sensitivity to multiple-choice answer ordering, aggressive content filtering, and others. We also identify areas where Gemini demonstrates comparably high performance, including generation into non-English languages, and handling longer and more complex reasoning chains. Code and data for reproduction can be found at https://github.com/neulab/gemini-benchmark\"),\n",
       " Document(page_content='Author: Eric P. Xing Title: Learning to Prompt Segment Anything Models Publication year: 2024 Coauthors: Jiaxing Huang, Kai Jiang, Jingyi Zhang, Han Qiu, Lewei Lu, Shijian Lu, Eric P. Xing Abstract: Segment Anything Models (SAMs) like SEEM and SAM have demonstrated great potential in learning to segment anything. The core design of SAMs lies with Promptable Segmentation, which takes a handcrafted prompt as input and returns the expected segmentation mask. SAMs work with two types of prompts including spatial prompts (e.g., points) and semantic prompts (e.g., texts), which work together to prompt SAMs to segment anything on downstream datasets. Despite the important role of prompts, how to acquire suitable prompts for SAMs is largely under-explored. In this work, we examine the architecture of SAMs and identify two challenges for learning effective prompts for SAMs. To this end, we propose spatial-semantic prompt learning (SSPrompt) that learns effective semantic and spatial prompts for better SAMs. Specifically, SSPrompt introduces spatial prompt learning and semantic prompt learning, which optimize spatial prompts and semantic prompts directly over the embedding space and selectively leverage the knowledge encoded in pre-trained prompt encoders. Extensive experiments show that SSPrompt achieves superior image segmentation performance consistently across multiple widely adopted datasets.'),\n",
       " Document(page_content='Author: Eric P. Xing Title: 3D Open-vocabulary Segmentation with Foundation Models Publication year: 2023 Coauthors: Kunhao Liu, Fangneng Zhan, Jiahui Zhang, Muyu Xu, Yingchen Yu, Abdulmotaleb El-Saddik, Christian Theobalt, Eric P. Xing, Shijian Lu Abstract: Open-vocabulary segmentation of 3D scenes is a fundamental function of human perception and thus a crucial objective in computer vision research. However, this task is heavily impeded by the lack of large-scale and diverse 3D open-vocabulary segmentation datasets for training robust and generalizable models. Distilling knowledge from pre-trained 2D open-vocabulary segmentation models helps but it compromises the open-vocabulary feature significantly as the 2D models are mostly finetuned with close-vocabulary datasets. We tackle the challenges in 3D open-vocabulary segmentation by exploiting the open-vocabulary multimodal knowledge and object reasoning capability of pre-trained foundation models CLIP and DINO, without necessitating any fine-tuning. Specifically, we distill open-vocabulary visual and textual knowledge from CLIP into a neural radiance field (NeRF) which effectively lifts 2D features into view-consistent 3D segmentation. Furthermore, we introduce the Relevancy-Distribution Alignment loss and Feature-Distribution Alignment loss to respectively mitigate the ambiguities of CLIP features and distill precise object boundaries from DINO features, eliminating the need for segmentation annotations during training. Extensive experiments show that our method even outperforms fully supervised models trained with segmentation annotations, suggesting that 3D open-vocabulary segmentation can be effectively learned from 2D images and text-image pairs.'),\n",
       " Document(page_content='Author: Eric P. Xing Title: TrustLLM: Trustworthiness in Large Language Models Publication year: 2024 Coauthors: Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, Zheng Liu, Yixin Liu, Yijue Wang, Zhikun Zhang, B. Kailkhura, Caiming Xiong, Chaowei Xiao, Chun-Yan Li, Eric P. Xing, Furong Huang, Haodong Liu, Heng Ji, Hongyi Wang, Huan Zhang, Huaxiu Yao, M. Kellis, M. Zitnik, Meng Jiang, Mohit Bansal, James Zou, Jian Pei, Jian Liu, Jianfeng Gao, Jiawei Han, Jieyu Zhao, Jiliang Tang, Jindong Wang, John Mitchell, Kai Shu, Kaidi Xu, Kai-Wei Chang, Lifang He, Lifu Huang, M. Backes, Neil Zhenqiang Gong, Philip S. Yu, Pin-Yu Chen, Quanquan Gu, Ran Xu, Rex Ying, Shuiwang Ji, S. Jana, Tian-Xiang Chen, Tianming Liu, Tianying Zhou, William Wang, Xiang Li, Xiang-Yu Zhang, Xiao Wang, Xingyao Xie, Xun Chen, Xuyu Wang, Yan Liu, Yanfang Ye, Yinzhi Cao, Yue Zhao Abstract: Large language models (LLMs), exemplified by ChatGPT, have gained considerable attention for their excellent natural language processing capabilities. Nonetheless, these LLMs present many challenges, particularly in the realm of trustworthiness. Therefore, ensuring the trustworthiness of LLMs emerges as an important topic. This paper introduces TrustLLM, a comprehensive study of trustworthiness in LLMs, including principles for different dimensions of trustworthiness, established benchmark, evaluation, and analysis of trustworthiness for mainstream LLMs, and discussion of open challenges and future directions. Specifically, we first propose a set of principles for trustworthy LLMs that span eight different dimensions. Based on these principles, we further establish a benchmark across six dimensions including truthfulness, safety, fairness, robustness, privacy, and machine ethics. We then present a study evaluating 16 mainstream LLMs in TrustLLM, consisting of over 30 datasets. Our findings firstly show that in general trustworthiness and utility (i.e., functional effectiveness) are positively related. Secondly, our observations reveal that proprietary LLMs generally outperform most open-source counterparts in terms of trustworthiness, raising concerns about the potential risks of widely accessible open-source LLMs. However, a few open-source LLMs come very close to proprietary ones. Thirdly, it is important to note that some LLMs may be overly calibrated towards exhibiting trustworthiness, to the extent that they compromise their utility by mistakenly treating benign prompts as harmful and consequently not responding. Finally, we emphasize the importance of ensuring transparency not only in the models themselves but also in the technologies that underpin trustworthiness. Knowing the specific trustworthy technologies that have been employed is crucial for analyzing their effectiveness.'),\n",
       " Document(page_content=\"Author: Eric P. Xing Title: SlimPajama-DC: Understanding Data Combinations for LLM Training Publication year: 2023 Coauthors: Zhiqiang Shen, Tianhua Tao, Liqun Ma, W. Neiswanger, Zhengzhong Liu, Hongyi Wang, Bowen Tan, Joel Hestness, Natalia Vassilieva, Daria Soboleva, Eric P. Xing Abstract: This paper aims to understand the impacts of various data combinations (e.g., web text, wikipedia, github, books) on the training of large language models using SlimPajama. SlimPajama is a rigorously deduplicated, multi-source dataset, which has been refined and further deduplicated to 627B tokens from the extensive 1.2T tokens RedPajama dataset contributed by Together. We've termed our research as SlimPajama-DC, an empirical analysis designed to uncover fundamental characteristics and best practices associated with employing SlimPajama in the training of large language models. During our research with SlimPajama, two pivotal observations emerged: (1) Global deduplication vs. local deduplication. We analyze and discuss how global (across different sources of datasets) and local (within the single source of dataset) deduplications affect the performance of trained models. (2) Proportions of high-quality/highly-deduplicated multi-source datasets in the combination. To study this, we construct six configurations of SlimPajama dataset and train individual ones using 1.3B Cerebras-GPT model with Alibi and SwiGLU. Our best configuration outperforms the 1.3B model trained on RedPajama using the same number of training tokens by a significant margin. All our 1.3B models are trained on Cerebras 16$\\\\times$ CS-2 cluster with a total of 80 PFLOP/s in bf16 mixed precision. We further extend our discoveries (such as increasing data diversity is crucial after global deduplication) on a 7B model with large batch-size training. Our models and the separate SlimPajama-DC datasets are available at: https://huggingface.co/MBZUAI-LLM and https://huggingface.co/datasets/cerebras/SlimPajama-627B.\"),\n",
       " Document(page_content='Author: Eric P. Xing Title: LLM360: Towards Fully Transparent Open-Source LLMs Publication year: 2023 Coauthors: Zhengzhong Liu, Aurick Qiao, W. Neiswanger, Hongyi Wang, Bowen Tan, Tianhua Tao, Junbo Li, Yuqi Wang, Suqi Sun, Omkar Pangarkar, Richard Fan, Yi Gu, Victor Miller, Yonghao Zhuang, Guowei He, Haonan Li, Fajri Koto, Liping Tang, Nikhil Ranjan, Zhiqiang Shen, Xuguang Ren, Roberto Iriondo, Cun Mu, Zhiting Hu, Mark Schulze, Preslav Nakov, Timothy Baldwin, Eric P. Xing Abstract: The recent surge in open-source Large Language Models (LLMs), such as LLaMA, Falcon, and Mistral, provides diverse options for AI practitioners and researchers. However, most LLMs have only released partial artifacts, such as the final model weights or inference code, and technical reports increasingly limit their scope to high-level design choices and surface statistics. These choices hinder progress in the field by degrading transparency into the training of LLMs and forcing teams to rediscover many details in the training process. We present LLM360, an initiative to fully open-source LLMs, which advocates for all training code and data, model checkpoints, and intermediate results to be made available to the community. The goal of LLM360 is to support open and collaborative AI research by making the end-to-end LLM training process transparent and reproducible by everyone. As a first step of LLM360, we release two 7B parameter LLMs pre-trained from scratch, Amber and CrystalCoder, including their training code, data, intermediate checkpoints, and analyses (at https://www.llm360.ai). We are committed to continually pushing the boundaries of LLMs through this open-source effort. More large-scale and stronger models are underway and will be released in the future.'),\n",
       " Document(page_content='Author: Eric P. Xing Title: Fusing Models with Complementary Expertise Publication year: 2023 Coauthors: Hongyi Wang, Felipe Maia Polo, Yuekai Sun, Souvik Kundu, Eric P. Xing, M. Yurochkin Abstract: Training AI models that generalize across tasks and domains has long been among the open problems driving AI research. The emergence of Foundation Models made it easier to obtain expert models for a given task, but the heterogeneity of data that may be encountered at test time often means that any single expert is insufficient. We consider the Fusion of Experts (FoE) problem of fusing outputs of expert models with complementary knowledge of the data distribution and formulate it as an instance of supervised learning. Our method is applicable to both discriminative and generative tasks and leads to significant performance improvements in image and text classification, text summarization, multiple-choice QA, and automatic evaluation of generated text. We also extend our method to the\"frugal\"setting where it is desired to reduce the number of expert model evaluations at test time.'),\n",
       " Document(page_content='Author: Eric P. Xing Title: Making Scalable Meta Learning Practical Publication year: 2023 Coauthors: Sang Keun Choe, Sanket Vaibhav Mehta, Hwijeen Ahn, W. Neiswanger, Pengtao Xie, Emma Strubell, Eric P. Xing Abstract: Despite its flexibility to learn diverse inductive biases in machine learning programs, meta learning (i.e., learning to learn) has long been recognized to suffer from poor scalability due to its tremendous compute/memory costs, training instability, and a lack of efficient distributed training support. In this work, we focus on making scalable meta learning practical by introducing SAMA, which combines advances in both implicit differentiation algorithms and systems. Specifically, SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients. Evaluated on multiple large-scale meta learning benchmarks, SAMA showcases up to 1.7/4.8x increase in throughput and 2.0/3.8x decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms. Furthermore, we show that SAMA-based data optimization leads to consistent improvements in text classification accuracy with BERT and RoBERTa large language models, and achieves state-of-the-art results in both small- and large-scale data pruning on image classification tasks, demonstrating the practical applicability of scalable meta learning across language and vision domains.'),\n",
       " Document(page_content='Author: Eric P. Xing Title: PromptAgent: Strategic Planning with Language Models Enables Expert-level Prompt Optimization Publication year: 2023 Coauthors: Xinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Haotian Luo, Jiayou Zhang, Nebojsa Jojic, Eric P. Xing, Zhiting Hu Abstract: Highly effective, task-specific prompts are often heavily engineered by experts to integrate detailed instructions and domain insights based on a deep understanding of both instincts of large language models (LLMs) and the intricacies of the target task. However, automating the generation of such expert-level prompts remains elusive. Existing prompt optimization methods tend to overlook the depth of domain knowledge and struggle to efficiently explore the vast space of expert-level prompts. Addressing this, we present PromptAgent, an optimization method that autonomously crafts prompts equivalent in quality to those handcrafted by experts. At its core, PromptAgent views prompt optimization as a strategic planning problem and employs a principled planning algorithm, rooted in Monte Carlo tree search, to strategically navigate the expert-level prompt space. Inspired by human-like trial-and-error exploration, PromptAgent induces precise expert-level insights and in-depth instructions by reflecting on model errors and generating constructive error feedback. Such a novel framework allows the agent to iteratively examine intermediate prompts (states), refine them based on error feedbacks (actions), simulate future rewards, and search for high-reward paths leading to expert prompts. We apply PromptAgent to 12 tasks spanning three practical domains: BIG-Bench Hard (BBH), as well as domain-specific and general NLP tasks, showing it significantly outperforms strong Chain-of-Thought and recent prompt optimization baselines. Extensive analyses emphasize its capability to craft expert-level, detailed, and domain-insightful prompts with great efficiency and generalizability.'),\n",
       " Document(page_content='Author: Eric P. Xing Title: Cappy: Outperforming and Boosting Large Multi-Task LMs with a Small Scorer Publication year: 2023 Coauthors: Bowen Tan, Yun Zhu, Lijuan Liu, Eric P. Xing, Zhiting Hu, Jindong Chen Abstract: Large language models (LLMs) such as T0, FLAN, and OPT-IML, excel in multi-tasking under a unified instruction-following paradigm, where they also exhibit remarkable generalization abilities to unseen tasks. Despite their impressive performance, these LLMs, with sizes ranging from several billion to hundreds of billions of parameters, demand substantial computational resources, making their training and inference expensive and inefficient. Furthermore, adapting these models to downstream applications, particularly complex tasks, is often unfeasible due to the extensive hardware requirements for finetuning, even when utilizing parameter-efficient approaches such as prompt tuning. Additionally, the most powerful multi-task LLMs, such as OPT-IML-175B and FLAN-PaLM-540B, are not publicly accessible, severely limiting their customization potential. To address these challenges, we introduce a pretrained small scorer, Cappy, designed to enhance the performance and efficiency of multi-task LLMs. With merely 360 million parameters, Cappy functions either independently on classification tasks or serve as an auxiliary component for LLMs, boosting their performance. Moreover, Cappy enables efficiently integrating downstream supervision without requiring LLM finetuning nor the access to their parameters. Our experiments demonstrate that, when working independently on 11 language understanding tasks from PromptSource, Cappy outperforms LLMs that are several orders of magnitude larger. Besides, on 45 complex tasks from BIG-Bench, Cappy boosts the performance of the advanced multi-task LLM, FLAN-T5, by a large margin. Furthermore, Cappy is flexible to cooperate with other LLM adaptations, including finetuning and in-context learning, offering additional performance enhancement.'),\n",
       " Document(page_content=\"Author: Eric P. Xing Title: RedCoast: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU/TPUs Publication year: 2023 Coauthors: Bowen Tan, Yun Zhu, Lijuan Liu, Hongyi Wang, Yonghao Zhuang, Jindong Chen, Eric P. Xing, Zhiting Hu Abstract: The recent progress of AI can be largely attributed to large language models (LLMs). However, their escalating memory requirements introduce challenges for machine learning (ML) researchers and engineers. Addressing this requires developers to partition a large model to distribute it across multiple GPUs or TPUs. This necessitates considerable coding and intricate configuration efforts with existing model parallel tools, such as Megatron-LM, DeepSpeed, and Alpa. These tools require users' expertise in machine learning systems (MLSys), creating a bottleneck in LLM development, particularly for developers without MLSys background. In this work, we present RedCoast(Redco), a lightweight and user-friendly tool crafted to automate distributed training and inference for LLMs, as well as to simplify ML pipeline development. The design of Redco emphasizes two key aspects. Firstly, to automate model parallism, our study identifies two straightforward rules to generate tensor parallel strategies for any given LLM. Integrating these rules into Redco facilitates effortless distributed LLM training and inference, eliminating the need of additional coding or complex configurations. We demonstrate the effectiveness by applying Redco on a set of LLM architectures, such as GPT-J, LLaMA, T5, and OPT, up to the size of 66B. Secondly, we propose a mechanism that allows for the customization of diverse ML pipelines through the definition of merely three functions, avoiding redundant and formulaic code like multi-host related processing. This mechanism proves adaptable across a spectrum of ML algorithms, from foundational language modeling to complex algorithms like meta-learning and reinforcement learning. Consequently, Redco implementations exhibit much fewer code lines compared to their official counterparts.\"),\n",
       " Document(page_content='Author: Eric P. Xing Title: Neural-Symbolic Interaction and Co-Evolving Publication year: 2023 Coauthors: Bowen Tan, Shibo Hao, Eric P. Xing, Zhiting Hu '),\n",
       " Document(page_content='Author: Eric P. Xing Title: Redco: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU/TPUs Publication year: 2023 Coauthors: Bowen Tan, Yun Zhu, Lijuan Liu, Hongyi Wang, Yonghao Zhuang, Jindong Chen, Eric P. Xing, Zhiting Hu Abstract: .'),\n",
       " Document(page_content='Author: Eric P. Xing Title: Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale From A New Perspective Publication year: 2023 Coauthors: Zeyuan Yin, Eric P. Xing, Zhiqiang Shen Abstract: We present a new dataset condensation framework termed Squeeze, Recover and Relabel (SRe$^2$L) that decouples the bilevel optimization of model and synthetic data during training, to handle varying scales of datasets, model architectures and image resolutions for efficient dataset condensation. The proposed method demonstrates flexibility across diverse dataset scales and exhibits multiple advantages in terms of arbitrary resolutions of synthesized images, low training cost and memory consumption with high-resolution synthesis, and the ability to scale up to arbitrary evaluation network architectures. Extensive experiments are conducted on Tiny-ImageNet and full ImageNet-1K datasets. Under 50 IPC, our approach achieves the highest 42.5% and 60.8% validation accuracy on Tiny-ImageNet and ImageNet-1K, outperforming all previous state-of-the-art methods by margins of 14.5% and 32.9%, respectively. Our approach also surpasses MTT in terms of speed by approximately 52$\\\\times$ (ConvNet-4) and 16$\\\\times$ (ResNet-18) faster with less memory consumption of 11.6$\\\\times$ and 6.4$\\\\times$ during data synthesis. Our code and condensed datasets of 50, 200 IPC with 4K recovery budget are available at https://github.com/VILA-Lab/SRe2L.'),\n",
       " Document(page_content='Author: Eric P. Xing Title: One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning Publication year: 2023 Coauthors: Arnav Chavan, Zhuang Liu, D. Gupta, Eric P. Xing, Zhiqiang Shen Abstract: We present Generalized LoRA (GLoRA), an advanced approach for universal parameter-efficient fine-tuning tasks. Enhancing Low-Rank Adaptation (LoRA), GLoRA employs a generalized prompt module to optimize pre-trained model weights and adjust intermediate activations, providing more flexibility and capability across diverse tasks and datasets. Moreover, GLoRA facilitates efficient parameter adaptation by employing a scalable, modular, layer-wise structure search that learns individual adapter of each layer. Originating from a unified mathematical formulation, GLoRA exhibits strong transfer learning, few-shot learning and domain generalization abilities, as it adapts to new tasks through not only weights but also additional dimensions like activations. Comprehensive experiments demonstrate that GLoRA outperforms all previous methods in natural, specialized, and structured vision benchmarks, achieving superior accuracy with fewer parameters and computations. The proposed method on LLaMA-1 and LLaMA-2 also show considerable enhancements compared to the original LoRA in the language domain. Furthermore, our structural re-parameterization design ensures that GLoRA incurs no extra inference cost, rendering it a practical solution for resource-limited applications. Code and models are available at: https://github.com/Arnav0400/ViT-Slim/tree/master/GLoRA.'),\n",
       " Document(page_content='Author: Eric P. Xing Title: Defending Against Malicious Behaviors in Federated Learning with Blockchain Publication year: 2023 Coauthors: Nanqing Dong, Zhipeng Wang, Jiahao Sun, Michael C. Kampffmeyer, Yizhe Wen, Shuoying Zhang, W. Knottenbelt, Eric P. Xing Abstract: In the era of deep learning, federated learning (FL) presents a promising approach that allows multi-institutional data owners, or clients, to collaboratively train machine learning models without compromising data privacy. However, most existing FL approaches rely on a centralized server for global model aggregation, leading to a single point of failure. This makes the system vulnerable to malicious attacks when dealing with dishonest clients. In this work, we address this problem by proposing a secure and reliable FL system based on blockchain and distributed ledger technology. Our system incorporates a peer-to-peer voting mechanism and a reward-and-slash mechanism, which are powered by on-chain smart contracts, to detect and deter malicious behaviors. Both theoretical and empirical analyses are presented to demonstrate the effectiveness of the proposed approach, showing that our framework is robust against malicious client-side behaviors.'),\n",
       " Document(page_content='Author: Eric P. Xing Title: Weakly Supervised 3D Open-vocabulary Segmentation Publication year: 2023 Coauthors: Kunhao Liu, Fangneng Zhan, Jiahui Zhang, Muyu Xu, Yingchen Yu, A. E. Saddik, C. Theobalt, Eric P. Xing, Shijian Lu Abstract: Open-vocabulary segmentation of 3D scenes is a fundamental function of human perception and thus a crucial objective in computer vision research. However, this task is heavily impeded by the lack of large-scale and diverse 3D open-vocabulary segmentation datasets for training robust and generalizable models. Distilling knowledge from pre-trained 2D open-vocabulary segmentation models helps but it compromises the open-vocabulary feature as the 2D models are mostly finetuned with close-vocabulary datasets. We tackle the challenges in 3D open-vocabulary segmentation by exploiting pre-trained foundation models CLIP and DINO in a weakly supervised manner. Specifically, given only the open-vocabulary text descriptions of the objects in a scene, we distill the open-vocabulary multimodal knowledge and object reasoning capability of CLIP and DINO into a neural radiance field (NeRF), which effectively lifts 2D features into view-consistent 3D segmentation. A notable aspect of our approach is that it does not require any manual segmentation annotations for either the foundation models or the distillation process. Extensive experiments show that our method even outperforms fully supervised models trained with segmentation annotations in certain scenes, suggesting that 3D open-vocabulary segmentation can be effectively learned from 2D images and text-image pairs. Code is available at \\\\url{https://github.com/Kunhao-Liu/3D-OVS}.'),\n",
       " Document(page_content='Author: Eric P. Xing Title: Supplementary Material for KD-DLGAN: Data Limited Image Generation via Knowledge Distillation Publication year: 2023 Coauthors: Kaiwen Cui, Yingchen Yu, Fangneng Zhan, Shengcai Liao, Shijian Lu, Eric P. Xing Abstract: We conduct experiments over multiple widely adopted datasets including: 100-shot, AFHQ, CIFAR-10, CIFAR100 and ImageNet. 100-shot: 100-shot contains three datasets each of which has 100 samples of resolution 256 × 256. The three datasets are 100-shot Obama, 100-shot Grumpy Cat and 100-shot Panda. AFHQ: AFHQ consists of face images of three types of animals including Cat, Dog and Wildlife, each of which has 5k training images. We follow DA [9] and use 160 AFHQCat images and 389 AFHQ-Dog images (at a resolution of 256 × 256) for training. CIFAR-10: CIFAR-10 contains 50k training images and 10k validation images with 10 classes. The image resolution is 32 × 32. In our experiments, three networks are trained with 100%, 20% or 10% training images, respectively, and the trained models are valuated over all the validation images. CIFAR-100: CIFAR-100 contains 50k training images and 10k validation images of 100 classes. The image resolution is 32 × 32. In our experiments, three networks are trained with 100%, 20% or 10% training images, respectively, and the trained models are evaluated over all the validation data.'),\n",
       " Document(page_content='Author: Eric P. Xing Title: Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming Publication year: 2023 Coauthors: Hanlin Zhang, Jiani Huang, Ziyang Li, M. Naik, Eric P. Xing Abstract: Pre-trained large language models (LMs) struggle to perform logical reasoning reliably despite advances in scale and compositionality. In this work, we tackle this challenge through the lens of symbolic programming. We propose DSR-LM, a Differentiable Symbolic Reasoning framework where pre-trained LMs govern the perception of factual knowledge, and a symbolic module performs deductive reasoning. In contrast to works that rely on hand-crafted logic rules, our differentiable symbolic reasoning framework efficiently learns weighted rules and applies semantic loss to further improve LMs. DSR-LM is scalable, interpretable, and allows easy integration of prior knowledge, thereby supporting extensive symbolic programming to robustly derive a logical conclusion. The results of our experiments suggest that DSR-LM improves the logical reasoning abilities of pre-trained language models, resulting in a significant increase in accuracy of over 20% on deductive reasoning benchmarks. Furthermore, DSR-LM outperforms a variety of competitive baselines when faced with systematic changes in sequence length.'),\n",
       " Document(page_content='Author: Eric P. Xing Title: Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models Publication year: 2023 Coauthors: Neha Sengupta, Sunil Kumar Sahu, Bokang Jia, Satheesh Katipomu, Haonan Li, Fajri Koto, Osama Mohammed Afzal, Samta Kamboj, O. Pandit, Rahul Pal, Lalit Pradhan, Zainul Mujahid, Massa Baali, Xudong Han, Alham Fikri Aji, Zhengzhong Liu, Andy Hock, Andrew Feldman, Jonathan Lee, A. Jackson, Preslav Nakov, Timothy Baldwin, Eric P. Xing Abstract: We introduce Jais and Jais-chat, new state-of-the-art Arabic-centric foundation and instruction-tuned open generative large language models (LLMs). The models are based on the GPT-3 decoder-only architecture and are pretrained on a mixture of Arabic and English texts, including source code in various programming languages. With 13 billion parameters, they demonstrate better knowledge and reasoning capabilities in Arabic than any existing open Arabic and multilingual models by a sizable margin, based on extensive evaluation. Moreover, the models are competitive in English compared to English-centric open models of similar size, despite being trained on much less English data. We provide a detailed description of the training, the tuning, the safety alignment, and the evaluation of the models. We release two open versions of the model -- the foundation Jais model, and an instruction-tuned Jais-chat variant -- with the aim of promoting research on Arabic LLMs. Available at https://huggingface.co/inception-mbzuai/jais-13b-chat'),\n",
       " Document(page_content='Author: Eric P. Xing Title: KD-DLGAN: Data Limited Image Generation via Knowledge Distillation Publication year: 2023 Coauthors: Kaiwen Cui, Yingchen Yu, Fangneng Zhan, Shengcai Liao, Shijian Lu1, Eric P. Xing Abstract: Generative Adversarial Networks (GANs) rely heavily on large-scale training data for training high-quality image generation models. With limited training data, the GAN discriminator often suffers from severe overfitting which directly leads to degraded generation especially in generation diversity. Inspired by the recent advances in knowledge distillation (KD), we propose KD-DLGAN, a knowledge-distillation based generation framework that introduces pre-trained vision-language models for training effective data-limited generation models. KD-DLGAN consists of two innovative designs. The first is aggregated generative KD that mitigates the discriminator overfitting by challenging the discriminator with harder learning tasks and distilling more generalizable knowledge from the pre-trained models. The second is correlated generative KD that improves the generation diversity by distilling and preserving the diverse image-text correlation within the pre-trained models. Extensive experiments over multiple benchmarks show that KD-DLGAN achieves superior image generation with limited training data. In addition, KD-DLGAN complements the state-of-the-art with consistent and substantial performance gains. Note that codes will be released.'),\n",
       " Document(page_content='Author: Eric P. Xing Title: 3D Semantic Segmentation in the Wild: Learning Generalized Models for Adverse-Condition Point Clouds Publication year: 2023 Coauthors: Aoran Xiao, Jiaxing Huang, Weihao Xuan, Ruijie Ren, Kangcheng Liu, Dayan Guan, A. E. Saddik, Shijian Lu, Eric P. Xing Abstract: Robust point cloud parsing under all-weather conditions is crucial to level-5 autonomy in autonomous driving. However, how to learn a universal 3D semantic segmentation (3DSS) model is largely neglected as most existing benchmarks are dominated by point clouds captured under normal weather. We introduce SemanticSTF, an adverse-weather point cloud dataset that provides dense point-level annotations and allows to study 3DSS under various adverse weather conditions. We study all-weather 3DSS modeling under two setups: 1) domain adaptive 3DSS that adapts from normal-weather data to adverse-weather data; 2) domain generalizable 3DSS that learns all-weather 3DSS models from normal-weather data. Our studies reveal the challenge while existing 3DSS methods encounter adverse-weather data, showing the great value of SemanticSTF in steering the future endeavor along this very meaningful research direction. In addition, we design a domain randomization technique that alternatively randomizes the geometry styles of point clouds and aggregates their embeddings, ultimately leading to a generalizable model that can improve 3DSS under various adverse weather effectively. The SemanticSTF and related codes are available at https://github.com/xiaoaoran/SemanticSTF.'),\n",
       " Document(page_content='Author: Eric P. Xing Title: Cuttlefish: Low-Rank Model Training without All the Tuning Publication year: 2023 Coauthors: Hongyi Wang, Saurabh Agarwal, Pongsakorn U-chupala, Yoshiki Tanaka, Eric P. Xing, Dimitris Papailiopoulos Abstract: Recent research has shown that training low-rank neural networks can effectively reduce the total number of trainable parameters without sacrificing predictive accuracy, resulting in end-to-end speedups. However, low-rank model training necessitates adjusting several additional factorization hyperparameters, such as the rank of the factorization at each layer. In this paper, we tackle this challenge by introducing Cuttlefish, an automated low-rank training approach that eliminates the need for tuning factorization hyperparameters. Cuttlefish leverages the observation that after a few epochs of full-rank training, the stable rank (i.e., an approximation of the true rank) of each layer stabilizes at a constant value. Cuttlefish switches from full-rank to low-rank training once the stable ranks of all layers have converged, setting the dimension of each factorization to its corresponding stable rank. Our results show that Cuttlefish generates models up to 5.6 times smaller than full-rank models, and attains up to a 1.2 times faster end-to-end training process while preserving comparable accuracy. Moreover, Cuttlefish outperforms state-of-the-art low-rank model training methods and other prominent baselines. The source code for our implementation can be found at: https://github.com/hwang595/Cuttlefish.'),\n",
       " Document(page_content='Author: Eric P. Xing Title: Does compressing activations help model parallel training? Publication year: 2023 Coauthors: S. Bian, Dacheng Li, Hongyi Wang, Eric P. Xing, S. Venkataraman Abstract: Large-scale Transformer models are known for their exceptional performance in a range of tasks, but training them can be difficult due to the requirement for communication-intensive model parallelism. One way to improve training speed is to compress the message size in communication. Previous approaches have primarily focused on compressing gradients in a data parallelism setting, but compression in a model-parallel setting is an understudied area. We have discovered that model parallelism has fundamentally different characteristics than data parallelism. In this work, we present the first empirical study on the effectiveness of compression methods for model parallelism. We implement and evaluate three common classes of compression algorithms - pruning-based, learning-based, and quantization-based - using a popular Transformer training framework. We evaluate these methods across more than 160 settings and 8 popular datasets, taking into account different hyperparameters, hardware, and both fine-tuning and pre-training stages. We also provide analysis when the model is scaled up. Finally, we provide insights for future development of model parallelism compression algorithms.'),\n",
       " Document(page_content='Author: Eric P. Xing Title: Federated Learning as Variational Inference: A Scalable Expectation Propagation Approach Publication year: 2023 Coauthors: Han Guo, P. Greengard, Hongyi Wang, A. Gelman, Yoon Kim, Eric P. Xing Abstract: The canonical formulation of federated learning treats it as a distributed optimization problem where the model parameters are optimized against a global loss function that decomposes across client loss functions. A recent alternative formulation instead treats federated learning as a distributed inference problem, where the goal is to infer a global posterior from partitioned client data (Al-Shedivat et al., 2021). This paper extends the inference view and describes a variational inference formulation of federated learning where the goal is to find a global variational posterior that well-approximates the true posterior. This naturally motivates an expectation propagation approach to federated learning (FedEP), where approximations to the global posterior are iteratively refined through probabilistic message-passing between the central server and the clients. We conduct an extensive empirical study across various algorithmic considerations and describe practical strategies for scaling up expectation propagation to the modern federated setting. We apply FedEP on standard federated learning benchmarks and find that it outperforms strong baselines in terms of both convergence speed and accuracy.'),\n",
       " Document(page_content=\"Author: Eric P. Xing Title: Memory-adaptive Depth-wise Heterogenous Federated Learning Publication year: 2023 Coauthors: Kai Zhang, Yutong Dai, Hongyi Wang, Eric P. Xing, Xun Chen, Lichao Sun Abstract: Federated learning is a promising paradigm that allows multiple clients to collaboratively train a model without sharing the local data. However, the presence of heterogeneous devices in federated learning, such as mobile phones and IoT devices with varying memory capabilities, would limit the scale and hence the performance of the model could be trained. The mainstream approaches to address memory limitations focus on width-slimming techniques, where different clients train subnetworks with reduced widths locally and then the server aggregates the subnetworks. The global model produced from these methods suffers from performance degradation due to the negative impact of the actions taken to handle the varying subnetwork widths in the aggregation phase. In this paper, we introduce a memory-adaptive depth-wise learning solution in FL called FeDepth, which adaptively decomposes the full model into blocks according to the memory budgets of each client and trains blocks sequentially to obtain a full inference model. Our method outperforms state-of-the-art approaches, achieving 5% and more than 10% improvements in top-1 accuracy on CIFAR-10 and CIFAR-100, respectively. We also demonstrate the effectiveness of depth-wise fine-tuning on ViT. Our findings highlight the importance of memory-aware techniques for federated learning with heterogeneous devices and the success of depth-wise training strategy in improving the global model's performance.\"),\n",
       " Document(page_content=\"Author: E. Xing Title: Identification of Nonlinear Latent Hierarchical Models Publication year: 2023 Coauthors: Lingjing Kong, Biwei Huang, Feng Xie, E. Xing, Yuejie Chi, Kun Zhang Abstract: Identifying latent variables and causal structures from observational data is essential to many real-world applications involving biological data, medical data, and unstructured data such as images and languages. However, this task can be highly challenging, especially when observed variables are generated by causally related latent variables and the relationships are nonlinear. In this work, we investigate the identification problem for nonlinear latent hierarchical causal models in which observed variables are generated by a set of causally related latent variables, and some latent variables may not have observed children. We show that the identifiability of causal structures and latent variables (up to invertible transformations) can be achieved under mild assumptions: on causal structures, we allow for multiple paths between any pair of variables in the graph, which relaxes latent tree assumptions in prior work; on structural functions, we permit general nonlinearity and multi-dimensional continuous variables, alleviating existing work's parametric assumptions. Specifically, we first develop an identification criterion in the form of novel identifiability guarantees for an elementary latent variable model. Leveraging this criterion, we show that both causal structures and latent variables of the hierarchical model can be identified asymptotically by explicitly constructing an estimation procedure. To the best of our knowledge, our work is the first to establish identifiability guarantees for both causal structures and latent variables in nonlinear latent hierarchical models.\"),\n",
       " Document(page_content='Author: E. Xing Title: StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields Publication year: 2023 Coauthors: Kunhao Liu, Fangneng Zhan, Yiwen Chen, Jiahui Zhang, Yingchen Yu, Abdulmotaleb El Saddik, Shijian Lu, E. Xing Abstract: 3D style transfer aims to render stylized novel views of a 3D scene with multiview consistency. However, most existing work suffers from a three-way dilemma over accurate geometry reconstruction, high-quality stylization, and being generalizable to arbitrary new styles. We propose StyleRF (Style Radiance Fields), an innovative 3D style transfer technique that resolves the three-way dilemma by performing style transformation within the feature space of a radiance field. StyleRF employs an explicit grid of high-level features to represent 3D scenes, with which highfidelity geometry can be reliably restored via volume rendering. In addition, it transforms the grid features according to the reference style which directly leads to high-quality zero-shot style transfer. StyleRF consists of two innovative designs. The first is sampling-invariant content transformation that makes the transformation invariant to the holistic statistics of the sampled 3D points and accordingly ensures multi-view consistency. The second is deferred style transformation of 2D feature maps which is equivalent to the transformation of 3D points but greatly reduces memory footprint without degrading multi-view consistency. Extensive experiments show that StyleRF achieves superior 3D stylization quality with precise geometry reconstruction and it can generalize to various new styles in a zero-shot manner. Project website: https://kunhao-liu.github.io/StyleRF/'),\n",
       " Document(page_content='Author: E. Xing Title: Judging LLM-as-a-judge with MT-Bench and Chatbot Arena Publication year: 2023 Coauthors: Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, E. Xing, Haotong Zhang, Joseph Gonzalez, I. Stoica Abstract: Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'),\n",
       " Document(page_content='Author: E. Xing Title: Understanding Masked Autoencoders via Hierarchical Latent Variable Models Publication year: 2023 Coauthors: Lingjing Kong, Martin Q. Ma, Guan-Hong Chen, E. Xing, Yuejie Chi, Louis-Philippe Morency, Kun Zhang Abstract: Masked autoencoder (MAE), a simple and effective self-supervised learning framework based on the reconstruction of masked image regions, has recently achieved prominent success in a variety of vision tasks. Despite the emergence of intriguing empirical observations on MAE, a theoretically principled understanding is still lacking. In this work, we formally characterize and justify existing empirical in-sights and provide theoretical guarantees of MAE. We formulate the underlying data-generating process as a hierarchical latent variable model, and show that under reasonable assumptions, MAE provably identifies a set of latent variables in the hierarchical model, explaining why MAE can extract high-level information from pixels. Further, we show how key hyperparameters in MAE (the masking ratio and the patch size) determine which true latent variables to be recovered, therefore influencing the level of semantic information in the representation. Specifically, extremely large or small masking ratios inevitably lead to low-level representations. Our theory offers coherent explanations of existing empirical observations and provides insights for potential empirical improvements and fundamental limitations of the masked-reconstruction paradigm. We conduct extensive experiments to validate our theoretical insights.'),\n",
       " Document(page_content='Author: Eric P. Xing Title: LightSeq: Sequence Level Parallelism for Distributed Training of Long Context Transformers Publication year: 2023 Coauthors: Dacheng Li, Rulin Shao, Anze Xie, Eric P. Xing, Joseph E. Gonzalez, I. Stoica, Xuezhe Ma, Hao Zhang Abstract: Increasing the context length of large language models (LLMs) unlocks fundamentally new capabilities, but also significantly increases the memory footprints of training. Previous model-parallel systems such as Megatron-LM partition and compute different attention heads in parallel, resulting in large communication volumes, so they cannot scale beyond the number of attention heads, thereby hindering its adoption. In this paper, we introduce a new approach, LightSeq, for long-context LLMs training. LightSeq has many notable advantages. First, LightSeq partitions over the sequence dimension, hence is agnostic to model architectures and readily applicable for models with varying numbers of attention heads, such as Multi-Head, Multi-Query and Grouped-Query attention. Second, LightSeq not only requires up to 4.7x less communication than Megatron-LM on popular LLMs but also overlaps the communication with computation. To further reduce the training time, LightSeq features a novel gradient checkpointing scheme to bypass an forward computation for memory-efficient attention. We evaluate LightSeq on Llama-7B and its variants with sequence lengths from 32K to 512K. Through comprehensive experiments on single and cross-node training, we show that LightSeq achieves up to 1.24-2.01x end-to-end speedup, and a 2-8x longer sequence length on models with fewer heads, compared to Megatron-LM. Codes will be available at https://github.com/RulinShao/LightSeq.'),\n",
       " Document(page_content=\"Author: Eric P. Xing Title: LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset Publication year: 2023 Coauthors: Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric P. Xing, Joseph E. Gonzalez, I. Stoica, Haotong Zhang Abstract: Studying how people interact with large language models (LLMs) in real-world scenarios is increasingly important due to their widespread use in various applications. In this paper, we introduce LMSYS-Chat-1M, a large-scale dataset containing one million real-world conversations with 25 state-of-the-art LLMs. This dataset is collected from 210K unique IP addresses in the wild on our Vicuna demo and Chatbot Arena website. We offer an overview of the dataset's content, including its curation process, basic statistics, and topic distribution, highlighting its diversity, originality, and scale. We demonstrate its versatility through four use cases: developing content moderation models that perform similarly to GPT-4, building a safety benchmark, training instruction-following models that perform similarly to Vicuna, and creating challenging benchmark questions. We believe that this dataset will serve as a valuable resource for understanding and advancing LLM capabilities. The dataset is publicly available at https://huggingface.co/datasets/lmsys/lmsys-chat-1m.\"),\n",
       " Document(page_content=\"Author: Eric Xing Title: ALISON: Fast and Effective Stylometric Authorship Obfuscation Publication year: 2024 Coauthors: Eric Xing, Saranya Venkatraman, Thai Le, Dongwon Lee Abstract: Authorship Attribution (AA) and Authorship Obfuscation (AO) are two competing tasks of increasing importance in privacy research. Modern AA leverages an author's consistent writing style to match a text to its author using an AA classifier. AO is the corresponding adversarial task, aiming to modify a text in such a way that its semantics are preserved, yet an AA model cannot correctly infer its authorship. To address privacy concerns raised by state-of-the-art (SOTA) AA methods, new AO methods have been proposed but remain largely impractical to use due to their prohibitively slow training and obfuscation speed, often taking hours. To this challenge, we propose a practical AO method, ALISON, that (1) dramatically reduces training/obfuscation time, demonstrating more than 10x faster obfuscation than SOTA AO methods, (2) achieves better obfuscation success through attacking three transformer-based AA methods on two benchmark datasets, typically performing 15% better than competing methods, (3) does not require direct signals from a target AA classifier during obfuscation, and (4) utilizes unique stylometric features, allowing sound model interpretation for explainable obfuscation. We also demonstrate that ALISON can effectively prevent four SOTA AA methods from accurately determining the authorship of ChatGPT-generated texts, all while minimally changing the original text semantics. To ensure the reproducibility of our findings, our code and data are available at: https://github.com/EricX003/ALISON.\"),\n",
       " Document(page_content='Author: Eric Xing Title: A Study on the Calibration of In-context Learning Publication year: 2023 Coauthors: Hanlin Zhang, Yi-Fan Zhang, Yaodong Yu, Dhruv Madeka, Dean Foster, Eric Xing, Hima Lakkaraju, S. Kakade Abstract: Accurate uncertainty quantification is crucial for the safe deployment of language models (LMs), and prior research has demonstrated improvements in the calibration of modern LMs. Our study focuses on in-context learning (ICL), a prevalent method for adapting static LMs through tailored prompts, and examines the balance between performance and calibration across a broad spectrum of natural language understanding and reasoning tasks. Through comprehensive experiments, we observe that, with an increasing number of ICL examples, models initially exhibit increased miscalibration before achieving better calibration and miscalibration tends to arise in low-shot settings. Moreover, we find that methods aimed at improving usability, such as fine-tuning and chain-of-thought (CoT) prompting, can lead to miscalibration and unreliable natural language explanations, suggesting that new methods may be required for scenarios where models are expected to be reliable.'),\n",
       " Document(page_content=\"Author: Eric Xing Title: SegMix: A Simple Structure-Aware Data Augmentation Method Publication year: 2023 Coauthors: Yuxin Pei, Pushkar Bhuse, Zhengzhong Liu, Eric Xing Abstract: Interpolation-based Data Augmentation (DA) methods (Mixup) linearly interpolate the inputs and labels of two or more training examples. Mixup has more recently been adapted to the field of Natural Language Processing (NLP), mainly for sequence labeling tasks. However, such a simple adoption yields mixed or unstable improvements over the baseline models. We argue that the direct-adoption methods do not account for structures in NLP tasks. To this end, we propose SegMix, a collection of interpolation-based DA algorithms that can adapt to task-specific structures. SegMix poses fewer constraints on data structures, is robust to various hyperparameter settings, applies to more task settings, and adds little computational overhead. In the algorithm's core, we apply interpolation methods on task-specific meaningful segments, in contrast to applying them on sequences as in prior work. We find SegMix to be a flexible framework that combines rule-based DA methods with interpolation-based methods, creating interesting mixtures of DA techniques. We show that SegMix consistently improves performance over strong baseline models in Named Entity Recognition (NER) and Relation Extraction (RE) tasks, especially under data-scarce settings. Furthermore, this method is easy to implement and adds negligible training overhead.\"),\n",
       " Document(page_content=\"Author: Eric Xing Title: Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large Language Models Publication year: 2024 Coauthors: Loka Li, Guan-Hong Chen, Yusheng Su, Zhenhao Chen, Yixuan Zhang, Eric Xing, Kun Zhang Abstract: The recent success of Large Language Models (LLMs) has catalyzed an increasing interest in their self-correction capabilities. This paper presents a comprehensive investigation into the intrinsic self-correction of LLMs, attempting to address the ongoing debate about its feasibility. Our research has identified an important latent factor - the ``confidence'' of LLMs - during the self-correction process. Overlooking this factor may cause the models to over-criticize themselves, resulting in unreliable conclusions regarding the efficacy of self-correction. We have experimentally observed that LLMs possess the capability to understand the ``confidence'' in their own responses. It motivates us to develop an ``If-or-Else'' (IoE) prompting framework, designed to guide LLMs in assessing their own ``confidence'', facilitating intrinsic self-corrections. We conduct extensive experiments and demonstrate that our IoE-based Prompt can achieve a consistent improvement regarding the accuracy of self-corrected responses over the initial answers. Our study not only sheds light on the underlying factors affecting self-correction in LLMs, but also introduces a practical framework that utilizes the IoE prompting principle to efficiently improve self-correction capabilities with ``confidence''. The code is available at \\\\url{https://github.com/MBZUAI-CLeaR/IoE-Prompting.git}.\"),\n",
       " Document(page_content='Author: Eric Xing Title: Temporally Disentangled Representation Learning under Unknown Nonstationarity Publication year: 2023 Coauthors: Xiangchen Song, Weiran Yao, Yewen Fan, Xinshuai Dong, Guan-Hong Chen, Juan Carlos Niebles, Eric Xing, Kun Zhang Abstract: In unsupervised causal representation learning for sequential data with time-delayed latent causal influences, strong identifiability results for the disentanglement of causally-related latent variables have been established in stationary settings by leveraging temporal structure. However, in nonstationary setting, existing work only partially addressed the problem by either utilizing observed auxiliary variables (e.g., class labels and/or domain indexes) as side information or assuming simplified latent causal dynamics. Both constrain the method to a limited range of scenarios. In this study, we further explored the Markov Assumption under time-delayed causally related process in nonstationary setting and showed that under mild conditions, the independent latent components can be recovered from their nonlinear mixture up to a permutation and a component-wise transformation, without the observation of auxiliary variables. We then introduce NCTRL, a principled estimation framework, to reconstruct time-delayed latent causal variables and identify their relations from measured sequential data only. Empirical evaluations demonstrated the reliable identification of time-delayed latent causal influences, with our methodology substantially outperforming existing baselines that fail to exploit the nonstationarity adequately and then, consequently, cannot distinguish distribution shifts.'),\n",
       " Document(page_content='Author: Eric P. Xing Title: LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning Publication year: 2023 Coauthors: Han Guo, P. Greengard, Eric P. Xing, Yoon Kim Abstract: We propose a simple approach for memory-efficient adaptation of pretrained language models. Our approach uses an iterative algorithm to decompose each pretrained matrix into a high-precision low-rank component and a memory-efficient quantized component. During finetuning, the quantized component remains fixed and only the low-rank component is updated. We present an integer linear programming formulation of the quantization component which enables dynamic configuration of quantization parameters (e.g., bit-width, block size) for each matrix given an overall target memory budget. We further explore a data-aware version of the algorithm which uses an approximation of the Fisher information matrix to weight the reconstruction objective during matrix decomposition. Experiments on finetuning RoBERTa and LLaMA-2 (7B and 70B) demonstrate that our low-rank plus quantized matrix decomposition approach (LQ-LoRA) outperforms strong QLoRA and GPTQ-LoRA baselines and enables aggressive quantization to sub-3 bits with only minor performance degradations. When finetuned on a language modeling calibration dataset, LQ-LoRA can also be used for model compression; in this setting our 2.75-bit LLaMA-2-70B model (which has 2.85 bits on average when including the low-rank components and requires 27GB of GPU memory) performs respectably compared to the 16-bit baseline.'),\n",
       " Document(page_content='Author: Bhiksha Ramakrishnan Title: Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive Instruction-Tuning Benchmark for Speech Publication year: 2023 Coauthors: Chien-yu Huang, Ke-Han Lu, Shi Wang, Chi-Yuan Hsiao, Chun-Yi Kuan, Haibin Wu, Siddhant Arora, Kai-Wei Chang, Jiatong Shi, Yifan Peng, Roshan Sharma, Shinji Watanabe, Bhiksha Ramakrishnan, Shady Shehata, Hung-yi Lee Abstract: Text language models have shown remarkable zero-shot capability in generalizing to unseen tasks when provided with well-formulated instructions. However, existing studies in speech processing primarily focus on limited or specific tasks. Moreover, the lack of standardized benchmarks hinders a fair comparison across different approaches. Thus, we present Dynamic-SUPERB, a benchmark designed for building universal speech models capable of leveraging instruction tuning to perform multiple tasks in a zero-shot fashion. To achieve comprehensive coverage of diverse speech tasks and harness instruction tuning, we invite the community to collaborate and contribute, facilitating the dynamic growth of the benchmark. To initiate, Dynamic-SUPERB features 55 evaluation instances by combining 33 tasks and 22 datasets. This spans a broad spectrum of dimensions, providing a comprehensive platform for evaluation. Additionally, we propose several approaches to establish benchmark baselines. These include the utilization of speech models, text language models, and the multimodal encoder. Evaluation results indicate that while these baselines perform reasonably on seen tasks, they struggle with unseen ones. We also conducted an ablation study to assess the robustness and seek improvements in the performance. We release all materials to the public and welcome researchers to collaborate on the project, advancing technologies in the field together.'),\n",
       " Document(page_content='Author: C. Rosé Title: High school students’ data modeling practices and processes: from modeling unstructured data to evaluating automated decisions Publication year: 2023 Coauthors: Shiyan Jiang, Hengtao Tang, Can Tatar, C. Rosé, J. Chao Abstract: ABSTRACT It’s critical to foster artificial intelligence (AI) literacy for high school students, the first generation to grow up surrounded by AI, to understand working mechanism of data-driven AI technologies and critically evaluate automated decisions from predictive models. While efforts have been made to engage youth in understanding AI through developing machine learning models, few provided in-depth insights into the nuanced learning processes. In this study, we examined high school students’ data modeling practices and processes. Twenty-eight students developed machine learning models with text data for classifying negative and positive reviews of ice cream stores. We identified nine data modeling practices that describe students’ processes of model exploration, development, and testing and two themes about evaluating automated decisions from data technologies. The results provide implications for designing accessible data modeling experiences for students to understand data justice as well as the role and responsibility of data modelers in creating AI technologies.'),\n",
       " Document(page_content='Author: C. Rosé Title: Linguistic representations for fewer-shot relation extraction across domains Publication year: 2023 Coauthors: Sireesh Gururaja, Ritam Dutt, Ting-gen Liao, C. Rosé Abstract: Recent work has demonstrated the positive impact of incorporating linguistic representations as additional context and scaffolds on the in-domain performance of several NLP tasks. We extend this work by exploring the impact of linguistic representations on cross-domain performance in a few-shot transfer setting. An important question is whether linguistic representations enhance generalizability by providing features that function as cross-domain pivots. We focus on the task of relation extraction on three datasets of procedural text in two domains, cooking and materials science. Our approach augments a popular transformer-based architecture by alternately incorporating syntactic and semantic graphs constructed by freely available off-the-shelf tools. We examine their utility for enhancing generalization, and investigate whether earlier findings, e.g. that semantic representations can be more helpful than syntactic ones, extend to relation extraction in multiple domains. We find that while the inclusion of these graphs results in significantly higher performance in few-shot transfer, both types of graph exhibit roughly equivalent utility.'),\n",
       " Document(page_content='Author: C. Rosé Title: Using counterfactual contrast to improve compositional generalization for multi-step quantitative reasoning Publication year: 2023 Coauthors: Armineh Nourbakhsh, Sameena Shah, C. Rosé Abstract: In quantitative question answering, compositional generalization is one of the main challenges of state of the art models, especially when longer sequences of reasoning steps are required. In this paper we propose CounterComp, a method that uses counterfactual scenarios to generate samples with compositional contrast. Instead of a data augmentation approach, CounterComp is based on metric learning, which allows for direct sampling from the training set and circumvents the need for additional human labels. Our proposed auxiliary metric learning loss improves the performance of three state of the art models on four recently released datasets. We also show how the approach can improve OOD performance on unseen domains, as well as unseen compositions. Lastly, we demonstrate how the method can lead to better compositional attention patterns during training.'),\n",
       " Document(page_content='Author: C. Rosé Title: Towards Extracting and Understanding the Implicit Rubrics of Transformer Based Automatic Essay Scoring Models Publication year: 2023 Coauthors: James Fiacco, David Adamson, C. Rosé Abstract: By aligning the functional components derived from the activations of transformer models trained for AES with external knowledge such as human-understandable feature groups, the proposed method improves the interpretability of a Longformer Automatic Essay Scoring (AES) system and provides tools for performing such analyses on further neural AES systems. The analysis focuses on models trained to score essays based on organization, main idea, support, and language. The findings provide insights into the models’ decision-making processes, biases, and limitations, contributing to the development of more transparent and reliable AES systems.'),\n",
       " Document(page_content='Author: C. Rosé Title: Exploring Artificial Intelligence in English Language Arts with StoryQ Publication year: 2023 Coauthors: J. Chao, Rebecca Ellis, Shiyan Jiang, C. Rosé, W. Finzer, Can Tatar, James Fiacco, Kenia Wiedemann Abstract: Exploring Artificial Intelligence (AI) in English Language Arts (ELA) with StoryQ is a 10-hour curriculum module designed for high school ELA classes. The module introduces students to fundamental AI concepts and essential machine learning workflow using StoryQ, a web-based GUI environment for Grades 6-12 learners. In this module, students work with unstructured text data and learn to train, test, and improve text classification models such as intent recognition, clickbait filter, and sentiment analysis. As they interact with machine-learning language models deeply, students also gain a nuanced understanding of language and how to wield it, not just as a data structure, but as a tool in our human-human encounters as well. The current version contains eight lessons, all delivered through a full-featured online learning and teaching platform. Computers and Internet access are required to implement the module. The module was piloted in an ELA class in the Spring of 2022, and the student learning outcomes were positive. The module is currently undergoing revision and will be further tested and improved in Fall 2022.'),\n",
       " Document(page_content='Author: Carolyn Rosé Title: SPEERLoom: An Open-Source Loom Kit for Interdisciplinary Engagement in Math, Engineering, and Textiles Publication year: 2023 Coauthors: Samantha Speer, Ana P Garcia-Alonzo, Joey Huang, N. Yankova, Carolyn Rosé, Kylie A Peppler, James Mccann, Melisa Orta Martinez Abstract: Weaving is a fabrication process that is grounded in mathematics and engineering: from the binary, matrix-like nature of the pattern drafts weavers have used for centuries, to the punch card programming of the first Jacquard looms. This intersection of disciplines provides an opportunity to ground abstract mathematical concepts in a concrete and embodied art, viewing this textile art through the lens of engineering. Currently, available looms are not optimized to take advantage of this opportunity to increase mathematics learning by providing hands-on interdisciplinary learning in collegiate classrooms. In this work, we present SPEERLoom: an open-source, robotic Jacquard loom kit designed to be a tool for interweaving cloth fabrication, mathematics, and engineering to support interdisciplinary learning in the classroom. We discuss the design requirements and subsequent design of SPEERLoom. We also present the results of a pilot study in a post-secondary class finding that SPEERLoom supports hands-on, interdisciplinary learning of math, engineering, and textiles.'),\n",
       " Document(page_content='Author: Alexander I. Rudnicky Title: Overview of the Tenth Dialog System Technology Challenge: DSTC10 Publication year: 2024 Coauthors: Koichiro Yoshino, Yun-Nung (Vivian) Chen, Paul A. Crook, Satwik Kottur, Jinchao Li, Behnam Hedayatnia, Seungwhan Moon, Zhengcong Fei, Zekang Li, Jinchao Zhang, Yang Feng, Jie Zhou, Seokhwan Kim, Yang Liu, Di Jin, A. Papangelis, Karthik Gopalakrishnan, Dilek Z. Hakkani-Tür, B. Damavandi, A. Geramifard, Chiori Hori, Ankit Shah, Chen Zhang, Haizhou Li, João Sedoc, L. F. D’Haro, Rafael E. Banchs, Alexander I. Rudnicky Abstract: This article introduces the Tenth Dialog System Technology Challenge (DSTC-10). This edition of the DSTC focuses on applying end-to-end dialog technologies for five distinct tasks in dialog systems, namely 1. Incorporation of Meme images into open domain dialogs, 2. Knowledge-grounded Task-oriented Dialogue Modeling on Spoken Conversations, 3. Situated Interactive Multimodal dialogs, 4. Reasoning for Audio Visual Scene-Aware Dialog, and 5. Automatic Evaluation and Moderation of Open-domainDialogue Systems. This article describes the task definition, provided datasets, baselines, and evaluation setup for each track. We also summarize the results of the submitted systems to highlight the general trends of the state-of-the-art technologies for the tasks.'),\n",
       " Document(page_content='Author: Alexander I. Rudnicky Title: Advancing Regular Language Reasoning in Linear Recurrent Neural Networks Publication year: 2023 Coauthors: Ting-Han Fan, Ta-Chung Chi, Alexander I. Rudnicky Abstract: In recent studies, linear recurrent neural networks (LRNNs) have achieved Transformer-level performance in natural language modeling and long-range modeling while offering rapid parallel training and constant inference costs. With the resurged interest in LRNNs, we study whether they can learn the hidden rules in training sequences, such as the grammatical structures of regular language. We theoretically analyze some existing LRNNs and discover their limitations on regular language. Motivated by the analysis, we propose a new LRNN equipped with a block-diagonal and input-dependent transition matrix. Experiments suggest that the proposed model is the only LRNN that can perform length extrapolation on regular language tasks such as Sum, Even Pair, and Modular Arithmetic.'),\n",
       " Document(page_content='Author: Alexander I. Rudnicky Title: Structured Dialogue Discourse Parsing Publication year: 2023 Coauthors: Ta-Chung Chi, Alexander I. Rudnicky Abstract: Dialogue discourse parsing aims to uncover the internal structure of a multi-participant conversation by finding all the discourse links and corresponding relations. Previous work either treats this task as a series of independent multiple-choice problems, in which the link existence and relations are decoded separately, or the encoding is restricted to only local interaction, ignoring the holistic structural information. In contrast, we propose a principled method that improves upon previous work from two perspectives: encoding and decoding. From the encoding side, we perform structured encoding on the adjacency matrix followed by the matrix-tree learning algorithm, where all discourse links and relations in the dialogue are jointly optimized based on latent tree-level distribution. From the decoding side, we perform structured inference using the modified Chiu-Liu-Edmonds algorithm, which explicitly generates the labeled multi-root non-projective spanning tree that best captures the discourse structure. In addition, unlike in previous work, we do not rely on hand-crafted features; this improves the model’s robustness. Experiments show that our method achieves new state-of-the-art, surpassing the previous model by 2.3 on STAC and 1.5 on Molweni (F1 scores).'),\n",
       " Document(page_content='Author: Alexander I. Rudnicky Title: A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech Publication year: 2023 Coauthors: Li-Wei Chen, Shinji Watanabe, Alexander I. Rudnicky Abstract: Recent Text-to-Speech (TTS) systems trained on reading or acted corpora have achieved near human-level naturalness. The diversity of human speech, however, often goes beyond the coverage of these corpora. We believe the ability to handle such diversity is crucial for AI systems to achieve human-level communication. Our work explores the use of more abundant real-world data for building speech synthesizers. We train TTS systems using real-world speech from YouTube and podcasts. We observe the mismatch between training and inference alignments in mel-spectrogram based autoregressive models, leading to unintelligible synthesis, and demonstrate that learned discrete codes within multiple code groups effectively resolves this issue. We introduce our MQTTS system whose architecture is designed for multiple code generation and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality. We conduct ablation analyses to identify the efficacy of our methods. We show that MQTTS outperforms existing TTS systems in several objective and subjective measures.'),\n",
       " Document(page_content='Author: Alexander I. Rudnicky Title: Tartan: an LLM Driven SocialBot Publication year: 2023 Coauthors: Liangze Li, Ziyuan Liu, Li-Wei Chen, Ta-Chung Chi, Alexander I. Rudnicky Abstract: We describe our work to date on using large language models (LLMs) in our Alexa Prize Socialbot. Our work has laid the groundwork for looking more closely at using LLMs in a conversational system. We also analyze common patterns in conversations our bot has had with users'),\n",
       " Document(page_content=\"Author: Alexander I. Rudnicky Title: Overview of Robust and Multilingual Automatic Evaluation Metrics\\n\\nfor Open-Domain Dialogue Systems at DSTC 11 Track 4 Publication year: 2023 Coauthors: Mario Rodr'iguez-Cantelar, Chen Zhang, Chengguang Tang, Ke Shi, Sarik Ghazarian, João Sedoc, L. F. D’Haro, Alexander I. Rudnicky Abstract: The advent and fast development of neural networks have revolutionized the research on dialogue systems and subsequently have triggered various challenges regarding their automatic evaluation. Automatic evaluation of open-domain dialogue systems as an open challenge has been the center of the attention of many researchers. Despite the consistent efforts to improve automatic metrics’ correlations with human evaluation, there have been very few attempts to assess their robustness over multiple domains and dimensions. Also, their focus is mainly on the English language. All of these challenges prompt the development of automatic evaluation metrics that are reliable in various domains, dimensions, and languages. This track in the 11th Dialogue System Technology Challenge (DSTC11) is part of the ongoing effort to promote robust and multilingual automatic evaluation metrics. This article describes the datasets and baselines provided to participants and discusses the submission and result details of the two proposed subtasks.\"),\n",
       " Document(page_content='Author: Alexander I. Rudnicky Title: Learning to Ask Questions for Zero-shot Dialogue State Tracking Publication year: 2023 Coauthors: Diogo Tavares, David Semedo, Alexander I. Rudnicky, João Magalhães Abstract: We present a method for performing zero-shot Dialogue State Tracking (DST) by casting the task as a learning-to-ask-questions framework. The framework learns to pair the best question generation (QG) strategy with in-domain question answering (QA) methods to extract slot values from a dialogue without any human intervention. A novel self-supervised QA pretraining step using in-domain data is essential to learn the structure without requiring any slot-filling annotations. Moreover, we show that QG methods need to be aligned with the same grammatical person used in the dialogue. Empirical evaluation on the MultiWOZ 2.1 dataset demonstrates that our approach, when used alongside robust QA models, outperforms existing zero-shot methods in the challenging task of zero-shot cross domain adaptation-given a comparable amount of domain knowledge during data creation. Finally, we analyze the impact of the types of questions used, and demonstrate that the algorithmic approach outperforms template-based question generation.'),\n",
       " Document(page_content='Author: A. Rudnicky Title: Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings Publication year: 2023 Coauthors: Ta-Chung Chi, Ting-Han Fan, Li-Wei Chen, A. Rudnicky, P. Ramadge Abstract: The use of positional embeddings in transformer language models is widely accepted. However, recent research has called into question the necessity of such embeddings. We further extend this inquiry by demonstrating that a randomly initialized and frozen transformer language model, devoid of positional embeddings, inherently encodes strong positional information through the shrinkage of self-attention variance. To quantify this variance, we derive the underlying distribution of each step within a transformer layer. Through empirical validation using a fully pretrained model, we show that the variance shrinkage effect still persists after extensive gradient updates. Our findings serve to justify the decision to discard positional embeddings and thus facilitate more efficient pretraining of transformer language models.'),\n",
       " Document(page_content='Author: A. Rudnicky Title: Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation Publication year: 2023 Coauthors: Ta-Chung Chi, Ting-Han Fan, A. Rudnicky, P. Ramadge Abstract: Unlike recurrent models, conventional wisdom has it that Transformers cannot perfectly model regular languages. Inspired by the notion of working memory, we propose a new Transformer variant named RegularGPT. With its novel combination of Weight-Sharing, Adaptive-Depth, and Sliding-Dilated-Attention, RegularGPT constructs working memory along the depth dimension, thereby enabling efficient and successful modeling of regular languages such as PARITY. We further test RegularGPT on the task of natural language length extrapolation and surprisingly find that it rediscovers the local windowed attention effect deemed necessary in prior work for length extrapolation.'),\n",
       " Document(page_content=\"Author: Maarten Sap Title: Relying on the Unreliable: The Impact of Language Models' Reluctance to Express Uncertainty Publication year: 2024 Coauthors: Kaitlyn Zhou, Jena D. Hwang, Xiang Ren, Maarten Sap Abstract: As natural language becomes the default interface for human-AI interaction, there is a critical need for LMs to appropriately communicate uncertainties in downstream applications. In this work, we investigate how LMs incorporate confidence about their responses via natural language and how downstream users behave in response to LM-articulated uncertainties. We examine publicly deployed models and find that LMs are unable to express uncertainties when answering questions even when they produce incorrect responses. LMs can be explicitly prompted to express confidences, but tend to be overconfident, resulting in high error rates (on average 47%) among confident responses. We test the risks of LM overconfidence by running human experiments and show that users rely heavily on LM generations, whether or not they are marked by certainty. Lastly, we investigate the preference-annotated datasets used in RLHF alignment and find that humans have a bias against texts with uncertainty. Our work highlights a new set of safety harms facing human-LM interactions and proposes design recommendations and mitigating strategies moving forward.\"),\n",
       " Document(page_content='Author: Maarten Sap Title: FANToM: A Benchmark for Stress-testing Machine Theory of Mind in Interactions Publication year: 2023 Coauthors: Hyunwoo Kim, Melanie Sclar, Xuhui Zhou, R. L. Bras, Gunhee Kim, Yejin Choi, Maarten Sap Abstract: Theory of mind (ToM) evaluations currently focus on testing models using passive narratives that inherently lack interactivity. We introduce FANToM, a new benchmark designed to stress-test ToM within information-asymmetric conversational contexts via question answering. Our benchmark draws upon important theoretical requisites from psychology and necessary empirical considerations when evaluating large language models (LLMs). In particular, we formulate multiple types of questions that demand the same underlying reasoning to identify illusory or false sense of ToM capabilities in LLMs. We show that FANToM is challenging for state-of-the-art LLMs, which perform significantly worse than humans even with chain-of-thought reasoning or fine-tuning.'),\n",
       " Document(page_content=\"Author: Maarten Sap Title: Modeling Empathic Similarity in Personal Narratives Publication year: 2023 Coauthors: Jocelyn Shen, Maarten Sap, Pedro Colon-Hernandez, Hae Won Park, C. Breazeal Abstract: The most meaningful connections between people are often fostered through expression of shared vulnerability and emotional experiences in personal narratives. We introduce a new task of identifying similarity in personal stories based on empathic resonance, i.e., the extent to which two people empathize with each others' experiences, as opposed to raw semantic or lexical similarity, as has predominantly been studied in NLP. Using insights from social psychology, we craft a framework that operationalizes empathic similarity in terms of three key features of stories: main events, emotional trajectories, and overall morals or takeaways. We create EmpathicStories, a dataset of 1,500 personal stories annotated with our empathic similarity features, and 2,000 pairs of stories annotated with empathic similarity scores. Using our dataset, we fine-tune a model to compute empathic similarity of story pairs, and show that this outperforms semantic similarity models on automated correlation and retrieval metrics. Through a user study with 150 participants, we also assess the effect our model has on retrieving stories that users empathize with, compared to naive semantic similarity-based retrieval, and find that participants empathized significantly more with stories retrieved by our model. Our work has strong implications for the use of empathy-aware models to foster human connection and empathy between people.\"),\n",
       " Document(page_content='Author: Maarten Sap Title: COBRA Frames: Contextual Reasoning about Effects and Harms of Offensive Statements Publication year: 2023 Coauthors: Xuhui Zhou, Haojie Zhu, Akhila Yerukola, Thomas Davidson, Jena D. Hwang, Swabha Swayamdipta, Maarten Sap Abstract: Warning: This paper contains content that may be offensive or upsetting. Understanding the harms and offensiveness of statements requires reasoning about the social and situational context in which statements are made. For example, the utterance\"your English is very good\"may implicitly signal an insult when uttered by a white man to a non-white colleague, but uttered by an ESL teacher to their student would be interpreted as a genuine compliment. Such contextual factors have been largely ignored by previous approaches to toxic language detection. We introduce COBRA frames, the first context-aware formalism for explaining the intents, reactions, and harms of offensive or biased statements grounded in their social and situational context. We create COBRACORPUS, a dataset of 33k potentially offensive statements paired with machine-generated contexts and free-text explanations of offensiveness, implied biases, speaker intents, and listener reactions. To study the contextual dynamics of offensiveness, we train models to generate COBRA explanations, with and without access to the context. We find that explanations by context-agnostic models are significantly worse than by context-aware ones, especially in situations where the context inverts the statement\\'s offensiveness (29% accuracy drop). Our work highlights the importance and feasibility of contextualized NLP by modeling social factors.'),\n",
       " Document(page_content='Author: Maarten Sap Title: Riveter: Measuring Power and Social Dynamics Between Entities Publication year: 2023 Coauthors: Maria Antoniak, Anjalie Field, Jimin Mun, Melanie Walsh, Lauren F. Klein, Maarten Sap Abstract: Riveter provides a complete easy-to-use pipeline for analyzing verb connotations associated with entities in text corpora. We prepopulate the package with connotation frames of sentiment, power, and agency, which have demonstrated usefulness for capturing social phenomena, such as gender bias, in a broad range of corpora. For decades, lexical frameworks have been foundational tools in computational social science, digital humanities, and natural language processing, facilitating multifaceted analysis of text corpora. But working with verb-centric lexica specifically requires natural language processing skills, reducing their accessibility to other researchers. By organizing the language processing pipeline, providing complete lexicon scores and visualizations for all entities in a corpus, and providing functionality for users to target specific research questions, Riveter greatly improves the accessibility of verb lexica and can facilitate a broad range of future research.'),\n",
       " Document(page_content=\"Author: Maarten Sap Title: Don't Take This Out of Context!: On the Need for Contextual Models and Evaluations for Stylistic Rewriting Publication year: 2023 Coauthors: Akhila Yerukola, Xuhui Zhou, Elizabeth Clark, Maarten Sap \"),\n",
       " Document(page_content=\"Author: Maarten Sap Title: Beyond Denouncing Hate: Strategies for Countering Implied Biases and Stereotypes in Language Publication year: 2023 Coauthors: Jimin Mun, Emily Allaway, Akhila Yerukola, Laura Vianna, Sarah-Jane Leslie, Maarten Sap Abstract: Counterspeech, i.e., responses to counteract potential harms of hateful speech, has become an increasingly popular solution to address online hate speech without censorship. However, properly countering hateful language requires countering and dispelling the underlying inaccurate stereotypes implied by such language. In this work, we draw from psychology and philosophy literature to craft six psychologically inspired strategies to challenge the underlying stereotypical implications of hateful language. We first examine the convincingness of each of these strategies through a user study, and then compare their usages in both human- and machine-generated counterspeech datasets. Our results show that human-written counterspeech uses countering strategies that are more specific to the implied stereotype (e.g., counter examples to the stereotype, external factors about the stereotype's origins), whereas machine-generated counterspeech uses less specific strategies (e.g., generally denouncing the hatefulness of speech). Furthermore, machine-generated counterspeech often employs strategies that humans deem less convincing compared to human-produced counterspeech. Our findings point to the importance of accounting for the underlying stereotypical implications of speech when generating counterspeech and for better machine reasoning about anti-stereotypical examples.\"),\n",
       " Document(page_content='Author: Maarten Sap Title: BiasX: \"Thinking Slow\" in Toxic Content Moderation with Explanations of Implied Social Biases Publication year: 2023 Coauthors: Yiming Zhang, Sravani Nanduri, Liwei Jiang, Tongshuang Wu, Maarten Sap Abstract: Toxicity annotators and content moderators often default to mental shortcuts when making decisions. This can lead to subtle toxicity being missed, and seemingly toxic but harmless content being over-detected. We introduce BiasX, a framework that enhances content moderation setups with free-text explanations of statements\\' implied social biases, and explore its effectiveness through a large-scale crowdsourced user study. We show that indeed, participants substantially benefit from explanations for correctly identifying subtly (non-)toxic content. The quality of explanations is critical: imperfect machine-generated explanations (+2.4% on hard toxic examples) help less compared to expert-written human explanations (+7.2%). Our results showcase the promise of using free-text explanations to encourage more thoughtful toxicity moderation.'),\n",
       " Document(page_content='Author: Maarten Sap Title: Can LLMs Keep a Secret? Testing Privacy Implications of Language Models via Contextual Integrity Theory Publication year: 2023 Coauthors: Niloofar Mireshghallah, Hyunwoo Kim, Xuhui Zhou, Yulia Tsvetkov, Maarten Sap, Reza Shokri, Yejin Choi Abstract: The interactive use of large language models (LLMs) in AI assistants (at work, home, etc.) introduces a new set of inference-time privacy risks: LLMs are fed different types of information from multiple sources in their inputs and are expected to reason about what to share in their outputs, for what purpose and with whom, within a given context. In this work, we draw attention to the highly critical yet overlooked notion of contextual privacy by proposing ConfAIde, a benchmark designed to identify critical weaknesses in the privacy reasoning capabilities of instruction-tuned LLMs. Our experiments show that even the most capable models such as GPT-4 and ChatGPT reveal private information in contexts that humans would not, 39% and 57% of the time, respectively. This leakage persists even when we employ privacy-inducing prompts or chain-of-thought reasoning. Our work underscores the immediate need to explore novel inference-time privacy-preserving approaches, based on reasoning and theory of mind.'),\n",
       " Document(page_content=\"Author: Maarten Sap Title: Improving Language Models with Advantage-based Offline Policy Gradients Publication year: 2023 Coauthors: Ashutosh Baheti, Ximing Lu, Faeze Brahman, Ronan Le Bras, Maarten Sap, Mark O. Riedl Abstract: Language Models (LMs) achieve substantial language capabilities when finetuned using Reinforcement Learning with Human Feedback (RLHF). However, RLHF is an unstable and data-hungry process that continually requires new high-quality LM-generated data for finetuning. We introduce Advantage-Leftover Lunch RL (A-LoL), a new class of offline policy gradient algorithms that enable RL training on any pre-existing data. By assuming the entire LM output sequence as a single action, A-LoL allows incorporating sequence-level classifiers or human-designed scoring functions as rewards. Subsequently, by using LM's internal sequence-level value estimate, A-LoL filters negative advantage (low-quality) data points during training, making it resilient to noise. Overall, A-LoL is an easy-to-implement LM training recipe that is sample-efficient and stable. We demonstrate the effectiveness of A-LoL and its variants with a set of four different language generation tasks. We compare against both online RL (PPO) and recent preference-based (DPO, PRO) and reward-based (GOLD) offline RL baselines. On the commonly-used RLHF benchmark, Helpful and Harmless Assistant (HHA), LMs trained with A-LoL methods achieve the highest diversity while also being rated more safe and helpful than baselines according to humans. Additionally, in the remaining three tasks, A-LoL could optimize multiple distinct reward functions even when using noisy or suboptimal training data. We also release our experimental code. https://github.com/abaheti95/LoL-RL\"),\n",
       " Document(page_content='Author: Maarten Sap Title: From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models Publication year: 2023 Coauthors: Julia Mendelsohn, Ronan Le Bras, Yejin Choi, Maarten Sap Abstract: Dogwhistles are coded expressions that simultaneously convey one meaning to a broad audience and a second, often hateful or provocative, meaning to a narrow in-group; they are deployed to evade both political repercussions and algorithmic content moderation. For example, the word “cosmopolitan” in a sentence such as “we need to end the cosmopolitan experiment” can mean “worldly” to many but also secretly mean “Jewish” to a select few. We present the first large-scale computational investigation of dogwhistles. We develop a typology of dogwhistles, curate the largest-to-date glossary of over 300 dogwhistles with rich contextual information and examples, and analyze their usage in historical U.S. politicians’ speeches. We then assess whether a large language model (GPT-3) can identify dogwhistles and their meanings, and find that GPT-3’s performance varies widely across types of dogwhistles and targeted groups. Finally, we show that harmful content containing dogwhistles avoids toxicity detection, highlighting online risks presented by such coded language. This work sheds light on the theoretical and applied importance of dogwhistles in both NLP and computational social science, and provides resources to facilitate future research in modeling dogwhistles and mitigating their online harms.'),\n",
       " Document(page_content='Author: Maarten Sap Title: NLPositionality: Characterizing Design Biases of Datasets and Models Publication year: 2023 Coauthors: Sebastin Santy, Jenny T Liang, Ronan Le Bras, Katharina Reinecke, Maarten Sap Abstract: Design biases in NLP systems, such as performance differences for different populations, often stem from their creator’s positionality, i.e., views and lived experiences shaped by identity and background. Despite the prevalence and risks of design biases, they are hard to quantify because researcher, system, and dataset positionality is often unobserved. We introduce NLPositionality, a framework for characterizing design biases and quantifying the positionality of NLP datasets and models. Our framework continuously collects annotations from a diverse pool of volunteer participants on LabintheWild, and statistically quantifies alignment with dataset labels and model predictions. We apply NLPositionality to existing datasets and models for two tasks—social acceptability and hate speech detection. To date, we have collected 16,299 annotations in over a year for 600 instances from 1,096 annotators across 87 countries.We find that datasets and models align predominantly with Western, White, college-educated, and younger populations. Additionally, certain groups, such as non-binary people and non-native English speakers, are further marginalized by datasets and models as they rank least in alignment across all tasks. Finally, we draw from prior literature to discuss how researchers can examine their own positionality and that of their datasets and models, opening the door for more inclusive NLP systems.'),\n",
       " Document(page_content=\"Author: Maarten Sap Title: Don't Take This Out of Context! On the Need for Contextual Models and Evaluations for Stylistic Rewriting Publication year: 2023 Coauthors: Akhila Yerukola, Xuhui Zhou, Maarten Sap Abstract: Most existing stylistic text rewriting methods and evaluation metrics operate on a sentence level, but ignoring the broader context of the text can lead to preferring generic, ambiguous, and incoherent rewrites. In this paper, we investigate integrating the preceding textual context into both the $\\\\textit{rewriting}$ and $\\\\textit{evaluation}$ stages of stylistic text rewriting, and introduce a new composite contextual evaluation metric $\\\\texttt{CtxSimFit}$ that combines similarity to the original sentence with contextual cohesiveness. We comparatively evaluate non-contextual and contextual rewrites in formality, toxicity, and sentiment transfer tasks. Our experiments show that humans significantly prefer contextual rewrites as more fitting and natural over non-contextual ones, yet existing sentence-level automatic metrics (e.g., ROUGE, SBERT) correlate poorly with human preferences ($\\\\rho$=0--0.3). In contrast, human preferences are much better reflected by both our novel $\\\\texttt{CtxSimFit}$ ($\\\\rho$=0.7--0.9) as well as proposed context-infused versions of common metrics ($\\\\rho$=0.4--0.7). Overall, our findings highlight the importance of integrating context into the generation and especially the evaluation stages of stylistic text rewriting.\"),\n",
       " Document(page_content='Author: Maarten Sap Title: Where Do People Tell Stories Online? Story Detection Across Online Communities Publication year: 2023 Coauthors: Maria Antoniak, Joel Mire, Maarten Sap, Elliott Ash, Andrew Piper Abstract: People share stories online for a myriad of purposes, whether as a means of self-disclosure, processing difficult personal experiences, providing needed information or entertainment, or persuading others to share their beliefs. Better understanding of online storytelling can illuminate the dynamics of social movements, sensemaking practices, persuasion strategies, and more. However, unlike other media such as books and visual content where the narrative nature of the content is often overtly signaled at the document level, studying storytelling in online communities is challenging due to the mixture of storytelling and non-storytelling behavior, which can be interspersed within documents and across diverse topics and settings. We introduce a codebook and create the Storytelling in Online Communities Corpus, an expert-annotated dataset of 502 English-language posts and comments with labeled story and event spans. Using our corpus, we train and evaluate an online story detection model, which we use to investigate the role storytelling of in different social contexts. We identify distinctive features of online storytelling, the prevalence of storytelling among different communities, and the conversational patterns of storytelling.'),\n",
       " Document(page_content='Author: Maarten Sap Title: Queer In AI: A Case Study in Community-Led Participatory AI Publication year: 2023 Coauthors: AI OrganizersOfQueerin, Anaelia Ovalle, Arjun Subramonian, Ashwin Singh, C. Voelcker, Danica J. Sutherland, Davide Locatelli, Eva Breznik, Filip Klubicka, Hang Yuan, J. Hetvi, Huan Zhang, Jaidev Shriram, Kruno Lehman, Luca Soldaini, Maarten Sap, M. Deisenroth, Maria Leonor Pacheco, Maria Ryskina, Martin Mundt, M. Agarwal, Nyx McLean, Pan Xu, Pranav A, Raj Korpan, Ruchira Ray, Sarah Mathew, Sarthak Arora, S. T. John, Tanvi Anand, Vishakha Agrawal, William Agnew, Yanan Long, Zijie J. Wang, Zeerak Talat, Avijit Ghosh, N. Dennler, Michael Noseworthy, Sharvani Jha, Emi Baylor, Aditya Joshi, Natalia Y. Bilenko, Andrew McNamara, Raphael Gontijo-Lopes, Alex Markham, Evyn Dǒng, J. Kay, Manu Saraswat, Nikhil Vytla, Luke Stark Abstract: Queerness and queer people face an uncertain future in the face of ever more widely deployed and invasive artificial intelligence (AI). These technologies have caused numerous harms to queer people, including privacy violations, censoring and downranking queer content, exposing queer people and spaces to harassment by making them hypervisible, deadnaming and outing queer people. More broadly, they have violated core tenets of queerness by classifying and controlling queer identities. In response to this, the queer community in AI has organized Queer in AI, a global, decentralized, volunteer-run grassroots organization that employs intersectional and community-led participatory design to build an inclusive and equitable AI future. In this paper, we present Queer in AI as a case study for community-led participatory design in AI. We examine how participatory design and intersectional tenets started and shaped this community’s programs over the years. We discuss different challenges that emerged in the process, look at ways this organization has fallen short of operationalizing participatory and intersectional principles, and then assess the organization’s impact. Queer in AI provides important lessons and insights for practitioners and theorists of participatory methods broadly through its rejection of hierarchy in favor of decentralization, success at building aid and programs by and for the queer community, and effort to change actors and institutions outside of the queer community. Finally, we theorize how communities like Queer in AI contribute to the participatory design in AI more broadly by fostering cultures of participation in AI, welcoming and empowering marginalized participants, critiquing poor or exploitative participatory practices, and bringing participation to institutions outside of individual research projects. Queer in AI’s work serves as a case study of grassroots activism and participatory methods within AI, demonstrating the potential of community-led participatory methods and intersectional praxis, while also providing challenges, case studies, and nuanced insights to researchers developing and using participatory methods.'),\n",
       " Document(page_content=\"Author: Maarten Sap Title: Value Kaleidoscope: Engaging AI with Pluralistic Human Values, Rights, and Duties Publication year: 2023 Coauthors: Taylor Sorensen, Liwei Jiang, Jena D. Hwang, Sydney Levine, Valentina Pyatkin, Peter West, Nouha Dziri, Ximing Lu, Kavel Rao, Chandra Bhagavatula, Maarten Sap, J. Tasioulas, Yejin Choi Abstract: Human values are crucial to human decision-making. Value pluralism is the view that multiple correct values may be held in tension with one another (e.g., when considering lying to a friend to protect their feelings, how does one balance honesty with friendship?). As statistical learners, AI systems fit to averages by default, washing out these potentially irreducible value conflicts. To improve AI systems to better reflect value pluralism, the first-order challenge is to explore the extent to which AI systems can model pluralistic human values, rights, and duties as well as their interaction. We introduce ValuePrism, a large-scale dataset of 218k values, rights, and duties connected to 31k human-written situations. ValuePrism's contextualized values are generated by GPT-4 and deemed high-quality by human annotators 91% of the time. We conduct a large-scale study with annotators across diverse social and demographic backgrounds to try to understand whose values are represented. With ValuePrism, we build Kaleido, an open, light-weight, and structured language-based multi-task model that generates, explains, and assesses the relevance and valence (i.e., support or oppose) of human values, rights, and duties within a specific context. Humans prefer the sets of values output by our system over the teacher GPT-4, finding them more accurate and with broader coverage. In addition, we demonstrate that Kaleido can help explain variability in human decision-making by outputting contrasting values. Finally, we show that Kaleido's representations transfer to other philosophical frameworks and datasets, confirming the benefit of an explicit, modular, and interpretable approach to value pluralism. We hope that our work will serve as a step to making more explicit the implicit values behind human decision-making and to steering AI systems to make decisions that are more in accordance with them.\"),\n",
       " Document(page_content='Author: Maarten Sap Title: Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models Publication year: 2023 Coauthors: Natalie Shapira, Mosh Levy, S. Alavi, Xuhui Zhou, Yejin Choi, Yoav Goldberg, Maarten Sap, Vered Shwartz Abstract: The escalating debate on AI\\'s capabilities warrants developing reliable metrics to assess machine\"intelligence\". Recently, many anecdotal examples were used to suggest that newer large language models (LLMs) like ChatGPT and GPT-4 exhibit Neural Theory-of-Mind (N-ToM); however, prior work reached conflicting conclusions regarding those abilities. We investigate the extent of LLMs\\' N-ToM through an extensive evaluation on 6 tasks and find that while LLMs exhibit certain N-ToM abilities, this behavior is far from being robust. We further examine the factors impacting performance on N-ToM tasks and discover that LLMs struggle with adversarial examples, indicating reliance on shallow heuristics rather than robust ToM abilities. We caution against drawing conclusions from anecdotal examples, limited benchmark testing, and using human-designed psychological tests to evaluate models.'),\n",
       " Document(page_content=\"Author: Maarten Sap Title: Towards Countering Essentialism through Social Bias Reasoning Publication year: 2023 Coauthors: Emily Allaway, Nina Taneja, S. Leslie, Maarten Sap Abstract: Essentialist beliefs (i.e., believing that members of the same group are fundamentally alike) play a central role in social stereotypes and can lead to harm when left unchallenged. In our work, we conduct exploratory studies into the task of countering essentialist beliefs (e.g., ``liberals are stupid''). Drawing on prior work from psychology and NLP, we construct five types of counterstatements and conduct human studies on the effectiveness of these different strategies. Our studies also investigate the role in choosing a counterstatement of the level of explicitness with which an essentialist belief is conveyed. We find that statements that broaden the scope of a stereotype (e.g., to other groups, as in ``conservatives can also be stupid'') are the most popular countering strategy. We conclude with a discussion of challenges and open questions for future work in this area (e.g., improving factuality, studying community-specific variation) and we emphasize the importance of work at the intersection of NLP and psychology.\"),\n",
       " Document(page_content=\"Author: Maarten Sap Title: SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents Publication year: 2023 Coauthors: Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, Maarten Sap Abstract: Humans are social beings; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and interact under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills. These findings demonstrate SOTOPIA's promise as a general platform for research on evaluating and improving social intelligence in artificial agents.\"),\n",
       " Document(page_content='Author: Rita Singh Title: BASS: Block-wise Adaptation for Speech Summarization Publication year: 2023 Coauthors: Roshan Sharma, Kenneth Zheng, Siddhant Arora, Shinji Watanabe, Rita Singh, B. Raj Abstract: End-to-end speech summarization has been shown to improve performance over cascade baselines. However, such models are difficult to train on very large inputs (dozens of minutes or hours) owing to compute restrictions and are hence trained with truncated model inputs. Truncation leads to poorer models, and a solution to this problem rests in block-wise modeling, i.e., processing a portion of the input frames at a time. In this paper, we develop a method that allows one to train summarization models on very long sequences in an incremental manner. Speech summarization is realized as a streaming process, where hypothesis summaries are updated every block based on new acoustic information. We devise and test strategies to pass semantic context across the blocks. Experiments on the How2 dataset demonstrate that the proposed block-wise training method improves by 3 points absolute on ROUGE-L over a truncated input baseline.'),\n",
       " Document(page_content='Author: Rita Singh Title: Rethinking Voice-Face Correlation: A Geometry View Publication year: 2023 Coauthors: Xiang Li, Yandong Wen, Muqiao Yang, Jinglu Wang, Rita Singh, B. Raj Abstract: Previous works on voice-face matching and voice-guided face synthesis demonstrate strong correlations between voice and face, but mainly rely on coarse semantic cues such as gender, age, and emotion. In this paper, we aim to investigate the capability of reconstructing the 3D facial shape from voice from a geometry perspective without any semantic information. We propose a voice-anthropometric measurement (AM)-face paradigm, which identifies predictable facial AMs from the voice and uses them to guide 3D face reconstruction. By leveraging AMs as a proxy to link the voice and face geometry, we can eliminate the influence of unpredictable AMs and make the face geometry tractable. Our approach is evaluated on our proposed dataset with ground-truth 3D face scans and corresponding voice recordings, and we find significant correlations between voice and specific parts of the face geometry, such as the nasal cavity and cranium. Our work offers a new perspective on voice-face correlation and can serve as a good empirical study for anthropometry science.'),\n",
       " Document(page_content='Author: Rita Singh Title: A Gene-Based Algorithm for Identifying Factors That May Affect a Speaker’s Voice Publication year: 2023 Coauthors: Rita Singh Abstract: Over the past decades, many machine-learning- and artificial-intelligence-based technologies have been created to deduce biometric or bio-relevant parameters of speakers from their voice. These voice profiling technologies have targeted a wide range of parameters, from diseases to environmental factors, based largely on the fact that they are known to influence voice. Recently, some have also explored the prediction of parameters whose influence on voice is not easily observable through data-opportunistic biomarker discovery techniques. However, given the enormous range of factors that can possibly influence voice, more informed methods for selecting those that may be potentially deducible from voice are needed. To this end, this paper proposes a simple path-finding algorithm that attempts to find links between vocal characteristics and perturbing factors using cytogenetic and genomic data. The links represent reasonable selection criteria for use by computational by profiling technologies only, and are not intended to establish any unknown biological facts. The proposed algorithm is validated using a simple example from medical literature—that of the clinically observed effects of specific chromosomal microdeletion syndromes on the vocal characteristics of affected people. In this example, the algorithm attempts to link the genes involved in these syndromes to a single example gene (FOXP2) that is known to play a broad role in voice production. We show that in cases where strong links are exposed, vocal characteristics of the patients are indeed reported to be correspondingly affected. Validation experiments and subsequent analyses confirm that the methodology could be potentially useful in predicting the existence of vocal signatures in naïve cases where their existence has not been otherwise observed.'),\n",
       " Document(page_content='Author: Rita Singh Title: Deriving Vocal Fold Oscillation Information from Recorded Voice Signals Using Models of Phonation Publication year: 2023 Coauthors: Wayne Zhao, Rita Singh Abstract: During phonation, the vocal folds exhibit a self-sustained oscillatory motion, which is influenced by the physical properties of the speaker’s vocal folds and driven by the balance of bio-mechanical and aerodynamic forces across the glottis. Subtle changes in the speaker’s physical state can affect voice production and alter these oscillatory patterns. Measuring these can be valuable in developing computational tools that analyze voice to infer the speaker’s state. Traditionally, vocal fold oscillations (VFOs) are measured directly using physical devices in clinical settings. In this paper, we propose a novel analysis-by-synthesis approach that allows us to infer the VFOs directly from recorded speech signals on an individualized, speaker-by-speaker basis. The approach, called the ADLES-VFT algorithm, is proposed in the context of a joint model that combines a phonation model (with a glottal flow waveform as the output) and a vocal tract acoustic wave propagation model such that the output of the joint model is an estimated waveform. The ADLES-VFT algorithm is a forward-backward algorithm which minimizes the error between the recorded waveform and the output of this joint model to estimate its parameters. Once estimated, these parameter values are used in conjunction with a phonation model to obtain its solutions. Since the parameters correlate with the physical properties of the vocal folds of the speaker, model solutions obtained using them represent the individualized VFOs for each speaker. The approach is flexible and can be applied to various phonation models. In addition to presenting the methodology, we show how the VFOs can be quantified from a dynamical systems perspective for classification purposes. Mathematical derivations are provided in an appendix for better readability.'),\n",
       " Document(page_content='Author: Rita Singh Title: Imprecise Label Learning: A Unified Framework for Learning with Various Imprecise Label Configurations Publication year: 2023 Coauthors: Hao Chen, Ankit Shah, Jindong Wang, R. Tao, Yidong Wang, Xingxu Xie, Masashi Sugiyama, Rita Singh, B. Raj Abstract: Learning with reduced labeling standards, such as noisy label, partial label, and multiple label candidates, which we generically refer to as \\\\textit{imprecise} labels, is a commonplace challenge in machine learning tasks. Previous methods tend to propose specific designs for every emerging imprecise label configuration, which is usually unsustainable when multiple configurations of imprecision coexist. In this paper, we introduce imprecise label learning (ILL), a framework for the unification of learning with various imprecise label configurations. ILL leverages expectation-maximization (EM) for modeling the imprecise label information, treating the precise labels as latent variables.Instead of approximating the correct labels for training, it considers the entire distribution of all possible labeling entailed by the imprecise information. We demonstrate that ILL can seamlessly adapt to partial label learning, semi-supervised learning, noisy label learning, and, more importantly, a mixture of these settings. Notably, ILL surpasses the existing specified techniques for handling imprecise labels, marking the first unified framework with robust and effective performance across various challenging settings. We hope our work will inspire further research on this topic, unleashing the full potential of ILL in wider scenarios where precise labels are expensive and complicated to obtain.'),\n",
       " Document(page_content='Author: Rita Singh Title: The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features Publication year: 2023 Coauthors: Liao Qu, X. Zou, Xiang Li, Yandong Wen, Rita Singh, B. Raj Abstract: This work unveils the enigmatic link between phonemes and facial features. Traditional studies on voice-face correlations typically involve using a long period of voice input, including generating face images from voices and reconstructing 3D face meshes from voices. However, in situations like voice-based crimes, the available voice evidence may be short and limited. Additionally, from a physiological perspective, each segment of speech -- phoneme -- corresponds to different types of airflow and movements in the face. Therefore, it is advantageous to discover the hidden link between phonemes and face attributes. In this paper, we propose an analysis pipeline to help us explore the voice-face relationship in a fine-grained manner, i.e., phonemes v.s. facial anthropometric measurements (AM). We build an estimator for each phoneme-AM pair and evaluate the correlation through hypothesis testing. Our results indicate that AMs are more predictable from vowels compared to consonants, particularly with plosives. Additionally, we observe that if a specific AM exhibits more movement during phoneme pronunciation, it is more predictable. Our findings support those in physiology regarding correlation and lay the groundwork for future research on speech-face multimodal learning.'),\n",
       " Document(page_content='Author: Rita Singh Title: Pengi: An Audio Language Model for Audio Tasks Publication year: 2023 Coauthors: Soham Deshmukh, Benjamin Elizalde, Rita Singh, Huaming Wang Abstract: In the domain of audio processing, Transfer Learning has facilitated the rise of Self-Supervised Learning and Zero-Shot Learning techniques. These approaches have led to the development of versatile models capable of tackling a wide array of tasks, while delivering state-of-the-art performance. However, current models inherently lack the capacity to produce the requisite language for open-ended tasks, such as Audio Captioning or Audio Question&Answering. We introduce Pengi, a novel Audio Language Model that leverages Transfer Learning by framing all audio tasks as text-generation tasks. It takes as input, an audio recording, and text, and generates free-form text as output. The input audio is represented as a sequence of continuous embeddings by an audio encoder. A text encoder does the same for the corresponding text input. Both sequences are combined as a prefix to prompt a pre-trained frozen language model. The unified architecture of Pengi enables open-ended tasks and close-ended tasks without any additional fine-tuning or task-specific extensions. When evaluated on 22 downstream tasks, our approach yields state-of-the-art performance in several of them. Our results show that connecting language models with audio models is a major step towards general-purpose audio understanding'),\n",
       " Document(page_content='Author: Rita Singh Title: Domain Adaptation for Contrastive Audio-Language Models Publication year: 2024 Coauthors: Soham Deshmukh, Rita Singh, Bhiksha Raj Abstract: Audio-Language Models (ALM) aim to be general-purpose audio models by providing zero-shot capabilities at test time. The zero-shot performance of ALM improves by using suitable text prompts for each domain. The text prompts are usually hand-crafted through an ad-hoc process and lead to a drop in ALM generalization and out-of-distribution performance. Existing approaches to improve domain performance, like few-shot learning or fine-tuning, require access to annotated data and iterations of training. Therefore, we propose a test-time domain adaptation method for ALMs that does not require access to annotations. Our method learns a domain vector by enforcing consistency across augmented views of the testing audio. We extensively evaluate our approach on 12 downstream tasks across domains. With just one example, our domain adaptation method leads to 3.2% (max 8.4%) average zero-shot performance improvement. After adaptation, the model still retains the generalization property of ALMs.'),\n",
       " Document(page_content='Author: Rita Singh Title: PAM: Prompting Audio-Language Models for Audio Quality Assessment Publication year: 2024 Coauthors: Soham Deshmukh, Dareen Alharthi, Benjamin Elizalde, Hannes Gamper, Mahmoud Al Ismail, Rita Singh, Bhiksha Raj, Huaming Wang Abstract: While audio quality is a key performance metric for various audio processing tasks, including generative modeling, its objective measurement remains a challenge. Audio-Language Models (ALMs) are pre-trained on audio-text pairs that may contain information about audio quality, the presence of artifacts, or noise. Given an audio input and a text prompt related to quality, an ALM can be used to calculate a similarity score between the two. Here, we exploit this capability and introduce PAM, a no-reference metric for assessing audio quality for different audio processing tasks. Contrary to other\"reference-free\"metrics, PAM does not require computing embeddings on a reference dataset nor training a task-specific model on a costly set of human listening scores. We extensively evaluate the reliability of PAM against established metrics and human listening scores on four tasks: text-to-audio (TTA), text-to-music generation (TTM), text-to-speech (TTS), and deep noise suppression (DNS). We perform multiple ablation studies with controlled distortions, in-the-wild setups, and prompt choices. Our evaluation shows that PAM correlates well with existing metrics and human listening scores. These results demonstrate the potential of ALMs for computing a general-purpose audio quality metric.'),\n",
       " Document(page_content='Author: Rita Singh Title: A General Framework for Learning from Weak Supervision Publication year: 2024 Coauthors: Hao Chen, Jindong Wang, Lei Feng, Xiang Li, Yidong Wang, Xing Xie, Masashi Sugiyama, Rita Singh, Bhiksha Raj Abstract: Weakly supervised learning generally faces challenges in applicability to various scenarios with diverse weak supervision and in scalability due to the complexity of existing algorithms, thereby hindering the practical deployment. This paper introduces a general framework for learning from weak supervision (GLWS) with a novel algorithm. Central to GLWS is an Expectation-Maximization (EM) formulation, adeptly accommodating various weak supervision sources, including instance partial labels, aggregate statistics, pairwise observations, and unlabeled data. We further present an advanced algorithm that significantly simplifies the EM computational demands using a Non-deterministic Finite Automaton (NFA) along with a forward-backward algorithm, which effectively reduces time complexity from quadratic or factorial often required in existing solutions to linear scale. The problem of learning from arbitrary weak supervision is therefore converted to the NFA modeling of them. GLWS not only enhances the scalability of machine learning models but also demonstrates superior performance and versatility across 11 weak supervision scenarios. We hope our work paves the way for further advancements and practical deployment in this field.'),\n",
       " Document(page_content='Author: Rita Singh Title: Towards Robust Audiovisual Segmentation in Complex Environments with Quantization-based Semantic Decomposition Publication year: 2023 Coauthors: Xiang Li, Jinglu Wang, Xiaohao Xu, Xiulian Peng, Rita Singh, Yan Lu, Bhiksha Raj Abstract: Audiovisual segmentation (AVS) is a challenging task that aims to segment visual objects in videos according to their associated acoustic cues. With multiple sound sources and background disturbances involved, establishing robust correspondences between audio and visual contents poses unique challenges due to (1) complex entanglement across sound sources and (2) frequent changes in the occurrence of distinct sound events. Assuming sound events occur independently, the multi-source semantic space can be represented as the Cartesian product of single-source sub-spaces. We are motivated to decompose the multi-source audio semantics into single-source semantics for more effective interactions with visual content. We propose a semantic decomposition method based on product quantization, where the multi-source semantics can be decomposed and represented by several disentangled and noise-suppressed single-source semantics. Furthermore, we introduce a global-to-local quantization mechanism, which distills knowledge from stable global (clip-level) features into local (frame-level) ones, to handle frequent changes in audio semantics. Extensive experiments demonstrate that our semantically decomposed audio representation significantly improves AVS performance, e.g., +21.2% mIoU on the challenging AVS-Semantic benchmark with ResNet50 backbone. https://github.com/lxa9867/QSD.'),\n",
       " Document(page_content=\"Author: Rita Singh Title: LoFT: Local Proxy Fine-tuning For Improving Transferability Of Adversarial Attacks Against Large Language Model Publication year: 2023 Coauthors: Muhammad Ahmed Shah, Roshan Sharma, Hira Dhamyal, R. Olivier, Ankit Shah, Dareen Alharthi, Hazim T Bukhari, Massa Baali, Soham Deshmukh, Michael Kuhlmann, Bhiksha Raj, Rita Singh Abstract: It has been shown that Large Language Model (LLM) alignments can be circumvented by appending specially crafted attack suffixes with harmful queries to elicit harmful responses. To conduct attacks against private target models whose characterization is unknown, public models can be used as proxies to fashion the attack, with successful attacks being transferred from public proxies to private target models. The success rate of attack depends on how closely the proxy model approximates the private model. We hypothesize that for attacks to be transferrable, it is sufficient if the proxy can approximate the target model in the neighborhood of the harmful query. Therefore, in this paper, we propose \\\\emph{Local Fine-Tuning (LoFT)}, \\\\textit{i.e.}, fine-tuning proxy models on similar queries that lie in the lexico-semantic neighborhood of harmful queries to decrease the divergence between the proxy and target models. First, we demonstrate three approaches to prompt private target models to obtain similar queries given harmful queries. Next, we obtain data for local fine-tuning by eliciting responses from target models for the generated similar queries. Then, we optimize attack suffixes to generate attack prompts and evaluate the impact of our local fine-tuning on the attack's success rate. Experiments show that local fine-tuning of proxy models improves attack transferability and increases attack success rate by $39\\\\%$, $7\\\\%$, and $0.5\\\\%$ (absolute) on target models ChatGPT, GPT-4, and Claude respectively.\"),\n",
       " Document(page_content='Author: Rita Singh Title: Evaluating Speech Synthesis by Training Recognizers on Synthetic Speech Publication year: 2023 Coauthors: Dareen Alharthi, Roshan Sharma, Hira Dhamyal, Soumi Maiti, Bhiksha Raj, Rita Singh Abstract: Modern speech synthesis systems have improved significantly, with synthetic speech being indistinguishable from real speech. However, efficient and holistic evaluation of synthetic speech still remains a significant challenge. Human evaluation using Mean Opinion Score (MOS) is ideal, but inefficient due to high costs. Therefore, researchers have developed auxiliary automatic metrics like Word Error Rate (WER) to measure intelligibility. Prior works focus on evaluating synthetic speech based on pre-trained speech recognition models, however, this can be limiting since this approach primarily measures speech intelligibility. In this paper, we propose an evaluation technique involving the training of an ASR model on synthetic speech and assessing its performance on real speech. Our main assumption is that by training the ASR model on the synthetic speech, the WER on real speech reflects the similarity between distributions, a broader assessment of synthetic speech quality beyond intelligibility. Our proposed metric demonstrates a strong correlation with both MOS naturalness and MOS intelligibility when compared to SpeechLMScore and MOSNet on three recent Text-to-Speech (TTS) systems: MQTTS, StyleTTS, and YourTTS.'),\n",
       " Document(page_content='Author: Rita Singh Title: Rethinking Audiovisual Segmentation with Semantic Quantization and Decomposition Publication year: 2023 Coauthors: Xiang Li, Jinglu Wang, Xiaohao Xu, Xiulian Peng, Rita Singh, Yan Lu, Bhiksha Raj Abstract: Audiovisual segmentation (AVS) is a challenging task that aims to segment visual objects in videos based on their associated acoustic cues. With multiple sound sources involved, establishing robust correspondences between audio and visual contents poses unique challenges due to its (1) intricate entanglement across sound sources and (2) frequent shift among sound events. Assuming sound events occur independently, the multi-source semantic space (which encompasses all possible semantic categories) can be represented as the Cartesian product of single-source sub-spaces. This motivates us to decompose the multi-source audio semantics into single-source semantics, enabling more effective interaction with visual content. Specifically, we propose a semantic decomposition method based on product quantization, where the multi-source semantics can be decomposed and represented by several quantized single-source semantics. Furthermore, we introduce a global-to-local quantization mechanism, which distills knowledge from stable global (clip-level) features into local (frame-level) ones, to handle the constant shift of audio semantics. Extensive experiments demonstrate that semantically quantized and decomposed audio representation significantly improves AVS performance, e.g., +21.2% mIoU on the most challenging AVS-Semantic benchmark.'),\n",
       " Document(page_content='Author: Rita Singh Title: Towards Noise-Tolerant Speech-Referring Video Object Segmentation: Bridging Speech and Text Publication year: 2023 Coauthors: Xiang Li, Jinglu Wang, Xiaohao Xu, Muqiao Yang, Fan Yang, Yizhou Zhao, Rita Singh, Bhiksha Raj '),\n",
       " Document(page_content='Author: Rita Singh Title: Espnet-Summ: Introducing a Novel Large Dataset, Toolkit, and a Cross-Corpora Evaluation of Speech Summarization Systems Publication year: 2023 Coauthors: Roshan Sharma, William Chen, Takatomo Kano, Ruchira Sharma, Siddhant Arora, Shinji Watanabe, A. Ogawa, Marc Delcroix, Rita Singh, Bhiksha Raj Abstract: Speech summarization has garnered significant interest and progressed rapidly over the past few years. In particular, end-to-end models have recently emerged as a competitive alternative to cascade systems for abstractive video summarization. This paper aims to establish progress in this rapidly evolving research field, by introducing ESPNet-SUMM, a new open-source toolkit that facilitates a comprehensive comparison of end-to-end and cascade speech summarization models on 4 different speech summarization tasks spanning diverse applications. Experiments demonstrate that end-to-end models perform better for larger corpora with shorter inputs. This work also introduces Interview, the largest public open-domain multiparty interview corpus with $4400 \\\\mathrm{~h}$ of conversations between radio hosts and guests. Finally, this work explores the use of multiple datasets to improve end-to-end summarization, and experiments demonstrate the benefit of multi-style training over fine-tuning. 1'),\n",
       " Document(page_content='Author: Rita Singh Title: Importance of negative sampling in weak label learning Publication year: 2023 Coauthors: Ankit Shah, Fuyu Tang, Zelin Ye, Rita Singh, Bhiksha Raj Abstract: Weak-label learning is a challenging task that requires learning from data\"bags\"containing positive and negative instances, but only the bag labels are known. The pool of negative instances is usually larger than positive instances, thus making selecting the most informative negative instance critical for performance. Such a selection strategy for negative instances from each bag is an open problem that has not been well studied for weak-label learning. In this paper, we study several sampling strategies that can measure the usefulness of negative instances for weak-label learning and select them accordingly. We test our method on CIFAR-10 and AudioSet datasets and show that it improves the weak-label classification performance and reduces the computational cost compared to random sampling methods. Our work reveals that negative instances are not all equally irrelevant, and selecting them wisely can benefit weak-label learning.'),\n",
       " Document(page_content='Author: Rita Singh Title: Completing Visual Objects via Bridging Generation and Segmentation Publication year: 2023 Coauthors: Xiang Li, Yinpeng Chen, Chung-Ching Lin, Rita Singh, Bhiksha Raj, Zicheng Liu Abstract: This paper presents a novel approach to object completion, with the primary goal of reconstructing a complete object from its partially visible components. Our method, named MaskComp, delineates the completion process through iterative stages of generation and segmentation. In each iteration, the object mask is provided as an additional condition to boost image generation, and, in return, the generated images can lead to a more accurate mask by fusing the segmentation of images. We demonstrate that the combination of one generation and one segmentation stage effectively functions as a mask denoiser. Through alternation between the generation and segmentation stages, the partial object mask is progressively refined, providing precise shape guidance and yielding superior object completion results. Our experiments demonstrate the superiority of MaskComp over existing approaches, e.g., ControlNet and Stable Diffusion, establishing it as an effective solution for object completion.'),\n",
       " Document(page_content='Author: Rita Singh Title: Training Audio Captioning Models without Audio Publication year: 2023 Coauthors: Soham Deshmukh, Benjamin Elizalde, Dimitra Emmanouilidou, Bhiksha Raj, Rita Singh, Huaming Wang Abstract: Automated Audio Captioning (AAC) is the task of generating natural language descriptions given an audio stream. A typical AAC system requires manually curated training data of audio segments and corresponding text caption annotations. The creation of these audio-caption pairs is costly, resulting in general data scarcity for the task. In this work, we address this major limitation and propose an approach to train AAC systems using only text. Our approach leverages the multimodal space of contrastively trained audio-text models, such as CLAP. During training, a decoder generates captions conditioned on the pretrained CLAP text encoder. During inference, the text encoder is replaced with the pretrained CLAP audio encoder. To bridge the modality gap between text and audio embeddings, we propose the use of noise injection or a learnable adapter, during training. We find that the proposed text-only framework performs competitively with state-of-the-art models trained with paired audio, showing that efficient text-to-audio transfer is possible. Finally, we showcase both stylized audio captioning and caption enrichment while training without audio or human-created text captions.'),\n",
       " Document(page_content=\"Author: Rita Singh Title: Prompting Audios Using Acoustic Properties For Emotion Representation Publication year: 2023 Coauthors: Hira Dhamyal, Benjamin Elizalde, Soham Deshmukh, Huaming Wang, Bhiksha Raj, Rita Singh Abstract: Emotions lie on a continuum, but current models treat emotions as a finite valued discrete variable. This representation does not capture the diversity in the expression of emotion. To better represent emotions we propose the use of natural language descriptions (or prompts). In this work, we address the challenge of automatically generating these prompts and training a model to better learn emotion representations from audio and prompt pairs. We use acoustic properties that are correlated to emotion like pitch, intensity, speech rate, and articulation rate to automatically generate prompts i.e. 'acoustic prompts'. We use a contrastive learning objective to map speech to their respective acoustic prompts. We evaluate our model on Emotion Audio Retrieval and Speech Emotion Recognition. Our results show that the acoustic prompts significantly improve the model's performance in EAR, in various Precision@K metrics. In SER, we observe a 3.8% relative accuracy improvement on the Ravdess dataset.\"),\n",
       " Document(page_content='Author: Rita Singh Title: Pairwise Similarity Learning is SimPLE Publication year: 2023 Coauthors: Yandong Wen, Weiyang Liu, Yao Feng, Bhiksha Raj, Rita Singh, Adrian Weller, Michael J. Black, Bernhard Schölkopf Abstract: In this paper, we focus on a general yet important learning problem, pairwise similarity learning (PSL). PSL subsumes a wide range of important applications, such as open-set face recognition, speaker verification, image retrieval and person re-identification. The goal of PSL is to learn a pairwise similarity function assigning a higher similarity score to positive pairs (i.e., a pair of samples with the same label) than to negative pairs (i.e., a pair of samples with different label). We start by identifying a key desideratum for PSL, and then discuss how existing methods can achieve this desideratum. We then propose a surprisingly simple proxy-free method, called SimPLE, which requires neither feature/proxy normalization nor angular margin and yet is able to generalize well in open-set recognition. We apply the proposed method to three challenging PSL tasks: open-set face recognition, image retrieval and speaker verification. Comprehensive experimental results on large-scale benchmarks show that our method performs significantly better than current state-of-the-art methods. Our project page is available at simple.is.tue.mpg.de.'),\n",
       " Document(page_content='Author: Emma Strubell Title: AboutMe: Using Self-Descriptions in Webpages to Document the Effects of English Pretraining Data Filters Publication year: 2024 Coauthors: Li Lucy, Suchin Gururangan, Luca Soldaini, Emma Strubell, David Bamman, Lauren Klein, Jesse Dodge Abstract: Large language models\\' (LLMs) abilities are drawn from their pretraining data, and model development begins with data curation. However, decisions around what data is retained or removed during this initial stage is under-scrutinized. In our work, we ground web text, which is a popular pretraining data source, to its social and geographic contexts. We create a new dataset of 10.3 million self-descriptions of website creators, and extract information about who they are and where they are from: their topical interests, social roles, and geographic affiliations. Then, we conduct the first study investigating how ten\"quality\"and English language identification (langID) filters affect webpages that vary along these social dimensions. Our experiments illuminate a range of implicit preferences in data curation: we show that some quality classifiers act like topical domain filters, and langID can overlook English content from some regions of the world. Overall, we hope that our work will encourage a new line of research on pretraining data curation practices and its social implications.'),\n",
       " Document(page_content='Author: Emma Strubell Title: OLMo: Accelerating the Science of Language Models Publication year: 2024 Coauthors: Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, A. Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Raghavi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Daniel Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, Will Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah A. Smith, Hanna Hajishirzi Abstract: Language models (LMs) have become ubiquitous in both NLP research and in commercial product offerings. As their commercial importance has surged, the most powerful models have become closed off, gated behind proprietary interfaces, with important details of their training data, architectures, and development undisclosed. Given the importance of these details in scientifically studying these models, including their biases and potential risks, we believe it is essential for the research community to have access to powerful, truly open LMs. To this end, this technical report details the first release of OLMo, a state-of-the-art, truly Open Language Model and its framework to build and study the science of language modeling. Unlike most prior efforts that have only released model weights and inference code, we release OLMo and the whole framework, including training data and training and evaluation code. We hope this release will empower and strengthen the open research community and inspire a new wave of innovation.'),\n",
       " Document(page_content='Author: Emma Strubell Title: Dolma: an Open Corpus of Three Trillion Tokens for Language Model Pretraining Research Publication year: 2024 Coauthors: Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Raghavi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, A. Jha, Sachin Kumar, L. Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Daniel Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew E. Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Pete Walsh, Luke Zettlemoyer, Noah A. Smith, Hanna Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, Kyle Lo Abstract: Language models have become a critical technology to tackling a wide range of natural language processing tasks, yet many details about how the best-performing language models were developed are not reported. In particular, information about their pretraining corpora is seldom discussed: commercial language models rarely provide any information about their data; even open models rarely release datasets they are trained on, or an exact recipe to reproduce them. As a result, it is challenging to conduct certain threads of language modeling research, such as understanding how training data impacts model capabilities and shapes their limitations. To facilitate open research on language model pretraining, we release Dolma, a three trillion tokens English corpus, built from a diverse mixture of web content, scientific papers, code, public-domain books, social media, and encyclopedic materials. In addition, we open source our data curation toolkit to enable further experimentation and reproduction of our work. In this report, we document Dolma, including its design principles, details about its construction, and a summary of its contents. We interleave this report with analyses and experimental results from training language models on intermediate states of Dolma to share what we have learned about important data curation practices, including the role of content or quality filters, deduplication, and multi-source mixing. Dolma has been used to train OLMo, a state-of-the-art, open language model and framework designed to build and study the science of language modeling.'),\n",
       " Document(page_content=\"Author: Emma Strubell Title: To Build Our Future, We Must Know Our Past: Contextualizing Paradigm Shifts in Natural Language Processing Publication year: 2023 Coauthors: Sireesh Gururaja, Amanda Bertsch, Clara Na, D. Widder, Emma Strubell Abstract: NLP is in a period of disruptive change that is impacting our methodologies, funding sources, and public perception. In this work, we seek to understand how to shape our future by better understanding our past. We study factors that shape NLP as a field, including culture, incentives, and infrastructure by conducting long-form interviews with 26 NLP researchers of varying seniority, research area, institution, and social identity. Our interviewees identify cyclical patterns in the field, as well as new shifts without historical parallel, including changes in benchmark culture and software infrastructure. We complement this discussion with quantitative analysis of citation, authorship, and language use in the ACL Anthology over time. We conclude by discussing shared visions, concerns, and hopes for the future of NLP. We hope that this study of our field's past and present can prompt informed discussion of our community's implicit norms and more deliberate action to consciously shape the future.\"),\n",
       " Document(page_content=\"Author: Emma Strubell Title: Energy and Carbon Considerations of Fine-Tuning BERT Publication year: 2023 Coauthors: Xiaorong Wang, Clara Na, Emma Strubell, Sorelle A. Friedler, Sasha Luccioni Abstract: Despite the popularity of the `pre-train then fine-tune' paradigm in the NLP community, existing work quantifying energy costs and associated carbon emissions has largely focused on language model pre-training. Although a single pre-training run draws substantially more energy than fine-tuning, fine-tuning is performed more frequently by many more individual actors, and thus must be accounted for when considering the energy and carbon footprint of NLP. In order to better characterize the role of fine-tuning in the landscape of energy and carbon emissions in NLP, we perform a careful empirical study of the computational costs of fine-tuning across tasks, datasets, hardware infrastructure and measurement modalities. Our experimental results allow us to place fine-tuning energy and carbon costs into perspective with respect to pre-training and inference, and outline recommendations to NLP researchers and practitioners who wish to improve their fine-tuning energy efficiency.\"),\n",
       " Document(page_content=\"Author: Emma Strubell Title: Understanding the Effect of Model Compression on Social Bias in Large Language Models Publication year: 2023 Coauthors: Gustavo Gonçalves, Emma Strubell Abstract: Large Language Models (LLMs) trained with self-supervision on vast corpora of web text fit to the social biases of that text. Without intervention, these social biases persist in the model's predictions in downstream tasks, leading to representational harm. Many strategies have been proposed to mitigate the effects of inappropriate social biases learned during pretraining. Simultaneously, methods for model compression have become increasingly popular to reduce the computational burden of LLMs. Despite the popularity and need for both approaches, little work has been done to explore the interplay between these two. We perform a carefully controlled study of the impact of model compression via quantization and knowledge distillation on measures of social bias in LLMs. Longer pretraining and larger models led to higher social bias, and quantization showed a regularizer effect with its best trade-off around 20% of the original pretraining time.\"),\n",
       " Document(page_content=\"Author: Emma Strubell Title: Surveying (Dis)Parities and Concerns of Compute Hungry NLP Research Publication year: 2023 Coauthors: Ji-Ung Lee, Haritz Puerto, Betty van Aken, Yuki Arase, J. Forde, Leon Derczynski, Andreas Ruckl'e, Iryna Gurevych, Roy Schwartz, Emma Strubell, Jesse Dodge Abstract: Many recent improvements in NLP stem from the development and use of large pre-trained language models (PLMs) with billions of parameters. Large model sizes makes computational cost one of the main limiting factors for training and evaluating such models; and has raised severe concerns about the sustainability, reproducibility, and inclusiveness for researching PLMs. These concerns are often based on personal experiences and observations. However, there had not been any large-scale surveys that investigate them. In this work, we provide a first attempt to quantify these concerns regarding three topics, namely, environmental impact, equity, and impact on peer reviewing. By conducting a survey with 312 participants from the NLP community, we capture existing (dis)parities between different and within groups with respect to seniority, academia, and industry; and their impact on the peer reviewing process. For each topic, we provide an analysis and devise recommendations to mitigate found disparities, some of which already successfully implemented. Finally, we discuss additional concerns raised by many participants in free-text responses.\"),\n",
       " Document(page_content='Author: Emma Strubell Title: On the Interactions of Structural Constraints and Data Resources for Structured Prediction Publication year: 2023 Coauthors: Zhisong Zhang, Emma Strubell, E. Hovy Abstract: In this work, we provide an analysis on the interactions of the effectiveness of decoding with structural constraints and the amount of available training data for structured prediction tasks in NLP. Our exploration adopts a simple protocol that enforces constraints upon constraint-agnostic local models at testing time. With evaluations on three typical structured prediction tasks (named entity recognition, dependency parsing, and event argument extraction), we find that models trained with less data predict outputs with more structural violations in greedy decoding mode. Incorporating constraints provides consistent performance improvements and such benefits are larger in lower resource scenarios. Moreover, there are similar patterns with regard to the model sizes and more efficient models tend to enjoy more benefits. Finally, we also investigate settings with genre transfer and discover patterns that are related to domain discrepancies.'),\n",
       " Document(page_content=\"Author: Emma Strubell Title: Efficiency Pentathlon: A Standardized Arena for Efficiency Evaluation Publication year: 2023 Coauthors: Hao Peng, Qingqing Cao, Jesse Dodge, Matthew E. Peters, Jared Fernandez, Tom Sherborne, Kyle Lo, Sam Skjonsberg, Emma Strubell, Darrell Plessas, Iz Beltagy, Pete Walsh, Noah A. Smith, Hannaneh Hajishirzi Abstract: Rising computational demands of modern natural language processing (NLP) systems have increased the barrier to entry for cutting-edge research while posing serious environmental concerns. Yet, progress on model efficiency has been impeded by practical challenges in model evaluation and comparison. For example, hardware is challenging to control due to disparate levels of accessibility across different institutions. Moreover, improvements in metrics such as FLOPs often fail to translate to progress in real-world applications. In response, we introduce Pentathlon, a benchmark for holistic and realistic evaluation of model efficiency. Pentathlon focuses on inference, which accounts for a majority of the compute in a model's lifecycle. It offers a strictly-controlled hardware platform, and is designed to mirror real-world applications scenarios. It incorporates a suite of metrics that target different aspects of efficiency, including latency, throughput, memory overhead, and energy consumption. Pentathlon also comes with a software library that can be seamlessly integrated into any codebase and enable evaluation. As a standardized and centralized evaluation platform, Pentathlon can drastically reduce the workload to make fair and reproducible efficiency comparisons. While initially focused on natural language processing (NLP) models, Pentathlon is designed to allow flexible extension to other fields. We envision Pentathlon will stimulate algorithmic innovations in building efficient models, and foster an increased awareness of the social and environmental implications in the development of future-generation NLP models.\"),\n",
       " Document(page_content='Author: Emma Strubell Title: Queer People are People First: Deconstructing Sexual Identity Stereotypes in Large Language Models Publication year: 2023 Coauthors: Harnoor Dhingra, Preetiha Jayashanker, Sayali S. Moghe, Emma Strubell Abstract: Large Language Models (LLMs) are trained primarily on minimally processed web text, which exhibits the same wide range of social biases held by the humans who created that content. Consequently, text generated by LLMs can inadvertently perpetuate stereotypes towards marginalized groups, like the LGBTQIA+ community. In this paper, we perform a comparative study of how LLMs generate text describing people with different sexual identities. Analyzing bias in the text generated by an LLM using regard score shows measurable bias against queer people. We then show that a post-hoc method based on chain-of-thought prompting using SHAP analysis can increase the regard of the sentence, representing a promising approach towards debiasing the output of LLMs in this setting.'),\n",
       " Document(page_content='Author: Emma Strubell Title: Power Hungry Processing: Watts Driving the Cost of AI Deployment? Publication year: 2023 Coauthors: A. Luccioni, Yacine Jernite, Emma Strubell Abstract: Recent years have seen a surge in the popularity of commercial AI products based on generative, multi-purpose AI systems promising a unified approach to building machine learning (ML) models into technology. However, this ambition of\"generality\"comes at a steep cost to the environment, given the amount of energy these systems require and the amount of carbon that they emit. In this work, we propose the first systematic comparison of the ongoing inference cost of various categories of ML systems, covering both task-specific (i.e. finetuned models that carry out a single task) and `general-purpose\\' models, (i.e. those trained for multiple tasks). We measure deployment cost as the amount of energy and carbon required to perform 1,000 inferences on representative benchmark dataset using these models. We find that multi-purpose, generative architectures are orders of magnitude more expensive than task-specific systems for a variety of tasks, even when controlling for the number of model parameters. We conclude with a discussion around the current trend of deploying multi-purpose generative ML systems, and caution that their utility should be more intentionally weighed against increased costs in terms of energy and emissions. All the data from our study can be accessed via an interactive demo to carry out further exploration and analysis.'),\n",
       " Document(page_content='Author: Emma Strubell Title: Making Scalable Meta Learning Practical Publication year: 2023 Coauthors: Sang Keun Choe, Sanket Vaibhav Mehta, Hwijeen Ahn, W. Neiswanger, Pengtao Xie, Emma Strubell, Eric P. Xing Abstract: Despite its flexibility to learn diverse inductive biases in machine learning programs, meta learning (i.e., learning to learn) has long been recognized to suffer from poor scalability due to its tremendous compute/memory costs, training instability, and a lack of efficient distributed training support. In this work, we focus on making scalable meta learning practical by introducing SAMA, which combines advances in both implicit differentiation algorithms and systems. Specifically, SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients. Evaluated on multiple large-scale meta learning benchmarks, SAMA showcases up to 1.7/4.8x increase in throughput and 2.0/3.8x decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms. Furthermore, we show that SAMA-based data optimization leads to consistent improvements in text classification accuracy with BERT and RoBERTa large language models, and achieves state-of-the-art results in both small- and large-scale data pruning on image classification tasks, demonstrating the practical applicability of scalable meta learning across language and vision domains.'),\n",
       " Document(page_content='Author: Emma Strubell Title: Regularizing Self-training for Unsupervised Domain Adaptation via Structural Constraints Publication year: 2023 Coauthors: Rajshekhar Das, Jonathan M Francis, Sanket Vaibhav Mehta, Jean Oh, Emma Strubell, Jose Moura Abstract: Self-training based on pseudo-labels has emerged as a dominant approach for addressing conditional distribution shifts in unsupervised domain adaptation (UDA) for semantic segmentation problems. A notable drawback, however, is that this family of approaches is susceptible to erroneous pseudo labels that arise from confirmation biases in the source domain and that manifest as nuisance factors in the target domain. A possible source for this mismatch is the reliance on only photometric cues provided by RGB image inputs, which may ultimately lead to sub-optimal adaptation. To mitigate the effect of mismatched pseudo-labels, we propose to incorporate structural cues from auxiliary modalities, such as depth, to regularise conventional self-training objectives. Specifically, we introduce a contrastive pixel-level objectness constraint that pulls the pixel representations within a region of an object instance closer, while pushing those from different object categories apart. To obtain object regions consistent with the true underlying object, we extract information from both depth maps and RGB-images in the form of multimodal clustering. Crucially, the objectness constraint is agnostic to the ground-truth semantic labels and, hence, appropriate for unsupervised domain adaptation. In this work, we show that our regularizer significantly improves top performing self-training methods (by up to $2$ points) in various UDA benchmarks for semantic segmentation. We include all code in the supplementary.'),\n",
       " Document(page_content='Author: Emma Strubell Title: The Framework Tax: Disparities Between Inference Efficiency in Research and Deployment Publication year: 2023 Coauthors: Jared Fernandez, Jacob Kahn, Clara Na, Yonatan Bisk, Emma Strubell Abstract: Increased focus on the computational efficiency of NLP systems has motivated the design of efficient model architectures and improvements to underlying hardware accelerators. However, the resulting increases in computational throughput and reductions in floating point operations have not directly translated to improvements in wall-clock inference latency. We demonstrate that these discrepancies can be largely attributed to bottlenecks introduced by deep learning frameworks. We denote this phenomenon as the \\\\textit{framework tax}, and observe that the disparity is growing as hardware speed increases over time. In this work, we examine this phenomenon through a series of case studies analyzing the effects of model design decisions, framework paradigms, and hardware platforms on total model latency. Code is available at https://github.com/JaredFern/Framework-Tax.'),\n",
       " Document(page_content=\"Author: Emma Strubell Title: Data-efficient Active Learning for Structured Prediction with Partial Annotation and Self-Training Publication year: 2023 Coauthors: Zhisong Zhang, Emma Strubell, E. Hovy Abstract: In this work we propose a pragmatic method that reduces the annotation cost for structured label spaces using active learning. Our approach leverages partial annotation, which reduces labeling costs for structured outputs by selecting only the most informative sub-structures for annotation. We also utilize self-training to incorporate the current model's automatic predictions as pseudo-labels for un-annotated sub-structures. A key challenge in effectively combining partial annotation with self-training to reduce annotation cost is determining which sub-structures to select to label. To address this challenge, we adopt an error estimator to adaptively decide the partial selection ratio according to the current model's capability. In evaluations spanning four structured prediction tasks, we show that our combination of partial annotation and self-training using an adaptive selection ratio reduces annotation cost over strong full annotation baselines under a fair comparison scheme that takes reading time into consideration.\"),\n",
       " Document(page_content=\"Author: Emma Strubell Title: How To Train Your (Compressed) Large Language Model Publication year: 2023 Coauthors: A. Jha, Dirk Groeneveld, Emma Strubell, Iz Beltagy Abstract: With the increase in the size of large language models (LLMs), we need compression methods that can reduce the model size while preserving the generality and zero-shot promptability of the model. This goal is more ambitious than the typical compression setup, which reduces the model's size at the expense of specializing it to a specific end-task. To study this, we develop a task-agnostic compression pipeline with a large-scale evaluation comprising language modeling perplexity and 12 zero-shot end-tasks. Our results show that a simple layer-wise pruning followed by continued language model pretraining matches or outperforms three existing state-of-the-art baselines while being 1.5x more computationally efficient. However, unlike typical task-specialized compression, our best-compressed model significantly underperforms a similar-sized model trained from scratch. We posit the half-sized pretrained model as an upper bound for task-agnostic compression and call for future work to bridge this gap under a reasonable token budget. Our findings highlight the inadequacy of existing compression methods for LLMs and establish a requirement for new methods that preserve a model's generality and zero-shot promptability under compression. We release our code and evaluation setup to facilitate reproducibility and help iterate on method design.\"),\n",
       " Document(page_content='Author: A. Waibel Title: Audio-driven Talking Face Generation by Overcoming Unintended Information Flow Publication year: 2023 Coauthors: Dogucan Yaman, Fevziye Irem Eyiokur, Leonard Barmann, H. K. Ekenel, A. Waibel Abstract: Audio-driven talking face generation is the task of creating a lip-synchronized, realistic face video from given audio and reference frames. This involves two major challenges: overall visual quality of generated images on the one hand, and audio-visual synchronization of the mouth part on the other hand. In this paper, we start by identifying several problematic aspects of synchronization methods in recent audio-driven talking face generation approaches. Specifically, this involves unintended flow of lip, pose and other information from the reference to the generated image, as well as instabilities during model training. Subsequently, we propose various techniques for obviating these issues: First, a silent-lip reference image generator prevents leaking of lips from the reference to the generated image. Second, an adaptive triplet loss handles the pose leaking problem. Finally, we propose a stabilized formulation of synchronization loss, circumventing aforementioned training instabilities while additionally further alleviating the lip leaking issue. Combining the individual improvements, we present state-of-the-art visual quality and synchronization performance on LRS2 in five out of seven and LRW in six out of seven metrics, and competitive results on the remaining ones. We further validate our design in various ablation experiments, confirming the individual contributions as well as their complementary effects.'),\n",
       " Document(page_content='Author: A. Waibel Title: AdapITN: A Fast, Reliable, and Dynamic Adaptive Inverse Text Normalization Publication year: 2023 Coauthors: T. Nguyen, Le Duc Minh Nhat, Quang Minh Nguyen, Quoc Truong Do, C. Luong, A. Waibel Abstract: Inverse text normalization (ITN) is the task that transforms text in spoken-form into written-form. While automatic speech recognition (ASR) produces text in spoken-form, human and natural language understanding systems prefer to consume text in written-form. ITN generally deals with semiotic phrases (e.g., numbers, date, time). However, lack of studies to deal with phonetization phrases, which is ASR’s output when it handles unseen data (e.g., foreign-named entities, domain names), although these exist in the same form in the spoken-form text. The reason is that phonetization phrases are infinite patterns and language-dependent. In this study, we introduce a novel end2end model that can handle both semiotic phrases (SEP) and phonetization phrases (PHP), named AdapITN. We call it \"Adap\" because it allows for handling unseen PHP. The model performs only when necessary by providing a mechanism to narrow normalized regions and external query knowledge, reducing the runtime significantly.'),\n",
       " Document(page_content='Author: A. Waibel Title: Train Global, Tailor Local: Minimalist Multilingual Translation into Endangered Languages Publication year: 2023 Coauthors: Zhong Zhou, J. Niehues, A. Waibel Abstract: In many humanitarian scenarios, translation into severely low resource languages often does not require a universal translation engine, but a dedicated text-specific translation engine. For example, healthcare records, hygienic procedures, government communication, emergency procedures and religious texts are all limited texts. While generic translation engines for all languages do not exist, translation of multilingually known limited texts into new, endangered languages may be possible and reduce human translation effort. We attempt to leverage translation resources from rich resource languages to efficiently produce best possible translation quality for well known texts, which is available in multiple languages, in a new, severely low resource language. We examine two approaches: 1.) best selection of seed sentences to jump start translations in a new language in view of best generalization to the remainder of a larger targeted text(s), and 2.) we adapt large general multilingual translation engines from many other languages to focus on a specific text in a new, unknown language. We find that adapting large pretrained multilingual models to the domain/text first and then to the severely low resource language works best. If we also select a best set of seed sentences, we can improve average chrF performance on new test languages from a baseline of 21.9 to 50.7, while reducing the number of seed sentences to only ∼1,000 in the new, unknown language.'),\n",
       " Document(page_content='Author: A. Waibel Title: Towards Efficient Simultaneous Speech Translation: CUNI-KIT System for Simultaneous Track at IWSLT 2023 Publication year: 2023 Coauthors: Peter Polák, Danni Liu, Ngoc-Quan Pham, J. Niehues, A. Waibel, Ondrej Bojar Abstract: In this paper, we describe our submission to the Simultaneous Track at IWSLT 2023. This year, we continue with the successful setup from the last year, however, we adopt the latest methods that further improve the translation quality. Additionally, we propose a novel online policy for attentional encoder-decoder models. The policy prevents the model to generate translation beyond the current speech input by using an auxiliary CTC output layer. We show that the proposed simultaneous policy can be applied to both streaming blockwise models and offline encoder-decoder models. We observe significant improvements in quality (up to 1.1 BLEU) and the computational footprint (up to 45% relative RTF).'),\n",
       " Document(page_content='Author: A. Waibel Title: SYNTACC : Synthesizing Multi-Accent Speech By Weight Factorization Publication year: 2023 Coauthors: Tuan-Nam Nguyen, Ngoc-Quan Pham, A. Waibel Abstract: Conventional multi-speaker text-to-speech synthesis (TTS) is known to be capable of synthesizing speech for multiple voices, yet it cannot generate speech in different accents. This limitation has motivated us to develop SYNTACC (Synthesizing speech with accents) which adapts conventional multi-speaker TTS to produce multi-accent speech. Our method uses the YourTTS model and involves a novel multi-accent training mechanism. The method works by decomposing each weight matrix into a shared component and an accent-dependent component, with the former being initialized by the pretrained multi-speaker TTS model and the latter being factorized into vectors using rank-1 matrices to reduce the number of training parameters per accent. This weight factorization method proves to be effective in fine-tuning the SYNTACC on multi-accent data sets in a low-resource condition. Our SYNTACC model eventually allows speech synthesis in not only different voices but also in different accents.'),\n",
       " Document(page_content='Author: A. Waibel Title: KIT’s Multilingual Speech Translation System for IWSLT 2023 Publication year: 2023 Coauthors: Danni Liu, T. Nguyen, Sai Koneru, Enes Yavuz Ugan, Ngoc-Quan Pham, Tuan-Nam Nguyen, Tu Anh Dinh, Carlos Mullov, A. Waibel, J. Niehues Abstract: Many existing speech translation benchmarks focus on native-English speech in high-quality recording conditions, which often do not match the conditions in real-life use-cases. In this paper, we describe our speech translation system for the multilingual track of IWSLT 2023, which focuses on the translation of scientific conference talks. The test condition features accented input speech and terminology-dense contents. The tasks requires translation into 10 languages of varying amounts of resources. In absence of training data from the target domain, we use a retrieval-based approach (kNN-MT) for effective adaptation (+0.8 BLEU for speech translation). We also use adapters to easily integrate incremental training data from data augmentation, and show that it matches the performance of re-training. We observe that cascaded systems are more easily adaptable towards specific target domains, due to their separate modules. Our cascaded speech system outperforms its end-to-end counterpart on scientific talk translation, although their performance remains similar on TED talks.'),\n",
       " Document(page_content=\"Author: A. Waibel Title: Convoifilter: A case study of doing cocktail party speech recognition Publication year: 2023 Coauthors: T. Nguyen, A. Waibel Abstract: This paper presents an end-to-end model designed to improve automatic speech recognition (ASR) for a particular speaker in a crowded, noisy environment. The model utilizes a single-channel speech enhancement module that isolates the speaker's voice from background noise (ConVoiFilter) and an ASR module. The model can decrease ASR's word error rate (WER) from 80% to 26.4% through this approach. Typically, these two components are adjusted independently due to variations in data requirements. However, speech enhancement can create anomalies that decrease ASR efficiency. By implementing a joint fine-tuning strategy, the model can reduce the WER from 26.4% in separate tuning to 14.5% in joint tuning. We openly share our pre-trained model to foster further research hf.co/nguyenvulebinh/voice-filter.\"),\n",
       " Document(page_content='Author: A. Waibel Title: Incremental Blockwise Beam Search for Simultaneous Speech Translation with Controllable Quality-Latency Tradeoff Publication year: 2023 Coauthors: Peter Polák, Brian Yan, Shinji Watanabe, A. Waibel, Ondrej Bojar Abstract: Blockwise self-attentional encoder models have recently emerged as one promising end-to-end approach to simultaneous speech translation. These models employ a blockwise beam search with hypothesis reliability scoring to determine when to wait for more input speech before translating further. However, this method maintains multiple hypotheses until the entire speech input is consumed -- this scheme cannot directly show a single \\\\textit{incremental} translation to users. Further, this method lacks mechanisms for \\\\textit{controlling} the quality vs. latency tradeoff. We propose a modified incremental blockwise beam search incorporating local agreement or hold-$n$ policies for quality-latency control. We apply our framework to models trained for online or offline translation and demonstrate that both types can be effectively used in online mode. Experimental results on MuST-C show 0.6-3.6 BLEU improvement without changing latency or 0.8-1.4 s latency improvement without changing quality.'),\n",
       " Document(page_content='Author: A. Waibel Title: End-to-End Evaluation for Low-Latency Simultaneous Speech Translation Publication year: 2023 Coauthors: Christian Huber, Tu Anh Dinh, Carlos Mullov, Ngoc-Quan Pham, T. Nguyen, Fabian Retkowski, Stefan Constantin, Enes Yavuz Ugan, Danni Liu, Zhaolin Li, Sai Koneru, J. Niehues, A. Waibel Abstract: The challenge of low-latency speech translation has recently draw significant interest in the research community as shown by several publications and shared tasks. Therefore, it is essential to evaluate these different approaches in realistic scenarios. However, currently only specific aspects of the systems are evaluated and often it is not possible to compare different approaches. In this work, we propose the first framework to perform and evaluate the various aspects of low-latency speech translation under realistic conditions. The evaluation is carried out in an end-to-end fashion. This includes the segmentation of the audio as well as the run-time of the different components. Secondly, we compare different approaches to low-latency speech translation using this framework. We evaluate models with the option to revise the output as well as methods with fixed output. Furthermore, we directly compare state-of-the-art cascaded as well as end-to-end systems. Finally, the framework allows to automatically evaluate the translation quality as well as latency and also provides a web interface to show the low-latency model outputs to the user.'),\n",
       " Document(page_content='Author: A. Waibel Title: FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN Publication year: 2023 Coauthors: Sweta Agrawal, Antonios Anastasopoulos, L. Bentivogli, Ondrej Bojar, Claudia Borg, Marine Carpuat, Roldano Cattoni, Mauro Cettolo, Mingda Chen, William Chen, K. Choukri, Alexandra Chronopoulou, Anna Currey, T. Declerck, Qianqian Dong, Kevin Duh, Y. Estève, Marcello Federico, Souhir Gahbiche, B. Haddow, B. Hsu, Phu Mon Htut, H. Inaguma, Dávid Javorský, J. Judge, Yasumasa Kano, Tom Ko, Rishu Kumar, Peng Li, Xutai Ma, Prashant Mathur, E. Matusov, Paul McNamee, John P. McCrae, Kenton Murray, Maria Nadejde, Satoshi Nakamura, Matteo Negri, H. Nguyen, J. Niehues, Xing Niu, Atul Kr. Ojha, John E. Ortega, Proyag Pal, J. Pino, Lonneke van der Plas, Peter Polák, Elijah Matthew Rippeth, Elizabeth Salesky, Jiatong Shi, Matthias Sperber, Sebastian Stüker, Katsuhito Sudoh, Yun Tang, Brian Thompson, Ke M. Tran, M. Turchi, A. Waibel, Mingxuan Wang, Shinji Watanabe, Rodolfo Zevallos Abstract: This paper reports on the shared tasks organized by the 20th IWSLT Conference. The shared tasks address 9 scientific challenges in spoken language translation: simultaneous and offline translation, automatic subtitling and dubbing, speech-to-speech translation, multilingual, dialect and low-resource speech translation, and formality control. The shared tasks attracted a total of 38 submissions by 31 teams. The growing interest towards spoken language translation is also witnessed by the constantly increasing number of shared task organizers and contributors to the overview paper, almost evenly distributed across industry and academia.'),\n",
       " Document(page_content='Author: Alexander Waibel Title: Modular Design of a Front-End and Back-End Speech-to-Speech Translation Application for Psychiatric Treatment of Refugees Publication year: 2023 Coauthors: Enes Yavuz Ugan, Mohammed Mediani, Omar Al Jawabra, Aya Khader, Yining Liu, Alexander Waibel Abstract: One of the inevitable impacts happening in areas with political conflicts is the significant influx of displaced individuals. The psychological consequences on individuals enduring such events are profound. Therefore, the imperative of providing adequate mental health care to refugees coming from conflict areas becomes apparent. However, providing this necessary care faces two obstacles. On the one hand, not all this target population is expected to have an acceptable level of proficiency of the hosting country’s local language. On the other hand, finding enough number of suitable interpreters is a very challenging task. Moreover, even when the availability of the human interpreters is no problem, the refugees may hesitate to share their experiences with interpreters due to the associated stigma. To address these challenges and enhance mental health care for refugees, we propose the design of a modular front-end and back-end Speech-to-Speech translation system, with a focus on safeguarding patient data privacy. As our system is Speech-to-Speech, it also enables dialogue with dyslexic people and removes barriers for their treatment as well.'),\n",
       " Document(page_content='Author: Alexander Waibel Title: Continuously Learning New Words in Automatic Speech Recognition Publication year: 2024 Coauthors: Christian Huber, Alexander Waibel Abstract: Despite recent advances, Automatic Speech Recognition (ASR) systems are still far from perfect. Typical errors include acronyms, named entities and domain-specific special words for which little or no data is available. To address the problem of recognizing these words, we propose an self-supervised continual learning approach. Given the audio of a lecture talk with corresponding slides, we bias the model towards decoding new words from the slides by using a memory-enhanced ASR model from previous work. Then, we perform inference on the talk, collecting utterances that contain detected new words into an adaptation dataset. Continual learning is then performed on this set by adapting low-rank matrix weights added to each weight matrix of the model. The whole procedure is iterated for many talks. We show that with this approach, we obtain increasing performance on the new words when they occur more frequently (more than 80% recall) while preserving the general performance of the model.'),\n",
       " Document(page_content=\"Author: Alexander H. Waibel Title: Incremental Learning of Humanoid Robot Behavior from Natural Interaction and Large Language Models Publication year: 2023 Coauthors: Leonard Bärmann, Rainer Kartmann, Fabian Peller-Konrad, Alexander H. Waibel, T. Asfour Abstract: Natural-language dialog is key for intuitive human-robot interaction. It can be used not only to express humans' intents, but also to communicate instructions for improvement if a robot does not understand a command correctly. Of great importance is to endow robots with the ability to learn from such interaction experience in an incremental way to allow them to improve their behaviors or avoid mistakes in the future. In this paper, we propose a system to achieve incremental learning of complex behavior from natural interaction, and demonstrate its implementation on a humanoid robot. Building on recent advances, we present a system that deploys Large Language Models (LLMs) for high-level orchestration of the robot's behavior, based on the idea of enabling the LLM to generate Python statements in an interactive console to invoke both robot perception and action. The interaction loop is closed by feeding back human instructions, environment observations, and execution results to the LLM, thus informing the generation of the next statement. Specifically, we introduce incremental prompt learning, which enables the system to interactively learn from its mistakes. For that purpose, the LLM can call another LLM responsible for code-level improvements of the current interaction based on human feedback. The improved interaction is then saved in the robot's memory, and thus retrieved on similar requests. We integrate the system in the robot cognitive architecture of the humanoid robot ARMAR-6 and evaluate our methods both quantitatively (in simulation) and qualitatively (in simulation and real-world) by demonstrating generalized incrementally-learned knowledge.\"),\n",
       " Document(page_content='Author: Alexander H. Waibel Title: Multimodal Error Correction with Natural Language and Pointing Gestures Publication year: 2023 Coauthors: Stefan Constantin, Fevziye Irem Eyiokur, Dogucan Yaman, Leonard Bärmann, Alexander H. Waibel Abstract: Error correction is crucial in human-computer interaction, as it can provide supervision for incrementally learning artificial intelligence. If a system maps entities like objects or persons with unknown class to inappropriate existing classes, or misrecognizes entities from known classes when there is too high train-test discrepancy, error correction is a natural way for a user to improve the system. Provided an agent with visual perception, if such entity is in the view of the system, pointing gestures can dramatically simplify the error correction. Therefore, we propose a modularized system for multimodal error correction using natural language and pointing gestures. First, pointing line generation and region proposal detects whether there is a pointing gesture, and if yes, which candidate objects (i.e. RoIs) are on the pointing line. Second, these RoIs (if any) and the user’s utterances are fed into a VL-T5 network to extract and link both the class name and the corresponding RoI of the referred entity, or to output that there is no error correction. In the latter case, the utterances can be passed to a downstream component for Natural Language Understanding. We use additional, challenging annotations for an existing real-world pointing gesture dataset to evaluate our proposed system. Furthermore, we demonstrate our approach by integrating it on a real-world steerable laser pointer robot, enabling interactive multimodal error correction and thus incremental learning of new objects.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Saturation time of exposure interval for cross-neutralization response to SARS-CoV-2: Implications for vaccine dose interval Publication year: 2023 Coauthors: Sho Miyamoto, Y. Kuroda, T. Kanno, A. Ueno, N. Shiwa-Sudo, N. Iwata-Yoshikawa, Yusuke Sakai, N. Nagata, T. Arashiro, A. Ainai, Saya Moriyama, N. Kishida, Shinji Watanabe, K. Nojima, Y. Seki, T. Mizukami, H. Hasegawa, H. Ebihara, S. Fukushi, Yoshimasa Takahashi, Maeda Ken, Tadaki Suzuki '),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Joint Prediction and Denoising for Large-Scale Multilingual Self-Supervised Learning Publication year: 2023 Coauthors: William Chen, Jiatong Shi, Brian Yan, Dan Berrebbi, Wangyou Zhang, Yifan Peng, Xuankai Chang, Soumi Maiti, Shinji Watanabe Abstract: Multilingual self-supervised learning (SSL) has often lagged behind state-of-the-art (SOTA) methods due to the expenses and complexity required to handle many languages. This further harms the reproducibility of SSL, which is already limited to few research groups due to its resource usage. We show that more powerful techniques can actually lead to more efficient pre-training, opening SSL to more research groups. We propose WavLabLM, which extends WavLM’s joint prediction and denoising to 40k hours of data across 136 languages. To build WavLabLM, we devise a novel multi-stage pre-training method, designed to address the language imbalance of multilingual data. WavLabLM achieves comparable performance to XLS-R on ML-SUPERB with less than $10 \\\\%$ of the training data, making SSL realizable with academic compute. We show that further efficiency can be achieved with a vanilla HuBERT Base model, which can maintain $94 \\\\%$ of XLS-R’s performance with only $3 \\\\%$ of the data, 4 GPUs, and limited trials. We open-source all code and models in ESPnet.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Tensor decomposition for minimization of E2E SLU model toward on-device processing Publication year: 2023 Coauthors: Yosuke Kashiwagi, Siddhant Arora, Hayato Futami, Jessica Huynh, Shih-Lun Wu, Yifan Peng, Brian Yan, E. Tsunoo, Shinji Watanabe Abstract: Spoken Language Understanding (SLU) is a critical speech recognition application and is often deployed on edge devices. Consequently, on-device processing plays a significant role in the practical implementation of SLU. This paper focuses on the end-to-end (E2E) SLU model due to its small latency property, unlike a cascade system, and aims to minimize the computational cost. We reduce the model size by applying tensor decomposition to the Conformer and E-Branchformer architectures used in our E2E SLU models. We propose to apply singular value decomposition to linear layers and the Tucker decomposition to convolution layers, respectively. We also compare COMP/PARFAC decomposition and Tensor-Train decomposition to the Tucker decomposition. Since the E2E model is represented by a single neural network, our tensor decomposition can flexibly control the number of parameters without changing feature dimensions. On the STOP dataset, we achieved 70.9% exact match accuracy under the tight constraint of only 15 million parameters.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Decoder-only Architecture for Speech Recognition with CTC Prompts and Text Data Augmentation Publication year: 2023 Coauthors: E. Tsunoo, Hayato Futami, Yosuke Kashiwagi, Siddhant Arora, Shinji Watanabe Abstract: Collecting audio-text pairs is expensive; however, it is much easier to access text-only data. Unless using shallow fusion, end-to-end automatic speech recognition (ASR) models require architecture modifications or additional training schemes to use text-only data. Inspired by recent advances in decoder-only language models (LMs), such as GPT-3 and PaLM adopted for speech-processing tasks, we propose using a decoder-only architecture for ASR with simple text augmentation. To provide audio information, encoder features compressed by CTC prediction are used as prompts for the decoder, which can be regarded as refining CTC prediction using the decoder-only model. Because the decoder architecture is the same as an autoregressive LM, it is simple to enhance the model by leveraging external text data with LM training. An experimental comparison using LibriSpeech and Switchboard shows that our proposed models with text augmentation training reduced word error rates from ordinary CTC by 0.3% and 1.4% on LibriSpeech test-clean and testother set, respectively, and 2.9% and 5.0% on Switchboard and CallHome. The proposed model had advantage on computational efficiency compared with conventional encoder-decoder ASR models with a similar parameter setup, and outperformed them on the LibriSpeech 100h and Switchboard training scenarios.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: ML-SUPERB: Multilingual Speech Universal PERformance Benchmark Publication year: 2023 Coauthors: Jiatong Shi, Dan Berrebbi, William Chen, Ho-Lam Chung, En-Pei Hu, Wei Huang, Xuankai Chang, Shang-Wen Li, Abdel-rahman Mohamed, Hung-yi Lee, Shinji Watanabe Abstract: Speech processing Universal PERformance Benchmark (SUPERB) is a leaderboard to benchmark the performance of Self-Supervised Learning (SSL) models on various speech processing tasks. However, SUPERB largely considers English speech in its evaluation. This paper presents multilingual SUPERB (ML-SUPERB), covering 143 languages (ranging from high-resource to endangered), and considering both automatic speech recognition and language identification. Following the concept of SUPERB, ML-SUPERB utilizes frozen SSL features and employs a simple framework for multilingual tasks by learning a shallow downstream model. Similar to the SUPERB benchmark, we find speech SSL models can significantly improve performance compared to FBANK features. Furthermore, we find that multilingual models do not always perform better than their monolingual counterparts. We will release ML-SUPERB as a challenge with organized datasets and reproducible training scripts for future multilingual representation research.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Structured Pruning of Self-Supervised Pre-Trained Models for Speech Recognition and Understanding Publication year: 2023 Coauthors: Yifan Peng, Kwangyoun Kim, Felix Wu, Prashant Sridhar, Shinji Watanabe Abstract: Self-supervised speech representation learning (SSL) has shown to be effective in various downstream tasks, but SSL models are usually large and slow. Model compression techniques such as pruning aim to reduce the model size and computation without degradation in accuracy. Prior studies focus on the pruning of Transformers; however, speech models not only utilize a stack of Transformer blocks, but also combine a frontend network based on multiple convolutional layers for low-level feature representation learning. This frontend has a small size but a heavy computational cost. In this work, we propose three task-specific structured pruning methods to deal with such heterogeneous networks. Experiments on LibriSpeech and SLURP show that the proposed method is more accurate than the original wav2vec2-base with 10% to 30% less computation, and is able to reduce the computation by 40% to 50% without any degradation.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization Publication year: 2023 Coauthors: Puyuan Peng, Brian Yan, Shinji Watanabe, David F. Harwath Abstract: We investigate the emergent abilities of the recently proposed web-scale speech model Whisper, by adapting it to unseen tasks with prompt engineering. We selected three tasks: audio-visual speech recognition (AVSR), code-switched speech recognition (CS-ASR), and speech translation (ST) on unseen language pairs. We design task-specific prompts, by either leveraging another large-scale model, or simply manipulating the special tokens in the default prompts. Experiments show that compared to the default prompts, our proposed prompts improve performance by 10% to 45% on the three zero-shot tasks, and even outperform SotA supervised models on some datasets. In addition, our experiments reveal many interesting properties of Whisper, including its robustness to prompts, bias on accents, and the multilingual understanding in its latent space. Code is available at https://github.com/jasonppy/PromptingWhisper'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks Publication year: 2023 Coauthors: Yifan Peng, Kwangyoun Kim, Felix Wu, Brian Yan, Siddhant Arora, William Chen, Jiyang Tang, Suwon Shon, Prashant Sridhar, Shinji Watanabe Abstract: Conformer, a convolution-augmented Transformer variant, has become the de facto encoder architecture for speech processing due to its superior performance in various tasks, including automatic speech recognition (ASR), speech translation (ST) and spoken language understanding (SLU). Recently, a new encoder called E-Branchformer has outperformed Conformer in the LibriSpeech ASR benchmark, making it promising for more general speech applications. This work compares E-Branchformer and Conformer through extensive experiments using different types of end-to-end sequence-to-sequence models. Results demonstrate that E-Branchformer achieves comparable or better performance than Conformer in almost all evaluation sets across 15 ASR, 2 ST, and 3 SLU benchmarks, while being more stable during training. We will release our training configurations and pre-trained models for reproducibility, which can benefit the speech community.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Efficient Sequence Transduction by Jointly Predicting Tokens and Durations Publication year: 2023 Coauthors: Hainan Xu, Fei Jia, Somshubra Majumdar, Hengguan Huang, Shinji Watanabe, Boris Ginsburg Abstract: This paper introduces a novel Token-and-Duration Transducer (TDT) architecture for sequence-to-sequence tasks. TDT extends conventional RNN-Transducer architectures by jointly predicting both a token and its duration, i.e. the number of input frames covered by the emitted token. This is achieved by using a joint network with two outputs which are independently normalized to generate distributions over tokens and durations. During inference, TDT models can skip input frames guided by the predicted duration output, which makes them significantly faster than conventional Transducers which process the encoder output frame by frame. TDT models achieve both better accuracy and significantly faster inference than conventional Transducers on different sequence transduction tasks. TDT models for Speech Recognition achieve better accuracy and up to 2.82X faster inference than conventional Transducers. TDT models for Speech Translation achieve an absolute gain of over 1 BLEU on the MUST-C test compared with conventional Transducers, and its inference is 2.27X faster. In Speech Intent Classification and Slot Filling tasks, TDT models improve the intent accuracy by up to over 1% (absolute) over conventional Transducers, while running up to 1.28X faster. Our implementation of the TDT model will be open-sourced with the NeMo (https://github.com/NVIDIA/NeMo) toolkit.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Speech Summarization of Long Spoken Document: Improving Memory Efficiency of Speech/Text Encoders Publication year: 2023 Coauthors: Takatomo Kano, A. Ogawa, Marc Delcroix, Roshan Sharma, Kohei Matsuura, Shinji Watanabe Abstract: Speech summarization requires processing several minute-long speech sequences to allow exploiting the whole context of a spoken document. A conventional approach is a cascade of automatic speech recognition (ASR) and text summarization (TS). However, the cascade systems are sensitive to ASR errors. Moreover, the cascade system cannot be optimized for input speech and utilize para-linguistic information. Recently, there has been an increased interest in end-to-end (E2E) approaches optimized to output summaries directly from speech. Such systems can thus mitigate the ASR errors of cascade approaches. However, E2E speech summarization requires massive computational resources because it needs to encode long speech sequences. We propose a speech summarization system that enables E2E summarization from 100 seconds, which is the limit of the conventional method, to up to 10 minutes (i.e., the duration of typical instructional videos on YouTube). However, the modeling capability of this model for minute-long speech sequences is weaker than the conventional approach. We thus exploit auxiliary text information from ASR transcriptions to improve the modeling capabilities. The resultant system consists of a dual speech/text encoder decoder-based summarization system. We perform experiments on the How2 dataset showing the proposed system improved METEOR scores by up to 2.7 points by fully exploiting the long spoken documents.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: UNSSOR: Unsupervised Neural Speech Separation by Leveraging Over-determined Training Mixtures Publication year: 2023 Coauthors: Zhong-Qiu Wang, Shinji Watanabe Abstract: In reverberant conditions with multiple concurrent speakers, each microphone acquires a mixture signal of multiple speakers at a different location. In over-determined conditions where the microphones out-number speakers, we can narrow down the solutions to speaker images and realize unsupervised speech separation by leveraging each mixture signal as a constraint (i.e., the estimated speaker images at a microphone should add up to the mixture). Equipped with this insight, we propose UNSSOR, an algorithm for $\\\\textbf{u}$nsupervised $\\\\textbf{n}$eural $\\\\textbf{s}$peech $\\\\textbf{s}$eparation by leveraging $\\\\textbf{o}$ver-determined training mixtu$\\\\textbf{r}$es. At each training step, we feed an input mixture to a deep neural network (DNN) to produce an intermediate estimate for each speaker, linearly filter the estimates, and optimize a loss so that, at each microphone, the filtered estimates of all the speakers can add up to the mixture to satisfy the above constraint. We show that this loss can promote unsupervised separation of speakers. The linear filters are computed in each sub-band based on the mixture and DNN estimates through the forward convolutive prediction (FCP) algorithm. To address the frequency permutation problem incurred by using sub-band FCP, a loss term based on minimizing intra-source magnitude scattering is proposed. Although UNSSOR requires over-determined training mixtures, we can train DNNs to achieve under-determined separation (e.g., unsupervised monaural speech separation). Evaluation results on two-speaker separation in reverberant conditions show the effectiveness and potential of UNSSOR.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Segment-Level Vectorized Beam Search Based on Partially Autoregressive Inference Publication year: 2023 Coauthors: Masao Someki, N. Eng, Yosuke Higuchi, Shinji Watanabe Abstract: Attention-based encoder-decoder models with autoregressive (AR) decoding have proven to be the dominant approach for automatic speech recognition (ASR) due to their superior accuracy. However, they often suffer from slow inference. This is primarily attributed to the incremental calculation of the decoder. This work proposes a partially AR framework, which employs segment-level vectorized beam search for improving the inference speed of an ASR model based on the hybrid connectionist temporal classification (CTC) attention-based architecture. It first generates an initial hypothesis using greedy CTC decoding, identifying low-confidence tokens based on their output probabilities. We then utilize the decoder to perform segment-level vectorized beam search on these tokens, re-predicting in parallel with minimal decoder calculations. Experimental results show that our method is 12 to 13 times faster in inference on the LibriSpeech corpus over AR decoding whilst preserving high accuracy.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Unsupervised Data Selection for TTS: Using Arabic Broadcast News as a Case Study Publication year: 2023 Coauthors: Massa Baali, Tomoki Hayashi, Hamdy Mubarak, Soumi Maiti, Shinji Watanabe, W. El-Hajj, Ahmed Ali Abstract: .'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: BASS: Block-wise Adaptation for Speech Summarization Publication year: 2023 Coauthors: Roshan Sharma, Kenneth Zheng, Siddhant Arora, Shinji Watanabe, Rita Singh, B. Raj Abstract: End-to-end speech summarization has been shown to improve performance over cascade baselines. However, such models are difficult to train on very large inputs (dozens of minutes or hours) owing to compute restrictions and are hence trained with truncated model inputs. Truncation leads to poorer models, and a solution to this problem rests in block-wise modeling, i.e., processing a portion of the input frames at a time. In this paper, we develop a method that allows one to train summarization models on very long sequences in an incremental manner. Speech summarization is realized as a streaming process, where hypothesis summaries are updated every block based on new acoustic information. We devise and test strategies to pass semantic context across the blocks. Experiments on the How2 dataset demonstrate that the proposed block-wise training method improves by 3 points absolute on ROUGE-L over a truncated input baseline.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: I3D: Transformer Architectures with Input-Dependent Dynamic Depth for Speech Recognition Publication year: 2023 Coauthors: Yifan Peng, Jaesong Lee, Shinji Watanabe Abstract: Transformer-based end-to-end speech recognition has achieved great success. However, the large footprint and computational overhead make it difficult to deploy these models in some real-world applications. Model compression techniques can reduce the model size and speed up inference, but the compressed model has a fixed architecture which might be suboptimal. We propose a novel Transformer encoder with Input-Dependent Dynamic Depth (I3D) to achieve strong performance-efficiency trade-offs. With a similar number of layers at inference time, I3D-based models outperform the vanilla Transformer and the static pruned model via iterative layer pruning. We also present interesting analysis on the gate probabilities and the input-dependency, which helps us better understand deep encoders.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech Publication year: 2023 Coauthors: Li-Wei Chen, Shinji Watanabe, Alexander I. Rudnicky Abstract: Recent Text-to-Speech (TTS) systems trained on reading or acted corpora have achieved near human-level naturalness. The diversity of human speech, however, often goes beyond the coverage of these corpora. We believe the ability to handle such diversity is crucial for AI systems to achieve human-level communication. Our work explores the use of more abundant real-world data for building speech synthesizers. We train TTS systems using real-world speech from YouTube and podcasts. We observe the mismatch between training and inference alignments in mel-spectrogram based autoregressive models, leading to unintelligible synthesis, and demonstrate that learned discrete codes within multiple code groups effectively resolves this issue. We introduce our MQTTS system whose architecture is designed for multiple code generation and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality. We conduct ablation analyses to identify the efficacy of our methods. We show that MQTTS outperforms existing TTS systems in several objective and subjective measures.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: A New Benchmark of Aphasia Speech Recognition and Detection Based on E-Branchformer and Multi-task Learning Publication year: 2023 Coauthors: Jiyang Tang, William Chen, Xuankai Chang, Shinji Watanabe, B. MacWhinney Abstract: Aphasia is a language disorder that affects the speaking ability of millions of patients. This paper presents a new benchmark for Aphasia speech recognition and detection tasks using state-of-the-art speech recognition techniques with the AphsiaBank dataset. Specifically, we introduce two multi-task learning methods based on the CTC/Attention architecture to perform both tasks simultaneously. Our system achieves state-of-the-art speaker-level detection accuracy (97.3%), and a relative WER reduction of 11% for moderate Aphasia patients. In addition, we demonstrate the generalizability of our approach by applying it to another disordered speech database, the DementiaBank Pitt corpus. We will make our all-in-one recipes and pre-trained model publicly available to facilitate reproducibility. Our standardized data preprocessing pipeline and open-source recipes enable researchers to compare results directly, promoting progress in disordered speech processing.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Antiviral Susceptibilities of Distinct Lineages of Influenza C and D Viruses Publication year: 2023 Coauthors: E. Takashita, S. Murakami, Y. Matsuzaki, Seiichiro Fujisaki, H. Morita, Shiho Nagata, Misa Katayama, K. Mizuta, H. Nishimura, Shinji Watanabe, T. Horimoto, H. Hasegawa Abstract: The emergence and spread of antiviral-resistant influenza viruses are of great concern. To minimize the public health risk, it is important to monitor antiviral susceptibilities of influenza viruses. Analyses of the antiviral susceptibilities of influenza A and B viruses have been conducted globally; however, those of influenza C and D viruses are limited. Here, we determined the susceptibilities of influenza C viruses representing all six lineages (C/Taylor, C/Yamagata, C/Sao Paulo, C/Aichi, C/Kanagawa, and C/Mississippi) and influenza D viruses representing four lineages (D/OK, D/660, D/Yama2016, and D/Yama2019) to RNA polymerase inhibitors (baloxavir and favipiravir) by using a focus reduction assay. All viruses tested were susceptible to both drugs. We then performed a genetic analysis to check for amino acid substitutions associated with baloxavir and favipiravir resistance and found that none of the viruses tested possessed these substitutions. Use of the focus reduction assay with the genotypic assay has proven valuable for monitoring the antiviral susceptibilities of influenza C and D viruses as well as influenza A and B viruses. Antiviral susceptibility monitoring of all influenza virus types should continue in order to assess the public health risks posed by these viruses.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Paaploss: A Phonetic-Aligned Acoustic Parameter Loss for Speech Enhancement Publication year: 2023 Coauthors: Muqiao Yang, Joseph Konan, David Bick, YUNYANG ZENG, Shuo Han, Anurag Kumar, Shinji Watanabe, B. Raj Abstract: Despite rapid advancement in recent years, current speech enhancement models often produce speech that differs in perceptual quality from real clean speech. We propose a learning objective that formalizes differences in perceptual quality, by using domain knowledge of acoustic-phonetics. We identify temporal acoustic parameters – such as spectral tilt, spectral flux, shimmer, etc. – that are non-differentiable, and we develop a neural network estimator that can accurately predict their time-series values across an utterance. We also model phoneme-specific weights for each feature, as the acoustic parameters are known to show different behavior in different phonemes. We can add this criterion as an auxiliary loss to any model that produces speech, to optimize speech outputs to match the values of clean speech in these features. Experimentally we show that it improves speech enhancement workflows in both time-domain and time-frequency domain, as measured by standard evaluation metrics. We also provide an analysis of phoneme-dependent improvement on acoustic parameters, demonstrating the additional interpretability that our method provides. This analysis can suggest which features are currently the bottleneck for improvement.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing Publication year: 2023 Coauthors: Taiqi He, Lindia Tjuatja, Nathaniel R. Robinson, Shinji Watanabe, David R. Mortensen, Graham Neubig, L. Levin Abstract: In our submission to the SIGMORPHON 2023 Shared Task on interlinear glossing (IGT), we explore approaches to data augmentation and modeling across seven low-resource languages. For data augmentation, we explore two approaches: creating artificial data from the provided training data and utilizing existing IGT resources in other languages. On the modeling side, we test an enhanced version of the provided token classification baseline as well as a pretrained multilingual seq2seq model. Additionally, we apply post-correction using a dictionary for Gitksan, the language with the smallest amount of data. We find that our token classification models are the best performing, with the highest word-level accuracy for Arapaho and highest morpheme-level accuracy for Gitksan out of all submissions. We also show that data augmentation is an effective strategy, though applying artificial data pretraining has very different effects across both models tested.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation Publication year: 2023 Coauthors: Yoshiki Masuyama, Xuankai Chang, Wangyou Zhang, Samuele Cornell, Zhongqiu Wang, Nobutaka Ono, Y. Qian, Shinji Watanabe Abstract: Neural speech separation has made remarkable progress and its integration with automatic speech recognition (ASR) is an important direction towards realizing multi-speaker ASR. This work provides an insightful investigation of speech separation in reverberant and noisy-reverberant scenarios as an ASR front-end. In detail, we explore multi-channel separation methods, mask-based beamforming and complex spectral mapping, as well as the best features to use in the ASR back-end model. We employ the recent self-supervised learning representation (SSLR) as a feature and improve the recognition performance from the case with filterbank features. To further improve multi-speaker recognition performance, we present a carefully designed training strategy for integrating speech separation and recognition with SSLR. The proposed integration using TF-GridNet-based complex spectral mapping and WavLM-based SSLR achieves a 2.5% word error rate in reverberant WHAMR! test set, significantly outperforming an existing mask-based MVDR beamforming and filterbank integration (28.9%).'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive Instruction-Tuning Benchmark for Speech Publication year: 2023 Coauthors: Chien-yu Huang, Ke-Han Lu, Shi Wang, Chi-Yuan Hsiao, Chun-Yi Kuan, Haibin Wu, Siddhant Arora, Kai-Wei Chang, Jiatong Shi, Yifan Peng, Roshan Sharma, Shinji Watanabe, Bhiksha Ramakrishnan, Shady Shehata, Hung-yi Lee Abstract: Text language models have shown remarkable zero-shot capability in generalizing to unseen tasks when provided with well-formulated instructions. However, existing studies in speech processing primarily focus on limited or specific tasks. Moreover, the lack of standardized benchmarks hinders a fair comparison across different approaches. Thus, we present Dynamic-SUPERB, a benchmark designed for building universal speech models capable of leveraging instruction tuning to perform multiple tasks in a zero-shot fashion. To achieve comprehensive coverage of diverse speech tasks and harness instruction tuning, we invite the community to collaborate and contribute, facilitating the dynamic growth of the benchmark. To initiate, Dynamic-SUPERB features 55 evaluation instances by combining 33 tasks and 22 datasets. This spans a broad spectrum of dimensions, providing a comprehensive platform for evaluation. Additionally, we propose several approaches to establish benchmark baselines. These include the utilization of speech models, text language models, and the multimodal encoder. Evaluation results indicate that while these baselines perform reasonably on seen tasks, they struggle with unseen ones. We also conducted an ablation study to assess the robustness and seek improvements in the performance. We release all materials to the public and welcome researchers to collaborate on the project, advancing technologies in the field together.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Speaker-Independent Acoustic-to-Articulatory Speech Inversion Publication year: 2023 Coauthors: Peter Wu, Li-Wei Chen, Cheol Jun Cho, Shinji Watanabe, L. Goldstein, A. Black, G. Anumanchipalli Abstract: To build speech processing methods that can handle speech as naturally as humans, researchers have explored multiple ways of building an invertible mapping from speech to an interpretable space. The articulatory space is a promising inversion target, since this space captures the mechanics of speech production. To this end, we build an acoustic-to-articulatory inversion (AAI) model that leverages autoregression, adversarial training, and self supervision to generalize to unseen speakers. Our approach obtains 0.784 correlation on an electromagnetic articulography (EMA) dataset, improving the state-of-the-art by 12.5%. Additionally, we show the interpretability of these representations through directly com-paring the behavior of estimated representations with speech production behavior. Finally, we propose a resynthesis-based AAI evaluation metric that does not rely on articulatory labels, demonstrating its efficacy with an 18-speaker dataset.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Towards Practical and Efficient Image-to-Speech Captioning with Vision-Language Pre-training and Multi-modal Tokens Publication year: 2023 Coauthors: Minsu Kim, J. Choi, Soumi Maiti, Jeong Hun Yeo, Shinji Watanabe, Y. Ro Abstract: In this paper, we propose methods to build a powerful and efficient Image-to-Speech captioning (Im2Sp) model. To this end, we start with importing the rich knowledge related to image comprehension and language modeling from a large-scale pre-trained vision-language model into Im2Sp. We set the output of the proposed Im2Sp as discretized speech units, i.e., the quantized speech features of a self-supervised speech model. The speech units mainly contain linguistic information while suppressing other characteristics of speech. This allows us to incorporate the language modeling capability of the pre-trained vision-language model into the spoken language modeling of Im2Sp. With the vision-language pre-training strategy, we set new state-of-the-art Im2Sp performances on two widely used benchmark databases, COCO and Flickr8k. Then, we further improve the efficiency of the Im2Sp model. Similar to the speech unit case, we convert the original image into image units, which are derived through vector quantization of the raw image. With these image units, we can drastically reduce the required data storage for saving image data to just 0.8% when compared to the original image data in terms of bits. Demo page: https://ms-dot-k.github.io/Image-to-Speech-Captioning.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Enhancing Speech-To-Speech Translation with Multiple TTS Targets Publication year: 2023 Coauthors: Jiatong Shi, Yun Tang, Ann Lee, H. Inaguma, Changhan Wang, J. Pino, Shinji Watanabe Abstract: It has been known that direct speech-to-speech translation (S2ST) models usually suffer from the data scarcity issue because of the limited existing parallel materials for both source and target speech. Therefore to train a direct S2ST system, previous works usually utilize text-to-speech (TTS) systems to generate samples in the target language by augmenting the data from speech-to-text translation (S2TT). However, there is a limited investigation into how the synthesized target speech would affect the S2ST models. In this work, we analyze the effect of changing synthesized target speech for direct S2ST models. We find that simply combining the target speech from different TTS systems can potentially improve the S2ST performances. Following that, we also propose a multi-task framework that jointly optimizes the S2ST system with multiple targets from different TTS systems. Extensive experiments demonstrate that our proposed framework achieves consistent improvements (2.8 BLEU) over the baselines on the Fisher Spanish-English dataset.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head Publication year: 2023 Coauthors: Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jia-Bin Huang, Jinglin Liu, Yixiang Ren, Zhou Zhao, Shinji Watanabe Abstract: Large language models (LLMs) have exhibited remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. Despite the recent success, current LLMs are not capable of processing complex audio information or conducting spoken conversations (like Siri or Alexa). In this work, we propose a multi-modal AI system named AudioGPT, which complements LLMs (i.e., ChatGPT) with 1) foundation models to process complex audio information and solve numerous understanding and generation tasks; and 2) the input/output interface (ASR, TTS) to support spoken dialogue. With an increasing demand to evaluate multi-modal LLMs of human intention understanding and cooperation with foundation models, we outline the principles and processes and test AudioGPT in terms of consistency, capability, and robustness. Experimental results demonstrate the capabilities of AudioGPT in solving AI tasks with speech, music, sound, and talking head understanding and generation in multi-round dialogues, which empower humans to create rich and diverse audio content with unprecedented ease. Our system is publicly available at \\\\url{https://github.com/AIGC-Audio/AudioGPT}.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Improving Audio Captioning Models with Fine-grained Audio Features, Text Embedding Supervision, and LLM Mix-up Augmentation Publication year: 2023 Coauthors: Shih-Lun Wu, Xuankai Chang, G. Wichern, Jee-weon Jung, Franccois G. Germain, Jonathan Le Roux, Shinji Watanabe Abstract: Automated audio captioning (AAC) aims to generate informative descriptions for various sounds from nature and/or human activities. In recent years, AAC has quickly attracted research interest, with state-of-the-art systems now relying on a sequence-to-sequence (seq2seq) backbone powered by strong models such as Transformers. Following the macro-trend of applied machine learning research, in this work, we strive to improve the performance of seq2seq AAC models by extensively leveraging pretrained models and large language models (LLMs). Specifically, we utilize BEATs to extract fine-grained audio features. Then, we employ Instructor LLM to fetch text embeddings of captions, and infuse their language-modality knowledge into BEATs audio features via an auxiliary InfoNCE loss function. Moreover, we propose a novel data augmentation method that uses ChatGPT to produce caption mix-ups (i.e., grammatical and compact combinations of two captions) which, together with the corresponding audio mixtures, increase not only the amount but also the complexity and diversity of training data. During inference, we propose to employ nucleus sampling and a hybrid reranking algorithm, which has not been explored in AAC research. Combining our efforts, our model achieves a new state-of-the-art 32.6 SPIDEr-FL score on the Clotho evaluation split, and wins the 2023 DCASE AAC challenge.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Voxtlm: unified decoder-only models for consolidating speech recognition/synthesis and speech/text continuation tasks Publication year: 2023 Coauthors: Soumi Maiti, Yifan Peng, Shukjae Choi, Jee-weon Jung, Xuankai Chang, Shinji Watanabe Abstract: We propose a decoder-only language model, VoxtLM, that can perform four tasks: speech recognition, speech synthesis, text generation, and speech continuation. VoxtLM integrates text vocabulary with discrete speech tokens from self-supervised speech features and uses special tokens to enable multitask learning. Compared to a single-task model, VoxtLM exhibits a significant improvement in speech synthesis, with improvements in both speech intelligibility from 28.9 to 5.6 and objective quality from 2.68 to 3.90. VoxtLM also improves speech generation and speech recognition performance over the single-task counterpart. Further, VoxtLM is trained with publicly available data and training recipes and model checkpoints are open-sourced to make fully reproducible work.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Learning to Speak from Text: Zero-Shot Multilingual Text-to-Speech with Unsupervised Text Pretraining Publication year: 2023 Coauthors: Takaaki Saeki, Soumi Maiti, Xinjian Li, Shinji Watanabe, Shinnosuke Takamichi, H. Saruwatari Abstract: While neural text-to-speech (TTS) has achieved human-like natural synthetic speech, multilingual TTS systems are limited to resource-rich languages due to the need for paired text and studio-quality audio data. This paper proposes a method for zero-shot multilingual TTS using text-only data for the target language. The use of text-only data allows the development of TTS systems for low-resource languages for which only textual resources are available, making TTS accessible to thousands of languages. Inspired by the strong cross-lingual transferability of multilingual language models, our framework first performs masked language model pretraining with multilingual text-only data. Then we train this model with a paired data in a supervised manner, while freezing a language-aware embedding layer. This allows inference even for languages not included in the paired data but present in the text-only data. Evaluation results demonstrate highly intelligible zero-shot TTS with a character error rate of less than 12% for an unseen language.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: FNeural Speech Enhancement with Very Low Algorithmic Latency and Complexity via Integrated full- and sub-band Modeling Publication year: 2023 Coauthors: Zhongqiu Wang, Samuele Cornell, Shukjae Choi, Younglo Lee, Byeonghak Kim, Shinji Watanabe Abstract: We propose FSB-LSTM, a novel long short-term memory (LSTM) based architecture that integrates full- and sub-band (FSB) modeling, for single- and multi-channel speech enhancement in the short-time Fourier transform (STFT) domain. The model maintains an information highway to flow an over-complete input representation through multiple FSB-LSTM modules. Each FSB-LSTM module consists of a full-band block to model spectro-temporal patterns at all frequencies and a sub-band block to model patterns within each sub-band, where each of the two blocks takes a down-sampled representation as input and returns an up-sampled discriminative representation to be added to the block input via a residual connection. The model is designed to have a low algorithmic complexity, a small run-time buffer and a very low algorithmic latency, at the same time producing a strong enhancement performance on a noisy-reverberant speech enhancement task even if the hop size is as low as 2 ms.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Improving Massively Multilingual ASR with Auxiliary CTC Objectives Publication year: 2023 Coauthors: William Chen, Brian Yan, Jiatong Shi, Yifan Peng, Soumi Maiti, Shinji Watanabe Abstract: Multilingual Automatic Speech Recognition (ASR) models have extended the usability of speech technologies to a wide variety of languages. With how many languages these models have to handle, however, a key to understanding their imbalanced performance across different languages is to examine if the model actually knows which language it should transcribe. In this paper, we introduce our work on improving performance on FLEURS, a 102-language open ASR benchmark, by conditioning the entire model on language identity (LID). We investigate techniques inspired from recent Connectionist Temporal Classification (CTC) studies to help the model handle the large number of languages, conditioning on the LID predictions of auxiliary tasks. Our experimental results demonstrate the effectiveness of our technique over standard CTC/Attention-based hybrid models. Furthermore, our state-of-the-art systems using self-supervised models with the Conformer architecture improve over the results of prior work on FLEURS by a relative 28.4% CER. Trained models are reproducible recipes are available at https://github.com/espnet/espnet/tree/master/egs2/fleurs/asr1.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Toward Universal Speech Enhancement For Diverse Input Conditions Publication year: 2023 Coauthors: Wangyou Zhang, Kohei Saijo, Zhong-Qiu Wang, Shinji Watanabe, Yanmin Qian Abstract: The past decade has witnessed substantial growth of data-driven speech enhancement (SE) techniques thanks to deep learning. While existing approaches have shown impressive performance in some common datasets, most of them are designed only for a single condition (e.g., single-channel, multi-channel, or a fixed sampling frequency) or only consider a single task (e.g., denoising or dereverberation). Currently, there is no universal SE approach that can effectively handle diverse input conditions with a single model. In this paper, we make the first attempt to investigate this line of research. First, we devise a single SE model that is independent of microphone channels, signal lengths, and sampling frequencies. Second, we design a universal SE benchmark by combining existing public corpora with multiple conditions. Our experiments on a wide range of datasets show that the proposed single model can successfully handle diverse conditions with strong performance.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: The Pipeline System of ASR and NLU with MLM-based data Augmentation Toward Stop Low-Resource Challenge Publication year: 2023 Coauthors: Hayato Futami, Jessica Huynh, Siddhant Arora, Shih-Lun Wu, Yosuke Kashiwagi, Yifan Peng, Brian Yan, E. Tsunoo, Shinji Watanabe Abstract: This paper describes our system for the low-resource domain adaptation track (Track 3) in Spoken Language Understanding Grand Challenge, which is a part of ICASSP Signal Processing Grand Challenge 2023. In the track, we adopt a pipeline approach of ASR and NLU. For ASR, we fine-tune Whisper for each domain with upsampling. For NLU, we fine-tune BART on all the Track3 data and then on low-resource domain data. We apply masked LM (MLM) -based data augmentation, where some of input tokens and corresponding target labels are replaced using MLM. We also apply a retrieval-based approach, where model input is augmented with similar training samples. As a result, we achieved exact match (EM) accuracy 63.3/75.0 (average: 69.15) for reminder/weather domain, and won the 1st place at the challenge.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: A Study on the Integration of Pipeline and E2E SLU Systems for Spoken Semantic Parsing Toward Stop Quality Challenge Publication year: 2023 Coauthors: Siddhant Arora, Hayato Futami, Shih-Lun Wu, Jessica Huynh, Yifan Peng, Yosuke Kashiwagi, E. Tsunoo, Brian Yan, Shinji Watanabe Abstract: Recently there have been efforts to introduce new benchmark tasks for spoken language understanding (SLU), like semantic parsing. In this paper, we describe our proposed spoken semantic parsing system for the quality track (Track 1) in Spoken Language Understanding Grand Challenge which is part of ICASSP Signal Processing Grand Challenge 2023. We experiment with both end-to-end and pipeline systems for this task. Strong automatic speech recognition (ASR) models like Whisper and pretrained Language models (LM) like BART are utilized inside our SLU framework to boost performance. We also investigate the output level combination of various models to get an exact match accuracy of 80.8, which won the 1st place at the challenge.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: A community cluster of influenza A(H3N2) virus infection with reduced susceptibility to baloxavir due to a PA E199G substitution in Japan, February to March 2023 Publication year: 2023 Coauthors: E. Takashita, Seiichiro Fujisaki, H. Morita, Shiho Nagata, H. Miura, Yuki Matsuura, Saya Yamamoto, Shoko Chiba, Yumiko Inoue, Iori Minami, Sayaka Yoshikawa, Seiko Yamazaki, N. Kishida, Kazuya Nakamura, Masayuki Shirakura, Shinji Watanabe, Hideki Hasegawa Abstract: A community cluster of influenza A(H3N2) caused by viruses with an E199G substitution in PA was detected in Nara, Japan, between February and March 2023. The three patients with these mutant viruses had not received antiviral treatment before specimen collection but patients in the same hospital had. The sequences of the mutant viruses were closely related, suggesting clonal spread in Nara. They showed reduced susceptibility to baloxavir in vitro; however, the clinical significance of the PA E199G substitution remains unclear.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study Publication year: 2023 Coauthors: Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma, Jiatong Shi, Jinchuan Tian, Shinji Watanabe, Yuya Fujita, Takashi Maekaku, Pengcheng Guo, Yao-Fei Cheng, Pavel Denisov, Kohei Saijo, Hsiu-Hsuan Wang Abstract: Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of discrete speech units derived from self-supervised learning representations, which significantly compresses the size of speech data. Applying various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech processing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good results in almost all the settings. We intend to release our configurations and trained models to foster future research efforts.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Joint Modelling of Spoken Language Understanding Tasks with Integrated Dialog History Publication year: 2023 Coauthors: Siddhant Arora, Hayato Futami, E. Tsunoo, Brian Yan, Shinji Watanabe Abstract: Most human interactions occur in the form of spoken conversations where the semantic meaning of a given utterance depends on the context. Each utterance in spoken conversation can be represented by many semantic and speaker attributes, and there has been an interest in building Spoken Language Understanding (SLU) systems for automatically predicting these attributes. Recent work has shown that incorporating dialogue history can help advance SLU performance. However, separate models are used for each SLU task, leading to an increase in inference time and computation cost. Motivated by this, we aim to ask: can we jointly model all the SLU tasks while incorporating context to facilitate low-latency and lightweight inference? To answer this, we propose a novel model architecture that learns dialog context to jointly predict the intent, dialog act, speaker role, and emotion for the spoken utterance. Note that our joint prediction is based on an autoregressive model and we need to decide the prediction order of dialog attributes, which is not trivial. To mitigate the issue, we also propose an order agnostic training method. Our experiments show that our joint model achieves similar results to task-specific classifiers and can effectively integrate dialog context to further improve the SLU performance.1'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Enhancing End-to-End Conversational Speech Translation Through Target Language Context Utilization Publication year: 2023 Coauthors: A. Hussein, Brian Yan, Antonios Anastasopoulos, Shinji Watanabe, S. Khudanpur Abstract: Incorporating longer context has been shown to benefit machine translation, but the inclusion of context in end-to-end speech translation (E2E-ST) remains under-studied. To bridge this gap, we introduce target language context in E2E-ST, enhancing coherence and overcoming memory constraints of extended audio segments. Additionally, we propose context dropout to ensure robustness to the absence of context, and further improve performance by adding speaker information. Our proposed contextual E2E-ST outperforms the isolated utterance-based E2E-ST approach. Lastly, we demonstrate that in conversational speech, contextual information primarily contributes to capturing context style, as well as resolving anaphora and named entities.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Challenges of Corporate Alliance CLOMA toward Plastic Litter Publication year: 2023 Coauthors: Shinji Watanabe '),\n",
       " Document(page_content='Author: Shinji Watanabe Title: The Multimodal Information Based Speech Processing (MISP) 2023 Challenge: Audio-Visual Target Speaker Extraction Publication year: 2023 Coauthors: Shilong Wu, Chenxi Wang, Hang Chen, Yusheng Dai, Chenyue Zhang, Ruoyu Wang, Hongbo Lan, Jun Du, Chin-Hui Lee, Jingdong Chen, Shinji Watanabe, Sabato Marco Siniscalchi, O. Scharenborg, Zhong-Qiu Wang, Jia Pan, Jianqing Gao Abstract: Previous Multimodal Information based Speech Processing (MISP) challenges mainly focused on audio-visual speech recognition (AVSR) with commendable success. However, the most advanced back-end recognition systems often hit performance limits due to the complex acoustic environments. This has prompted a shift in focus towards the Audio-Visual Target Speaker Extraction (AVTSE) task for the MISP 2023 challenge in ICASSP 2024 Signal Processing Grand Challenges. Unlike existing audio-visual speech enhance-ment challenges primarily focused on simulation data, the MISP 2023 challenge uniquely explores how front-end speech processing, combined with visual clues, impacts back-end tasks in real-world scenarios. This pioneering effort aims to set the first benchmark for the AVTSE task, offering fresh insights into enhancing the ac-curacy of back-end speech recognition systems through AVTSE in challenging and real acoustic environments. This paper delivers a thorough overview of the task setting, dataset, and baseline system of the MISP 2023 challenge. It also includes an in-depth analysis of the challenges participants may encounter. The experimental results highlight the demanding nature of this task, and we look forward to the innovative solutions participants will bring forward.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Intrusion of Coastal Oyashio water to Funka Bay and Tsugaru Strait occasionally disturbed by Kuroshio-originating warm core ring Publication year: 2023 Coauthors: H. Abe, Y. Yahiro, T. Hasegawa, T. Hirawake, H. Onishi, A. Ooki, T. Takatsu, K. Sasaki, M. Wakita, H. Kaneko, Shinji Watanabe, T. Tanaka, T. Okunishi, S. Ohno, S. Hashizume '),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Reproducing Whisper-Style Training Using An Open-Source Toolkit And Publicly Available Data Publication year: 2023 Coauthors: Yifan Peng, Jinchuan Tian, Brian Yan, Dan Berrebbi, Xuankai Chang, Xinjian Li, Jiatong Shi, Siddhant Arora, William Chen, Roshan Sharma, Wangyou Zhang, Yui Sudo, Muhammad Shakeel, Jee-weon Jung, Soumi Maiti, Shinji Watanabe Abstract: Pre-training speech models on large volumes of data has achieved remarkable success. OpenAI Whisper is a multilingual multitask model trained on 680k hours of supervised speech data. It generalizes well to various speech recognition and translation benchmarks even in a zero-shot setup. However, the full pipeline for developing such models (from data collection to training) is not publicly accessible, which makes it difficult for researchers to further improve its performance and address training-related issues such as efficiency, robustness, fairness, and bias. This work presents an Open Whisper-style Speech Model (OWSM), which reproduces Whisperstyle training using an open-source toolkit and publicly available data. OWSM even supports more translation directions and can be more efficient to train. We will publicly release all scripts used for data preparation, training, inference, and scoring as well as pretrained models and training logs to promote open science. 11https://github.com/espnet/espnet'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Visual Speech Recognition for Languages with Limited Labeled Data using Automatic Labels from Whisper Publication year: 2023 Coauthors: Jeong Hun Yeo, Minsu Kim, Shinji Watanabe, Y. Ro Abstract: This paper proposes a powerful Visual Speech Recognition (VSR) method for multiple languages, especially for low-resource languages that have a limited number of labeled data. Different from previous methods that tried to improve the VSR performance for the target language by using knowledge learned from other languages, we explore whether we can increase the amount of training data itself for the different languages without human intervention. To this end, we employ a Whisper model which can conduct both language identification and audio-based speech recognition. It serves to filter data of the desired languages and transcribe labels from the unannotated, multilingual audio-visual data pool. By comparing the performances of VSR models trained on automatic labels and the human-annotated labels, we show that we can achieve similar VSR performance to that of human-annotated labels even without utilizing human annotations. Through the automated labeling process, we label large-scale unlabeled multilingual databases, VoxCeleb2 and AVSpeech, producing 1,002 hours of data for four low VSR resource languages, French, Italian, Spanish, and Portuguese. With the automatic labels, we achieve new state-of-the-art performance on mTEDx in four languages, significantly surpassing the previous methods. The automatic labels are available online: https://github.com/JeongHun0716/Visual-Speech-Recognition-for-Low-Resource-Languages'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: The CHiME-7 DASR Challenge: Distant Meeting Transcription with Multiple Devices in Diverse Scenarios Publication year: 2023 Coauthors: Samuele Cornell, Matthew Wiesner, Shinji Watanabe, Desh Raj, Xuankai Chang, Paola García, Yoshiki Masuyama, Zhong-Qiu Wang, S. Squartini, S. Khudanpur Abstract: The CHiME challenges have played a significant role in the development and evaluation of robust automatic speech recognition (ASR) systems. We introduce the CHiME-7 distant ASR (DASR) task, within the 7th CHiME challenge. This task comprises joint ASR and diarization in far-field settings with multiple, and possibly heterogeneous, recording devices. Different from previous challenges, we evaluate systems on 3 diverse scenarios: CHiME-6, DiPCo, and Mixer 6. The goal is for participants to devise a single system that can generalize across different array geometries and use cases with no a-priori information. Another departure from earlier CHiME iterations is that participants are allowed to use open-source pre-trained models and datasets. In this paper, we describe the challenge design, motivation, and fundamental research questions in detail. We also present the baseline system, which is fully array-topology agnostic and features multi-channel diarization, channel selection, guided source separation and a robust ASR model that leverages self-supervised speech representations (SSLR).'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Cross-Modal Multi-Tasking for Speech-to-Text Translation via Hard Parameter Sharing Publication year: 2023 Coauthors: Brian Yan, Xuankai Chang, Antonios Anastasopoulos, Yuya Fujita, Shinji Watanabe Abstract: Recent works in end-to-end speech-to-text translation (ST) have proposed multi-tasking methods with soft parameter sharing which leverage machine translation (MT) data via secondary encoders that map text inputs to an eventual cross-modal representation. In this work, we instead propose a ST/MT multi-tasking framework with hard parameter sharing in which all model parameters are shared cross-modally. Our method reduces the speech-text modality gap via a pre-processing stage which converts speech and text inputs into two discrete token sequences of similar length -- this allows models to indiscriminately process both modalities simply using a joint vocabulary. With experiments on MuST-C, we demonstrate that our multi-tasking framework improves attentional encoder-decoder, Connectionist Temporal Classification (CTC), transducer, and joint CTC/attention models by an average of +0.5 BLEU without any external MT data. Further, we show that this framework incorporates external MT data, yielding +0.8 BLEU, and also improves transfer learning from pre-trained textual models, yielding +1.8 BLEU.'),\n",
       " Document(page_content=\"Author: Shinji Watanabe Title: ESPnet-ST-v2: Multipurpose Spoken Language Translation Toolkit Publication year: 2023 Coauthors: Brian Yan, Jiatong Shi, Yun Tang, H. Inaguma, Yifan Peng, Siddharth Dalmia, Peter Pol'ak, Patrick Fernandes, Dan Berrebbi, Tomoki Hayashi, Xiaohui Zhang, Zhaoheng Ni, Moto Hira, Soumi Maiti, J. Pino, Shinji Watanabe Abstract: ESPnet-ST-v2 is a revamp of the open-source ESPnet-ST toolkit necessitated by the broadening interests of the spoken language translation community. ESPnet-ST-v2 supports 1) offline speech-to-text translation (ST), 2) simultaneous speech-to-text translation (SST), and 3) offline speech-to-speech translation (S2ST) – each task is supported with a wide variety of approaches, differentiating ESPnet-ST-v2 from other open source spoken language translation toolkits. This toolkit offers state-of-the-art architectures such as transducers, hybrid CTC/attention, multi-decoders with searchable intermediates, time-synchronous blockwise CTC/attention, Translatotron models, and direct discrete unit models. In this paper, we describe the overall design, example models for each task, and performance benchmarking behind ESPnet-ST-v2, which is publicly available at https://github.com/espnet/espnet.\"),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Multi-Channel Target Speaker Extraction with Refinement: The WavLab Submission to the Second Clarity Enhancement Challenge Publication year: 2023 Coauthors: Samuele Cornell, Zhongqiu Wang, Yoshiki Masuyama, Shinji Watanabe, Manuel Pariente, Nobutaka Ono Abstract: This paper describes our submission to the Second Clarity Enhancement Challenge (CEC2), which consists of target speech enhancement for hearing-aid (HA) devices in noisy-reverberant environments with multiple interferers such as music and competing speakers. Our approach builds upon the powerful iterative neural/beamforming enhancement (iNeuBe) framework introduced in our recent work, and this paper extends it for target speaker extraction. We therefore name the proposed approach as iNeuBe-X, where the X stands for extraction. To address the challenges encountered in the CEC2 setting, we introduce four major novelties: (1) we extend the state-of-the-art TF-GridNet model, originally designed for monaural speaker separation, for multi-channel, causal speech enhancement, and large improvements are observed by replacing the TCNDenseNet used in iNeuBe with this new architecture; (2) we leverage a recent dual window size approach with future-frame prediction to ensure that iNueBe-X satisfies the 5 ms constraint on algorithmic latency required by CEC2; (3) we introduce a novel speaker-conditioning branch for TF-GridNet to achieve target speaker extraction; (4) we propose a fine-tuning step, where we compute an additional loss with respect to the target speaker signal compensated with the listener audiogram. Without using external data, on the official development set our best model reaches a hearing-aid speech perception index (HASPI) score of 0.942 and a scale-invariant signal-to-distortion ratio improvement (SI-SDRi) of 18.8 dB. These results are promising given the fact that the CEC2 data is extremely challenging (e.g., on the development set the mixture SI-SDR is -12.3 dB). A demo of our submitted system is available at WAVLab CEC2 demo.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Fully Unsupervised Topic Clustering of Unlabelled Spoken Audio Using Self-Supervised Representation Learning and Topic Model Publication year: 2023 Coauthors: Takashi Maekaku, Yuya Fujita, Xuankai Chang, Shinji Watanabe Abstract: Unsupervised topic clustering of spoken audio is an important research topic for zero-resourced unwritten languages. A classical approach is to find a set of spoken terms from only the audio based on dynamic time warping or generative modeling (e.g., hidden Markov model), and apply a topic model to classify topics. The spoken term discovery is the most important and difficult part. In this paper, we propose to combine self-supervised representation learning (SSRL) methods as a component of spoken term discovery and probabilistic topic models. Most SSRL methods pre-train a model which predicts high-quality pseudo labels generated from an audio-only corpus. These pseudo labels can be used to produce a sequence of pseudo subwords by applying deduplication and a subword model. Then, we apply a topic model based on latent Dirichlet allocation for these pseudo-subword sequences in an unsupervised manner. The clustering performance is evaluated on the Fisher corpus using normalized mutual information. We confirm the improvement of the proposed method and its effectiveness compared to an existing approach using dynamic time warping and topic models although the experimental setups are not directly comparable.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: TAPLoss: A Temporal Acoustic Parameter Loss for Speech Enhancement Publication year: 2023 Coauthors: YUNYANG ZENG, Joseph Konan, Shuo Han, David Bick, Muqiao Yang, Anurag Kumar, Shinji Watanabe, B. Raj Abstract: Speech enhancement models have greatly progressed in recent years, but still show limits in perceptual quality of their speech outputs. We propose an objective for perceptual quality based on temporal acoustic parameters. These are fundamental speech features that play an essential role in various applications, including speaker recognition and paralinguistic analysis. We provide a differentiable estimator for four categories of low-level acoustic descriptors involving: frequency-related parameters, energy or amplitude-related parameters, spectral balance parameters, and temporal features. Un-like prior work that looks at aggregated acoustic parameters or a few categories of acoustic parameters, our temporal acoustic parameter (TAP) loss enables auxiliary optimization and improvement of many fine-grained speech characteristics in enhancement workflows. We show that adding TAPLoss as an auxiliary objective in speech enhancement produces speech with improved perceptual quality and intelligibility. We use data from the Deep Noise Suppression 2020 Challenge to demonstrate that both time-domain models and time-frequency domain models can benefit from our method.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: End-to-End Speech Recognition: A Survey Publication year: 2023 Coauthors: Rohit Prabhavalkar, Takaaki Hori, Tara N. Sainath, R. Schluter, Shinji Watanabe Abstract: In the last decade of automatic speech recognition (ASR) research, the introduction of deep learning has brought considerable reductions in word error rate of more than 50% relative, compared to modeling without deep learning. In the wake of this transition, a number of all-neural ASR architectures have been introduced. These so-called end-to-end (E2E) models provide highly integrated, completely neural ASR models, which rely strongly on general machine learning knowledge, learn more consistently from data, with lower dependence on ASR domain-specific experience. The success and enthusiastic adoption of deep learning, accompanied by more generic model architectures has led to E2E models now becoming the prominent ASR approach. The goal of this survey is to provide a taxonomy of E2E ASR models and corresponding improvements, and to discuss their properties and their relationship to classical hidden Markov model (HMM) based ASR architectures. All relevant aspects of E2E ASR are covered in this work: modeling, training, decoding, and external language model integration, discussions of performance and deployment opportunities, as well as an outlook into potential future developments.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: The Multimodal Information Based Speech Processing (Misp) 2022 Challenge: Audio-Visual Diarization And Recognition Publication year: 2023 Coauthors: Zhe Wang, Shilong Wu, Hang Chen, Maokui He, Jun Du, Chin-Hui Lee, Jingdong Chen, Shinji Watanabe, Sabato Marco Siniscalchi, O. Scharenborg, Diyuan Liu, Baocai Yin, Jia Pan, Jianqing Gao, Cong Liu Abstract: The Multi-modal Information based Speech Processing (MISP) challenge aims to extend the application of signal processing technology in specific scenarios by promoting the research into wake-up words, speaker diarization, speech recognition, and other technologies. The MISP2022 challenge has two tracks: 1) audio-visual speaker diarization (AVSD), aiming to solve \"who spoken when\" using both audio and visual data; 2) a novel audio-visual diarization and recognition (AVDR) task that focuses on addressing \"who spoken what when\" with audio-visual speaker diarization results. Both tracks focus on the Chinese language, and use far-field audio and video in real home-tv scenarios: 2-6 people communicating each other with TV noise in the background. This paper introduces the dataset, track settings, and baselines of the MISP2022 challenge. Our analyses of experiments and examples indicate the good performance of AVDR baseline system, and the potential difficulties in this challenge due to, e.g., the far-field video quality, the presence of TV noise in the background, and the indistinguishable speakers.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models Publication year: 2023 Coauthors: Yifan Peng, Yui Sudo, Muhammad Shakeel, Shinji Watanabe Abstract: Self-supervised learning (SSL) has achieved notable success in many speech processing tasks, but the large model size and heavy computational cost hinder the deployment. Knowledge distillation trains a small student model to mimic the behavior of a large teacher model. However, the student architecture usually needs to be manually designed and will remain fixed during training, which requires prior knowledge and can lead to suboptimal performance. Inspired by recent success of task-specific structured pruning, we propose DPHuBERT, a novel task-agnostic compression method for speech SSL based on joint distillation and pruning. Experiments on SUPERB show that DPHuBERT outperforms pure distillation methods in almost all tasks. Moreover, DPHuBERT requires little training time and performs well with limited training data, making it suitable for resource-constrained applications. Our method can also be applied to various speech SSL models. Our code and models will be publicly available.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: An external quality assessment feasibility study; cross laboratory comparison of haemagglutination inhibition assay and microneutralization assay performance for seasonal influenza serology testing: A FLUCOP study Publication year: 2023 Coauthors: J. Waldock, C. Weiss, Wei Wang, M. Levine, Stacie N. Jefferson, S. Ho, K. Hoschler, B. Londt, E. Masat, Louise A. Carolan, Stephany Sánchez-Ovando, A. Fox, Shinji Watanabe, Miki Akimoto, Aya Sato, N. Kishida, A. Buys, Lorens Maake, Cardia Fourie, Catherine Caillet, Sandrine Raynaud, R. Webby, J. Debeauchamp, R. Cox, Sarah Lartey, C. Trombetta, S. Marchi, E. Montomoli, I. Sanz-Muñoz, J. Eiros, Javier Sánchez-Martínez, D. Duijsings, O. Engelhardt Abstract: Introduction External Quality Assessment (EQA) schemes are designed to provide a snapshot of laboratory proficiency, identifying issues and providing feedback to improve laboratory performance and inter-laboratory agreement in testing. Currently there are no international EQA schemes for seasonal influenza serology testing. Here we present a feasibility study for conducting an EQA scheme for influenza serology methods. Methods We invited participant laboratories from industry, contract research organizations (CROs), academia and public health institutions who regularly conduct hemagglutination inhibition (HAI) and microneutralization (MN) assays and have an interest in serology standardization. In total 16 laboratories returned data including 19 data sets for HAI assays and 9 data sets for MN assays. Results Within run analysis demonstrated good laboratory performance for HAI, with intrinsically higher levels of intra-assay variation for MN assays. Between run analysis showed laboratory and strain specific issues, particularly with B strains for HAI, whilst MN testing was consistently good across labs and strains. Inter-laboratory variability was higher for MN assays than HAI, however both assays showed a significant reduction in inter-laboratory variation when a human sera pool is used as a standard for normalization. Discussion This study has received positive feedback from participants, highlighting the benefit such an EQA scheme would have on improving laboratory performance, reducing inter laboratory variation and raising awareness of both harmonized protocol use and the benefit of biological standards for seasonal influenza serology testing.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Multi-Channel Speaker Extraction with Adversarial Training: The Wavlab Submission to The Clarity ICASSP 2023 Grand Challenge Publication year: 2023 Coauthors: Samuele Cornell, Zhongqiu Wang, Yoshiki Masuyama, Shinji Watanabe, Manuel Pariente, Nobutaka Ono, S. Squartini Abstract: In this work we detail our submission to the Clarity ICASSP 2023 grand challenge, in which participants have to develop a strong target speech enhancement system for hearing-aid (HA) devices in noisy-reverberant environments. Our system builds on our previous submission at the Second Clarity Enhancement Challenge (CEC2): iNeuBe-X, which consists in an iterative neural/conventional beamforming enhancement pipeline, guided by an enrollment utterance from the target speaker. This model, which won by a large margin the CEC2, is an extension of the state-of-the-art TF-GridNet model for multi-channel, streamable target-speaker speech enhancement. Here, this approach is extended and further improved by leveraging generative adversarial training, which we show proves especially useful when the training data is limited. Using only the official 6k training scenes data, our best model achieves 0.80 hearing-aid speech perception index (HASPI) and 0.41 hearing-aid speech quality index (HASQI) scores on the synthetic evaluation set. However, our model generalized poorly on the semi-real evaluation set. This highlights the fact that our community should focus more on real-world evaluation and less on fully synthetic datasets.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: AV-SUPERB: A Multi-Task Evaluation Benchmark for Audio-Visual Representation Models Publication year: 2023 Coauthors: Yuan Tseng, Layne Berry, Yi-Ting Chen, I-Hsiang Chiu, Hsuan-Hao Lin, Max Liu, Puyuan Peng, Yi-Jen Shih, Hung-Yu Wang, Haibin Wu, Po-Yao Huang, Chun-Mao Lai, Shang-Wen Li, David F. Harwath, Yu Tsao, Shinji Watanabe, Abdel-rahman Mohamed, Chi-Luen Feng, Hung-yi Lee Abstract: Audio-visual representation learning aims to develop systems with human-like perception by utilizing correlation between auditory and visual information. However, current models often focus on a limited set of tasks, and generalization abilities of learned representations are unclear. To this end, we propose the AV-SUPERB benchmark that enables general-purpose evaluation of unimodal audio/visual and bimodal fusion representations on 7 datasets covering 5 audio-visual tasks in speech and audio processing. We evaluate 5 recent self-supervised models and show that none of these models generalize to all tasks, emphasizing the need for future study on improving universal model performance. In addition, we show that representations may be improved with intermediate-task fine-tuning and audio event classification with AudioSet serves as a strong intermediate task. We release our benchmark with evaluation code and a model submission platform to encourage further research in audio-visual learning.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation Publication year: 2023 Coauthors: Yu-Kuan Fu, Liang-Hsuan Tseng, Jiatong Shi, Chen-An Li, Tsung-Yuan Hsu, Shinji Watanabe, Hung-yi Lee Abstract: Most of the speech translation models heavily rely on parallel data, which is hard to collect especially for low-resource languages. To tackle this issue, we propose to build a cascaded speech translation system without leveraging any kind of paired data. We use fully unpaired data to train our unsupervised systems and evaluate our results on CoVoST 2 and CVSS. The results show that our work is comparable with some other early supervised methods in some language pairs. While cascaded systems always suffer from severe error propagation problems, we proposed denoising back-translation (DBT), a novel approach to building robust unsupervised neural machine translation (UNMT). DBT successfully increases the BLEU score by 0.7--0.9 in all three translation directions. Moreover, we simplified the pipeline of our cascaded system to reduce inference latency and conducted a comprehensive analysis of every part of our work. We also demonstrate our unsupervised speech translation results on the established website.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Deep Speech Synthesis from MRI-Based Articulatory Representations Publication year: 2023 Coauthors: Peter Wu, Tingle Li, Yijingxiu Lu, Yubin Zhang, Jiachen Lian, A. Black, L. Goldstein, Shinji Watanabe, G. Anumanchipalli Abstract: In this paper, we study articulatory synthesis, a speech synthesis method using human vocal tract information that offers a way to develop efficient, generalizable and interpretable synthesizers. While recent advances have enabled intelligible articulatory synthesis using electromagnetic articulography (EMA), these methods lack critical articulatory information like excitation and nasality, limiting generalization capabilities. To bridge this gap, we propose an alternative MRI-based feature set that covers a much more extensive articulatory space than EMA. We also introduce normalization and denoising procedures to enhance the generalizability of deep learning methods trained on MRI data. Moreover, we propose an MRI-to-speech model that improves both computational efficiency and speech fidelity. Finally, through a series of ablations, we show that the proposed MRI representation is more comprehensive than EMA and identify the most suitable MRI feature subset for articulatory synthesis.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN Publication year: 2023 Coauthors: Sweta Agrawal, Antonios Anastasopoulos, L. Bentivogli, Ondrej Bojar, Claudia Borg, Marine Carpuat, Roldano Cattoni, Mauro Cettolo, Mingda Chen, William Chen, K. Choukri, Alexandra Chronopoulou, Anna Currey, T. Declerck, Qianqian Dong, Kevin Duh, Y. Estève, Marcello Federico, Souhir Gahbiche, B. Haddow, B. Hsu, Phu Mon Htut, H. Inaguma, Dávid Javorský, J. Judge, Yasumasa Kano, Tom Ko, Rishu Kumar, Peng Li, Xutai Ma, Prashant Mathur, E. Matusov, Paul McNamee, John P. McCrae, Kenton Murray, Maria Nadejde, Satoshi Nakamura, Matteo Negri, H. Nguyen, J. Niehues, Xing Niu, Atul Kr. Ojha, John E. Ortega, Proyag Pal, J. Pino, Lonneke van der Plas, Peter Polák, Elijah Matthew Rippeth, Elizabeth Salesky, Jiatong Shi, Matthias Sperber, Sebastian Stüker, Katsuhito Sudoh, Yun Tang, Brian Thompson, Ke M. Tran, M. Turchi, A. Waibel, Mingxuan Wang, Shinji Watanabe, Rodolfo Zevallos Abstract: This paper reports on the shared tasks organized by the 20th IWSLT Conference. The shared tasks address 9 scientific challenges in spoken language translation: simultaneous and offline translation, automatic subtitling and dubbing, speech-to-speech translation, multilingual, dialect and low-resource speech translation, and formality control. The shared tasks attracted a total of 38 submissions by 31 teams. The growing interest towards spoken language translation is also witnessed by the constantly increasing number of shared task organizers and contributors to the overview paper, almost evenly distributed across industry and academia.'),\n",
       " Document(page_content=\"Author: Shinji Watanabe Title: Speech collage: code-switched audio generation by collaging monolingual corpora Publication year: 2023 Coauthors: A. Hussein, Dorsa Zeinali, Ondrej Klejch, Matthew Wiesner, Brian Yan, Shammur A. Chowdhury, Ahmed Ali, Shinji Watanabe, S. Khudanpur Abstract: Designing effective automatic speech recognition (ASR) systems for Code-Switching (CS) often depends on the availability of the transcribed CS resources. To address data scarcity, this paper introduces Speech Collage, a method that synthesizes CS data from monolingual corpora by splicing audio segments. We further improve the smoothness quality of audio generation using an overlap-add approach. We investigate the impact of generated data on speech recognition in two scenarios: using in-domain CS text and a zero-shot approach with synthesized CS text. Empirical results highlight up to 34.4% and 16.2% relative reductions in Mixed-Error Rate and Word-Error Rate for in-domain and zero-shot scenarios, respectively. Lastly, we demonstrate that CS augmentation bolsters the model's code-switching inclination and reduces its monolingual bias.\"),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Exploration on HuBERT with Multiple Resolutions Publication year: 2023 Coauthors: Jiatong Shi, Yun Tang, H. Inaguma, Hongyu Gong, J. Pino, Shinji Watanabe Abstract: Hidden-unit BERT (HuBERT) is a widely-used self-supervised learning (SSL) model in speech processing. However, we argue that its fixed 20ms resolution for hidden representations would not be optimal for various speech-processing tasks since their attributes (e.g., speaker characteristics and semantics) are based on different time scales. To address this limitation, we propose utilizing HuBERT representations at multiple resolutions for downstream tasks. We explore two approaches, namely the parallel and hierarchical approaches, for integrating HuBERT features with different resolutions. Through experiments, we demonstrate that HuBERT with multiple resolutions outperforms the original model. This highlights the potential of utilizing multiple resolutions in SSL models like HuBERT to capture diverse information from speech signals.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Integrating Pretrained ASR and LM to Perform Sequence Generation for Spoken Language Understanding Publication year: 2023 Coauthors: Siddhant Arora, Hayato Futami, Yosuke Kashiwagi, E. Tsunoo, Brian Yan, Shinji Watanabe Abstract: There has been an increased interest in the integration of pretrained speech recognition (ASR) and language models (LM) into the SLU framework. However, prior methods often struggle with a vocabulary mismatch between pretrained models, and LM cannot be directly utilized as they diverge from its NLU formulation. In this study, we propose a three-pass end-to-end (E2E) SLU system that effectively integrates ASR and LM subnetworks into the SLU formulation for sequence generation tasks. In the first pass, our architecture predicts ASR transcripts using the ASR subnetwork. This is followed by the LM subnetwork, which makes an initial SLU prediction. Finally, in the third pass, the deliberation subnetwork conditions on representations from the ASR and LM subnetworks to make the final prediction. Our proposed three-pass SLU system shows improved performance over cascaded and E2E SLU models on two benchmark SLU datasets, SLURP and SLUE, especially on acoustically challenging utterances.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Reducing Barriers to Self-Supervised Learning: HuBERT Pre-training with Academic Compute Publication year: 2023 Coauthors: William Chen, Xuankai Chang, Yifan Peng, Zhaoheng Ni, Soumi Maiti, Shinji Watanabe Abstract: Self-supervised learning (SSL) has led to great strides in speech processing. However, the resources needed to train these models has become prohibitively large as they continue to scale. Currently, only a few groups with substantial resources are capable of creating SSL models, which harms reproducibility. In this work, we optimize HuBERT SSL to fit in academic constraints. We reproduce HuBERT independently from the original implementation, with no performance loss. Our code and training optimizations make SSL feasible with only 8 GPUs, instead of the 32 used in the original work. We also explore a semi-supervised route, using an ASR model to skip the first pre-training iteration. Within one iteration of pre-training, our models improve over HuBERT on several tasks. Furthermore, our HuBERT Large variant requires only 8 GPUs, achieving similar performance to the original trained on 128. As our contribution to the community, all models, configurations, and code are made open-source in ESPnet.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: FindAdaptNet: Find and Insert Adapters by Learned Layer Importance Publication year: 2023 Coauthors: Junwei Huang, Karthik Ganesan, Soumi Maiti, Young Min Kim, Xuankai Chang, Paul Liang, Shinji Watanabe Abstract: Adapters are lightweight bottleneck modules introduced to assist pre-trained self-supervised learning (SSL) models to be customized to new tasks. However, searching the appropriate layers to insert adapters on large models has become difficult due to the large number of possible layers and thus a vast search space (2N possibilities for N layers). In this paper, we propose a technique that achieves automatic insertion of adapters for downstream automatic speech recognition (ASR) and spoken language understanding (SLU) tasks. Our approach is based on two-stage training. First, we train our model for a specific downstream task with additional shallow learnable layers and weight parameters to obtain the weighted summation over the output of each layer in SSL. This training method is established by the SUPERB baseline [1]. This first-stage training determines the most important layers given their respective weights. In the second stage, we proceed to insert adapters to the most important layers, retaining both performance and neural architecture search efficiency. On the CommonVoice dataset[2] we obtain 20.6% absolute improvement in Word Error Rate (WER) on the Welsh language against the conventional method, which inserts the adapter modules into the highest layers without search. In the SLURP SLU task, our method yields 4.0% intent accuracy improvement against the same conventional baseline.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: CMU’s IWSLT 2023 Simultaneous Speech Translation System Publication year: 2023 Coauthors: Brian Yan, Jiatong Shi, Soumi Maiti, William Chen, Xinjian Li, Yifan Peng, Siddhant Arora, Shinji Watanabe Abstract: This paper describes CMU’s submission to the IWSLT 2023 simultaneous speech translation shared task for translating English speech to both German text and speech in a streaming fashion. We first build offline speech-to-text (ST) models using the joint CTC/attention framework. These models also use WavLM front-end features and mBART decoder initialization. We adapt our offline ST models for simultaneous speech-to-text translation (SST) by 1) incrementally encoding chunks of input speech, re-computing encoder states for each new chunk and 2) incrementally decoding output text, pruning beam search hypotheses to 1-best after processing each chunk. We then build text-to-speech (TTS) models using the VITS framework and achieve simultaneous speech-to-speech translation (SS2ST) by cascading our SST and TTS models.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Exploration of Efficient End-to-End ASR using Discretized Input from Self-Supervised Learning Publication year: 2023 Coauthors: Xuankai Chang, Brian Yan, Yuya Fujita, Takashi Maekaku, Shinji Watanabe Abstract: Self-supervised learning (SSL) of speech has shown impressive results in speech-related tasks, particularly in automatic speech recognition (ASR). While most methods employ the output of intermediate layers of the SSL model as real-valued features for downstream tasks, there is potential in exploring alternative approaches that use discretized token sequences. This approach offers benefits such as lower storage requirements and the ability to apply techniques from natural language processing. In this paper, we propose a new protocol that utilizes discretized token sequences in ASR tasks, which includes de-duplication and sub-word modeling to enhance the input sequence. It reduces computational cost by decreasing the length of the sequence. Our experiments on the LibriSpeech dataset demonstrate that our proposed protocol performs competitively with conventional ASR systems using continuous input features, while reducing computational and storage costs.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Integration of Frame- and Label-synchronous Beam Search for Streaming Encoder-decoder Speech Recognition Publication year: 2023 Coauthors: E. Tsunoo, Hayato Futami, Yosuke Kashiwagi, Siddhant Arora, Shinji Watanabe Abstract: Although frame-based models, such as CTC and transducers, have an affinity for streaming automatic speech recognition, their decoding uses no future knowledge, which could lead to incorrect pruning. Conversely, label-based attention encoder-decoder mitigates this issue using soft attention to the input, while it tends to overestimate labels biased towards its training domain, unlike CTC. We exploit these complementary attributes and propose to integrate the frame- and label-synchronous (F-/L-Sync) decoding alternately performed within a single beam-search scheme. F-Sync decoding leads the decoding for block-wise processing, while L-Sync decoding provides the prioritized hypotheses using look-ahead future frames within a block. We maintain the hypotheses from both decoding methods to perform effective pruning. Experiments demonstrate that the proposed search algorithm achieves lower error rates compared to the other search methods, while being robust against out-of-domain situations.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: E-Branchformer-Based E2E SLU Toward Stop on-Device Challenge Publication year: 2023 Coauthors: Yosuke Kashiwagi, Siddhant Arora, Hayato Futami, Jessica Huynh, Shih-Lun Wu, Yifan Peng, Brian Yan, E. Tsunoo, Shinji Watanabe Abstract: In this paper, we report our team’s study on track 2 of the Spoken Language Understanding Grand Challenge, which is a component of the ICASSP Signal Processing Grand Challenge 2023. The task is intended for on-device processing and involves estimating semantic parse labels from speech using a model with 15 million parameters. We use E2E E-Branchformer-based spoken language understanding model, which is more parameter controllable than cascade models, and reduced the parameter size through sequential distillation and tensor decomposition techniques. On the STOP dataset, we achieved an exact match accuracy of 70.9% under the tight constraint of 15 million parameters.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Incremental Blockwise Beam Search for Simultaneous Speech Translation with Controllable Quality-Latency Tradeoff Publication year: 2023 Coauthors: Peter Polák, Brian Yan, Shinji Watanabe, A. Waibel, Ondrej Bojar Abstract: Blockwise self-attentional encoder models have recently emerged as one promising end-to-end approach to simultaneous speech translation. These models employ a blockwise beam search with hypothesis reliability scoring to determine when to wait for more input speech before translating further. However, this method maintains multiple hypotheses until the entire speech input is consumed -- this scheme cannot directly show a single \\\\textit{incremental} translation to users. Further, this method lacks mechanisms for \\\\textit{controlling} the quality vs. latency tradeoff. We propose a modified incremental blockwise beam search incorporating local agreement or hold-$n$ policies for quality-latency control. We apply our framework to models trained for online or offline translation and demonstrate that both types can be effectively used in online mode. Experimental results on MuST-C show 0.6-3.6 BLEU improvement without changing latency or 0.8-1.4 s latency improvement without changing quality.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Bayes Risk Transducer: Transducer with Controllable Alignment Prediction Publication year: 2023 Coauthors: Jinchuan Tian, Jianwei Yu, Hangting Chen, Brian Yan, Chao Weng, Dong Yu, Shinji Watanabe Abstract: Automatic speech recognition (ASR) based on transducers is widely used. In training, a transducer maximizes the summed posteriors of all paths. The path with the highest posterior is commonly defined as the predicted alignment between the speech and the transcription. While the vanilla transducer does not have a prior preference for any of the valid paths, this work intends to enforce the preferred paths and achieve controllable alignment prediction. Specifically, this work proposes Bayes Risk Transducer (BRT), which uses a Bayes risk function to set lower risk values to the preferred paths so that the predicted alignment is more likely to satisfy specific desired properties. We further demonstrate that these predicted alignments with intentionally designed properties can provide practical advantages over the vanilla transducer. Experimentally, the proposed BRT saves inference cost by up to 46% for non-streaming ASR and reduces overall system latency by 41% for streaming ASR.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Time-synchronous one-pass Beam Search for Parallel Online and Offline Transducers with Dynamic Block Training Publication year: 2023 Coauthors: Yui Sudo, Muhammad Shakeel, Yifan Peng, Shinji Watanabe '),\n",
       " Document(page_content=\"Author: Shinji Watanabe Title: ESPnet-SPK: full pipeline speaker embedding toolkit with reproducible recipes, self-supervised front-ends, and off-the-shelf models Publication year: 2024 Coauthors: Jee-weon Jung, Wangyou Zhang, Jiatong Shi, Zakaria Aldeneh, Takuya Higuchi, B. Theobald, A. H. Abdelaziz, Shinji Watanabe Abstract: This paper introduces ESPnet-SPK, a toolkit designed with several objectives for training speaker embedding extractors. First, we provide an open-source platform for researchers in the speaker recognition community to effortlessly build models. We provide several models, ranging from x-vector to recent SKA-TDNN. Through the modularized architecture design, variants can be developed easily. We also aspire to bridge developed models with other domains, facilitating the broad research community to effortlessly incorporate state-of-the-art embedding extractors. Pre-trained embedding extractors can be accessed in an off-the-shelf manner and we demonstrate the toolkit's versatility by showcasing its integration with two tasks. Another goal is to integrate with diverse self-supervised learning features. We release a reproducible recipe that achieves an equal error rate of 0.39% on the Vox1-O evaluation protocol using WavLM-Large with ECAPA-TDNN.\"),\n",
       " Document(page_content='Author: Shinji Watanabe Title: AugSumm: towards generalizable speech summarization using synthetic labels from large language model Publication year: 2024 Coauthors: Jee-weon Jung, Roshan Sharma, William Chen, Bhiksha Raj, Shinji Watanabe Abstract: Abstractive speech summarization (SSUM) aims to generate human-like summaries from speech. Given variations in information captured and phrasing, recordings can be summarized in multiple ways. Therefore, it is more reasonable to consider a probabilistic distribution of all potential summaries rather than a single summary. However, conventional SSUM models are mostly trained and evaluated with a single ground-truth (GT) human-annotated deterministic summary for every recording. Generating multiple human references would be ideal to better represent the distribution statistically, but is impractical because annotation is expensive. We tackle this challenge by proposing AugSumm, a method to leverage large language models (LLMs) as a proxy for human annotators to generate augmented summaries for training and evaluation. First, we explore prompting strategies to generate synthetic summaries from ChatGPT. We validate the quality of synthetic summaries using multiple metrics including human evaluation, where we find that summaries generated using AugSumm are perceived as more valid to humans. Second, we develop methods to utilize synthetic summaries in training and evaluation. Experiments on How2 demonstrate that pre-training on synthetic summaries and fine-tuning on GT summaries improves ROUGE-L by 1 point on both GT and AugSumm-based test sets. AugSumm summaries are available at https://github.com/Jungjee/AugSumm.'),\n",
       " Document(page_content=\"Author: Shinji Watanabe Title: OWSM v3.1: Better and Faster Open Whisper-Style Speech Models based on E-Branchformer Publication year: 2024 Coauthors: Yifan Peng, Jinchuan Tian, William Chen, Siddhant Arora, Brian Yan, Yui Sudo, Muhammad Shakeel, Kwanghee Choi, Jiatong Shi, Xuankai Chang, Jee-weon Jung, Shinji Watanabe Abstract: Recent studies have advocated for fully open foundation models to promote transparency and open science. As an initial step, the Open Whisper-style Speech Model (OWSM) reproduced OpenAI's Whisper using publicly available data and open-source toolkits. With the aim of reproducing Whisper, the previous OWSM v1 through v3 models were still based on Transformer, which might lead to inferior performance compared to other state-of-the-art speech encoders. In this work, we aim to improve the performance and efficiency of OWSM without extra training data. We present E-Branchformer based OWSM v3.1 models at two scales, i.e., 100M and 1B. The 1B model is the largest E-Branchformer based speech model that has been made publicly available. It outperforms the previous OWSM v3 in a vast majority of evaluation benchmarks, while demonstrating up to 25% faster inference speed. We publicly release the data preparation scripts, pre-trained models and training logs.\"),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Can you Remove the Downstream Model for Speaker Recognition with Self-Supervised Speech Features? Publication year: 2024 Coauthors: Zakaria Aldeneh, Takuya Higuchi, Jee-weon Jung, Skyler Seto, T. Likhomanenko, Stephen Shum, A. H. Abdelaziz, Shinji Watanabe, B. Theobald Abstract: Self-supervised features are typically used in place of filter-banks in speaker verification models. However, these models were originally designed to ingest filter-banks as inputs, and thus, training them on top of self-supervised features assumes that both feature types require the same amount of learning for the task. In this work, we observe that pre-trained self-supervised speech features inherently include information required for downstream speaker verification task, and therefore, we can simplify the downstream model without sacrificing performance. To this end, we revisit the design of the downstream model for speaker verification using self-supervised features. We show that we can simplify the model to use 97.51% fewer parameters while achieving a 29.93% average improvement in performance on SUPERB. Consequently, we show that the simplified downstream model is more data efficient compared to baseline--it achieves better performance with only 60% of the training data.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: UniverSLU: Universal Spoken Language Understanding for Diverse Classification and Sequence Generation Tasks with a Single Network Publication year: 2023 Coauthors: Siddhant Arora, Hayato Futami, Jee-weon Jung, Yifan Peng, Roshan Sharma, Yosuke Kashiwagi, E. Tsunoo, Shinji Watanabe Abstract: Recent studies have demonstrated promising outcomes by employing large language models with multi-tasking capabilities. They utilize prompts to guide the model\\'s behavior and surpass performance of task-specific models. Motivated by this, we ask: can we build a single model that jointly perform various spoken language understanding (SLU) tasks? To address this, we utilize pre-trained automatic speech recognition (ASR) models and employ various task and dataset specifiers as discrete prompts. We demonstrate efficacy of our single multi-task learning (MTL) model\"UniverSLU\"for 12 different speech classification and sequence generation tasks across 17 datasets and 9 languages. Results show that UniverSLU achieves competitive performance and even surpasses task-specific models. We also conduct preliminary investigations into enabling human-interpretable natural phrases instead of task specifiers as discrete prompts and test the model\\'s generalization capabilities to new paraphrases.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: One model to rule them all ? Towards End-to-End Joint Speaker Diarization and Speech Recognition Publication year: 2023 Coauthors: Samuele Cornell, Jee-weon Jung, Shinji Watanabe, S. Squartini Abstract: This paper presents a novel framework for joint speaker diarization (SD) and automatic speech recognition (ASR), named SLIDAR (sliding-window diarization-augmented recognition). SLIDAR can process arbitrary length inputs and can handle any number of speakers, effectively solving ``who spoke what, when\\'\\' concurrently. SLIDAR leverages a sliding window approach and consists of an end-to-end diarization-augmented speech transcription (E2E DAST) model which provides, locally, for each window: transcripts, diarization and speaker embeddings. The E2E DAST model is based on an encoder-decoder architecture and leverages recent techniques such as serialized output training and ``Whisper-style\"prompting. The local outputs are then combined to get the final SD+ASR result by clustering the speaker embeddings to get global speaker identities. Experiments performed on monaural recordings from the AMI corpus confirm the effectiveness of the method in both close-talk and far-field speech scenarios.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Understanding Probe Behaviors through Variational Bounds of Mutual Information Publication year: 2023 Coauthors: Kwanghee Choi, Jee-weon Jung, Shinji Watanabe Abstract: With the success of self-supervised representations, researchers seek a better understanding of the information encapsulated within a representation. Among various interpretability methods, we focus on classification-based linear probing. We aim to foster a solid understanding and provide guidelines for linear probing by constructing a novel mathematical framework leveraging information theory. First, we connect probing with the variational bounds of mutual information (MI) to relax the probe design, equating linear probing with fine-tuning. Then, we investigate empirical behaviors and practices of probing through our mathematical framework. We analyze the layer-wise performance curve being convex, which seemingly violates the data processing inequality. However, we show that the intermediate representations can have the biggest MI estimate because of the tradeoff between better separability and decreasing MI. We further suggest that the margin of linearly separable representations can be a criterion for measuring the\"goodness of representation.\"We also compare accuracy with MI as the measuring criteria. Finally, we empirically validate our claims by observing the self-supervised speech models on retaining word and phoneme information.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Improving ASR Contextual Biasing with Guided Attention Publication year: 2024 Coauthors: Jiyang Tang, Kwangyoun Kim, Suwon Shon, Felix Wu, Prashant Sridhar, Shinji Watanabe Abstract: In this paper, we propose a Guided Attention (GA) auxiliary training loss, which improves the effectiveness and robustness of automatic speech recognition (ASR) contextual biasing without introducing additional parameters. A common challenge in previous literature is that the word error rate (WER) reduction brought by contextual biasing diminishes as the number of bias phrases increases. To address this challenge, we employ a GA loss as an additional training objective besides the Transducer loss. The proposed GA loss aims to teach the cross attention how to align bias phrases with text tokens or audio frames. Compared to studies with similar motivations, the proposed loss operates directly on the cross attention weights and is easier to implement. Through extensive experiments based on Conformer Transducer with Contextual Adapter, we demonstrate that the proposed method not only leads to a lower WER but also retains its effectiveness as the number of bias phrases increases. Specifically, the GA loss decreases the WER of rare vocabularies by up to 19.2% on LibriSpeech compared to the contextual biasing baseline, and up to 49.3% compared to a vanilla Transducer.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Contextualized Automatic Speech Recognition with Attention-Based Bias Phrase Boosted Beam Search Publication year: 2024 Coauthors: Yui Sudo, Muhammad Shakeel, Yosuke Fukumoto, Yifan Peng, Shinji Watanabe Abstract: End-to-end (E2E) automatic speech recognition (ASR) methods exhibit remarkable performance. However, since the performance of such methods is intrinsically linked to the context present in the training data, E2E-ASR methods do not perform as desired for unseen user contexts (e.g., technical terms, personal names, and playlists). Thus, E2E-ASR methods must be easily contextualized by the user or developer. This paper proposes an attention-based contextual biasing method that can be customized using an editable phrase list (referred to as a bias list). The proposed method can be trained effectively by combining a bias phrase index loss and special tokens to detect the bias phrases in the input speech data. In addition, to improve the contextualization performance during inference further, we propose a bias phrase boosted (BPB) beam search algorithm based on the bias phrase index probability. Experimental results demonstrate that the proposed method consistently improves the word error rate and the character error rate of the target phrases in the bias list on both the Librispeech-960 (English) and our in-house (Japanese) dataset, respectively.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Generative Context-aware Fine-tuning of Self-supervised Speech Models Publication year: 2023 Coauthors: Suwon Shon, Kwangyoun Kim, Prashant Sridhar, Yi-Te Hsu, Shinji Watanabe, Karen Livescu Abstract: When performing tasks like automatic speech recognition or spoken language understanding for a given utterance, access to preceding text or audio provides contextual information can improve performance. Considering the recent advances in generative large language models (LLM), we hypothesize that an LLM could generate useful context information using the preceding text. With appropriate prompts, LLM could generate a prediction of the next sentence or abstractive text like titles or topics. In this paper, we study the use of LLM-generated context information and propose an approach to distill the generated information during fine-tuning of self-supervised speech models, which we refer to as generative context-aware fine-tuning. This approach allows the fine-tuned model to make improved predictions without access to the true surrounding segments or to the LLM at inference time, while requiring only a very small additional context module. We evaluate the proposed approach using the SLUE and Libri-light benchmarks for several downstream tasks: automatic speech recognition, named entity recognition, and sentiment analysis. The results show that generative context-aware fine-tuning outperforms a context injection fine-tuning approach that accesses the ground-truth previous text, and is competitive with a generative context injection fine-tuning approach that requires the LLM at inference time.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: SpeechComposer: Unifying Multiple Speech Tasks with Prompt Composition Publication year: 2024 Coauthors: Yihan Wu, Soumi Maiti, Yifan Peng, Wangyou Zhang, Chenda Li, Yuyue Wang, Xihua Wang, Shinji Watanabe, Ruihua Song Abstract: Recent advancements in language models have significantly enhanced performance in multiple speech-related tasks. Existing speech language models typically utilize task-dependent prompt tokens to unify various speech tasks in a single model. However, this design omits the intrinsic connections between different speech tasks, which can potentially boost the performance of each task. In this work, we propose a novel decoder-only speech language model, SpeechComposer, that can unify common speech tasks by composing a fixed set of prompt tokens. Built upon four primary tasks -- speech synthesis, speech recognition, speech language modeling, and text language modeling -- SpeechComposer can easily extend to more speech tasks via compositions of well-designed prompt tokens, like voice conversion and speech enhancement. The unification of prompt tokens also makes it possible for knowledge sharing among different speech tasks in a more structured manner. Experimental results demonstrate that our proposed SpeechComposer can improve the performance of both primary tasks and composite tasks, showing the effectiveness of the shared prompt tokens. Remarkably, the unified decoder-only model achieves a comparable and even better performance than the baselines which are expert models designed for single tasks.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: SpeechBERTScore: Reference-Aware Automatic Evaluation of Speech Generation Leveraging NLP Evaluation Metrics Publication year: 2024 Coauthors: Takaaki Saeki, Soumi Maiti, Shinnosuke Takamichi, Shinji Watanabe, H. Saruwatari Abstract: While subjective assessments have been the gold standard for evaluating speech generation, there is a growing need for objective metrics that are highly correlated with human subjective judgments due to their cost efficiency. This paper proposes reference-aware automatic evaluation methods for speech generation inspired by evaluation metrics in natural language processing. The proposed SpeechBERTScore computes the BERTScore for self-supervised dense speech features of the generated and reference speech, which can have different sequential lengths. We also propose SpeechBLEU and SpeechTokenDistance, which are computed on speech discrete tokens. The evaluations on synthesized speech show that our method correlates better with human subjective ratings than mel cepstral distortion and a recent mean opinion score prediction model. Also, they are effective in noisy speech evaluation and have cross-lingual applicability.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Multilingual TTS Accent Impressions for Accented ASR Publication year: 2023 Coauthors: G. Karakasidis, Nathaniel R. Robinson, Yaroslav Getman, Atieno Ogayo, Ragheb Al-Ghezi, Ananya Ayasi, Shinji Watanabe, David R. Mortensen, M. Kurimo '),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Domain Adaptation by Data Distribution Matching Via Submodularity For Speech Recognition Publication year: 2023 Coauthors: Yusuke Shinohara, Shinji Watanabe Abstract: We study the problem of building a domain-specific speech recognition model given some text from the target domain. One of the most popular approaches to this problem is shallow fusion, which incorporates a domain-specific language model build from the given text. However, shallow fusion significantly increases the model size and inference cost, which makes its deployment harder. In this paper, we propose domain adaptation by data distribution matching, where a subset is selected from an existing multi-domain training data to match the target-domain distribution, and a model is fine-tuned on the subset. A submodular optimization algorithm with a novel extension is employed for the subset selection. Experiments on LibriSpeech, a corpus of audiobooks, where we treat each book as a domain, show that the proposed distribution-matching approach achieves WERs equivalent with the conventional shallow-fusion approach, without any increase in the model size and inference cost.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Synthetic Data Augmentation for ASR with Domain Filtering Publication year: 2023 Coauthors: Tuan Vu Ho, Shota Horiguchi, Shinji Watanabe, Paola Garcia, Takashi Sumiyoshi Abstract: Recent studies have shown that synthetic speech can effectively serve as training data for automatic speech recognition models. Text data for synthetic speech is mostly obtained from in-domain text or generated text using augmentation. However, obtaining large amounts of in-domain text data with diverse lexical contexts is difficult, especially in low-resource scenarios. This paper proposes using text from a large generic-domain source and applying a domain filtering method to choose the relevant text data. This method involves two filtering steps: 1) selecting text based on its semantic similarity to the available in-domain text and 2) diversifying the vocabulary of the selected text using a greedy-search algorithm. Experimental results show that our proposed method outperforms the conventional text augmentation approach, with the relative reduction of word-error-rate ranging from 6% to 25% on the LibriSpeech dataset and 15% on a low-resource Vietnamese dataset.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: TorchAudio 2.1: Advancing Speech Recognition, Self-Supervised Learning, and Audio Processing Components for Pytorch Publication year: 2023 Coauthors: Jeff Hwang, Moto Hira, Caroline Chen, Xiaohui Zhang, Zhaoheng Ni, Guangzhi Sun, Pingchuan Ma, Ruizhe Huang, Vineel Pratap, Yuekai Zhang, Anurag Kumar, Chin-Yun Yu, Chuang Zhu, Chunxi Liu, Jacob Kahn, M. Ravanelli, Peng Sun, Shinji Watanabe, Yangyang Shi, Yumeng Tao, Robin Scheibler, Samuele Cornell, Sean Kim, Stavros Petridis Abstract: TorchAudio is an open-source audio and speech processing library built for PyTorch. It aims to accelerate the research and development of audio and speech technologies by providing well-designed, easy-to-use, and performant PyTorch components. Its contributors routinely engage with users to understand their needs and fulfill them by developing impactful features. Here, we survey TorchAudio’s development principles and contents and highlight key features we include in its latest version (2.1): self-supervised learning pre-trained pipelines and training recipes, high-performance CTC decoders, speech recognition models and training recipes, advanced media I/O capabilities, and tools for performing forced alignment, multi-channel speech enhancement, and reference-less speech assessment. For a selection of these features, through empirical studies, we demonstrate their efficacy and show that they achieve competitive or state-of-the-art performance.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Phoneme-aware Encoding for Prefix-tree-based Contextual ASR Publication year: 2023 Coauthors: Hayato Futami, E. Tsunoo, Yosuke Kashiwagi, Hiroaki Ogawa, Siddhant Arora, Shinji Watanabe Abstract: In speech recognition applications, it is important to recognize context-specific rare words, such as proper nouns. Tree-constrained Pointer Generator (TCPGen) has shown promise for this purpose, which efficiently biases such words with a prefix tree. While the original TCPGen relies on grapheme-based encoding, we propose extending it with phoneme-aware encoding to better recognize words of unusual pronunciations. As TCPGen handles biasing words as subword units, we propose obtaining subword-level phoneme-aware encoding by using alignment between phonemes and subwords. Furthermore, we propose injecting phoneme-level predictions from CTC into queries of TCPGen so that the model better interprets the phoneme-aware encodings. We conducted ASR experiments with TCPGen for RNN transducer. We observed that proposed phoneme-aware encoding outperformed ordinary grapheme-based encoding on both the English LibriSpeech and Japanese CSJ datasets, demonstrating the robustness of our approach across linguistically diverse languages.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Semi-Autoregressive Streaming ASR With Label Context Publication year: 2023 Coauthors: Siddhant Arora, G. Saon, Shinji Watanabe, Brian Kingsbury Abstract: Non-autoregressive (NAR) modeling has gained significant interest in speech processing since these models achieve dramatically lower inference time than autoregressive (AR) models while also achieving good transcription accuracy. Since NAR automatic speech recognition (ASR) models must wait for the completion of the entire utterance before processing, some works explore streaming NAR models based on blockwise attention for low-latency applications. However, streaming NAR models significantly lag in accuracy compared to streaming AR and non-streaming NAR models. To address this, we propose a streaming\"semi-autoregressive\"ASR model that incorporates the labels emitted in previous blocks as additional context using a Language Model (LM) subnetwork. We also introduce a novel greedy decoding algorithm that addresses insertion and deletion errors near block boundaries while not significantly increasing the inference time. Experiments show that our method outperforms the existing streaming NAR model by 19% relative on Tedlium2, 16%/8% on Librispeech-100 clean/other test sets, and 19%/8% on the Switchboard(SWB)/Callhome(CH) test sets. It also reduced the accuracy gap with streaming AR and non-streaming NAR models while achieving 2.5x lower latency. We also demonstrate that our approach can effectively utilize external text data to pre-train the LM subnetwork to further improve streaming ASR accuracy.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Summary on the Multimodal Information Based Speech Processing (MISP) 2022 Challenge Publication year: 2023 Coauthors: Hang Chen, Shilong Wu, Yusheng Dai, Zhe Wang, Jun Du, Chin-Hui Lee, Jingdong Chen, Shinji Watanabe, Sabato Marco Siniscalchi, O. Scharenborg, Diyuan Liu, Baocai Yin, Jia Pan, Jianqing Gao, Cong Liu Abstract: The Multimodal Information based Speech Processing (MISP) 2022 challenge aimed to enhance speech processing performance in harsh acoustic environments by leveraging additional modalities such as video or text. The challenge included two tracks: audio-visual speaker diarization (AVSD) and audio-visual diarization and recognition (AVDR). The training material was based on previous MISP 2021 recordings, but we have accurately synchronized audio and visual data. Additionally, a new evaluation set was provided. This paper gives an overview of the challenge setup, presents the results, and summarizes the effective techniques employed by the participants. We also analyze the current technical challenges and suggest directions for future research in AVSD and AVDR.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Singing Voice Data Scaling-up: An Introduction to ACE-Opencpop and KiSing-v2 Publication year: 2024 Coauthors: Jiatong Shi, Yueqian Lin, Xinyi Bai, Keyi Zhang, Yuning Wu, Yuxun Tang, Yifeng Yu, Qin Jin, Shinji Watanabe Abstract: In singing voice synthesis (SVS), generating singing voices from musical scores faces challenges due to limited data availability, a constraint less common in text-to-speech (TTS). This study proposes a new approach to address this data scarcity. We utilize an existing singing voice synthesizer for data augmentation and apply precise manual tuning to reduce unnatural voice synthesis. Our development of two extensive singing voice corpora, ACE-Opencpop and KiSing-v2, facilitates large-scale, multi-singer voice synthesis. Utilizing pre-trained models derived from these corpora, we achieve notable improvements in voice quality, evident in both in-domain and out-of-domain scenarios. The corpora, pre-trained models, and their related training recipes are publicly available at Muskits-ESPnet (https://github.com/espnet/espnet).'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: LV-CTC: Non-Autoregressive ASR With CTC and Latent Variable Models Publication year: 2023 Coauthors: Yuya Fujita, Shinji Watanabe, Xuankai Chang, Takashi Maekaku Abstract: Non-autoregressive (NAR) models for automatic speech recognition (ASR) aim to achieve high accuracy and fast inference by simplifying the autoregressive (AR) generation process of conventional models. Connectionist temporal classification (CTC) is one of the key techniques used in NAR ASR models. In this paper, we propose a new model combining CTC and a latent variable model, which is one of the state-of-the-art models in the neural machine translation research field. A new neural network architecture and formulation specialized for ASR application are introduced. In the proposed model, CTC alignment is assumed to be dependent on the latent variables that are expected to capture dependencies between tokens. Experimental results on a 100 hours subset of Librispeech corpus showed the best recognition accuracy among CTC-based NAR models. On the TED-LIUM2 corpus, the best recognition accuracy is achieved including AR E2E models with faster inference speed.'),\n",
       " Document(page_content=\"Author: Shinji Watanabe Title: EFFUSE: Efficient Self-Supervised Feature Fusion for E2E ASR in Multilingual and Low Resource Scenarios Publication year: 2023 Coauthors: Tejes Srivastava, Jiatong Shi, William Chen, Shinji Watanabe Abstract: Self-Supervised Learning (SSL) models have demonstrated exceptional performance in various speech tasks, particularly in low-resource and multilingual domains. Recent works show that fusing SSL models could achieve superior performance compared to using one SSL model. However, fusion models have increased model parameter size, leading to longer inference times. In this paper, we propose a novel approach of predicting other SSL models' features from a single SSL model, resulting in a light-weight framework with competitive performance. Our experiments show that SSL feature prediction models outperform individual SSL models in multilingual speech recognition tasks. The leading prediction model achieves an average SUPERB score increase of 135.4 in ML-SUPERB benchmarks. Moreover, our proposed framework offers an efficient solution, as it reduces the resulting model parameter size and inference times compared to previous fusion models.\"),\n",
       " Document(page_content=\"Author: Shinji Watanabe Title: HuBERTopic: Enhancing Semantic Representation of HuBERT through Self-supervision Utilizing Topic Model Publication year: 2023 Coauthors: Takashi Maekaku, Jiatong Shi, Xuankai Chang, Yuya Fujita, Shinji Watanabe Abstract: Recently, the usefulness of self-supervised representation learning (SSRL) methods has been confirmed in various downstream tasks. Many of these models, as exemplified by HuBERT and WavLM, use pseudo-labels generated from spectral features or the model's own representation features. From previous studies, it is known that the pseudo-labels contain semantic information. However, the masked prediction task, the learning criterion of HuBERT, focuses on local contextual information and may not make effective use of global semantic information such as speaker, theme of speech, and so on. In this paper, we propose a new approach to enrich the semantic representation of HuBERT. We apply topic model to pseudo-labels to generate a topic label for each utterance. An auxiliary topic classification task is added to HuBERT by using topic labels as teachers. This allows additional global semantic information to be incorporated in an unsupervised manner. Experimental results demonstrate that our method achieves comparable or better performance than the baseline in most tasks, including automatic speech recognition and five out of the eight SUPERB tasks. Moreover, we find that topic labels include various information about utterance, such as gender, speaker, and its theme. This highlights the effectiveness of our approach in capturing multifaceted semantic nuances.\"),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Findings of the 2023 ML-Superb Challenge: Pre-Training And Evaluation Over More Languages And Beyond Publication year: 2023 Coauthors: Jiatong Shi, William Chen, Dan Berrebbi, Hsiu-Hsuan Wang, Wei-Ping Huang, En-Pei Hu, Ho-Lam Chuang, Xuankai Chang, Yuxun Tang, Shang-Wen Li, Abdelrahman Mohamed, Hung-yi Lee, Shinji Watanabe Abstract: The 2023 Multilingual Speech Universal Performance Benchmark (ML-SUPERB) Challenge expands upon the acclaimed SUPERB framework, emphasizing self-supervised models in multilingual speech recognition and language identification. The challenge comprises a research track focused on applying ML-SUPERB to specific multilingual subjects, a Challenge Track for model submissions, and a New Language Track where language resource researchers can contribute and evaluate their low-resource language data in the context of the latest progress in multilingual speech recognition. The challenge garnered 12 model submissions and 54 language corpora, resulting in a comprehensive benchmark encompassing 154 languages. The findings indicate that merely scaling models is not the definitive solution for multilingual speech tasks, and a variety of speech/voice types present significant challenges in multilingual speech processing.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Boosting Unknown-number Speaker Separation with Transformer Decoder-based Attractor Publication year: 2024 Coauthors: Younglo Lee, Shukjae Choi, Byeonghak Kim, Zhong-Qiu Wang, Shinji Watanabe Abstract: We propose a novel speech separation model designed to separate mixtures with an unknown number of speakers. The proposed model stacks 1) a dual-path processing block that can model spectro-temporal patterns, 2) a transformer decoder-based attractor (TDA) calculation module that can deal with an unknown number of speakers, and 3) triple-path processing blocks that can model inter-speaker relations. Given a fixed, small set of learned speaker queries and the mixture embedding produced by the dual-path blocks, TDA infers the relations of these queries and generates an attractor vector for each speaker. The estimated attractors are then combined with the mixture embedding by feature-wise linear modulation conditioning, creating a speaker dimension. The mixture embedding, conditioned with speaker information produced by TDA, is fed to the final triple-path blocks, which augment the dual-path blocks with an additional pathway dedicated to inter-speaker processing. The proposed approach outperforms the previous best reported in the literature, achieving 24.0 and 23.7 dB SI-SDR improvement (SI-SDRi) on WSJ0-2 and 3mix respectively, with a single model trained to separate 2- and 3-speaker mixtures. The proposed model also exhibits strong performance and generalizability at counting sources and separating mixtures with up to 5 speakers.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Improving Design of Input Condition Invariant Speech Enhancement Publication year: 2024 Coauthors: Wangyou Zhang, Jee-weon Jung, Shinji Watanabe, Yanmin Qian Abstract: Building a single universal speech enhancement (SE) system that can handle arbitrary input is a demanded but underexplored research topic. Towards this ultimate goal, one direction is to build a single model that handles diverse audio duration, sampling frequencies, and microphone variations in noisy and reverberant scenarios, which we define here as\"input condition invariant SE\". Such a model was recently proposed showing promising performance; however, its multi-channel performance degraded severely in real conditions. In this paper we propose novel architectures to improve the input condition invariant SE model so that performance in simulated conditions remains competitive while real condition degradation is much mitigated. For this purpose, we redesign the key components that comprise such a system. First, we identify that the channel-modeling module\\'s generalization to unseen scenarios can be sub-optimal and redesign this module. We further introduce a two-stage training strategy to enhance training efficiency. Second, we propose two novel dual-path time-frequency blocks, demonstrating superior performance with fewer parameters and computational costs compared to the existing method. All proposals combined, experiments on various public datasets validate the efficacy of the proposed model, with significantly improved performance on real conditions. Recipe with full model details is released at https://github.com/espnet/espnet.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Software Design and User Interface of ESPnet-SE++: Speech Enhancement for Robust Speech Processing Publication year: 2023 Coauthors: Yen-Ju Lu, Xuankai Chang, Chenda Li, Wangyou Zhang, Samuele Cornell, Zhaoheng Ni, Yoshiki Masuyama, Brian Yan, Robin Scheibler, Zhong-Qiu Wang, Yu Tsao, Yanmin Qian, Shinji Watanabe '),\n",
       " Document(page_content='Author: Shinji Watanabe Title: A Single Speech Enhancement Model Unifying Dereverberation, Denoising, Speaker Counting, Separation, And Extraction Publication year: 2023 Coauthors: Kohei Saijo, Wangyou Zhang, Zhong-Qiu Wang, Shinji Watanabe, Tetsunori Kobayashi, Tetsuji Ogawa Abstract: We propose a multi-task universal speech enhancement (MUSE) model that can perform five speech enhancement (SE) tasks: dereverberation, denoising, speech separation (SS), target speaker extraction (TSE), and speaker counting. This is achieved by integrating two modules into an SE model: 1) an internal separation module that does both speaker counting and separation; and 2) a TSE module that extracts the target speech from the internal separation outputs using target speaker cues. The model is trained to perform TSE if the target speaker cue is given and SS otherwise. By training the model to remove noise and reverberation, we allow the model to tackle the five tasks mentioned above with a single model, which has not been accomplished yet. Evaluation results demonstrate that the proposed MUSE model can successfully handle multiple tasks with a single model.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Summarize While Translating: Universal Model With Parallel Decoding for Summarization and Translation Publication year: 2023 Coauthors: Takatomo Kano, A. Ogawa, Marc Delcroix, Kohei Matsuura, Takanori Ashihara, William Chen, Shinji Watanabe Abstract: Recently, multi-decoder and universal models have attracted increased interest in speech and language processing as they allow learning common representations across tasks. These models learn a common representation by sharing a part of or all network parameters. Moreover, such a universal model can handle tasks unseen during training (zero-shot tasks). However, these models do not fully exploit inter-dependencies between tasks during decoding since they usually perform decoding for each task independently. In this paper, we propose to address this issue by extending the universal model to perform multi-task parallel decoding with a cross-attention module between decoders to capture task inter-dependencies explicitly. We also introduce a novel multi-stream beam search algorithm to allow such parallel decoding. We test our proposed model on multi-lingual (English and Portuguese) text/speech translation and summarization, confirming its potential, especially in zero-shot tasks.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Yodas: Youtube-Oriented Dataset for Audio and Speech Publication year: 2023 Coauthors: Xinjian Li, Shinnosuke Takamichi, Takaaki Saeki, William Chen, Sayaka Shiota, Shinji Watanabe Abstract: In this study, we introduce YODAS (YouTube-Oriented Dataset for Audio and Speech), a large-scale, multilingual dataset comprising currently over 500k hours of speech data in more than 100 languages, sourced from both labeled and unlabeled YouTube speech datasets. The labeled subsets, including manual or automatic subtitles, facilitate supervised model training. Conversely, the unlabeled subsets are apt for self-supervised learning applications. YODAS is distinctive as the first publicly available dataset of its scale, and it will be distributed under a Creative Commons license. We introduce the collection methodology utilized for YODAS, which contributes to the large-scale speech dataset construction. Subsequently, we provide a comprehensive analysis of speech, text contained within the dataset. Finally, we describe the speech recognition baselines over the top-15 languages.'),\n",
       " Document(page_content='Author: Shinji Watanabe Title: Espnet-Summ: Introducing a Novel Large Dataset, Toolkit, and a Cross-Corpora Evaluation of Speech Summarization Systems Publication year: 2023 Coauthors: Roshan Sharma, William Chen, Takatomo Kano, Ruchira Sharma, Siddhant Arora, Shinji Watanabe, A. Ogawa, Marc Delcroix, Rita Singh, Bhiksha Raj Abstract: Speech summarization has garnered significant interest and progressed rapidly over the past few years. In particular, end-to-end models have recently emerged as a competitive alternative to cascade systems for abstractive video summarization. This paper aims to establish progress in this rapidly evolving research field, by introducing ESPNet-SUMM, a new open-source toolkit that facilitates a comprehensive comparison of end-to-end and cascade speech summarization models on 4 different speech summarization tasks spanning diverse applications. Experiments demonstrate that end-to-end models perform better for larger corpora with shorter inputs. This work also introduces Interview, the largest public open-domain multiparty interview corpus with $4400 \\\\mathrm{~h}$ of conversations between radio hosts and guests. Finally, this work explores the use of multiple datasets to improve end-to-end summarization, and experiments demonstrate the benefit of multi-style training over fine-tuning. 1'),\n",
       " Document(page_content='Author: S. Welleck Title: Self-Refine: Iterative Refinement with Self-Feedback Publication year: 2023 Coauthors: Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, S. Welleck, Bodhisattwa Prasad Majumder, Shashank Gupta, A. Yazdanbakhsh, Peter Clark Abstract: Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ~20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach.'),\n",
       " Document(page_content='Author: S. Welleck Title: D RAFT , S KETCH , AND P ROVE : G UIDING F ORMAL T HEOREM P ROVERS WITH I NFORMAL P ROOFS Publication year: 2023 Coauthors: Albert Qiaochu Jiang, S. Welleck, J. Zhou, Wen-Ding Li, Jiacheng Liu, M. Jamnik, Timothée Lacroix, Guillaume Lample, Yuhuai Wu Abstract: The formalization of existing mathematical proofs is a notoriously difficult process. Despite decades of research on automation and proof assistants, writing formal proofs remains arduous and only accessible to a few experts. While previous studies to automate formalization focused on powerful search algorithms, no attempts were made to take advantage of available informal proofs. In this work, we introduce Draft, Sketch, and Prove (DSP), a method that maps informal proofs to formal proof sketches, and uses the sketches to guide an automated prover by directing its search to easier sub-problems. We investigate two relevant setups where informal proofs are either written by humans or generated by a language model. Our experiments and ablation studies show that large language models are able to produce wellstructured formal sketches that follow the same reasoning steps as the informal proofs. Guiding an automated prover with these sketches enhances its performance from 20.9% to 39.3% on a collection of mathematical competition problems. Figure 1: Draft, Sketch, and Prove. Starting with an informal statement, our framework yields a formal proof through a three-stage process: drafting informal proofs, mapping them into formal sketches, and proving the remaining conjectures. Concretely, an informal statement is a mathematical problem described in a mixture of natural and mathematical languages (e.g., formulae in LTEX). Then, we use a large language model to autoformalize each informal proof into a formal sketch, which is a skeleton of the formal proof with open conjectures left unproven (indicated by the <proof> blocks). The formal sketch mirrors the structure of the informal proof. Finally, the open conjectures/gaps inside each formal sketch are proved by an off-the-shelf prover. †Equal contributions as leading authors. Correspondence to: qj213@cam.ac.uk. ‡Equal contributions as senior authors.'),\n",
       " Document(page_content=\"Author: S. Welleck Title: Faith and Fate: Limits of Transformers on Compositionality Publication year: 2023 Coauthors: Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jian, Bill Yuchen Lin, Peter West, Chandra Bhagavatula, Ronan Le Bras, Jena D. Hwang, Soumya Sanyal, S. Welleck, Xiang Ren, Allyson Ettinger, Zaïd Harchaoui, Yejin Choi Abstract: Transformer large language models (LLMs) have sparked admiration for their exceptional performance on tasks that demand intricate multi-step reasoning. Yet, these models simultaneously show failures on surprisingly trivial problems. This begs the question: Are these errors incidental, or do they signal more substantial limitations? In an attempt to demystify transformer LLMs, we investigate the limits of these models across three representative compositional tasks -- multi-digit multiplication, logic grid puzzles, and a classic dynamic programming problem. These tasks require breaking problems down into sub-steps and synthesizing these steps into a precise answer. We formulate compositional tasks as computation graphs to systematically quantify the level of complexity, and break down reasoning steps into intermediate sub-procedures. Our empirical findings suggest that transformer LLMs solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, without necessarily developing systematic problem-solving skills. To round off our empirical study, we provide theoretical arguments on abstract multi-step reasoning problems that highlight how autoregressive generations' performance can rapidly decay with\\\\,increased\\\\,task\\\\,complexity.\"),\n",
       " Document(page_content='Author: S. Welleck Title: STEER: Unified Style Transfer with Expert Reinforcement Publication year: 2023 Coauthors: Skyler Hallinan, Faeze Brahman, Ximing Lu, Jaehun Jung, S. Welleck, Yejin Choi Abstract: While text style transfer has many applications across natural language processing, the core premise of transferring from a single source style is unrealistic in a real-world setting. In this work, we focus on arbitrary style transfer: rewriting a text from an arbitrary, unknown style to a target style. We propose STEER: Unified Style Transfer with Expert Reinforcement, a unified frame-work developed to overcome the challenge of limited parallel data for style transfer. STEER involves automatically generating a corpus of style-transfer pairs using a product of experts during decoding. The generated offline data is then used to pre-train an initial policy before switching to online, off-policy reinforcement learning for further improvements via fine-grained reward signals. STEER is unified and can transfer to multiple target styles from an arbitrary, unknown source style, making it particularly flexible and efficient. Experimental results on a challenging dataset with text from a diverse set of styles demonstrate state-of-the-art results compared to competitive baselines. Remarkably, STEER outperforms the 175B parameter instruction-tuned GPT-3 on overall style transfer quality, despite being 226 times smaller in size. We also show STEER is robust, maintaining its style transfer capabilities on out-of-domain data, and surpassing nearly all baselines across various styles. The success of our method highlights the potential of RL algorithms when augmented with controllable decoding to overcome the challenge of limited data supervision.'),\n",
       " Document(page_content='Author: S. Welleck Title: Llemma: An Open Language Model For Mathematics Publication year: 2023 Coauthors: Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Q. Jiang, Jia Deng, Stella Biderman, S. Welleck Abstract: We present Llemma, a large language model for mathematics. We continue pretraining Code Llama on the Proof-Pile-2, a mixture of scientific papers, web data containing mathematics, and mathematical code, yielding Llemma. On the MATH benchmark Llemma outperforms all known open base models, as well as the unreleased Minerva model suite on an equi-parameter basis. Moreover, Llemma is capable of tool use and formal theorem proving without any further finetuning. We openly release all artifacts, including 7 billion and 34 billion parameter models, the Proof-Pile-2, and code to replicate our experiments.'),\n",
       " Document(page_content='Author: S. Welleck Title: Inference-Time Policy Adapters (IPA): Tailoring Extreme-Scale LMs without Fine-tuning Publication year: 2023 Coauthors: Ximing Lu, Faeze Brahman, Peter West, Jaehun Jang, Khyathi Raghavi Chandu, Abhilasha Ravichander, Lianhui Qin, Prithviraj Ammanabrolu, Liwei Jiang, Sahana Ramnath, Nouha Dziri, Jillian R. Fisher, Bill Yuchen Lin, Skyler Hallinan, Xiang Ren, S. Welleck, Yejin Choi Abstract: While extreme-scale language models have demonstrated exceptional performance on a variety of language tasks, the degree of control over these language models through pure prompting can often be limited. Directly fine-tuning such language models can be effective for tailoring them, but it can be either extremely costly (e.g., GPT-3) or not even feasible for the broader community (e.g., GPT-4). We propose Inference-time Policy Adapters (IPA), which efficiently tailors a language model such as GPT-3 without fine-tuning it. IPA guides a large base model during decoding time through a lightweight policy adapter trained to optimize an arbitrary user objective with reinforcement learning. On five challenging text generation tasks, such as toxicity reduction and lexically constrained generation, IPA consistently brings significant improvements over off-the-shelf language models. It outperforms competitive baseline methods, sometimes even including expensive fine-tuning. In particular, tailoring GPT-2 with IPA can outperform GPT-3, while tailoring GPT-3 with IPA brings a major performance boost over GPT-3 (and sometimes even over GPT-4). Our promising results highlight the potential of IPA as a lightweight alternative to tailoring extreme-scale language models.'),\n",
       " Document(page_content=\"Author: Sean Welleck Title: LLMSTEP: LLM proofstep suggestions in Lean Publication year: 2023 Coauthors: Sean Welleck, Rahul Saha Abstract: We present LLMSTEP, a tool for integrating a language model into the Lean proof assistant. LLMSTEP is a Lean 4 tactic that sends a user's proof state to a server hosting a language model. The language model generates suggestions, which are checked in Lean and displayed to a user in their development environment. We provide a baseline language model, along with code for fine-tuning and evaluation to support further development. We provide server implementations that run on CPU, a CUDA GPU, or a Google Colab notebook, as a step towards fast, effective language model suggestions for any user.\"),\n",
       " Document(page_content='Author: Eric P. Xing Title: TrustLLM: Trustworthiness in Large Language Models Publication year: 2024 Coauthors: Lichao Sun, Yue Huang, Haoran Wang, Siyuan Wu, Qihui Zhang, Chujie Gao, Yixin Huang, Wenhan Lyu, Yixuan Zhang, Xiner Li, Zheng Liu, Yixin Liu, Yijue Wang, Zhikun Zhang, B. Kailkhura, Caiming Xiong, Chaowei Xiao, Chun-Yan Li, Eric P. Xing, Furong Huang, Haodong Liu, Heng Ji, Hongyi Wang, Huan Zhang, Huaxiu Yao, M. Kellis, M. Zitnik, Meng Jiang, Mohit Bansal, James Zou, Jian Pei, Jian Liu, Jianfeng Gao, Jiawei Han, Jieyu Zhao, Jiliang Tang, Jindong Wang, John Mitchell, Kai Shu, Kaidi Xu, Kai-Wei Chang, Lifang He, Lifu Huang, M. Backes, Neil Zhenqiang Gong, Philip S. Yu, Pin-Yu Chen, Quanquan Gu, Ran Xu, Rex Ying, Shuiwang Ji, S. Jana, Tian-Xiang Chen, Tianming Liu, Tianying Zhou, William Wang, Xiang Li, Xiang-Yu Zhang, Xiao Wang, Xingyao Xie, Xun Chen, Xuyu Wang, Yan Liu, Yanfang Ye, Yinzhi Cao, Yue Zhao Abstract: Large language models (LLMs), exemplified by ChatGPT, have gained considerable attention for their excellent natural language processing capabilities. Nonetheless, these LLMs present many challenges, particularly in the realm of trustworthiness. Therefore, ensuring the trustworthiness of LLMs emerges as an important topic. This paper introduces TrustLLM, a comprehensive study of trustworthiness in LLMs, including principles for different dimensions of trustworthiness, established benchmark, evaluation, and analysis of trustworthiness for mainstream LLMs, and discussion of open challenges and future directions. Specifically, we first propose a set of principles for trustworthy LLMs that span eight different dimensions. Based on these principles, we further establish a benchmark across six dimensions including truthfulness, safety, fairness, robustness, privacy, and machine ethics. We then present a study evaluating 16 mainstream LLMs in TrustLLM, consisting of over 30 datasets. Our findings firstly show that in general trustworthiness and utility (i.e., functional effectiveness) are positively related. Secondly, our observations reveal that proprietary LLMs generally outperform most open-source counterparts in terms of trustworthiness, raising concerns about the potential risks of widely accessible open-source LLMs. However, a few open-source LLMs come very close to proprietary ones. Thirdly, it is important to note that some LLMs may be overly calibrated towards exhibiting trustworthiness, to the extent that they compromise their utility by mistakenly treating benign prompts as harmful and consequently not responding. Finally, we emphasize the importance of ensuring transparency not only in the models themselves but also in the technologies that underpin trustworthiness. Knowing the specific trustworthy technologies that have been employed is crucial for analyzing their effectiveness.'),\n",
       " Document(page_content=\"Author: Eric P. Xing Title: SlimPajama-DC: Understanding Data Combinations for LLM Training Publication year: 2023 Coauthors: Zhiqiang Shen, Tianhua Tao, Liqun Ma, W. Neiswanger, Zhengzhong Liu, Hongyi Wang, Bowen Tan, Joel Hestness, Natalia Vassilieva, Daria Soboleva, Eric P. Xing Abstract: This paper aims to understand the impacts of various data combinations (e.g., web text, wikipedia, github, books) on the training of large language models using SlimPajama. SlimPajama is a rigorously deduplicated, multi-source dataset, which has been refined and further deduplicated to 627B tokens from the extensive 1.2T tokens RedPajama dataset contributed by Together. We've termed our research as SlimPajama-DC, an empirical analysis designed to uncover fundamental characteristics and best practices associated with employing SlimPajama in the training of large language models. During our research with SlimPajama, two pivotal observations emerged: (1) Global deduplication vs. local deduplication. We analyze and discuss how global (across different sources of datasets) and local (within the single source of dataset) deduplications affect the performance of trained models. (2) Proportions of high-quality/highly-deduplicated multi-source datasets in the combination. To study this, we construct six configurations of SlimPajama dataset and train individual ones using 1.3B Cerebras-GPT model with Alibi and SwiGLU. Our best configuration outperforms the 1.3B model trained on RedPajama using the same number of training tokens by a significant margin. All our 1.3B models are trained on Cerebras 16$\\\\times$ CS-2 cluster with a total of 80 PFLOP/s in bf16 mixed precision. We further extend our discoveries (such as increasing data diversity is crucial after global deduplication) on a 7B model with large batch-size training. Our models and the separate SlimPajama-DC datasets are available at: https://huggingface.co/MBZUAI-LLM and https://huggingface.co/datasets/cerebras/SlimPajama-627B.\"),\n",
       " Document(page_content='Author: Eric P. Xing Title: LLM360: Towards Fully Transparent Open-Source LLMs Publication year: 2023 Coauthors: Zhengzhong Liu, Aurick Qiao, W. Neiswanger, Hongyi Wang, Bowen Tan, Tianhua Tao, Junbo Li, Yuqi Wang, Suqi Sun, Omkar Pangarkar, Richard Fan, Yi Gu, Victor Miller, Yonghao Zhuang, Guowei He, Haonan Li, Fajri Koto, Liping Tang, Nikhil Ranjan, Zhiqiang Shen, Xuguang Ren, Roberto Iriondo, Cun Mu, Zhiting Hu, Mark Schulze, Preslav Nakov, Timothy Baldwin, Eric P. Xing Abstract: The recent surge in open-source Large Language Models (LLMs), such as LLaMA, Falcon, and Mistral, provides diverse options for AI practitioners and researchers. However, most LLMs have only released partial artifacts, such as the final model weights or inference code, and technical reports increasingly limit their scope to high-level design choices and surface statistics. These choices hinder progress in the field by degrading transparency into the training of LLMs and forcing teams to rediscover many details in the training process. We present LLM360, an initiative to fully open-source LLMs, which advocates for all training code and data, model checkpoints, and intermediate results to be made available to the community. The goal of LLM360 is to support open and collaborative AI research by making the end-to-end LLM training process transparent and reproducible by everyone. As a first step of LLM360, we release two 7B parameter LLMs pre-trained from scratch, Amber and CrystalCoder, including their training code, data, intermediate checkpoints, and analyses (at https://www.llm360.ai). We are committed to continually pushing the boundaries of LLMs through this open-source effort. More large-scale and stronger models are underway and will be released in the future.'),\n",
       " Document(page_content='Author: Eric P. Xing Title: Fusing Models with Complementary Expertise Publication year: 2023 Coauthors: Hongyi Wang, Felipe Maia Polo, Yuekai Sun, Souvik Kundu, Eric P. Xing, M. Yurochkin Abstract: Training AI models that generalize across tasks and domains has long been among the open problems driving AI research. The emergence of Foundation Models made it easier to obtain expert models for a given task, but the heterogeneity of data that may be encountered at test time often means that any single expert is insufficient. We consider the Fusion of Experts (FoE) problem of fusing outputs of expert models with complementary knowledge of the data distribution and formulate it as an instance of supervised learning. Our method is applicable to both discriminative and generative tasks and leads to significant performance improvements in image and text classification, text summarization, multiple-choice QA, and automatic evaluation of generated text. We also extend our method to the\"frugal\"setting where it is desired to reduce the number of expert model evaluations at test time.'),\n",
       " Document(page_content='Author: Eric P. Xing Title: Making Scalable Meta Learning Practical Publication year: 2023 Coauthors: Sang Keun Choe, Sanket Vaibhav Mehta, Hwijeen Ahn, W. Neiswanger, Pengtao Xie, Emma Strubell, Eric P. Xing Abstract: Despite its flexibility to learn diverse inductive biases in machine learning programs, meta learning (i.e., learning to learn) has long been recognized to suffer from poor scalability due to its tremendous compute/memory costs, training instability, and a lack of efficient distributed training support. In this work, we focus on making scalable meta learning practical by introducing SAMA, which combines advances in both implicit differentiation algorithms and systems. Specifically, SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients. Evaluated on multiple large-scale meta learning benchmarks, SAMA showcases up to 1.7/4.8x increase in throughput and 2.0/3.8x decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms. Furthermore, we show that SAMA-based data optimization leads to consistent improvements in text classification accuracy with BERT and RoBERTa large language models, and achieves state-of-the-art results in both small- and large-scale data pruning on image classification tasks, demonstrating the practical applicability of scalable meta learning across language and vision domains.'),\n",
       " Document(page_content='Author: Eric P. Xing Title: PromptAgent: Strategic Planning with Language Models Enables Expert-level Prompt Optimization Publication year: 2023 Coauthors: Xinyuan Wang, Chenxi Li, Zhen Wang, Fan Bai, Haotian Luo, Jiayou Zhang, Nebojsa Jojic, Eric P. Xing, Zhiting Hu Abstract: Highly effective, task-specific prompts are often heavily engineered by experts to integrate detailed instructions and domain insights based on a deep understanding of both instincts of large language models (LLMs) and the intricacies of the target task. However, automating the generation of such expert-level prompts remains elusive. Existing prompt optimization methods tend to overlook the depth of domain knowledge and struggle to efficiently explore the vast space of expert-level prompts. Addressing this, we present PromptAgent, an optimization method that autonomously crafts prompts equivalent in quality to those handcrafted by experts. At its core, PromptAgent views prompt optimization as a strategic planning problem and employs a principled planning algorithm, rooted in Monte Carlo tree search, to strategically navigate the expert-level prompt space. Inspired by human-like trial-and-error exploration, PromptAgent induces precise expert-level insights and in-depth instructions by reflecting on model errors and generating constructive error feedback. Such a novel framework allows the agent to iteratively examine intermediate prompts (states), refine them based on error feedbacks (actions), simulate future rewards, and search for high-reward paths leading to expert prompts. We apply PromptAgent to 12 tasks spanning three practical domains: BIG-Bench Hard (BBH), as well as domain-specific and general NLP tasks, showing it significantly outperforms strong Chain-of-Thought and recent prompt optimization baselines. Extensive analyses emphasize its capability to craft expert-level, detailed, and domain-insightful prompts with great efficiency and generalizability.'),\n",
       " Document(page_content='Author: Eric P. Xing Title: Cappy: Outperforming and Boosting Large Multi-Task LMs with a Small Scorer Publication year: 2023 Coauthors: Bowen Tan, Yun Zhu, Lijuan Liu, Eric P. Xing, Zhiting Hu, Jindong Chen Abstract: Large language models (LLMs) such as T0, FLAN, and OPT-IML, excel in multi-tasking under a unified instruction-following paradigm, where they also exhibit remarkable generalization abilities to unseen tasks. Despite their impressive performance, these LLMs, with sizes ranging from several billion to hundreds of billions of parameters, demand substantial computational resources, making their training and inference expensive and inefficient. Furthermore, adapting these models to downstream applications, particularly complex tasks, is often unfeasible due to the extensive hardware requirements for finetuning, even when utilizing parameter-efficient approaches such as prompt tuning. Additionally, the most powerful multi-task LLMs, such as OPT-IML-175B and FLAN-PaLM-540B, are not publicly accessible, severely limiting their customization potential. To address these challenges, we introduce a pretrained small scorer, Cappy, designed to enhance the performance and efficiency of multi-task LLMs. With merely 360 million parameters, Cappy functions either independently on classification tasks or serve as an auxiliary component for LLMs, boosting their performance. Moreover, Cappy enables efficiently integrating downstream supervision without requiring LLM finetuning nor the access to their parameters. Our experiments demonstrate that, when working independently on 11 language understanding tasks from PromptSource, Cappy outperforms LLMs that are several orders of magnitude larger. Besides, on 45 complex tasks from BIG-Bench, Cappy boosts the performance of the advanced multi-task LLM, FLAN-T5, by a large margin. Furthermore, Cappy is flexible to cooperate with other LLM adaptations, including finetuning and in-context learning, offering additional performance enhancement.'),\n",
       " Document(page_content=\"Author: Eric P. Xing Title: RedCoast: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU/TPUs Publication year: 2023 Coauthors: Bowen Tan, Yun Zhu, Lijuan Liu, Hongyi Wang, Yonghao Zhuang, Jindong Chen, Eric P. Xing, Zhiting Hu Abstract: The recent progress of AI can be largely attributed to large language models (LLMs). However, their escalating memory requirements introduce challenges for machine learning (ML) researchers and engineers. Addressing this requires developers to partition a large model to distribute it across multiple GPUs or TPUs. This necessitates considerable coding and intricate configuration efforts with existing model parallel tools, such as Megatron-LM, DeepSpeed, and Alpa. These tools require users' expertise in machine learning systems (MLSys), creating a bottleneck in LLM development, particularly for developers without MLSys background. In this work, we present RedCoast(Redco), a lightweight and user-friendly tool crafted to automate distributed training and inference for LLMs, as well as to simplify ML pipeline development. The design of Redco emphasizes two key aspects. Firstly, to automate model parallism, our study identifies two straightforward rules to generate tensor parallel strategies for any given LLM. Integrating these rules into Redco facilitates effortless distributed LLM training and inference, eliminating the need of additional coding or complex configurations. We demonstrate the effectiveness by applying Redco on a set of LLM architectures, such as GPT-J, LLaMA, T5, and OPT, up to the size of 66B. Secondly, we propose a mechanism that allows for the customization of diverse ML pipelines through the definition of merely three functions, avoiding redundant and formulaic code like multi-host related processing. This mechanism proves adaptable across a spectrum of ML algorithms, from foundational language modeling to complex algorithms like meta-learning and reinforcement learning. Consequently, Redco implementations exhibit much fewer code lines compared to their official counterparts.\"),\n",
       " Document(page_content='Author: Eric P. Xing Title: Neural-Symbolic Interaction and Co-Evolving Publication year: 2023 Coauthors: Bowen Tan, Shibo Hao, Eric P. Xing, Zhiting Hu '),\n",
       " Document(page_content='Author: Eric P. Xing Title: Redco: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU/TPUs Publication year: 2023 Coauthors: Bowen Tan, Yun Zhu, Lijuan Liu, Hongyi Wang, Yonghao Zhuang, Jindong Chen, Eric P. Xing, Zhiting Hu Abstract: .'),\n",
       " Document(page_content='Author: Eric P. Xing Title: Learning to Prompt Segment Anything Models Publication year: 2024 Coauthors: Jiaxing Huang, Kai Jiang, Jingyi Zhang, Han Qiu, Lewei Lu, Shijian Lu, Eric P. Xing Abstract: Segment Anything Models (SAMs) like SEEM and SAM have demonstrated great potential in learning to segment anything. The core design of SAMs lies with Promptable Segmentation, which takes a handcrafted prompt as input and returns the expected segmentation mask. SAMs work with two types of prompts including spatial prompts (e.g., points) and semantic prompts (e.g., texts), which work together to prompt SAMs to segment anything on downstream datasets. Despite the important role of prompts, how to acquire suitable prompts for SAMs is largely under-explored. In this work, we examine the architecture of SAMs and identify two challenges for learning effective prompts for SAMs. To this end, we propose spatial-semantic prompt learning (SSPrompt) that learns effective semantic and spatial prompts for better SAMs. Specifically, SSPrompt introduces spatial prompt learning and semantic prompt learning, which optimize spatial prompts and semantic prompts directly over the embedding space and selectively leverage the knowledge encoded in pre-trained prompt encoders. Extensive experiments show that SSPrompt achieves superior image segmentation performance consistently across multiple widely adopted datasets.'),\n",
       " Document(page_content='Author: Eric P. Xing Title: 3D Open-vocabulary Segmentation with Foundation Models Publication year: 2023 Coauthors: Kunhao Liu, Fangneng Zhan, Jiahui Zhang, Muyu Xu, Yingchen Yu, Abdulmotaleb El-Saddik, Christian Theobalt, Eric P. Xing, Shijian Lu Abstract: Open-vocabulary segmentation of 3D scenes is a fundamental function of human perception and thus a crucial objective in computer vision research. However, this task is heavily impeded by the lack of large-scale and diverse 3D open-vocabulary segmentation datasets for training robust and generalizable models. Distilling knowledge from pre-trained 2D open-vocabulary segmentation models helps but it compromises the open-vocabulary feature significantly as the 2D models are mostly finetuned with close-vocabulary datasets. We tackle the challenges in 3D open-vocabulary segmentation by exploiting the open-vocabulary multimodal knowledge and object reasoning capability of pre-trained foundation models CLIP and DINO, without necessitating any fine-tuning. Specifically, we distill open-vocabulary visual and textual knowledge from CLIP into a neural radiance field (NeRF) which effectively lifts 2D features into view-consistent 3D segmentation. Furthermore, we introduce the Relevancy-Distribution Alignment loss and Feature-Distribution Alignment loss to respectively mitigate the ambiguities of CLIP features and distill precise object boundaries from DINO features, eliminating the need for segmentation annotations during training. Extensive experiments show that our method even outperforms fully supervised models trained with segmentation annotations, suggesting that 3D open-vocabulary segmentation can be effectively learned from 2D images and text-image pairs.'),\n",
       " Document(page_content='Author: Eric P. Xing Title: Squeeze, Recover and Relabel: Dataset Condensation at ImageNet Scale From A New Perspective Publication year: 2023 Coauthors: Zeyuan Yin, Eric P. Xing, Zhiqiang Shen Abstract: We present a new dataset condensation framework termed Squeeze, Recover and Relabel (SRe$^2$L) that decouples the bilevel optimization of model and synthetic data during training, to handle varying scales of datasets, model architectures and image resolutions for efficient dataset condensation. The proposed method demonstrates flexibility across diverse dataset scales and exhibits multiple advantages in terms of arbitrary resolutions of synthesized images, low training cost and memory consumption with high-resolution synthesis, and the ability to scale up to arbitrary evaluation network architectures. Extensive experiments are conducted on Tiny-ImageNet and full ImageNet-1K datasets. Under 50 IPC, our approach achieves the highest 42.5% and 60.8% validation accuracy on Tiny-ImageNet and ImageNet-1K, outperforming all previous state-of-the-art methods by margins of 14.5% and 32.9%, respectively. Our approach also surpasses MTT in terms of speed by approximately 52$\\\\times$ (ConvNet-4) and 16$\\\\times$ (ResNet-18) faster with less memory consumption of 11.6$\\\\times$ and 6.4$\\\\times$ during data synthesis. Our code and condensed datasets of 50, 200 IPC with 4K recovery budget are available at https://github.com/VILA-Lab/SRe2L.'),\n",
       " Document(page_content='Author: Eric P. Xing Title: One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning Publication year: 2023 Coauthors: Arnav Chavan, Zhuang Liu, D. Gupta, Eric P. Xing, Zhiqiang Shen Abstract: We present Generalized LoRA (GLoRA), an advanced approach for universal parameter-efficient fine-tuning tasks. Enhancing Low-Rank Adaptation (LoRA), GLoRA employs a generalized prompt module to optimize pre-trained model weights and adjust intermediate activations, providing more flexibility and capability across diverse tasks and datasets. Moreover, GLoRA facilitates efficient parameter adaptation by employing a scalable, modular, layer-wise structure search that learns individual adapter of each layer. Originating from a unified mathematical formulation, GLoRA exhibits strong transfer learning, few-shot learning and domain generalization abilities, as it adapts to new tasks through not only weights but also additional dimensions like activations. Comprehensive experiments demonstrate that GLoRA outperforms all previous methods in natural, specialized, and structured vision benchmarks, achieving superior accuracy with fewer parameters and computations. The proposed method on LLaMA-1 and LLaMA-2 also show considerable enhancements compared to the original LoRA in the language domain. Furthermore, our structural re-parameterization design ensures that GLoRA incurs no extra inference cost, rendering it a practical solution for resource-limited applications. Code and models are available at: https://github.com/Arnav0400/ViT-Slim/tree/master/GLoRA.'),\n",
       " Document(page_content='Author: Eric P. Xing Title: Defending Against Malicious Behaviors in Federated Learning with Blockchain Publication year: 2023 Coauthors: Nanqing Dong, Zhipeng Wang, Jiahao Sun, Michael C. Kampffmeyer, Yizhe Wen, Shuoying Zhang, W. Knottenbelt, Eric P. Xing Abstract: In the era of deep learning, federated learning (FL) presents a promising approach that allows multi-institutional data owners, or clients, to collaboratively train machine learning models without compromising data privacy. However, most existing FL approaches rely on a centralized server for global model aggregation, leading to a single point of failure. This makes the system vulnerable to malicious attacks when dealing with dishonest clients. In this work, we address this problem by proposing a secure and reliable FL system based on blockchain and distributed ledger technology. Our system incorporates a peer-to-peer voting mechanism and a reward-and-slash mechanism, which are powered by on-chain smart contracts, to detect and deter malicious behaviors. Both theoretical and empirical analyses are presented to demonstrate the effectiveness of the proposed approach, showing that our framework is robust against malicious client-side behaviors.'),\n",
       " Document(page_content='Author: Eric P. Xing Title: Weakly Supervised 3D Open-vocabulary Segmentation Publication year: 2023 Coauthors: Kunhao Liu, Fangneng Zhan, Jiahui Zhang, Muyu Xu, Yingchen Yu, A. E. Saddik, C. Theobalt, Eric P. Xing, Shijian Lu Abstract: Open-vocabulary segmentation of 3D scenes is a fundamental function of human perception and thus a crucial objective in computer vision research. However, this task is heavily impeded by the lack of large-scale and diverse 3D open-vocabulary segmentation datasets for training robust and generalizable models. Distilling knowledge from pre-trained 2D open-vocabulary segmentation models helps but it compromises the open-vocabulary feature as the 2D models are mostly finetuned with close-vocabulary datasets. We tackle the challenges in 3D open-vocabulary segmentation by exploiting pre-trained foundation models CLIP and DINO in a weakly supervised manner. Specifically, given only the open-vocabulary text descriptions of the objects in a scene, we distill the open-vocabulary multimodal knowledge and object reasoning capability of CLIP and DINO into a neural radiance field (NeRF), which effectively lifts 2D features into view-consistent 3D segmentation. A notable aspect of our approach is that it does not require any manual segmentation annotations for either the foundation models or the distillation process. Extensive experiments show that our method even outperforms fully supervised models trained with segmentation annotations in certain scenes, suggesting that 3D open-vocabulary segmentation can be effectively learned from 2D images and text-image pairs. Code is available at \\\\url{https://github.com/Kunhao-Liu/3D-OVS}.'),\n",
       " Document(page_content='Author: Eric P. Xing Title: Supplementary Material for KD-DLGAN: Data Limited Image Generation via Knowledge Distillation Publication year: 2023 Coauthors: Kaiwen Cui, Yingchen Yu, Fangneng Zhan, Shengcai Liao, Shijian Lu, Eric P. Xing Abstract: We conduct experiments over multiple widely adopted datasets including: 100-shot, AFHQ, CIFAR-10, CIFAR100 and ImageNet. 100-shot: 100-shot contains three datasets each of which has 100 samples of resolution 256 × 256. The three datasets are 100-shot Obama, 100-shot Grumpy Cat and 100-shot Panda. AFHQ: AFHQ consists of face images of three types of animals including Cat, Dog and Wildlife, each of which has 5k training images. We follow DA [9] and use 160 AFHQCat images and 389 AFHQ-Dog images (at a resolution of 256 × 256) for training. CIFAR-10: CIFAR-10 contains 50k training images and 10k validation images with 10 classes. The image resolution is 32 × 32. In our experiments, three networks are trained with 100%, 20% or 10% training images, respectively, and the trained models are valuated over all the validation images. CIFAR-100: CIFAR-100 contains 50k training images and 10k validation images of 100 classes. The image resolution is 32 × 32. In our experiments, three networks are trained with 100%, 20% or 10% training images, respectively, and the trained models are evaluated over all the validation data.'),\n",
       " Document(page_content='Author: Eric P. Xing Title: Improved Logical Reasoning of Language Models via Differentiable Symbolic Programming Publication year: 2023 Coauthors: Hanlin Zhang, Jiani Huang, Ziyang Li, M. Naik, Eric P. Xing Abstract: Pre-trained large language models (LMs) struggle to perform logical reasoning reliably despite advances in scale and compositionality. In this work, we tackle this challenge through the lens of symbolic programming. We propose DSR-LM, a Differentiable Symbolic Reasoning framework where pre-trained LMs govern the perception of factual knowledge, and a symbolic module performs deductive reasoning. In contrast to works that rely on hand-crafted logic rules, our differentiable symbolic reasoning framework efficiently learns weighted rules and applies semantic loss to further improve LMs. DSR-LM is scalable, interpretable, and allows easy integration of prior knowledge, thereby supporting extensive symbolic programming to robustly derive a logical conclusion. The results of our experiments suggest that DSR-LM improves the logical reasoning abilities of pre-trained language models, resulting in a significant increase in accuracy of over 20% on deductive reasoning benchmarks. Furthermore, DSR-LM outperforms a variety of competitive baselines when faced with systematic changes in sequence length.'),\n",
       " Document(page_content='Author: Eric P. Xing Title: Jais and Jais-chat: Arabic-Centric Foundation and Instruction-Tuned Open Generative Large Language Models Publication year: 2023 Coauthors: Neha Sengupta, Sunil Kumar Sahu, Bokang Jia, Satheesh Katipomu, Haonan Li, Fajri Koto, Osama Mohammed Afzal, Samta Kamboj, O. Pandit, Rahul Pal, Lalit Pradhan, Zainul Mujahid, Massa Baali, Xudong Han, Alham Fikri Aji, Zhengzhong Liu, Andy Hock, Andrew Feldman, Jonathan Lee, A. Jackson, Preslav Nakov, Timothy Baldwin, Eric P. Xing Abstract: We introduce Jais and Jais-chat, new state-of-the-art Arabic-centric foundation and instruction-tuned open generative large language models (LLMs). The models are based on the GPT-3 decoder-only architecture and are pretrained on a mixture of Arabic and English texts, including source code in various programming languages. With 13 billion parameters, they demonstrate better knowledge and reasoning capabilities in Arabic than any existing open Arabic and multilingual models by a sizable margin, based on extensive evaluation. Moreover, the models are competitive in English compared to English-centric open models of similar size, despite being trained on much less English data. We provide a detailed description of the training, the tuning, the safety alignment, and the evaluation of the models. We release two open versions of the model -- the foundation Jais model, and an instruction-tuned Jais-chat variant -- with the aim of promoting research on Arabic LLMs. Available at https://huggingface.co/inception-mbzuai/jais-13b-chat'),\n",
       " Document(page_content='Author: Eric P. Xing Title: KD-DLGAN: Data Limited Image Generation via Knowledge Distillation Publication year: 2023 Coauthors: Kaiwen Cui, Yingchen Yu, Fangneng Zhan, Shengcai Liao, Shijian Lu1, Eric P. Xing Abstract: Generative Adversarial Networks (GANs) rely heavily on large-scale training data for training high-quality image generation models. With limited training data, the GAN discriminator often suffers from severe overfitting which directly leads to degraded generation especially in generation diversity. Inspired by the recent advances in knowledge distillation (KD), we propose KD-DLGAN, a knowledge-distillation based generation framework that introduces pre-trained vision-language models for training effective data-limited generation models. KD-DLGAN consists of two innovative designs. The first is aggregated generative KD that mitigates the discriminator overfitting by challenging the discriminator with harder learning tasks and distilling more generalizable knowledge from the pre-trained models. The second is correlated generative KD that improves the generation diversity by distilling and preserving the diverse image-text correlation within the pre-trained models. Extensive experiments over multiple benchmarks show that KD-DLGAN achieves superior image generation with limited training data. In addition, KD-DLGAN complements the state-of-the-art with consistent and substantial performance gains. Note that codes will be released.'),\n",
       " Document(page_content='Author: Eric P. Xing Title: 3D Semantic Segmentation in the Wild: Learning Generalized Models for Adverse-Condition Point Clouds Publication year: 2023 Coauthors: Aoran Xiao, Jiaxing Huang, Weihao Xuan, Ruijie Ren, Kangcheng Liu, Dayan Guan, A. E. Saddik, Shijian Lu, Eric P. Xing Abstract: Robust point cloud parsing under all-weather conditions is crucial to level-5 autonomy in autonomous driving. However, how to learn a universal 3D semantic segmentation (3DSS) model is largely neglected as most existing benchmarks are dominated by point clouds captured under normal weather. We introduce SemanticSTF, an adverse-weather point cloud dataset that provides dense point-level annotations and allows to study 3DSS under various adverse weather conditions. We study all-weather 3DSS modeling under two setups: 1) domain adaptive 3DSS that adapts from normal-weather data to adverse-weather data; 2) domain generalizable 3DSS that learns all-weather 3DSS models from normal-weather data. Our studies reveal the challenge while existing 3DSS methods encounter adverse-weather data, showing the great value of SemanticSTF in steering the future endeavor along this very meaningful research direction. In addition, we design a domain randomization technique that alternatively randomizes the geometry styles of point clouds and aggregates their embeddings, ultimately leading to a generalizable model that can improve 3DSS under various adverse weather effectively. The SemanticSTF and related codes are available at https://github.com/xiaoaoran/SemanticSTF.'),\n",
       " Document(page_content='Author: Eric P. Xing Title: Cuttlefish: Low-Rank Model Training without All the Tuning Publication year: 2023 Coauthors: Hongyi Wang, Saurabh Agarwal, Pongsakorn U-chupala, Yoshiki Tanaka, Eric P. Xing, Dimitris Papailiopoulos Abstract: Recent research has shown that training low-rank neural networks can effectively reduce the total number of trainable parameters without sacrificing predictive accuracy, resulting in end-to-end speedups. However, low-rank model training necessitates adjusting several additional factorization hyperparameters, such as the rank of the factorization at each layer. In this paper, we tackle this challenge by introducing Cuttlefish, an automated low-rank training approach that eliminates the need for tuning factorization hyperparameters. Cuttlefish leverages the observation that after a few epochs of full-rank training, the stable rank (i.e., an approximation of the true rank) of each layer stabilizes at a constant value. Cuttlefish switches from full-rank to low-rank training once the stable ranks of all layers have converged, setting the dimension of each factorization to its corresponding stable rank. Our results show that Cuttlefish generates models up to 5.6 times smaller than full-rank models, and attains up to a 1.2 times faster end-to-end training process while preserving comparable accuracy. Moreover, Cuttlefish outperforms state-of-the-art low-rank model training methods and other prominent baselines. The source code for our implementation can be found at: https://github.com/hwang595/Cuttlefish.'),\n",
       " Document(page_content='Author: Eric P. Xing Title: Does compressing activations help model parallel training? Publication year: 2023 Coauthors: S. Bian, Dacheng Li, Hongyi Wang, Eric P. Xing, S. Venkataraman Abstract: Large-scale Transformer models are known for their exceptional performance in a range of tasks, but training them can be difficult due to the requirement for communication-intensive model parallelism. One way to improve training speed is to compress the message size in communication. Previous approaches have primarily focused on compressing gradients in a data parallelism setting, but compression in a model-parallel setting is an understudied area. We have discovered that model parallelism has fundamentally different characteristics than data parallelism. In this work, we present the first empirical study on the effectiveness of compression methods for model parallelism. We implement and evaluate three common classes of compression algorithms - pruning-based, learning-based, and quantization-based - using a popular Transformer training framework. We evaluate these methods across more than 160 settings and 8 popular datasets, taking into account different hyperparameters, hardware, and both fine-tuning and pre-training stages. We also provide analysis when the model is scaled up. Finally, we provide insights for future development of model parallelism compression algorithms.'),\n",
       " Document(page_content='Author: Eric P. Xing Title: Federated Learning as Variational Inference: A Scalable Expectation Propagation Approach Publication year: 2023 Coauthors: Han Guo, P. Greengard, Hongyi Wang, A. Gelman, Yoon Kim, Eric P. Xing Abstract: The canonical formulation of federated learning treats it as a distributed optimization problem where the model parameters are optimized against a global loss function that decomposes across client loss functions. A recent alternative formulation instead treats federated learning as a distributed inference problem, where the goal is to infer a global posterior from partitioned client data (Al-Shedivat et al., 2021). This paper extends the inference view and describes a variational inference formulation of federated learning where the goal is to find a global variational posterior that well-approximates the true posterior. This naturally motivates an expectation propagation approach to federated learning (FedEP), where approximations to the global posterior are iteratively refined through probabilistic message-passing between the central server and the clients. We conduct an extensive empirical study across various algorithmic considerations and describe practical strategies for scaling up expectation propagation to the modern federated setting. We apply FedEP on standard federated learning benchmarks and find that it outperforms strong baselines in terms of both convergence speed and accuracy.'),\n",
       " Document(page_content=\"Author: Eric P. Xing Title: Memory-adaptive Depth-wise Heterogenous Federated Learning Publication year: 2023 Coauthors: Kai Zhang, Yutong Dai, Hongyi Wang, Eric P. Xing, Xun Chen, Lichao Sun Abstract: Federated learning is a promising paradigm that allows multiple clients to collaboratively train a model without sharing the local data. However, the presence of heterogeneous devices in federated learning, such as mobile phones and IoT devices with varying memory capabilities, would limit the scale and hence the performance of the model could be trained. The mainstream approaches to address memory limitations focus on width-slimming techniques, where different clients train subnetworks with reduced widths locally and then the server aggregates the subnetworks. The global model produced from these methods suffers from performance degradation due to the negative impact of the actions taken to handle the varying subnetwork widths in the aggregation phase. In this paper, we introduce a memory-adaptive depth-wise learning solution in FL called FeDepth, which adaptively decomposes the full model into blocks according to the memory budgets of each client and trains blocks sequentially to obtain a full inference model. Our method outperforms state-of-the-art approaches, achieving 5% and more than 10% improvements in top-1 accuracy on CIFAR-10 and CIFAR-100, respectively. We also demonstrate the effectiveness of depth-wise fine-tuning on ViT. Our findings highlight the importance of memory-aware techniques for federated learning with heterogeneous devices and the success of depth-wise training strategy in improving the global model's performance.\"),\n",
       " Document(page_content=\"Author: E. Xing Title: Identification of Nonlinear Latent Hierarchical Models Publication year: 2023 Coauthors: Lingjing Kong, Biwei Huang, Feng Xie, E. Xing, Yuejie Chi, Kun Zhang Abstract: Identifying latent variables and causal structures from observational data is essential to many real-world applications involving biological data, medical data, and unstructured data such as images and languages. However, this task can be highly challenging, especially when observed variables are generated by causally related latent variables and the relationships are nonlinear. In this work, we investigate the identification problem for nonlinear latent hierarchical causal models in which observed variables are generated by a set of causally related latent variables, and some latent variables may not have observed children. We show that the identifiability of causal structures and latent variables (up to invertible transformations) can be achieved under mild assumptions: on causal structures, we allow for multiple paths between any pair of variables in the graph, which relaxes latent tree assumptions in prior work; on structural functions, we permit general nonlinearity and multi-dimensional continuous variables, alleviating existing work's parametric assumptions. Specifically, we first develop an identification criterion in the form of novel identifiability guarantees for an elementary latent variable model. Leveraging this criterion, we show that both causal structures and latent variables of the hierarchical model can be identified asymptotically by explicitly constructing an estimation procedure. To the best of our knowledge, our work is the first to establish identifiability guarantees for both causal structures and latent variables in nonlinear latent hierarchical models.\"),\n",
       " Document(page_content='Author: E. Xing Title: StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields Publication year: 2023 Coauthors: Kunhao Liu, Fangneng Zhan, Yiwen Chen, Jiahui Zhang, Yingchen Yu, Abdulmotaleb El Saddik, Shijian Lu, E. Xing Abstract: 3D style transfer aims to render stylized novel views of a 3D scene with multiview consistency. However, most existing work suffers from a three-way dilemma over accurate geometry reconstruction, high-quality stylization, and being generalizable to arbitrary new styles. We propose StyleRF (Style Radiance Fields), an innovative 3D style transfer technique that resolves the three-way dilemma by performing style transformation within the feature space of a radiance field. StyleRF employs an explicit grid of high-level features to represent 3D scenes, with which highfidelity geometry can be reliably restored via volume rendering. In addition, it transforms the grid features according to the reference style which directly leads to high-quality zero-shot style transfer. StyleRF consists of two innovative designs. The first is sampling-invariant content transformation that makes the transformation invariant to the holistic statistics of the sampled 3D points and accordingly ensures multi-view consistency. The second is deferred style transformation of 2D feature maps which is equivalent to the transformation of 3D points but greatly reduces memory footprint without degrading multi-view consistency. Extensive experiments show that StyleRF achieves superior 3D stylization quality with precise geometry reconstruction and it can generalize to various new styles in a zero-shot manner. Project website: https://kunhao-liu.github.io/StyleRF/'),\n",
       " Document(page_content='Author: E. Xing Title: Judging LLM-as-a-judge with MT-Bench and Chatbot Arena Publication year: 2023 Coauthors: Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, E. Xing, Haotong Zhang, Joseph Gonzalez, I. Stoica Abstract: Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.'),\n",
       " Document(page_content='Author: E. Xing Title: Understanding Masked Autoencoders via Hierarchical Latent Variable Models Publication year: 2023 Coauthors: Lingjing Kong, Martin Q. Ma, Guan-Hong Chen, E. Xing, Yuejie Chi, Louis-Philippe Morency, Kun Zhang Abstract: Masked autoencoder (MAE), a simple and effective self-supervised learning framework based on the reconstruction of masked image regions, has recently achieved prominent success in a variety of vision tasks. Despite the emergence of intriguing empirical observations on MAE, a theoretically principled understanding is still lacking. In this work, we formally characterize and justify existing empirical in-sights and provide theoretical guarantees of MAE. We formulate the underlying data-generating process as a hierarchical latent variable model, and show that under reasonable assumptions, MAE provably identifies a set of latent variables in the hierarchical model, explaining why MAE can extract high-level information from pixels. Further, we show how key hyperparameters in MAE (the masking ratio and the patch size) determine which true latent variables to be recovered, therefore influencing the level of semantic information in the representation. Specifically, extremely large or small masking ratios inevitably lead to low-level representations. Our theory offers coherent explanations of existing empirical observations and provides insights for potential empirical improvements and fundamental limitations of the masked-reconstruction paradigm. We conduct extensive experiments to validate our theoretical insights.'),\n",
       " Document(page_content='Author: Eric P. Xing Title: LightSeq: Sequence Level Parallelism for Distributed Training of Long Context Transformers Publication year: 2023 Coauthors: Dacheng Li, Rulin Shao, Anze Xie, Eric P. Xing, Joseph E. Gonzalez, I. Stoica, Xuezhe Ma, Hao Zhang Abstract: Increasing the context length of large language models (LLMs) unlocks fundamentally new capabilities, but also significantly increases the memory footprints of training. Previous model-parallel systems such as Megatron-LM partition and compute different attention heads in parallel, resulting in large communication volumes, so they cannot scale beyond the number of attention heads, thereby hindering its adoption. In this paper, we introduce a new approach, LightSeq, for long-context LLMs training. LightSeq has many notable advantages. First, LightSeq partitions over the sequence dimension, hence is agnostic to model architectures and readily applicable for models with varying numbers of attention heads, such as Multi-Head, Multi-Query and Grouped-Query attention. Second, LightSeq not only requires up to 4.7x less communication than Megatron-LM on popular LLMs but also overlaps the communication with computation. To further reduce the training time, LightSeq features a novel gradient checkpointing scheme to bypass an forward computation for memory-efficient attention. We evaluate LightSeq on Llama-7B and its variants with sequence lengths from 32K to 512K. Through comprehensive experiments on single and cross-node training, we show that LightSeq achieves up to 1.24-2.01x end-to-end speedup, and a 2-8x longer sequence length on models with fewer heads, compared to Megatron-LM. Codes will be available at https://github.com/RulinShao/LightSeq.'),\n",
       " Document(page_content=\"Author: Eric P. Xing Title: LMSYS-Chat-1M: A Large-Scale Real-World LLM Conversation Dataset Publication year: 2023 Coauthors: Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Tianle Li, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zhuohan Li, Zi Lin, Eric P. Xing, Joseph E. Gonzalez, I. Stoica, Haotong Zhang Abstract: Studying how people interact with large language models (LLMs) in real-world scenarios is increasingly important due to their widespread use in various applications. In this paper, we introduce LMSYS-Chat-1M, a large-scale dataset containing one million real-world conversations with 25 state-of-the-art LLMs. This dataset is collected from 210K unique IP addresses in the wild on our Vicuna demo and Chatbot Arena website. We offer an overview of the dataset's content, including its curation process, basic statistics, and topic distribution, highlighting its diversity, originality, and scale. We demonstrate its versatility through four use cases: developing content moderation models that perform similarly to GPT-4, building a safety benchmark, training instruction-following models that perform similarly to Vicuna, and creating challenging benchmark questions. We believe that this dataset will serve as a valuable resource for understanding and advancing LLM capabilities. The dataset is publicly available at https://huggingface.co/datasets/lmsys/lmsys-chat-1m.\"),\n",
       " Document(page_content=\"Author: Eric Xing Title: ALISON: Fast and Effective Stylometric Authorship Obfuscation Publication year: 2024 Coauthors: Eric Xing, Saranya Venkatraman, Thai Le, Dongwon Lee Abstract: Authorship Attribution (AA) and Authorship Obfuscation (AO) are two competing tasks of increasing importance in privacy research. Modern AA leverages an author's consistent writing style to match a text to its author using an AA classifier. AO is the corresponding adversarial task, aiming to modify a text in such a way that its semantics are preserved, yet an AA model cannot correctly infer its authorship. To address privacy concerns raised by state-of-the-art (SOTA) AA methods, new AO methods have been proposed but remain largely impractical to use due to their prohibitively slow training and obfuscation speed, often taking hours. To this challenge, we propose a practical AO method, ALISON, that (1) dramatically reduces training/obfuscation time, demonstrating more than 10x faster obfuscation than SOTA AO methods, (2) achieves better obfuscation success through attacking three transformer-based AA methods on two benchmark datasets, typically performing 15% better than competing methods, (3) does not require direct signals from a target AA classifier during obfuscation, and (4) utilizes unique stylometric features, allowing sound model interpretation for explainable obfuscation. We also demonstrate that ALISON can effectively prevent four SOTA AA methods from accurately determining the authorship of ChatGPT-generated texts, all while minimally changing the original text semantics. To ensure the reproducibility of our findings, our code and data are available at: https://github.com/EricX003/ALISON.\"),\n",
       " Document(page_content='Author: Eric Xing Title: A Study on the Calibration of In-context Learning Publication year: 2023 Coauthors: Hanlin Zhang, Yi-Fan Zhang, Yaodong Yu, Dhruv Madeka, Dean Foster, Eric Xing, Hima Lakkaraju, S. Kakade Abstract: Accurate uncertainty quantification is crucial for the safe deployment of language models (LMs), and prior research has demonstrated improvements in the calibration of modern LMs. Our study focuses on in-context learning (ICL), a prevalent method for adapting static LMs through tailored prompts, and examines the balance between performance and calibration across a broad spectrum of natural language understanding and reasoning tasks. Through comprehensive experiments, we observe that, with an increasing number of ICL examples, models initially exhibit increased miscalibration before achieving better calibration and miscalibration tends to arise in low-shot settings. Moreover, we find that methods aimed at improving usability, such as fine-tuning and chain-of-thought (CoT) prompting, can lead to miscalibration and unreliable natural language explanations, suggesting that new methods may be required for scenarios where models are expected to be reliable.'),\n",
       " Document(page_content=\"Author: Eric Xing Title: SegMix: A Simple Structure-Aware Data Augmentation Method Publication year: 2023 Coauthors: Yuxin Pei, Pushkar Bhuse, Zhengzhong Liu, Eric Xing Abstract: Interpolation-based Data Augmentation (DA) methods (Mixup) linearly interpolate the inputs and labels of two or more training examples. Mixup has more recently been adapted to the field of Natural Language Processing (NLP), mainly for sequence labeling tasks. However, such a simple adoption yields mixed or unstable improvements over the baseline models. We argue that the direct-adoption methods do not account for structures in NLP tasks. To this end, we propose SegMix, a collection of interpolation-based DA algorithms that can adapt to task-specific structures. SegMix poses fewer constraints on data structures, is robust to various hyperparameter settings, applies to more task settings, and adds little computational overhead. In the algorithm's core, we apply interpolation methods on task-specific meaningful segments, in contrast to applying them on sequences as in prior work. We find SegMix to be a flexible framework that combines rule-based DA methods with interpolation-based methods, creating interesting mixtures of DA techniques. We show that SegMix consistently improves performance over strong baseline models in Named Entity Recognition (NER) and Relation Extraction (RE) tasks, especially under data-scarce settings. Furthermore, this method is easy to implement and adds negligible training overhead.\"),\n",
       " Document(page_content=\"Author: Eric Xing Title: Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large Language Models Publication year: 2024 Coauthors: Loka Li, Guan-Hong Chen, Yusheng Su, Zhenhao Chen, Yixuan Zhang, Eric Xing, Kun Zhang Abstract: The recent success of Large Language Models (LLMs) has catalyzed an increasing interest in their self-correction capabilities. This paper presents a comprehensive investigation into the intrinsic self-correction of LLMs, attempting to address the ongoing debate about its feasibility. Our research has identified an important latent factor - the ``confidence'' of LLMs - during the self-correction process. Overlooking this factor may cause the models to over-criticize themselves, resulting in unreliable conclusions regarding the efficacy of self-correction. We have experimentally observed that LLMs possess the capability to understand the ``confidence'' in their own responses. It motivates us to develop an ``If-or-Else'' (IoE) prompting framework, designed to guide LLMs in assessing their own ``confidence'', facilitating intrinsic self-corrections. We conduct extensive experiments and demonstrate that our IoE-based Prompt can achieve a consistent improvement regarding the accuracy of self-corrected responses over the initial answers. Our study not only sheds light on the underlying factors affecting self-correction in LLMs, but also introduces a practical framework that utilizes the IoE prompting principle to efficiently improve self-correction capabilities with ``confidence''. The code is available at \\\\url{https://github.com/MBZUAI-CLeaR/IoE-Prompting.git}.\"),\n",
       " Document(page_content='Author: Eric Xing Title: Temporally Disentangled Representation Learning under Unknown Nonstationarity Publication year: 2023 Coauthors: Xiangchen Song, Weiran Yao, Yewen Fan, Xinshuai Dong, Guan-Hong Chen, Juan Carlos Niebles, Eric Xing, Kun Zhang Abstract: In unsupervised causal representation learning for sequential data with time-delayed latent causal influences, strong identifiability results for the disentanglement of causally-related latent variables have been established in stationary settings by leveraging temporal structure. However, in nonstationary setting, existing work only partially addressed the problem by either utilizing observed auxiliary variables (e.g., class labels and/or domain indexes) as side information or assuming simplified latent causal dynamics. Both constrain the method to a limited range of scenarios. In this study, we further explored the Markov Assumption under time-delayed causally related process in nonstationary setting and showed that under mild conditions, the independent latent components can be recovered from their nonlinear mixture up to a permutation and a component-wise transformation, without the observation of auxiliary variables. We then introduce NCTRL, a principled estimation framework, to reconstruct time-delayed latent causal variables and identify their relations from measured sequential data only. Empirical evaluations demonstrated the reliable identification of time-delayed latent causal influences, with our methodology substantially outperforming existing baselines that fail to exploit the nonstationarity adequately and then, consequently, cannot distinguish distribution shifts.'),\n",
       " Document(page_content='Author: Eric P. Xing Title: LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning Publication year: 2023 Coauthors: Han Guo, P. Greengard, Eric P. Xing, Yoon Kim Abstract: We propose a simple approach for memory-efficient adaptation of pretrained language models. Our approach uses an iterative algorithm to decompose each pretrained matrix into a high-precision low-rank component and a memory-efficient quantized component. During finetuning, the quantized component remains fixed and only the low-rank component is updated. We present an integer linear programming formulation of the quantization component which enables dynamic configuration of quantization parameters (e.g., bit-width, block size) for each matrix given an overall target memory budget. We further explore a data-aware version of the algorithm which uses an approximation of the Fisher information matrix to weight the reconstruction objective during matrix decomposition. Experiments on finetuning RoBERTa and LLaMA-2 (7B and 70B) demonstrate that our low-rank plus quantized matrix decomposition approach (LQ-LoRA) outperforms strong QLoRA and GPTQ-LoRA baselines and enables aggressive quantization to sub-3 bits with only minor performance degradations. When finetuned on a language modeling calibration dataset, LQ-LoRA can also be used for model compression; in this setting our 2.75-bit LLaMA-2-70B model (which has 2.85 bits on average when including the low-rank components and requires 27GB of GPU memory) performs respectably compared to the 16-bit baseline.'),\n",
       " Document(page_content='Author: Chenyan Xiong Title: Text Matching Improves Sequential Recommendation by Reducing Popularity Biases Publication year: 2023 Coauthors: Zhenghao Liu, Senkun Mei, Chenyan Xiong, Xiaohua Li, Shi Yu, Zhiyuan Liu, Yu Gu, Ge Yu Abstract: This paper proposes Text mAtching based SequenTial rEcommenda-tion model (TASTE), which maps items and users in an embedding space and recommends items by matching their text representations. TASTE verbalizes items and user-item interactions using identifiers and attributes of items. To better characterize user behaviors, TASTE additionally proposes an attention sparsity method, which enables TASTE to model longer user-item interactions by reducing the self-attention computations during encoding. Our experiments show that TASTE outperforms the state-of-the-art methods on widely used sequential recommendation datasets. TASTE alleviates the cold start problem by representing long-tail items using full-text modeling and bringing the benefits of pretrained language models to recommendation systems. Our further analyses illustrate that TASTE significantly improves the recommendation accuracy by reducing the popularity bias of previous item id based recommendation models and returning more appropriate and text-relevant items to satisfy users. All codes are available at https://github.com/OpenMatch/TASTE.'),\n",
       " Document(page_content='Author: Chenyan Xiong Title: Fusion-in-T5: Unifying Document Ranking Signals for Improved Information Retrieval Publication year: 2023 Coauthors: S. Yu, Cheng-Chung Fan, Chenyan Xiong, David Jin, Zhiyuan Liu, Zhenghao Liu Tsinghua University, Huazhong University of Science, Technology, Microsoft Research, M. I. O. Technology, N. University Abstract: Common IR pipelines are typically cascade systems that may involve multiple rankers and/or fusion models to integrate different information step-by-step. In this paper, we propose a novel re-ranker named Fusion-in-T5 (FiT5), which integrates document text information, retrieval features, and global document information into a single unified model using templated-based input and global attention. Experiments on passage ranking benchmarks MS MARCO and TREC DL show that FiT5 significantly improves ranking performance over prior pipelines. Analyses find that through global attention, FiT5 is able to jointly utilize the ranking features via gradually attending to related documents, and thus improve the detection of subtle nuances between them. Our code will be open-sourced.'),\n",
       " Document(page_content='Author: Chenyan Xiong Title: OpenMatch-v2: An All-in-one Multi-Modality PLM-based Information Retrieval Toolkit Publication year: 2023 Coauthors: Shi Yu, Zhenghao Liu, Chenyan Xiong, Zhiyuan Liu Abstract: Pre-trained language models (PLMs) have emerged as the foundation of the most advanced Information Retrieval (IR) models. Powered by PLMs, the latest IR research has proposed novel models, new domain adaptation algorithms as well as enlarged datasets. In this paper, we present a Python-based IR toolkit OpenMatch-v2. As a full upgrade of OpenMatch proposed in 2021, OpenMatch-v2 incorporates the most recent advancements of PLM-based IR research, providing support for new, cross-modality models and enhanced domain adaptation techniques with a streamlined, optimized infrastructure. The code of OpenMatch is publicly available at https://github.com/OpenMatch/OpenMatch.'),\n",
       " Document(page_content=\"Author: Chenyan Xiong Title: An In-depth Look at Gemini's Language Abilities Publication year: 2023 Coauthors: Syeda Nahida Akter, Zichun Yu, Aashiq Muhamed, Tianyue Ou, Alex Bauerle, Ángel Alexander Cabrera, Krish Dholakia, Chenyan Xiong, Graham Neubig Abstract: The recently released Google Gemini class of models are the first to comprehensively report results that rival the OpenAI GPT series across a wide variety of tasks. In this paper, we do an in-depth exploration of Gemini's language abilities, making two contributions. First, we provide a third-party, objective comparison of the abilities of the OpenAI GPT and Google Gemini models with reproducible code and fully transparent results. Second, we take a closer look at the results, identifying areas where one of the two model classes excels. We perform this analysis over 10 datasets testing a variety of language abilities, including reasoning, answering knowledge-based questions, solving math problems, translating between languages, generating code, and acting as instruction-following agents. From this analysis, we find that Gemini Pro achieves accuracy that is close but slightly inferior to the corresponding GPT 3.5 Turbo on all tasks that we benchmarked. We further provide explanations for some of this under-performance, including failures in mathematical reasoning with many digits, sensitivity to multiple-choice answer ordering, aggressive content filtering, and others. We also identify areas where Gemini demonstrates comparably high performance, including generation into non-English languages, and handling longer and more complex reasoning chains. Code and data for reproduction can be found at https://github.com/neulab/gemini-benchmark\"),\n",
       " Document(page_content='Author: Chenyan Xiong Title: CompleQA: Benchmarking the Impacts of Knowledge Graph Completion Methods on Question Answering Publication year: 2023 Coauthors: Donghan Yu, Yu Gu, Chenyan Xiong, Yiming Yang '),\n",
       " Document(page_content='Author: Chenyan Xiong Title: ActiveRAG: Revealing the Treasures of Knowledge via Active Learning Publication year: 2024 Coauthors: Zhipeng Xu, Zhenghao Liu, Yibin Liu, Chenyan Xiong, Yukun Yan, Shuo Wang, Shi Yu, Zhiyuan Liu, Ge Yu Abstract: Retrieval Augmented Generation (RAG) has introduced a new paradigm for Large Language Models (LLMs), aiding in the resolution of knowledge-intensive tasks. However, current RAG models position LLMs as passive knowledge receptors, thereby restricting their capacity for learning and comprehending external knowledge. In this paper, we present ActiveRAG, an innovative RAG framework that shifts from passive knowledge acquisition to an active learning mechanism. This approach utilizes the Knowledge Construction mechanism to develop a deeper understanding of external knowledge by associating it with previously acquired or memorized knowledge. Subsequently, it designs the Cognitive Nexus mechanism to incorporate the outcomes from both chains of thought and knowledge construction, thereby calibrating the intrinsic cognition of LLMs. Our experimental results demonstrate that ActiveRAG surpasses previous RAG models, achieving a 5% improvement on question-answering datasets. All data and codes are available at https://github.com/OpenMatch/ActiveRAG.'),\n",
       " Document(page_content='Author: Chenyan Xiong Title: Cleaner Pretraining Corpus Curation with Neural Web Scraping Publication year: 2024 Coauthors: Zhipeng Xu, Zhenghao Liu, Yukun Yan, Zhiyuan Liu, Chenyan Xiong, Ge Yu Abstract: The web contains large-scale, diverse, and abundant information to satisfy the information-seeking needs of humans. Through meticulous data collection, preprocessing, and curation, webpages can be used as a fundamental data resource for language model pretraining. However, when confronted with the progressively revolutionized and intricate nature of webpages, rule-based/feature-based web scrapers are becoming increasingly inadequate. This paper presents a simple, fast, and effective Neural web Scraper (NeuScraper) to help extract primary and clean text contents from webpages. Experimental results show that NeuScraper surpasses the baseline scrapers by achieving more than a 20% improvement, demonstrating its potential in extracting higher-quality data to facilitate the language model pretraining. All of the code is available at https://github.com/OpenMatch/NeuScraper.'),\n",
       " Document(page_content='Author: Chenyan Xiong Title: Improving Multitask Retrieval by Promoting Task Specialization Publication year: 2023 Coauthors: Wenzheng Zhang, Chenyan Xiong, K. Stratos, Arnold Overwijk Abstract: Abstract In multitask retrieval, a single retriever is trained to retrieve relevant contexts for multiple tasks. Despite its practical appeal, naive multitask retrieval lags behind task-specific retrieval, in which a separate retriever is trained for each task. We show that it is possible to train a multitask retriever that outperforms task-specific retrievers by promoting task specialization. The main ingredients are: (1) a better choice of pretrained model—one that is explicitly optimized for multitasking—along with compatible prompting, and (2) a novel adaptive learning method that encourages each parameter to specialize in a particular task. The resulting multitask retriever is highly performant on the KILT benchmark. Upon analysis, we find that the model indeed learns parameters that are more task-specialized compared to naive multitasking without prompting or adaptive learning.1'),\n",
       " Document(page_content='Author: Chenyan Xiong Title: Unsupervised Dense Retrieval Training with Web Anchors Publication year: 2023 Coauthors: Yiqing Xie, X. Liu, Chenyan Xiong Abstract: In this work, we present an unsupervised retrieval method with contrastive learning on web anchors. The anchor text describes the content that is referenced from the linked page. This shows similarities to search queries that aim to retrieve pertinent information from relevant documents. Based on their commonalities, we train an unsupervised dense retriever, Anchor-DR, with a contrastive learning task that matches the anchor text and the linked document. To filter out uninformative anchors (such as \"homepage\" or other functional anchors), we present a novel filtering technique to only select anchors that contain similar types of information as search queries. Experiments show that Anchor-DR outperforms state-of-the-art methods on unsupervised dense retrieval by a large margin (e.g., by 5.3% NDCG@10 on MSMARCO). The gain of our method is especially significant for search and question answering tasks. Our analysis further reveals that the pattern of anchor-document pairs is similar to that of search query-document pairs. Code available at https://github.com/Veronicium/AnchorDR.'),\n",
       " Document(page_content='Author: Chenyan Xiong Title: Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In Publication year: 2023 Coauthors: Zichun Yu, Chenyan Xiong, S. Yu, Zhiyuan Liu Abstract: Retrieval augmentation can aid language models (LMs) in knowledge-intensive tasks by supplying them with external information. Prior works on retrieval augmentation usually jointly fine-tune the retriever and the LM, making them closely coupled. In this paper, we explore the scheme of generic retrieval plug-in: the retriever is to assist target LMs that may not be known beforehand or are unable to be fine-tuned together. To retrieve useful documents for unseen target LMs, we propose augmentation-adapted retriever (AAR), which learns LM’s preferences obtained from a known source LM. Experiments on the MMLU and PopQA datasets demonstrate that our AAR trained with a small source LM is able to significantly improve the zero-shot generalization of larger target LMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates that the preferences of different LMs overlap, enabling AAR trained with a single source LM to serve as a generic plug-in for various target LMs. Our code is open-sourced at https://github.com/OpenMatch/Augmentation-Adapted-Retriever.'),\n",
       " Document(page_content='Author: Chenyan Xiong Title: Model-Generated Pretraining Signals Improves Zero-Shot Generalization of Text-to-Text Transformers Publication year: 2023 Coauthors: Linyuan Gong, Chenyan Xiong, Xiaodong Liu, Payal Bajaj, Yiqing Xie, Alvin Cheung, Jianfeng Gao, Xia Song Abstract: This paper explores the effectiveness of model-generated signals in improving zero-shot generalization of text-to-text Transformers such as T5. We study various designs to pretrain T5 using an auxiliary model to construct more challenging token replacements for the main model to denoise. Key aspects under study include the decoding target, the location of the RTD head, and the masking pattern. Based on these studies, we develop a new model, METRO-T0, which is pretrained using the redesigned ELECTRA-Style pretraining strategies and then prompt-finetuned on a mixture of NLP tasks. METRO-T0 outperforms all similar-sized baselines on prompted NLP benchmarks, such as _T0 Eval_ and MMLU, and rivals the state-of-the-art T0-11B model with only **8%** of its parameters. Our analysis on model’s neural activation and parameter sensitivity reveals that the effectiveness of METRO-T0 stems from more balanced contribution of parameters and better utilization of their capacity. The code and model checkpoints are available at [https://github.com/gonglinyuan/metro_t0](https://github.com/gonglinyuan/metro_t0).'),\n",
       " Document(page_content=\"Author: Chenyan Xiong Title: MARVEL: Unlocking the Multi-Modal Capability of Dense Retrieval via Visual Module Plugin Publication year: 2023 Coauthors: Tianshuo Zhou, Senkun Mei, Xinze Li, Zhenghao Liu, Chenyan Xiong, Zhiyuan Liu, Yu Gu, Ge Yu Abstract: This paper proposes Multi-modAl Retrieval model via Visual modulE pLugin (MARVEL), which learns an embedding space for queries and multi-modal documents to conduct retrieval. MARVEL encodes queries and multi-modal documents with a unified encoder model, which helps to alleviate the modality gap between images and texts. Specifically, we enable the image understanding ability of the well-trained dense retriever, T5-ANCE, by incorporating the visual module's encoded image features as its inputs. To facilitate the multi-modal retrieval tasks, we build the ClueWeb22-MM dataset based on the ClueWeb22 dataset, which regards anchor texts as queries, and exacts the related text and image documents from anchor-linked web pages. Our experiments show that MARVEL significantly outperforms the state-of-the-art methods on the multi-modal retrieval dataset WebQA and ClueWeb22-MM. MARVEL provides an opportunity to broaden the advantages of text retrieval to the multi-model scenario. Besides, we also illustrate that the language model has the ability to extract image semantics and partly map the image features to the input word embedding space. All codes are available at https://github.com/OpenMatch/MARVEL.\"),\n",
       " Document(page_content='Author: Chenyan Xiong Title: Distributionally Robust Unsupervised Dense Retrieval Training on Web Graphs Publication year: 2023 Coauthors: Peixuan Han, Zhenghao Liu, Zhiyuan Liu, Chenyan Xiong Abstract: This paper introduces Web-DRO, an unsupervised dense retrieval model, which clusters documents based on web structures and reweights the groups during contrastive training. Specifically, we first leverage web graph links and contrastively train an embedding model for clustering anchor-document pairs. Then we use Group Distributional Robust Optimization to reweight different clusters of anchor-document pairs, which guides the model to assign more weights to the group with higher contrastive loss and pay more attention to the worst case during training. Our experiments on MS MARCO and BEIR show that our model, Web-DRO, significantly improves the retrieval effectiveness in unsupervised scenarios. A comparison of clustering techniques shows that training on the web graph combining URL information reaches optimal performance on clustering. Further analysis confirms that group weights are stable and valid, indicating consistent model preferences as well as effective up-weighting of valuable groups and down-weighting of uninformative ones. The code of this paper can be obtained from https://github.com/OpenMatch/Web-DRO.'),\n",
       " Document(page_content='Author: Chenyan Xiong Title: Structure-Aware Language Model Pretraining Improves Dense Retrieval on Structured Data Publication year: 2023 Coauthors: Xinze Li, Zhenghao Liu, Chenyan Xiong, Shi Yu, Yu Gu, Zhiyuan Liu, Ge Yu Abstract: This paper presents Structure Aware Dense Retrieval (SANTA) model, which encodes user queries and structured data in one universal embedding space for retrieving structured data. SANTA proposes two pretraining methods to make language models structure-aware and learn effective representations for structured data: 1) Structured Data Alignment, which utilizes the natural alignment relations between structured data and unstructured data for structure-aware pretraining. It contrastively trains language models to represent multi-modal text data and teaches models to distinguish matched structured data for unstructured texts. 2) Masked Entity Prediction, which designs an entity-oriented mask strategy and asks language models to fill in the masked entities. Our experiments show that SANTA achieves state-of-the-art on code search and product search and conducts convincing results in the zero-shot setting. SANTA learns tailored representations for multi-modal text data by aligning structured and unstructured data pairs and capturing structural semantics by masking and predicting entities in the structured data. All codes are available at https://github.com/OpenMatch/OpenMatch.'),\n",
       " Document(page_content=\"Author: Chenyan Xiong Title: Toolink: Linking Toolkit Creation and Using through Chain-of-Solving on Open-Source Model Publication year: 2023 Coauthors: Cheng Qian, Chenyan Xiong, Zhenghao Liu, Zhiyuan Liu Abstract: Large Language Models (LLMs) have demonstrated remarkable progress in utilizing tools, but their closed-source nature and high inference costs pose limitations on their adaptability, necessitating a valid method that leverages smaller, open-sourced models. In this paper, we introduce Toolink, a comprehensive framework that performs task-solving by first creating a toolkit and then integrating the planning and calling of tools through a chain-of-solving (CoS) approach. We first validate the efficacy of Toolink in harnessing the model's creativity and CoS ability on ChatGPT. Subsequently, we curate CoS-GPT, a chain-of-solving dataset designed for tool-using, and finetune the LLaMA-7B model. It results in LLaMA-CoS, a powerful open-source model with advanced tool-planning and tool-calling capabilities. Evaluation on diverse tasks from BIG-bench demonstrates its CoS ability matches that of ChatGPT while its performance surpasses the chain-of-thought approach. Further studies highlight the generalization of LLaMA-CoS to unseen tasks and showcase its capability in using toolkits not explicitly tailored for the target task, affirming its robustness in real-world scenarios. All codes and data are released.\"),\n",
       " Document(page_content='Author: Chenyan Xiong Title: Augmenting Zero-Shot Dense Retrievers with Plug-in Mixture-of-Memories Publication year: 2023 Coauthors: Suyu Ge, Chenyan Xiong, Corby Rosset, Arnold Overwijk, Jiawei Han, Paul N. Bennett Abstract: In this paper we improve the zero-shot generalization ability of language models via Mixture-Of-Memory Augmentation (MoMA), a mechanism that retrieves augmentation documents from multiple information corpora (\"external memories\"), with the option to\"plug in\"new memory at inference time. We develop a joint learning mechanism that trains the augmentation component with latent labels derived from the end retrieval task, paired with hard negatives from the memory mixture. We instantiate the model in a zero-shot dense retrieval setting by augmenting a strong T5-based retriever with MoMA. Our model, MoMA, obtains strong zero-shot retrieval accuracy on the eighteen tasks included in the standard BEIR benchmark. It outperforms systems that seek generalization from increased model parameters and computation steps. Our analysis further illustrates the necessity of augmenting with mixture-of-memory for robust generalization, the benefits of augmentation learning, and how MoMA utilizes the plug-in memory at inference time without changing its parameters. We plan to open source our code.'),\n",
       " Document(page_content='Author: Yiming Yang Title: Accurate detection of reactive oxygen species by tuning an elastic motif (GPGGA)4 in nanopores. Publication year: 2023 Coauthors: Cunli Wang, Yiming Yang, Shuai Shao, Hangyu Zhang, Na Li, Zheng-Zhu Zhang, Bo Liu Abstract: We have developed a reactive oxygen species (ROS) sensor based on nanopores modified with GGGCEG(GPGGA)4CEG. The formation of an intramolecular disulfide bond oxidized by ROS leads to conformation changes in GGGCEG(GPGGA)4CEG, which then induces an obvious change in the size of the nanopores and a corresponding ionic current change. This work allows the accurate and dynamic monitoring of ROS through the combination of (GPGGA)4 and nanopores.'),\n",
       " Document(page_content='Author: Yiming Yang Title: Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation Publication year: 2023 Coauthors: Junwei Huang, Zhiqing Sun, Yiming Yang Abstract: Graph-based diffusion models have shown promising results in terms of generating high-quality solutions to NP-complete (NPC) combinatorial optimization (CO) problems. However, those models are often inefficient in inference, due to the iterative evaluation nature of the denoising diffusion process. This paper proposes to use progressive distillation to speed up the inference by taking fewer steps (e.g., forecasting two steps ahead within a single step) during the denoising process. Our experimental results show that the progressively distilled model can perform inference 16 times faster with only 0.019% degradation in performance on the TSP-50 dataset.'),\n",
       " Document(page_content='Author: Yiming Yang Title: Automatic synchrotron tomographic alignment schemes based on genetic algorithms and human-in-the-loop software Publication year: 2023 Coauthors: Zhen Zhang, Xiaoxue Bi, Pengcheng Li, Chenglong Zhang, Yiming Yang, Yu Liu, Gang Chen, Yuhui Dong, Gongfa Liu, Yi Zhang Abstract: A highly automatic alignment scheme is proposed to address the pressing challenge in tomographic alignment of future scanning tomography experiments. The results show that the proposed method exhibits excellent sub-pixel alignment accuracy and high time efficiency.'),\n",
       " Document(page_content='Author: Yiming Yang Title: High CD8+tumor-infiltrating lymphocytes indicate severe exhaustion and poor prognosis in angioimmunoblastic T-cell lymphoma Publication year: 2023 Coauthors: Qiqi Zhu, Yiming Yang, Xueqin Deng, Ningning Chao, Zihang Chen, Y. Ye, Wenyan Zhang, Weiping Liu, Sha Zhao Abstract: Background Exhaustion of CD8+ tumor-infiltrating lymphocytes (TILs), characterized by the overexpression of immune checkpoints (IC), is a major impediment to anti-tumor immunity. However, the exhaustion status of CD8+TILs in angioimmunoblastic T cell lymphoma (AITL) remains unclear. Therefore, we aimed to elucidate the exhaustion status of CD8+TILs in AITL and its influence on prognosis. Methods The correlation between CD8+TILs and IC expression in AITL was analyzed using single-cell RNA sequencing (n = 2), flow cytometry (n = 20), and RNA sequencing (n = 20). Biological changes related to CD8+TILs exhaustion at different cytotoxic T lymphocyte (CTL) levels (mean expression levels of CD8A, CD8B, GZMA, GZMB, and PRF1) in AITL were evaluated using RNA sequencing (n = 20) and further validated using the GEO dataset (n = 51). The impact of CD8 protein expression and CTL levels on patient prognosis was analyzed using flow cytometry and RNA sequencing, respectively. Results Our findings demonstrated that the higher the infiltration of CD8+TILs, the higher was the proportion of exhausted CD8+TILs characterized by the overexpression of multiple IC. This was accompanied by extensive exhaustion-related biological changes, which suggested severe exhaustion in CD8+TILs and may be one of the main reasons for the poor prognosis of patients with high CD8+TILs and CTL. Conclusion Our study comprehensively reveals the exhaustion status of CD8+TILs and their potential negative impact on AITL prognosis, which facilitates further mechanistic studies and is valuable for guiding immunotherapy strategies.'),\n",
       " Document(page_content='Author: Yiming Yang Title: Robust Cross-Domain Pseudo-Labeling and Contrastive Learning for Unsupervised Domain Adaptation NIR-VIS Face Recognition Publication year: 2023 Coauthors: Yiming Yang, Weipeng Hu, Haiqi Lin, Haifeng Hu Abstract: Near-infrared and visible face recognition (NIR-VIS) is attracting increasing attention because of the need to achieve face recognition in low-light conditions to enable 24-hour secure retrieval. However, annotating identity labels for a large number of heterogeneous face images is time-consuming and expensive, which limits the application of the NIR-VIS face recognition system to larger scale real-world scenarios. In this paper, we attempt to achieve NIR-VIS face recognition in an unsupervised domain adaptation manner. To get rid of the reliance on manual annotations, we propose a novel Robust cross-domain Pseudo-labeling and Contrastive learning (RPC) network which consists of three key components, i.e., NIR cluster-based Pseudo labels Sharing (NPS), Domain-specific cluster Contrastive Learning (DCL) and Inter-domain cluster Contrastive Learning (ICL). Firstly, NPS is presented to generate pseudo labels by exploring robust NIR clusters and sharing reliable label knowledge with VIS domain. Secondly, DCL is designed to learn intra-domain compact yet discriminative representations. Finally, ICL dynamically combines and refines intrinsic identity relationships to guide the instance-level features to learn robust and domain-independent representations. Extensive experiments are conducted to verify an accuracy of over 99% in pseudo label assignment and the advanced performance of RPC network on four mainstream NIR-VIS datasets.'),\n",
       " Document(page_content='Author: Yiming Yang Title: Numerical Investigation of Fatigue Crack Propagation Behaviour of 550E High-Performance Steel Publication year: 2023 Coauthors: Linfa Xiao, Heng Lin, Yongxiang Wang, Yiming Yang, Huapeng Chen Abstract: The fatigue crack propagation behaviour of Q550E high-performance steel (HPS) is studied in this paper. Static tensile testing and fatigue crack propagation testing were carried out, and the results were compared with those of Q235. Finite element models were developed and verified against the experimental results. The impacts of the initial crack angle, crack depth ratio, stress ratio, thickness, and corrosion pitting on the fatigue crack propagation behaviour of the HPS were analysed. The results show that the fatigue life of Q550 was reduced by 18% due to the corrosion pitting, but it did not change the crack propagation path. When the stress intensity factor is higher than a certain value, the fatigue performance of Q235 is better than that of Q550E. The initial crack angle of 52.5° is the critical angle of the crack stress intensity factor. The steel tends to fracture as the crack depth ratio increases, and more attention should be paid to the effective crack length in engineering practice. An increasing stress ratio leads to a smaller stress intensity factor, and the thickness affects the stress intensity factor in the later stage. The crack stress intensity factor around the corrosion pits gradually decreases along the thickness direction, and the crack tips around the corrosion pits tend to reach the yield state initially, accelerating the fatigue fracture of the specimen and ultimately leading to a decrease in fatigue life.'),\n",
       " Document(page_content=\"Author: Yiming Yang Title: Phase Behaviors of Charged Macromolecules in Aqueous Solutions Publication year: 2023 Coauthors: Yiming Yang, Di Jia Abstract: Compared to the charge–charge interaction, the role of the dipole‐dipole interaction has long been ignored in the phase behaviors of charged macromolecules in solutions. Charged macromolecules in solutions exhibit rich phase behaviors due to their complexity and they have been studied extensively. Phase separation can happen for charged macromolecules in the presence of monovalent salt, multivalent salt, and oppositely charged polymers, surfactants, etc., and for more advanced charged macromolecules such as polyzwitterions and polyampholytes, the phase diagram is even richer. In this perspective, the unacknowledged role of dipole‐dipole interaction in the phase behaviors of charged macromolecular solutions will be introduced. Dipolar polymers can form complex, self‐regulating structures which can be employed in various fields from drug‐delivery systems to next‐generation polymers. More importantly, it will shed light on how some of the life's basic and coherent structures such as biomolecular condensates and membrane‐less organelles are assembled and built by charged biomacromolecules such as DNA, RNA, and proteins.\"),\n",
       " Document(page_content='Author: Yiming Yang Title: Association between albumin-to-globulin ratio and the risk of overall survival in advanced non-small cell lung cancer patients with anlotinib treatment: a retrospective cohort study Publication year: 2023 Coauthors: Jinzhan Chen, Cong-jun Xie, Yiming Yang, Shuwen Yang, Jin-xiang Huang, Feiyang Ye, Zhenyang Lin, L. Tong, Jiaxin Liu '),\n",
       " Document(page_content='Author: Yiming Yang Title: Modification Effect of Pt on the Active Sites of Sulfated CeO2 Nanorods for the Selective Catalytic Reduction of NO Publication year: 2023 Coauthors: Hao Fan, Yiming Yang, Xu Yang, Xuefeng He, Jian Sun, Liu Yang, Jiao Li, Zhenxing Shen '),\n",
       " Document(page_content='Author: Yiming Yang Title: Analysis of Volatile Components in Dried Fruits and Branch Exudates of Schisandra chinensis with Different Fruit Colors Using GC-IMS Technology Publication year: 2023 Coauthors: Yiping Yan, Wenpeng Lu, Taiping Tian, Nan Shu, Yiming Yang, Shutian Fan, Xianyan Han, Yunhua Ge, Peilei Xu Abstract: To investigate the volatile components of Schisandra chinensis (Turcz.) Bail (commonly known as northern Schisandra) of different colors and to explore their similarities and differences, to identify the main flavor substances in the volatile components of the branch exudates of northern schisandra, and finally to establish a fingerprint map of the volatile components of the dried fruits and branch exudates of northern Schisandra of different colors, we used GC-IMS technology to analyze the volatile components of the dried fruits and branch exudates of three different colors of northern Schisandra and established a fingerprint spectra. The results showed that a total of 60 different volatile chemical components were identified in the branch exudates and dried fruits of Schisandra. The components of germplasm resources with different fruit colors were significantly different. The ion mobility spectrum and OPLS-DA results showed that white and yellow fruits were more similar compared to red fruits. The volatile components in dried fruits were significantly higher than those in branch exudates. After VIP (variable importance in projection) screening, 41 key volatile substances in dried fruits and 30 key volatile substances in branch exudates were obtained. After screening by odor activity value (OAV), there were 24 volatile components greater than 1 in both dried fruits and branch exudates. The most important contributing volatile substance was 3-methyl-butanal, and the most important contributing volatile substance in white fruit was (E)-2-hexenal.'),\n",
       " Document(page_content='Author: Yiming Yang Title: Secreted endogenous macrosomes reduce Aβ burden and ameliorate Alzheimer’s disease Publication year: 2023 Coauthors: Cunli Wang, Yiming Yang, Xiaoyu Zhang, Zhenqiang Shi, Huiling Gao, Manli Zhong, Yong-gang Fan, Hongyan Zhang, Bo Liu, Guangyan Qing Abstract: Innovative therapeutic strategies are urgently needed for Alzheimer’s disease (AD) due to the increasing size of the aging population and the lack of effective drug treatment. Here, we report the therapeutic effects of extracellular vesicles (EVs) secreted by microglia, including macrosomes and small EVs, on AD-associated pathology. Macrosomes strongly inhibited β-amyloid (Aβ) aggregation and rescued cells from Aβ misfolding–induced cytotoxicity. Furthermore, macrosome administration reduced Aβ plaques and ameliorated cognitive impairment in mice with AD. In contrast, small EVs slightly promoted Aβ aggregation and did not improve AD pathology. Proteomic analysis of small EVs and macrosomes revealed that macrosomes harbor several important neuroprotective proteins that inhibit Aβ misfolding. In particular, the small integral membrane protein 10–like protein 2B in macrosomes has been shown to inhibit Aβ aggregation. Our observations provide an alternative therapeutic strategy for the treatment of AD over conventional ineffective drug treatments.'),\n",
       " Document(page_content='Author: Yiming Yang Title: Neutral Face Learning and Progressive Fusion Synthesis Network for NIR-VIS Face Recognition Publication year: 2023 Coauthors: Yiming Yang, Weipeng Hu, Haifeng Hu Abstract: To meet the strong demand for deploying face recognition systems in low-light scenarios, the Near-InfraRed and VISible (NIR-VIS) face recognition task is receiving increasing attention. However, heterogeneous faces have the characteristics of heterogeneity and non-neutrality. Heterogeneity refers to the fact that the matching images are in different modalities, and non-neutrality means that the matching images are significantly different in pose, expression, lighting, etc. Both situations pose challenges for NIR-VIS face matching. To address this problem, we propose a novel Neutral face Learning and Progressive Fusion synthesis (NLPF) network to disentangle the latent attributes of heterogeneous faces and learn neutral face representations. Our approach naturally integrates Identity-related Neutral face Learning (INL) and Attribute Progressive Fusion (APF) into a joint framework. Firstly, INL eliminates modal variations and residual variations by guiding the network to learn homogeneous neutral face feature representations, which tackles the challenge of heterogeneity and non-neutrality by mapping cross-modal images to a common neutral representation subspace. Besides, APF is presented to perform the disentanglement and reintegration of identity-related features, modality-related features and residual features in a progressive fusion manner, which helps to further purify identity-related features. Comprehensive evaluations are carried out on three mainstream NIR-VIS datasets to verify the robustness and effectiveness of the NLPF model. In particular, NLPF has competitive recognition performance on LAMP-HQ, the most challenging NIR-VIS dataset so far.'),\n",
       " Document(page_content='Author: Yiming Yang Title: DIFUSCO: Graph-based Diffusion Solvers for Combinatorial Optimization Publication year: 2023 Coauthors: Zhiqing Sun, Yiming Yang Abstract: Neural network-based Combinatorial Optimization (CO) methods have shown promising results in solving various NP-complete (NPC) problems without relying on hand-crafted domain knowledge. This paper broadens the current scope of neural solvers for NPC problems by introducing a new graph-based diffusion framework, namely DIFUSCO. Our framework casts NPC problems as discrete {0, 1}-vector optimization problems and leverages graph-based denoising diffusion models to generate high-quality solutions. We investigate two types of diffusion models with Gaussian and Bernoulli noise, respectively, and devise an effective inference schedule to enhance the solution quality. We evaluate our methods on two well-studied NPC combinatorial optimization problems: Traveling Salesman Problem (TSP) and Maximal Independent Set (MIS). Experimental results show that DIFUSCO strongly outperforms the previous state-of-the-art neural solvers, improving the performance gap between ground-truth and neural solvers from 1.76% to 0.46% on TSP-500, from 2.46% to 1.17% on TSP-1000, and from 3.19% to 2.58% on TSP10000. For the MIS problem, DIFUSCO outperforms the previous state-of-the-art neural solver on the challenging SATLIB benchmark.'),\n",
       " Document(page_content='Author: Yiming Yang Title: Effects of S Content on Inclusion Formation in the Al and Ti–Mg Complex Deoxidized Steel Publication year: 2023 Coauthors: Pengliang Jin, Yiming Yang, Lei Cao, Xinghu Yuan, Guocheng Wang Abstract: Two groups of Al and Ti–Mg complex deoxidized steels with different S contents are designed, and inclusion characteristics of two groups of steel samples are observed by field‐emission scanning electron microscopy–energy‐dispersive spectroscopy. The results show that there are single TiS inclusions, MgAl2O4 (Al2O3)–TiS, and MgAl2O4–TiN–TiS complex inclusions in No. 1 steel (low‐sulfur content) and No. 2 steel (high‐sulfur content). However, there are also complex inclusions containing MnS in the No. 2 steel but not in No. 1 steel. In order to reveal the precipitation mechanism of MnS, equilibrium phase of inclusion from 1873\\u2009K to liquidus temperature is further analyzed, and the mass fractions of different inclusions from liquidus to solidus temperature are quantitatively calculated using the element segregation model combined with FactSage 7.2 thermodynamic software. Furthermore, the mismatch values between different crystal planes of MnS, TiS, and TiN are calculated. The results show that MnS (110) is most likely to precipitate on TiS (001), which is consistent with the observation that there is TiS–MnS interface in the complex inclusions containing MnS in No. 2 steel. This study could be helpful to the controlling sulfide precipitation in Al and Ti–Mg complex deoxidized steel.'),\n",
       " Document(page_content='Author: Yiming Yang Title: Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs Publication year: 2023 Coauthors: Qingyang Zhang, Yiming Yang, Jingqing Ruan, Xuantang Xiong, Dengpeng Xing, Bo Xu Abstract: Goal-Conditioned Hierarchical Reinforcement Learning (GCHRL) is a promising paradigm to address the exploration-exploitation dilemma in reinforcement learning. It decomposes the source task into sub goal conditional subtasks and conducts exploration and exploitation in the subgoal space. The effectiveness of GCHRL heavily relies on sub goal representation functions and sub goal selection strategy. However, existing works often overlook the temporal coherence in GCHRL when learning latent sub goal representations and lack an efficient sub goal selection strategy that balances exploration and exploitation. This paper proposes HIerarchical reinforcement learning via dynamically building Latent Landmark graphs (HILL) to overcome these limitations. HILL learns latent subgoal representations that satisfy temporal coherence using a contrastive representation learning objective. Based on these representations, HILL dynamically builds latent landmark graphs and employs a novelty measure on nodes and a utility measure on edges. Finally, HILL develops a subgoal selection strategy that balances exploration and exploitation by jointly considering both measures. Experimental results demonstrate that HILL outperforms state-of-the-art baselines on continuous control tasks with sparse rewards in sample efficiency and asymptotic performance. Our code is available at https://github.com/papercode2022/HILL.'),\n",
       " Document(page_content='Author: Yiming Yang Title: Comprehensive evaluation of nine grape varieties based on fundamental physical and chemical indicators, color and volatile compounds Publication year: 2023 Coauthors: W. Cao, Nan Shu, Yiming Yang, Jinli Wen, Wenpeng Lu Abstract: BACKGROUND: In todays’ society, the rapid development of the wine industry and the improvement of peoples’ living standards make people pay more and more attention to wine grape quality. OBJECTIVE: To evaluate the wine grape varieties in Northeast Chinas’ grape growing regions for better wine grape quality, we evaluated the quality of different varieties of wine grapes. METHODS: The grape varieties ‘Hassan’ ‘Zuoshaner’ ‘Beibinghong’ ‘Zuoyouhong’ ‘Beta’ ‘Shuanghong’ ‘Zijingganlu’ ‘Cabernet Sauvignon’ and ‘Syrah’ were planted in the grape growing area of Jilin, Northeast China, were used as the subjects of this study. The grape berries were analyzed and tested for morphological indicators, basic physicochemical indicators, color, and phenolic and aromatic composition. RESULTS: According to lab results, ‘Hassan’ contained the highest amount of total phenolics; ‘Zuoyouhong’ had the highest solids and total sugar content; ‘Shuanghong’ had the most elevated total acid and anthocyanin content; ‘Zijngganlu’ had the highest tannin content and acid fixation ratio; Seventy-one volatile compounds were detected in nine grape varieties. CONCLUSIONS: Each of the nine grape varieties has a distinctive flavor, and because of this, grape processing products with regional flavors can be created. The same offer valuable data for future scientific grape resource collection, conservation, and exploitation.'),\n",
       " Document(page_content='Author: Yiming Yang Title: Aligning Large Multimodal Models with Factually Augmented RLHF Publication year: 2023 Coauthors: Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liangyan Gui, Yu-Xiong Wang, Yiming Yang, K. Keutzer, Trevor Darrell Abstract: Large Multimodal Models (LMM) are built across modalities and the misalignment between two modalities can result in\"hallucination\", generating textual outputs that are not grounded by the multimodal information in context. To address the multimodal misalignment issue, we adapt the Reinforcement Learning from Human Feedback (RLHF) from the text domain to the task of vision-language alignment, where human annotators are asked to compare two responses and pinpoint the more hallucinated one, and the vision-language model is trained to maximize the simulated human rewards. We propose a new alignment algorithm called Factually Augmented RLHF that augments the reward model with additional factual information such as image captions and ground-truth multi-choice options, which alleviates the reward hacking phenomenon in RLHF and further improves the performance. We also enhance the GPT-4-generated training data (for vision instruction tuning) with previously available human-written image-text pairs to improve the general capabilities of our model. To evaluate the proposed approach in real-world scenarios, we develop a new evaluation benchmark MMHAL-BENCH with a special focus on penalizing hallucinations. As the first LMM trained with RLHF, our approach achieves remarkable improvement on the LLaVA-Bench dataset with the 94% performance level of the text-only GPT-4 (while previous best methods can only achieve the 87% level), and an improvement by 60% on MMHAL-BENCH over other baselines. We opensource our code, model, data at https://llava-rlhf.github.io.'),\n",
       " Document(page_content='Author: Yiming Yang Title: Experimental and numerical research on the static behavior of locally corroded OSBD Publication year: 2023 Coauthors: J. Peng, Yi Liu, Yiming Yang, Yadong Zhou, Longzhen Xie '),\n",
       " Document(page_content='Author: Yiming Yang Title: Syncretic Space Learning Network for NIR-VIS Face Recognition Publication year: 2023 Coauthors: Yiming Yang, Weipeng Hu, Haifeng Hu Abstract: To overcome the technical bottleneck of face recognition in low-light scenarios, Near-InfraRed and VISible (NIR-VIS) heterogeneous face recognition is proposed for matching well-lit VIS faces with poorly lit NIR faces. Current cross-modal synthesis methods visually convert the NIR modality to the VIS modality and then perform face matching in the VIS modality. However, using a heavyweight GAN network on unpaired NIR-VIS faces may lead to high synthesis difficulty, low inference efficiency, and other problems. To alleviate the above problems, we simultaneously synthesize NIR and VIS images into modality-independent syncretic images and propose a novel syncretic space learning (SSL) model to eliminate the modal gap. First, Syncretic Modality Generator (SMG) synthesizes NIR and VIS images into syncretic images using channel-level convolution with a shallow CNN. In particular, the discriminative structural information is well preserved and the face quality can be further improved with small modal variations in a self-supervised learning manner. Second, Modality-adversarial Syncretic space Learning (MSL) projects NIR and VIS images into the syncretic space by a syncretic-modality adversarial learning strategy with syncretic pattern guided objective, so the modal gap of NIR-VIS faces can be effectively reduced. Finally, the Syncretic Distribution Consistency (SDC) constructed by NIR-syncretic, syncretic-syncretic, and VIS-syncretic consistency can enhance the intra-class compactness and learn discriminative representations. Extensive experiments on three challenging datasets demonstrate the effectiveness of the SSL method.'),\n",
       " Document(page_content='Author: Yiming Yang Title: An Experimental Study on Secondary Transfer Performances of Prestress after Anchoring Failure of Steel Wire Strands Publication year: 2023 Coauthors: Rihua Yang, Yiming Yang, Xuhui Zhang, Xinzhong Wang Abstract: To understand the secondary transfer performances of residual prestress after the anchoring failure of end-anchored steel wire strands due to corrosion fracture, six steel wire strand components of post-tensioning prestress were designed and fabricated. One-side fast corrosion was applied to the steel wire strand components using the electrochemical method until anchoring failure was reached. The sphere of influence, stress changes, and the retraction and swelling effect of broken beams after failure were investigated. The influences of factors such as concrete strength, stirrup area, and the length of the component on the secondary transfer length of residual prestress were discussed. Based on the deformation relationship between prestressed steel wire strands and concrete in the stress transfer zone, a stress equation was established and solved through a bond constitutive model. A prediction model of the effective stress transfer length of prestressed steel wire strand after failure was proposed. The results demonstrated that residual prestress can have a secondary transfer after the corrosion fracture of end-anchored steel wire strands, but some effective prestress may be lost. Moreover, the loss of prestress is inversely proportional to concrete compressive strength. When the specimens are relatively short, the prestress loss increases significantly. Concrete strength has significant influences on the length of secondary transfer. The proposed simplified calculation method of the secondary transfer length of residual prestress has a relatively high accuracy, with an average error of 2.9% and a maximum error of 5.2%.'),\n",
       " Document(page_content='Author: Yiming Yang Title: Core loss analysis of soft magnetic composite under non-sinusoidal excitation based on finite element models Publication year: 2023 Coauthors: Lei Zhao, Chengcheng Liu, Youhua H. Wang, Yiming Yang Abstract: Due to the effect of higher harmonics on magnetic properties under actual complex operating conditions, the accurate calculation of core losses of soft magnetic composites (SMC) is complicated. First, this paper improves the existing SMC model by introducing a correction factor to correct the hysteresis loss coefficient so that the model can consider the local variation characteristics of the magnetic density waveform and then calculate the core loss under different harmonic excitation. Then, the influence of skin effect and inhomogeneous flux density within the ring sample model is analyzed. Finally, to validate the improved model, it is compared with other models in the reference based on experimental measurements, respectively. The results show that the core loss calculated by the improved model is closer to the experimental results under different harmonic excitations. In addition, the applicability of the improved SMC model under triangular and square wave excitations is also verified by the derivation of the equations.'),\n",
       " Document(page_content='Author: Yiming Yang Title: Distributed cooperative dual closed loop velocity-attitude consensus controller for rendezvous of the underactuated AUV swarm in 3-dimensional space Publication year: 2023 Coauthors: Yu Zhang, W. Zhang, G. Xia, Yiming Yang, Yan-luan Zheng, Peiyu Han '),\n",
       " Document(page_content='Author: Yiming Yang Title: MRI Features for Predicting Microvascular Invasion and Postoperative Recurrence in Hepatocellular Carcinoma Without Peritumoral Hypointensity Publication year: 2023 Coauthors: Zhiyuan Chen, Xiaohuan Li, Yu Zhang, Yiming Yang, Yan Zhang, Dongjing Zhou, Yu Yang, Shuping Zhang, Yupin Liu Abstract: Purpose To identify MRI features of hepatocellular carcinoma (HCC) that predict microvascular invasion (MVI) and postoperative intrahepatic recurrence in patients without peritumoral hepatobiliary phase (HBP) hypointensity. Patients and Methods One hundred and thirty patients with HCC who underwent preoperative gadoxetate-enhanced MRI and curative hepatic resection were retrospectively reviewed. Two radiologists reviewed all preoperative MR images and assessed the radiological features of HCCs. The ability of peritumoral HBP hypointensity to identify MVI and intrahepatic recurrence was analyzed. We then assessed the MRI features of HCC that predicted the MVI and intrahepatic recurrence-free survival (RFS) in the subgroup without peritumoral HBP hypointensity. Finally, a two-step flowchart was constructed to assist in clinical decision-making. Results Peritumoral HBP hypointensity (odds ratio, 3.019; 95% confidence interval: 1.071–8.512; P=0.037) was an independent predictor of MVI. The sensitivity, specificity, positive predictive value, negative predictive value, and AUROC of peritumoral HBP hypointensity in predicting MVI were 23.80%, 91.04%, 71.23%, 55.96%, and 0.574, respectively. Intrahepatic RFS was significantly shorter in patients with peritumoral HBP hypointensity (P<0.001). In patients without peritumoral HBP hypointensity, the only significant difference between MVI-positive and MVI-negative HCCs was the presence of a radiological capsule (P=0.038). Satellite nodule was an independent risk factor for intrahepatic RFS (hazard ratio,3.324; 95% CI: 1.733–6.378; P<0.001). The high-risk HCC detection rate was significantly higher when using the two-step flowchart that incorporated peritumoral HBP hypointensity and satellite nodule than when using peritumoral HBP hypointensity alone (P<0.001). Conclusion In patients without peritumoral HBP hypointensity, a radiological capsule is useful for identifying MVI and satellite nodule is an independent risk factor for intrahepatic RFS.'),\n",
       " Document(page_content='Author: Yiming Yang Title: Impact of local governments’ construction land allocation strategies on innovation-driven development of China Publication year: 2023 Coauthors: Jian Wang, Shangui Peng, Yuhao Feng, Yiming Yang, Qun Wu '),\n",
       " Document(page_content='Author: Yiming Yang Title: PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification Publication year: 2023 Coauthors: Yau-Shian Wang, Ta-Chung Chi, Ruohong Zhang, Yiming Yang Abstract: We present PESCO, a novel contrastive learning framework that substantially improves the performance of zero-shot text classification. We formulate text classification as a neural text retrieval problem where each document is treated as a query, and the system learns the mapping from each query to the relevant class labels by (1) adding prompts to enhance label retrieval, and (2) using retrieved labels to enrich the training set in a self-training loop of contrastive learning. PESCO achieves state-of-the-art performance on four benchmark text classification datasets. On DBpedia, we achieve 98.5% accuracy without any labeled data, which is close to the fully-supervised result. Extensive experiments and analyses show all the components of PESCO are necessary for improving the performance of zero-shot text classification.'),\n",
       " Document(page_content=\"Author: Yiming Yang Title: Learning Performance-Improving Code Edits Publication year: 2023 Coauthors: Aman Madaan, Alex Shypula, Uri Alon, Milad Hashemi, Parthasarathy Ranganathan, Yiming Yang, Graham Neubig, A. Yazdanbakhsh Abstract: The waning of Moore's Law has shifted the focus of the tech industry towards alternative methods for continued performance gains. While optimizing compilers are a standard tool to help increase program efficiency, programmers continue to shoulder much responsibility in crafting and refactoring code with better performance characteristics. In this paper, we investigate the ability of large language models (LLMs) to suggest functionally correct, performance improving code edits. We hypothesize that language models can suggest such edits in ways that would be impractical for static analysis alone. We investigate these questions by curating a large-scale dataset of Performance-Improving Edits, PIE. PIE contains trajectories of programs, where a programmer begins with an initial, slower version and iteratively makes changes to improve the program's performance. We use PIE to evaluate and improve the capacity of large language models. Specifically, use examples from PIE to fine-tune multiple variants of CODEGEN, a billion-scale Transformer-decoder model. Additionally, we use examples from PIE to prompt OpenAI's CODEX using a few-shot prompting. By leveraging PIE, we find that both CODEX and CODEGEN can generate performance-improving edits, with speedups of more than 2.5x for over 25% of the programs, for C++ and Python, even after the C++ programs were compiled using the O3 optimization level. Crucially, we show that PIE allows CODEGEN, an open-sourced and 10x smaller model than CODEX, to match the performance of CODEX on this challenging task. Overall, this work opens new doors for creating systems and methods that can help programmers write efficient code.\"),\n",
       " Document(page_content='Author: Yiming Yang Title: Generation-driven Contrastive Self-training for Zero-shot Text Classification with Instruction-tuned GPT Publication year: 2023 Coauthors: Ruohong Zhang, Yau-Shian Wang, Yiming Yang Abstract: Moreover, GPT-based zero-shot classification models tend to make independent predictions over test instances, which can be sub-optimal as the instance correlations and the decision boundaries in the target space are ignored. To address these difficulties and limitations, we propose a new approach to zero-shot text classification, namely \\\\ourmodelshort, which leverages the strong generative power of GPT to assist in training a smaller, more adaptable, and efficient sentence encoder classifier with contrastive self-training. Specifically, GenCo applies GPT in two ways: firstly, it generates multiple augmented texts for each input instance to enhance the semantic embedding of the instance and improve the mapping to relevant labels; secondly, it generates augmented texts conditioned on the predicted label during self-training, which makes the generative process tailored to the decision boundaries in the target space. In our experiments, GenCo outperforms previous state-of-the-art methods on multiple benchmark datasets, even when only limited in-domain text data is available.'),\n",
       " Document(page_content='Author: Yiming Yang Title: Self-Refine: Iterative Refinement with Self-Feedback Publication year: 2023 Coauthors: Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, S. Welleck, Bodhisattwa Prasad Majumder, Shashank Gupta, A. Yazdanbakhsh, Peter Clark Abstract: Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ~20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach.'),\n",
       " Document(page_content='Author: Yiming Yang Title: Long-tailed Extreme Multi-label Text Classification by the Retrieval of Generated Pseudo Label Descriptions Publication year: 2023 Coauthors: Ruohong Zhang, Yau-Shian Wang, Yiming Yang, Donghan Yu, Tom Vu, Li Lei Abstract: Extreme Multi-label Text Classification (XMTC) has been a tough challenge in machine learning research and applications due to the sheer sizes of the label spaces and the severe data scarcity problem associated with the long tail of rare labels in highly skewed distributions. This paper addresses the challenge of tail label prediction by leveraging the power of dense neural retrieval model in mapping input documents (as queries) to relevant label descriptions. To further enhance the quality of label descriptions, we propose to generate pseudo label descriptions from a trained bag-of-words (BoW) classifier, which demonstrates better classification performance under severe scarce data conditions.The proposed approach achieves the state-of-the-art (SOTA) performance of overall label prediction on XMTC benchmark datasets and especially outperforms the SOTA models in the tail label prediction. We also provide a theoretical analysis for relating the BoW and neural models w.r.t. performance lower bound.'),\n",
       " Document(page_content='Author: Yiming Yang Title: Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers Publication year: 2023 Coauthors: K. Choromanski, Shanda Li, Valerii Likhosherstov, Kumar Avinava Dubey, Shengjie Luo, Di He, Yiming Yang, Tamás Sarlós, Thomas Weingarten, Adrian Weller Abstract: We propose a new class of linear Transformers called FourierLearner-Transformers (FLTs), which incorporate a wide range of relative positional encoding mechanisms (RPEs). These include regular RPE techniques applied for nongeometric data, as well as novel RPEs operating on the sequences of tokens embedded in higher-dimensional Euclidean spaces (e.g. point clouds). FLTs construct the optimal RPE mechanism implicitly by learning its spectral representation. As opposed to other architectures combining efficient low-rank linear attention with RPEs, FLTs remain practical in terms of their memory usage and do not require additional assumptions about the structure of the RPE-mask. FLTs allow also for applying certain structural inductive bias techniques to specify masking strategies, e.g. they provide a way to learn the so-called local RPEs introduced in this paper and providing accuracy gains as compared with several other linear Transformers for language modeling. We also thoroughly tested FLTs on other data modalities and tasks, such as: image classification and 3D molecular modeling. For 3D-data FLTs are, to the best of our knowledge, the first Transformers architectures providing RPE-enhanced linear attention.'),\n",
       " Document(page_content='Author: Yiming Yang Title: Efficient Temporal Sentence Grounding in Videos with Multi-Teacher Knowledge Distillation Publication year: 2023 Coauthors: Renjie Liang, Yiming Yang, Hui Lu, Li Li Abstract: Temporal Sentence Grounding in Videos (TSGV) aims to detect the event timestamps described by the natural language query from untrimmed videos. This paper discusses the challenge of achieving efficient computation in TSGV models while maintaining high performance. Most existing approaches exquisitely design complex architectures to improve accuracy with extra layers and loss, suffering from inefficiency and heaviness. Although some works have noticed that, they only make an issue of feature fusion layers, which can hardly enjoy the highspeed merit in the whole clunky network. To tackle this problem, we propose a novel efficient multi-teacher model (EMTM) based on knowledge distillation to transfer diverse knowledge from both heterogeneous and isomorphic networks. Specifically, We first unify different outputs of the heterogeneous models into one single form. Next, a Knowledge Aggregation Unit (KAU) is built to acquire high-quality integrated soft labels from multiple teachers. After that, the KAU module leverages the multi-scale video and global query information to adaptively determine the weights of different teachers. A Shared Encoder strategy is then proposed to solve the problem that the student shallow layers hardly benefit from teachers, in which an isomorphic teacher is collaboratively trained with the student to align their hidden states. Extensive experimental results on three popular TSGV benchmarks demonstrate that our method is both effective and efficient without bells and whistles.'),\n",
       " Document(page_content='Author: Yiming Yang Title: Active Retrieval Augmented Generation Publication year: 2023 Coauthors: Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig Abstract: Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.'),\n",
       " Document(page_content='Author: Yiming Yang Title: Retrieval-Enhanced Generative Model for Large-Scale Knowledge Graph Completion Publication year: 2023 Coauthors: Donghan Yu, Yiming Yang Abstract: The task of knowledge graph completion (KGC) is of great importance. To achieve scalability when dealing with large-scale knowledge graphs, recent works formulate KGC as a sequence-to-sequence process, where the incomplete triplet (input) and the missing entity (output) are both verbalized as text sequences. However, inference with these methods relies solely on the model parameters for implicit reasoning and neglects the use of KG itself, which limits the performance since the model lacks the capacity to memorize a vast number of triplets. To tackle this issue, we introduce ReSKGC, a Retrieval-enhanced Seq2seq KGC model, which selects semantically relevant triplets from the KG and uses them as evidence to guide output generation with explicit reasoning. Our method has demonstrated state-of-the-art performance on benchmark datasets Wikidata5M and WikiKG90Mv2, which contain about 5M and 90M entities, respectively.'),\n",
       " Document(page_content=\"Author: Yiming Yang Title: Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs Publication year: 2023 Coauthors: Pranjal Aggarwal, Aman Madaan, Yiming Yang, Mausam Abstract: A popular approach for improving the correctness of output from large language models (LLMs) is Self-Consistency - poll the LLM multiple times and output the most frequent solution. Existing Self-Consistency techniques always generate a constant number of samples per question, where a better approach will be to non-uniformly distribute the available budget based on the amount of agreement in the samples generated so far. In response, we introduce Adaptive-Consistency, a cost-efficient, model-agnostic technique that dynamically adjusts the number of samples per question using a lightweight stopping criterion. Our experiments over 17 reasoning and code generation datasets and three LLMs demonstrate that Adaptive-Consistency reduces sample budget by up to 7.9 times with an average accuracy drop of less than 0.1%. Our code and data are available at https://www.sample-step-by-step.info\"),\n",
       " Document(page_content=\"Author: Yiming Yang Title: Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision Publication year: 2023 Coauthors: Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David D. Cox, Yiming Yang, Chuang Gan Abstract: Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised fine-tuning (SFT) with human annotations and reinforcement learning from human feedback (RLHF) to align the output of large language models (LLMs) with human intentions, ensuring they are helpful, ethical, and reliable. However, this dependence can significantly constrain the true potential of AI-assistant agents due to the high cost of obtaining human supervision and the related issues on quality, reliability, diversity, self-consistency, and undesirable biases. To address these challenges, we propose a novel approach called SELF-ALIGN, which combines principle-driven reasoning and the generative power of LLMs for the self-alignment of AI agents with minimal human supervision. Our approach encompasses four stages: first, we use an LLM to generate synthetic prompts, and a topic-guided method to augment the prompt diversity; second, we use a small set of human-written principles for AI models to follow, and guide the LLM through in-context learning from demonstrations (of principles application) to produce helpful, ethical, and reliable responses to user's queries; third, we fine-tune the original LLM with the high-quality self-aligned responses so that the resulting model can generate desirable responses for each query directly without the principle set and the demonstrations anymore; and finally, we offer a refinement step to address the issues of overly-brief or indirect responses. Applying SELF-ALIGN to the LLaMA-65b base language model, we develop an AI assistant named Dromedary. With fewer than 300 lines of human annotations (including<200 seed prompts, 16 generic principles, and 5 exemplars for in-context learning). Dromedary significantly surpasses the performance of several state-of-the-art AI systems, including Text-Davinci-003 and Alpaca, on benchmark datasets with various settings.\"),\n",
       " Document(page_content='Author: Yiming Yang Title: Policy Representation via Diffusion Probability Model for Reinforcement Learning Publication year: 2023 Coauthors: Long Yang, Zhixiong Huang, Fenghao Lei, Yucun Zhong, Yiming Yang, Cong Fang, Shiting Wen, Binbin Zhou, Zhouchen Lin Abstract: Popular reinforcement learning (RL) algorithms tend to produce a unimodal policy distribution, which weakens the expressiveness of complicated policy and decays the ability of exploration. The diffusion probability model is powerful to learn complicated multimodal distributions, which has shown promising and potential applications to RL. In this paper, we formally build a theoretical foundation of policy representation via the diffusion probability model and provide practical implementations of diffusion policy for online model-free RL. Concretely, we character diffusion policy as a stochastic process, which is a new approach to representing a policy. Then we present a convergence guarantee for diffusion policy, which provides a theory to understand the multimodality of diffusion policy. Furthermore, we propose the DIPO which is an implementation for model-free online RL with DIffusion POlicy. To the best of our knowledge, DIPO is the first algorithm to solve model-free online RL problems with the diffusion model. Finally, extensive empirical results show the effectiveness and superiority of DIPO on the standard continuous control Mujoco benchmark.'),\n",
       " Document(page_content=\"Author: Yiming Yang Title: Self-Imagine: Effective Unimodal Reasoning with Multimodal Models using Self-Imagination Publication year: 2024 Coauthors: Syeda Nahida Akter, Aman Madaan, Sangwu Lee, Yiming Yang, Eric Nyberg Abstract: The potential of Vision-Language Models (VLMs) often remains underutilized in handling complex text-based problems, particularly when these problems could benefit from visual representation. Resonating with humans' ability to solve complex text-based problems by (1) creating a visual diagram from the problem and (2) deducing what steps they need to take to solve it, we propose Self-Imagine. We leverage a single Vision-Language Model (VLM) to generate a structured representation of the question using HTML, then render the HTML as an image, and finally use the same VLM to answer the question using both the question and the image. Our approach does not require any additional training data or training. We evaluate our approach on three mathematics tasks and nine general-purpose reasoning tasks using state-of-the-art (LLAVA-1.5 and GEMINI PRO) VLMs. Our approach boosts the performance of LLAVA-1.5 and GEMINI PRO on all math tasks (on average GSM8K: +3.1%; ASDIV: +3.2%; SVAMP: +6.9%) and the majority of the general-purpose reasoning tasks by 3.2% to 6.0% on average.\"),\n",
       " Document(page_content=\"Author: Yiming Yang Title: Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning with LLMs Publication year: 2023 Coauthors: Pranjal Aggarwal, Aman Madaan, Yiming Yang, Mausam Abstract: A popular approach for improving the correctness of output from large language models (LLMs) is Self-Consistency – poll the LLM multiple times and output the most frequent so-lution. Existing Self-Consistency techniques always draw a constant number of samples per question, where a better approach will be to non-uniformly distribute the available bud-get based on the amount of agreement in the samples drawn so far. In response, we introduce Adaptive-Consistency, a cost-efﬁcient, model-agnostic technique that dynamically adjusts the number of samples per question using a lightweight stopping criterion. Our experiments over 13 datasets and two LLMs demonstrate that Adaptive-Consistency reduces sample budget by up to 6.0 times with an average accuracy drop of less than 0.1%. 1\"),\n",
       " Document(page_content='Author: Yiming Yang Title: AutoMix: Automatically Mixing Language Models Publication year: 2023 Coauthors: Aman Madaan, Pranjal Aggarwal, Ankit Anand, Srividya Pranavi Potharaju, Swaroop Mishra, Pei Zhou, Aditya Gupta, Dheeraj Rajagopal, Karthik Kappaganthu, Yiming Yang, Shyam Upadhyay, Mausam, Manaal Faruqui Abstract: Large language models (LLMs) are now available in various sizes and configurations from cloud API providers. While this diversity offers a broad spectrum of choices, effectively leveraging the options to optimize computational cost and performance remains challenging. In this work, we present AutoMix, an approach that strategically routes queries to larger LMs, based on the approximate correctness of outputs from a smaller LM. Central to AutoMix is a few-shot self-verification mechanism, which estimates the reliability of its own outputs without requiring training. Given that verifications can be noisy, we employ a meta verifier in AutoMix to refine the accuracy of these assessments. Our experiments using LLAMA2-13/70B, on five context-grounded reasoning datasets demonstrate that AutoMix surpasses established baselines, improving the incremental benefit per cost by up to 89%. Our code and data are available at https://github.com/automix-llm/automix.'),\n",
       " Document(page_content='Author: Yiming Yang Title: CompleQA: Benchmarking the Impacts of Knowledge Graph Completion Methods on Question Answering Publication year: 2023 Coauthors: Donghan Yu, Yu Gu, Chenyan Xiong, Yiming Yang '),\n",
       " Document(page_content='Author: Yiming Yang Title: KEEC: Embed to Control on An Equivariant Geometry Publication year: 2023 Coauthors: Xiaoyuan Cheng, Yiming Yang, Wei Jiang, Yukun Hu Abstract: This paper investigates how representation learning can enable optimal control in unknown and complex dynamics, such as chaotic and non-linear systems, without relying on prior domain knowledge of the dynamics. The core idea is to establish an equivariant geometry that is diffeomorphic to the manifold defined by a dynamical system and to perform optimal control within this corresponding geometry, which is a non-trivial task. To address this challenge, Koopman Embed to Equivariant Control (KEEC) is proposed for model learning and control. Inspired by Lie theory, KEEC begins by learning a non-linear dynamical system defined on a manifold and embedding trajectories into a Lie group. Subsequently, KEEC formulates an equivariant value function equation in reinforcement learning on the equivariant geometry, ensuring an invariant effect as the value function on the original manifold. By deriving analytical-form optimal actions on the equivariant value function, KEEC theoretically achieves quadratic convergence for the optimal equivariant value function by leveraging the differential information on the equivariant geometry. The effectiveness of KEEC is demonstrated in challenging dynamical systems, including chaotic ones like Lorenz-63. Notably, our results show that isometric functions, which maintain the compactness and completeness of geometry while preserving metric and differential information, consistently outperform loss functions lacking these characteristics.'),\n",
       " Document(page_content='Author: Yiming Yang Title: A Study on Semantic Understanding of Large Language Models from the Perspective of Ambiguity Resolution Publication year: 2023 Coauthors: Shuguang Yang, Feipeng Chen, Yiming Yang, Zude Zhu '),\n",
       " Document(page_content=\"Author: Yiming Yang Title: Functional Interpolation for Relative Positions Improves Long Context Transformers Publication year: 2023 Coauthors: Shanda Li, Chong You, Guru Guruganesh, J. Ainslie, Santiago Ontanon, M. Zaheer, Sumit K. Sanghai, Yiming Yang, Sanjiv Kumar, Srinadh Bhojanapalli Abstract: Preventing the performance decay of Transformers on inputs longer than those used for training has been an important challenge in extending the context length of these models. Though the Transformer architecture has fundamentally no limits on the input sequence lengths it can process, the choice of position encoding used during training can limit the performance of these models on longer inputs. We propose a novel functional relative position encoding with progressive interpolation, FIRE, to improve Transformer generalization to longer contexts. We theoretically prove that this can represent some of the popular relative position encodings, such as T5's RPE, Alibi, and Kerple. We next empirically show that FIRE models have better generalization to longer contexts on both zero-shot language modeling and long text benchmarks.\"),\n",
       " Document(page_content='Author: Yiming Yang Title: In-Context Principle Learning from Mistakes Publication year: 2024 Coauthors: Tianjun Zhang, Aman Madaan, Luyu Gao, Steven Zheng, Swaroop Mishra, Yiming Yang, Niket Tandon, Uri Alon Abstract: In-context learning (ICL, also known as few-shot prompting) has been the standard method of adapting LLMs to downstream tasks, by learning from a few input-output examples. Nonetheless, all ICL-based approaches only learn from correct input-output pairs. In this paper, we revisit this paradigm, by learning more from the few given input-output examples. We introduce Learning Principles (LEAP): First, we intentionally induce the model to make mistakes on these few examples; then we reflect on these mistakes, and learn explicit task-specific\"principles\"from them, which help solve similar problems and avoid common mistakes; finally, we prompt the model to answer unseen test questions using the original few-shot examples and these learned general principles. We evaluate LEAP on a wide range of benchmarks, including multi-hop question answering (Hotpot QA), textual QA (DROP), Big-Bench Hard reasoning, and math problems (GSM8K and MATH); in all these benchmarks, LEAP improves the strongest available LLMs such as GPT-3.5-turbo, GPT-4, GPT-4 turbo and Claude-2.1. For example, LEAP improves over the standard few-shot prompting using GPT-4 by 7.5% in DROP, and by 3.3% in HotpotQA. Importantly, LEAP does not require any more input or examples than the standard few-shot prompting settings.'),\n",
       " Document(page_content='Author: Yiming Yang Title: A Self-enhancement Approach for Domain-specific Chatbot Training via Knowledge Mining and Digest Publication year: 2023 Coauthors: Ruohong Zhang, Luyu Gao, Chen Zheng, Zhen Fan, Guokun Lai, Zheng Zhang, Fangzhou Ai, Yiming Yang, Hongxia Yang Abstract: Large Language Models (LLMs), despite their great power in language generation, often encounter challenges when dealing with intricate and knowledge-demanding queries in specific domains. This paper introduces a novel approach to enhance LLMs by effectively extracting the relevant knowledge from domain-specific textual sources, and the adaptive training of a chatbot with domain-specific inquiries. Our two-step approach starts from training a knowledge miner, namely LLMiner, which autonomously extracts Question-Answer pairs from relevant documents through a chain-of-thought reasoning process. Subsequently, we blend the mined QA pairs with a conversational dataset to fine-tune the LLM as a chatbot, thereby enriching its domain-specific expertise and conversational capabilities. We also developed a new evaluation benchmark which comprises four domain-specific text corpora and associated human-crafted QA pairs for testing. Our model shows remarkable performance improvement over generally aligned LLM and surpasses domain-adapted models directly fine-tuned on domain corpus. In particular, LLMiner achieves this with minimal human intervention, requiring only 600 seed instances, thereby providing a pathway towards self-improvement of LLMs through model-synthesized training data.'),\n",
       " Document(page_content='Author: Yiming Yang Title: SALMON: Self-Alignment with Principle-Following Reward Models Publication year: 2023 Coauthors: Zhiqing Sun, Yikang Shen, Hongxin Zhang, Qinhong Zhou, Zhenfang Chen, David D. Cox, Yiming Yang, Chuang Gan Abstract: Supervised Fine-Tuning (SFT) on response demonstrations combined with Reinforcement Learning from Human Feedback (RLHF) constitutes a powerful paradigm for aligning LLM-based AI agents. However, a significant limitation of such an approach is its dependency on high-quality human annotations, making its application to intricate tasks challenging due to difficulties in obtaining consistent response demonstrations and in-distribution response preferences. This paper presents a novel approach, namely SALMON (Self-ALignMent with principle-fOllowiNg reward models), to align base language models with minimal human supervision, using only a small set of human-defined principles, yet achieving superior performance. Central to our approach is a principle-following reward model. Trained on synthetic preference data, this model can generate reward scores based on arbitrary human-defined principles. By merely adjusting these principles during the RL training phase, we gain full control over the preferences with the reward model, subsequently influencing the behavior of the RL-trained policies, and eliminating the reliance on the collection of online human preferences. Applying our method to the LLaMA-2-70b base language model, we developed an AI assistant named Dromedary-2. With only 6 exemplars for in-context learning and 31 human-defined principles, Dromedary-2 significantly surpasses the performance of several state-of-the-art AI systems, including LLaMA-2-Chat-70b, on various benchmark datasets. We have open-sourced the code and model weights to encourage further research into aligning LLM-based AI agents with enhanced supervision efficiency, improved controllability, and scalable oversight.'),\n",
       " Document(page_content='Author: Yiming Yang Title: Generalized Robot Dynamics Learning and Gen2Real Transfer Publication year: 2023 Coauthors: Dengpeng Xing, Yiming Yang, Zechang Wang, Jiale Li, Bo Xu Abstract: Acquiring dynamics is critical for robot learning and is fundamental to planning and control. This paper concerns two fundamental questions: How can we learn a model that covers massive, diverse robot dynamics? Can we construct a model that lifts the data-collection pain and domain expertise required for building specific robot models? We learn the dynamics involved in a dataset containing a large number of serial articulated robots and propose a new concept, “Gen2Real”, to transfer simulated, generalized models to physical, specific robots. We generate a large-scale dataset by randomizing dynamics parameters, topology configurations, and model dimensions, which, in sequence, correspond to different properties, connections, and numbers of robot links. A structure modified from the generative pre-trained transformer is applied to approximate the dynamics of massive heterogeneous robots. In Gen2Real, we transfer the pre-trained model to a target robot using distillation, for the sake of real-time computation. The results demonstrate the superiority of the proposed method in terms of its accuracy in learning a tremendous amount of robot dynamics and its generality to transfer to different robots.'),\n",
       " Document(page_content='Author: Yiming Yang Title: Cardsformer: Grounding Language to Learn a Generalizable Policy in Hearthstone Publication year: 2023 Coauthors: Wannian Xia, Yiming Yang, Jingqing Ruan, Dengpeng Xing, Bo Xu '),\n",
       " Document(page_content='Andrew Carnegie \\nA self-educated \"working boy\" who loved books, Andrew Carnegie emigrated from Scotland in 1848 and settled in Pittsburgh, Pa. Attending night school and borrowing books, Carnegie went from factory worker in a textile mill to successful entrepreneur and industrialist. He rose to prominence by founding what became the world\\'s largest steel producing company by the end of the 19th century.\\n \\n Carnegie Technical Schools \\nAt one point the richest man in the world, Carnegie believed that \"to die rich is to die disgraced.\" He turned his attention to writing, social activism and philanthropy, determined to establish educational opportunities for the general public where few existed.\\nIn 1900, he donated $1 million for the creation of a technical institute for the city of Pittsburgh, envisioning a school where working-class men and women of Pittsburgh could learn practical skills, trades and crafts that would enhance their careers, lives and communities.'),\n",
       " Document(page_content='\"My heart is in the work,\" he stated, which would become part of the school\\'s official motto.\\nThe Carnegie Technical Schools offered two- and three-year certificates in the arts as well as in engineering disciplines and included a college for women, Margaret Morrison Carnegie College.\\n \\n Carnegie Tech – Early Years \\nSoon faced with the demand for baccalaureate programs, Carnegie Technical Schools began offering bachelor\\'s degrees through its College of Engineering and College of Fine Arts, becoming the Carnegie Institute of Technology, or \"Carnegie Tech.\"\\nDuring the first half of the 20th century, with support from Andrew Carnegie and other funders, Carnegie Tech laid the foundation for a school on the cutting edge. Some key developments were:\\nIt expanded from two buildings into an elegant 20th century campus designed in the beaux arts architectural style, housing a wealth of machine shops, studios and laboratories — the hands-on center of learning that persists today.'),\n",
       " Document(page_content=\"It pioneered conservatory degree programs in music and drama, in addition to visual art and design programs. The first U.S. drama degree was awarded in 1914 at Carnegie Tech.\\nIt began offering graduate degrees. In 1919, the first doctorate (in civil engineering) was awarded to Mao Yisheng, a student from China.\\nIt laid the groundwork for a research institution, recruiting leading scientists, offering sponsored fellowships with government and industry leaders and pioneering nontraditional interdisciplinary research, which brought together physicists, chemists and metallurgists, for example. Interdisciplinary research would become the hallmark of Carnegie Mellon research.\\nIt initiated the 'Carnegie Plan' in 1938, a new curriculum that required science and engineer students to take courses in humanities and social sciences in order to better understand the needs of society.\\nCarnegie died in 1919, but his vision for an educated public lived on after him.\\n \\n Carnegie Tech - Post-war Years\"),\n",
       " Document(page_content='Carnegie died in 1919, but his vision for an educated public lived on after him.\\n \\n Carnegie Tech - Post-war Years \\nWith the end of World War II, the latter half of the 20th century brought unprecedented growth to Carnegie Tech. In 1956, the arrival of the first IBM computer to campus was revolutionary, initiating a university culture where information technology pervaded virtually all areas of study.\\nUniversity culture also changed in 1973 when Margaret Morrison closed and women joined their male peers in classrooms and dorms.\\nThe times were changing, and Tech positioned itself at the forefront, opening three new schools:\\n1948:\\xa0The Graduate School of Industrial Administration, later renamed the\\xa0David A. Tepper School of Business, focusing on quantitative analysis and pioneering the field of management science.\\n1968:\\xa0School of Urban and Public Affairs, later renamed the\\xa0H. John Heinz III College, providing graduate training for work in the public sector.'),\n",
       " Document(page_content='1968:\\xa0School of Urban and Public Affairs, later renamed the\\xa0H. John Heinz III College, providing graduate training for work in the public sector.\\n1986:\\xa0School of Computer Science, pioneering computing and artificial intelligence, led by interdisciplinary efforts of Allen Newell and Herbert Simon.\\n \\n Carnegie Mellon University \\nIn 1967, Carnegie Tech merged with the Mellon Institute, a science research center founded by the Mellon family of Pittsburgh, to become known as Carnegie Mellon University. The merger built upon a long history of support from the Mellons.\\nIt allowed Carnegie Mellon to establish the last of its current pillars: the\\xa0Mellon College of Science\\xa0and the College of Humanities and Social Sciences, now known as\\xa0Marianna Brown Dietrich College of Humanities and Social Sciences.'),\n",
       " Document(page_content='In 2017, Carnegie Mellon celebrated the 50th anniversary of the Carnegie Tech-Mellon Institute merger, revisiting the shared vision of the founders and recognizing the impact it has had, and will continue to have, in the world of higher education, research and discovery.\\n \\n A Global Impact \\nIn its 115 years, Carnegie Mellon has soared to national and international leadership in higher education and research. A birthplace of innovation since its founding, it continues to be known for innovation, for solving real-world problems and for interdisciplinary collaboration.\\nIts\\xa0alumni can be found across the globe\\xa0— from Tony Award winners to Nobel Prize and Turing Award winners, from CEOs to entrepreneurs, from professors to artists.\\nIn the 2000s, in response to demand for expanded international educational opportunities, Carnegie Mellon began offering degree programs outside of Pittsburgh.'),\n",
       " Document(page_content='Today its global presence includes campuses in Qatar and Silicon Valley, Calif., more than a dozen degree-granting locations and more than 20 research partnerships such as Los Angeles; New York City; Washington, D.C.; Australia; China; Portugal and Rwanda.\\n \\n The Future \\nCMU is positioned like never before to meet the challenges of the 21st century. In the coming years, the university will see the largest expansion to the Pittsburgh campus since 1900.\\nAt the intersection of technology and humanity, CMU research, innovation and creativity will continue to guide our future as a world-class university.\\nAs outlined in the Strategic Plan 2025, the university will focus on advancing the individual student experience, the broader Carnegie Mellon community experience, and the social impact of Carnegie Mellon throughout the world.\\n \\nCarnegie Mellon University challenges the curious and passionate to deliver work that matters.\\nCalendar Careers'),\n",
       " Document(page_content='Yonatan Bisk Assistant Professor Email: ybisk@cs.cmu.edu Office: Gates & Hillman Centers Research Areas: Grounding, RoboNLP, Vision and Language, Embodiment, Unsupervised Learning'),\n",
       " Document(page_content='Ralf Brown Principal Systems Scientist Email: ralf@andrew.cmu.edu Office: 5711 Gates & Hillman Centers Phone: 412-268-8298 Research Areas: Information Extraction, Summarization and Question Answering, Information Retrieval, Text Mining and Analytics, Machine Translation, Natural Language Processing and Computational Linguistics'),\n",
       " Document(page_content='Jamie Callan Professor and PhD Program Director Email: callan@cs.cmu.edu Office: 5419 Gates & Hillman Centers Phone: 412-268-4525 Research Areas: Information Retrieval, Text Mining and Analytics'),\n",
       " Document(page_content='Justine Cassell Professor (On Leave) Email: jcassell@andrew.cmu.edu Office: 5107 Gates & Hillman Centers Phone: 412-204-6268'),\n",
       " Document(page_content='Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669'),\n",
       " Document(page_content='Fernando Diaz Associate Professor Email: diazf@cmu.edu Phone: 412-268-4229 Research Areas: Information Retrieval: Recommender Systems, Retrieval and Ranking Models, Natural Language Processing: Fairness and Ethics in Language Technology, Creativity, Evaluation'),\n",
       " Document(page_content='Scott Fahlman Research Professor Emeritus Email: sef@cs.cmu.edu Office: 6417 Gates & Hillman Centers Phone: 412-268-2575 Research Areas: AI, Knowledge Representation and Reasoning, Natural Language Understanding'),\n",
       " Document(page_content='Robert Frederking Principal Systems Scientist/Associate Dean of Doctoral Programs/MLT Program Director Email: ref@cs.cmu.edu Office: 6515 Gates & Hillman Centers Phone: 412-268-6656'),\n",
       " Document(page_content='Daniel Fried Assistant Professor Email: dfried@andrew.cmu.edu Research Areas: Natural Language Processing: Language and Code, Conversational AI, Intelligent Agents, and Dialogue, Discourse and Pragmatics, Multimodal AI'),\n",
       " Document(page_content='Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering'),\n",
       " Document(page_content='Alexander Hauptmann Research Professor Email: alex@cs.cmu.edu Office: 5519 Gates & Hillman Centers Phone: 412-268-1448 Research Areas: Information Extraction, Summarization and Question Answering, Information Retrieval, Text Mining and Analytics, Machine Learning, Multimodal Computing and Interaction'),\n",
       " Document(page_content='Daphne Ippolito Assistant Professor Email: daphnei@cmu.edu Phone: 412-268-7250 Research Areas: Natural Language Generation, Privacy and Security, Language Technology Application Areas/Issues, Creativity'),\n",
       " Document(page_content='Lori Levin Research Professor Email: lsl@cs.cmu.edu Office: 5717 Gates & Hillman Centers Phone: 412-268-6193 Research Areas: Machine Translation, Natural Language Processing and Computational Linguistics, Corpus Annotation and Resources'),\n",
       " Document(page_content='Lei Li Assistant Professor Email: leili@andrew.cmu.edu Phone: 412-268-6355 Research Areas: Machine Learning, Machine Translation, Large Language Models, AI Drug Discovery'),\n",
       " Document(page_content='Teruko Mitamura Research Professor Email: teruko@cs.cmu.edu Office: 6711 Gates & Hillman Centers Phone: 412-268-6596 Research Areas: Information Extraction, Summarization and Question Answering, Information Retrieval, Text Mining and Analytics, Language Technologies for Education, Natural Language Processing and Computational Linguistics'),\n",
       " Document(page_content='Louis-Philippe Morency Leonardo Associate Professor of Computer Science Email: morency@cs.cmu.edu Office: 5411 Gates & Hillman Centers Phone: 412-268-5508 Research Areas: Machine Learning, Multimodal Computing and Interaction, Spoken Interfaces and Dialogue Processing'),\n",
       " Document(page_content='David Mortensen Assistant Research Professor Email: dmortens@cs.cmu.edu Office: 5707 Gates & Hillman Centers Phone: 412-268-2894 Research Areas: Natural Language Processing and Computational Linguistics, Corpus Annotation and Resources'),\n",
       " Document(page_content='Graham Neubig Associate Professor Email: gneubig@cs.cmu.edu Office: 5409 Gates & Hillman Centers Research Areas: Machine Translation, Natural Language Processing, Spoken Language Processing, Machine Learning'),\n",
       " Document(page_content='Eric Nyberg Professor Email: ehn@cs.cmu.edu Office: 6715 Gates & Hillman Centers Phone: 412-268-7281 Research Areas: Information Extraction, Summarization and Question Answering, Information Retrieval, Text Mining and Analytics, Language Technologies for Education'),\n",
       " Document(page_content='Kemal Oflazer Teaching Professor of Computer Science Email: ko@qatar.cmu.edu Office: 1009 Carnegie Mellon - Qatar Campus Phone:'),\n",
       " Document(page_content='Bhiksha Ramakrishnan Professor Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826 Research Areas: Machine Learning, Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing, Privacy'),\n",
       " Document(page_content='Carolyn Rosé Professor Email: cprose@cs.cmu.edu Office: 5415 Gates & Hillman Centers Phone: 412-268-7130 Research Areas: Information Retrieval, Text Mining and Analytics, Language Technologies for Education, Natural Language Processing and Computational Linguistics, Computer Supported Collaborative Learning/MOOCs'),\n",
       " Document(page_content='Alexander Rudnicky Research Professor Emeritus Email: alex.rudnicky@cs.cmu.edu Office: 6511 Gates & Hillman Centers Phone: 412-268-2622 Research Areas: Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing'),\n",
       " Document(page_content='Maarten Sap Assistant Professor Email: msap2@andrew.cmu.edu Research Areas: Fairness and Ethics in Language Technology, Computational Social Science, Discourse and Pragmatics, Conversational AI, Intelligent Agents, and Dialogue'),\n",
       " Document(page_content='Michael Shamos Distinguished Career Professor Email: shamos@cs.cmu.edu Office: 6707 Gates & Hillman Centers Phone: 412-268-8193'),\n",
       " Document(page_content='Rita Singh Associate Research Professor Email: rsingh@cs.cmu.edu Office: 6703 Gates & Hillman Centers Phone: 412-268-9859'),\n",
       " Document(page_content='Emma Strubell Assistant Professor Email: estrubel@andrew.cmu.edu Office: Gates & Hillman Centers'),\n",
       " Document(page_content='Alexander Waibel Professor Email: waibel@cs.cmu.edu Office: 205 407 South Craig Street Phone: 412-268-7676 Research Areas: Spoken Language Translation, Machine Translation, Speech Processing, Neural Networks, Machine Learning, Multimodal Interaction, Dialog Processing'),\n",
       " Document(page_content='Shinji Watanabe Associate Professor Email: swatanab@andrew.cmu.edu Phone: 412-268-3687 Research Areas: Natural Language Processing: Conversational AI, Intelligent Agents, and Dialogue, Speech Processing (ASR, Speech Synthesis): Speech Recognition, Speech Synthesis, Multilingual/Low-Resource Speech Processing, Speech-to-Speech Translation, Speech Enhancement / Robust Speech Processing'),\n",
       " Document(page_content='Sean Welleck Assistant Professor (Starting January 2024) Email: swelleck@andrew.cmu.edu'),\n",
       " Document(page_content='Eric P. Xing Professor (On Leave) Email: epxing@andrew.cmu.edu Office: 8101 Gates & Hillman Centers Phone: 412-268-2559'),\n",
       " Document(page_content='Chenyan Xiong Associate Professor Email: cx@andrew.cmu.edu Phone: 412-268-7641'),\n",
       " Document(page_content='Yiming Yang Professor Email: yiming@cs.cmu.edu Office: 6717 Gates & Hillman Centers Phone: 412-268-1364'),\n",
       " Document(page_content='Spring offering: Course: 48105 Title: Architecture Design Studio: Poiesis Studio 2 Units: 15.0 Lec/Sec: Lecture Days: Monday, Wednesday, Friday Begin: 02:00PM End: 04:50PM Bldg/Room: MM A14 Location: Pittsburgh, Pennsylvania Instructor(s): Yang '),\n",
       " Document(page_content='Spring offering: Course: 48105 Title: Architecture Design Studio: Poiesis Studio 2 Units: 15.0 Lec/Sec: Section A Days: Monday, Wednesday, Friday Begin: 02:00PM End: 04:50PM Bldg/Room: CFA 200 Location: Pittsburgh, Pennsylvania Instructor(s): Abraham '),\n",
       " Document(page_content='Spring offering: Course: 48105 Title: Architecture Design Studio: Poiesis Studio 2 Units: 15.0 Lec/Sec: Section B Days: Monday, Wednesday, Friday Begin: 02:00PM End: 04:50PM Bldg/Room: CFA 200 Location: Pittsburgh, Pennsylvania Instructor(s): McFarland '),\n",
       " Document(page_content='Spring offering: Course: 48105 Title: Architecture Design Studio: Poiesis Studio 2 Units: 15.0 Lec/Sec: Section C Days: Monday, Wednesday, Friday Begin: 02:00PM End: 04:50PM Bldg/Room: CFA 200 Location: Pittsburgh, Pennsylvania Instructor(s): Saleh '),\n",
       " Document(page_content='Spring offering: Course: 48105 Title: Architecture Design Studio: Poiesis Studio 2 Units: 15.0 Lec/Sec: Section D Days: Monday, Wednesday, Friday Begin: 02:00PM End: 04:50PM Bldg/Room: CFA 200 Location: Pittsburgh, Pennsylvania Instructor(s): Jno Baptiste '),\n",
       " Document(page_content='Spring offering: Course: 48111 Title: Exploring Pittsburgh Units: 3.0 Lec/Sec: Section A Days: Friday Begin: 09:00AM End: 11:50AM Bldg/Room: CFA 214 Location: Pittsburgh, Pennsylvania Instructor(s): Torello '),\n",
       " Document(page_content='Spring offering: Course: 48111 Title: Exploring Pittsburgh Units: 3.0 Lec/Sec: Section B Days: Friday Begin: 09:00AM End: 11:50AM Bldg/Room: CFA 214 Location: Pittsburgh, Pennsylvania Instructor(s): Torello '),\n",
       " Document(page_content='Spring offering: Course: 48112 Title: Digital Fabrication Skills Units: 2.0 Lec/Sec: Section A3 Days: Tuesday Begin: 02:00PM End: 03:20PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Sontag '),\n",
       " Document(page_content='Spring offering: Course: 48112 Title: Digital Fabrication Skills Units: 2.0 Lec/Sec: Section A3 Days: Thursday Begin: 02:00PM End: 03:20PM Bldg/Room: GHC 4102 Location: Pittsburgh, Pennsylvania Instructor(s): Sontag '),\n",
       " Document(page_content='Spring offering: Course: 48112 Title: Digital Fabrication Skills Units: 2.0 Lec/Sec: Section A4 Days: Tuesday Begin: 02:00PM End: 03:20PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Sontag '),\n",
       " Document(page_content='Spring offering: Course: 48112 Title: Digital Fabrication Skills Units: 2.0 Lec/Sec: Section A4 Days: Thursday Begin: 02:00PM End: 03:20PM Bldg/Room: WEH 5415 Location: Pittsburgh, Pennsylvania Instructor(s): Sontag '),\n",
       " Document(page_content='Spring offering: Course: 48205 Title: Architecture Options Studios Units: 18.0 Lec/Sec: Lecture Days: Monday, Wednesday, Friday Begin: 02:00PM End: 04:50PM Bldg/Room: MM 103 Location: Pittsburgh, Pennsylvania Instructor(s): Arscott '),\n",
       " Document(page_content='Spring offering: Course: 48205 Title: Architecture Options Studios Units: 18.0 Lec/Sec: Section A Days: Monday, Wednesday, Friday Begin: 02:00PM End: 04:50PM Bldg/Room: CFA 200 Location: Pittsburgh, Pennsylvania Instructor(s): Anklesaria '),\n",
       " Document(page_content='Spring offering: Course: 48205 Title: Architecture Options Studios Units: 18.0 Lec/Sec: Section B Days: Tuesday, Thursday Begin: 01:00PM End: 04:50PM Bldg/Room: CFA 200 Location: Pittsburgh, Pennsylvania Instructor(s): Issaias '),\n",
       " Document(page_content='Spring offering: Course: 48205 Title: Architecture Options Studios Units: 18.0 Lec/Sec: Section C Days: Tuesday, Thursday Begin: 01:00PM End: 04:50PM Bldg/Room: CFA 200 Location: Pittsburgh, Pennsylvania Instructor(s): Lee '),\n",
       " Document(page_content='Spring offering: Course: 48205 Title: Architecture Options Studios Units: 18.0 Lec/Sec: Section D Days: Monday, Wednesday, Friday Begin: 02:00PM End: 04:50PM Bldg/Room: CFA 200 Location: Pittsburgh, Pennsylvania Instructor(s): Patel '),\n",
       " Document(page_content='Spring offering: Course: 48205 Title: Architecture Options Studios Units: 18.0 Lec/Sec: Section E Days: Monday, Wednesday, Friday Begin: 02:00PM End: 04:50PM Bldg/Room: CFA 200 Location: Pittsburgh, Pennsylvania Instructor(s): Sindi '),\n",
       " Document(page_content='Spring offering: Course: 48234 Title: Introduction to Structures Units: 3.0 Lec/Sec: Section A3 Days: Wednesday Begin: 11:00AM End: 12:20PM Bldg/Room: MM 303 Location: Pittsburgh, Pennsylvania Instructor(s): Segal '),\n",
       " Document(page_content='Spring offering: Course: 48234 Title: Introduction to Structures Units: 3.0 Lec/Sec: Section B3 Days: Wednesday Begin: 12:30PM End: 01:50PM Bldg/Room: MM 303 Location: Pittsburgh, Pennsylvania Instructor(s): Segal '),\n",
       " Document(page_content='Spring offering: Course: 48240 Title: History of World Architecture, I Units: 9.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 11:00AM End: 12:20PM Bldg/Room: MM A14 Location: Pittsburgh, Pennsylvania Instructor(s): Shaw '),\n",
       " Document(page_content='Spring offering: Course: 48241 Title: History of Modern Architecture Units: 9.0 Lec/Sec: Section A Days: Monday, Wednesday, Friday Begin: 10:00AM End: 10:50AM Bldg/Room: MM A14 Location: Pittsburgh, Pennsylvania Instructor(s): Gutschow '),\n",
       " Document(page_content='Spring offering: Course: 48305 Title: Architecture Design Studio: Praxis Studio 2 Units: 18.0 Lec/Sec: Lecture Days: Monday, Wednesday, Friday Begin: 02:00PM End: 04:50PM Bldg/Room: POS 146 Location: Pittsburgh, Pennsylvania Instructor(s): Ficca '),\n",
       " Document(page_content='Spring offering: Course: 48305 Title: Architecture Design Studio: Praxis Studio 2 Units: 18.0 Lec/Sec: Section A Days: Monday, Wednesday, Friday Begin: 02:00PM End: 04:50PM Bldg/Room: MM 312 Location: Pittsburgh, Pennsylvania Instructor(s): Achnani '),\n",
       " Document(page_content='Spring offering: Course: 48305 Title: Architecture Design Studio: Praxis Studio 2 Units: 18.0 Lec/Sec: Section B Days: Monday, Wednesday, Friday Begin: 02:00PM End: 04:50PM Bldg/Room: MM 312 Location: Pittsburgh, Pennsylvania Instructor(s): Cochran '),\n",
       " Document(page_content='Spring offering: Course: 48305 Title: Architecture Design Studio: Praxis Studio 2 Units: 18.0 Lec/Sec: Section C Days: Monday, Wednesday, Friday Begin: 02:00PM End: 04:50PM Bldg/Room: MM 312 Location: Pittsburgh, Pennsylvania Instructor(s): Kwon '),\n",
       " Document(page_content='Spring offering: Course: 48305 Title: Architecture Design Studio: Praxis Studio 2 Units: 18.0 Lec/Sec: Section D Days: Monday, Wednesday, Friday Begin: 02:00PM End: 04:50PM Bldg/Room: MM 312 Location: Pittsburgh, Pennsylvania Instructor(s): King '),\n",
       " Document(page_content='Spring offering: Course: 48314 Title: New Pedagogies: Units: 9.0 Lec/Sec: Section D Days: Monday, Wednesday, Friday Begin: 02:00PM End: 04:50PM Bldg/Room: MM 312 Location: Pittsburgh, Pennsylvania Instructor(s): King '),\n",
       " Document(page_content='Spring offering: Course: 48314 Title: Unsettling Ground- Retiring the God View Units: 9.0 Lec/Sec: Section A Days: Monday Begin: 10:00AM End: 12:50PM Bldg/Room: MM 303 Location: Pittsburgh, Pennsylvania Instructor(s): Sindi '),\n",
       " Document(page_content='Spring offering: Course: 48314 Title: New Pedagogies: Units: 9.0 Lec/Sec: Section A Days: Monday Begin: 10:00AM End: 12:50PM Bldg/Room: MM 303 Location: Pittsburgh, Pennsylvania Instructor(s): Sindi '),\n",
       " Document(page_content='Spring offering: Course: 48314 Title: Material Regeneration Units: 9.0 Lec/Sec: Section B Days: Wednesday Begin: 09:00AM End: 11:50AM Bldg/Room: MM 321 Location: Pittsburgh, Pennsylvania Instructor(s): Kwon '),\n",
       " Document(page_content='Spring offering: Course: 48314 Title: New Pedagogies: Units: 9.0 Lec/Sec: Section B Days: Wednesday Begin: 09:00AM End: 11:50AM Bldg/Room: MM 321 Location: Pittsburgh, Pennsylvania Instructor(s): Kwon '),\n",
       " Document(page_content='Spring offering: Course: 48314 Title: A Multiple-Making Approach to Inquiry in Craft + Computation Units: 9.0 Lec/Sec: Section C Days: Friday Begin: 09:00AM End: 12:20PM Bldg/Room: MM C4 Location: Pittsburgh, Pennsylvania Instructor(s): Noel '),\n",
       " Document(page_content='Spring offering: Course: 48317 Title: The Chair Units: 9.0 Lec/Sec: Section A Days: Monday, Friday Begin: 10:00AM End: 11:50AM Bldg/Room: CFA 211 Location: Pittsburgh, Pennsylvania Instructor(s): Achnani '),\n",
       " Document(page_content='Spring offering: Course: 48318 Title: Discourse and Praxis in the Climate Emergency Units: VAR Lec/Sec: Section A3 Days: Monday Begin: 11:00AM End: 12:20PM Bldg/Room: MM 307 Location: Pittsburgh, Pennsylvania Instructor(s): Anklesaria '),\n",
       " Document(page_content='Spring offering: Course: 48324 Title: Structural Design 1: Form and Forces Units: 6.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 11:00AM End: 12:20PM Bldg/Room: GHC 4307 Location: Pittsburgh, Pennsylvania Instructor(s): Lee '),\n",
       " Document(page_content='Spring offering: Course: 48328 Title: Detailing Architecture Units: 9.0 Lec/Sec: Section A Days: Friday Begin: 09:30AM End: 12:20PM Bldg/Room: MM 307 Location: Pittsburgh, Pennsylvania Instructor(s): Damiani '),\n",
       " Document(page_content='Spring offering: Course: 48355 Title: Perspective Units: 9.0 Lec/Sec: Section A Days: Thursday Begin: 02:00PM End: 04:50PM Bldg/Room: CFA 206A Location: Pittsburgh, Pennsylvania Instructor(s): Cooper '),\n",
       " Document(page_content='Spring offering: Course: 48367 Title: Material Histories Units: 9.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 11:00AM End: 12:20PM Bldg/Room: MM 409 Location: Pittsburgh, Pennsylvania Instructor(s): Torello '),\n",
       " Document(page_content='Spring offering: Course: 48380 Title: Constructing Value(s): Economies of Design Units: 6.0 Lec/Sec: Section A Days: Tuesday Begin: 09:00AM End: 10:50AM Bldg/Room: CFA 214 Location: Pittsburgh, Pennsylvania Instructor(s): Volcy '),\n",
       " Document(page_content='Spring offering: Course: 48381 Title: Issues of Practice Units: 6.0 Lec/Sec: Section A Days: Thursday Begin: 10:00AM End: 11:50AM Bldg/Room: MM 103 Location: Pittsburgh, Pennsylvania Instructor(s): Coppedge '),\n",
       " Document(page_content='Spring offering: Course: 48383 Title: Ethics and Decision Making in Architecture Units: 6.0 Lec/Sec: Section A Days: Tuesday Begin: 10:00AM End: 11:50AM Bldg/Room: MM 103 Location: Pittsburgh, Pennsylvania Instructor(s): Vavasis '),\n",
       " Document(page_content='Spring offering: Course: 48410 Title: Advanced Synthesis Options Studio II Units: 18.0 Lec/Sec: Lecture Days: Tuesday, Thursday Begin: 01:00PM End: 04:50PM Bldg/Room: POS 146 Location: Pittsburgh, Pennsylvania Instructor(s): Arscott '),\n",
       " Document(page_content='Spring offering: Course: 48410 Title: Advanced Synthesis Options Studio II Units: 18.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 01:00PM End: 04:50PM Bldg/Room: MM 312 Location: Pittsburgh, Pennsylvania Instructor(s): Bista '),\n",
       " Document(page_content='Spring offering: Course: 48410 Title: Advanced Synthesis Options Studio II Units: 18.0 Lec/Sec: Section B Days: Tuesday, Thursday Begin: 01:00PM End: 04:50PM Bldg/Room: MM 312 Location: Pittsburgh, Pennsylvania Instructor(s): Bizon '),\n",
       " Document(page_content='Spring offering: Course: 48410 Title: Advanced Synthesis Options Studio II Units: 18.0 Lec/Sec: Section C Days: Tuesday, Thursday Begin: 01:00PM End: 04:50PM Bldg/Room: MM 312 Location: Pittsburgh, Pennsylvania Instructor(s): Cupkova '),\n",
       " Document(page_content='Spring offering: Course: 48410 Title: Advanced Synthesis Options Studio II Units: 18.0 Lec/Sec: Section D Days: Tuesday, Thursday Begin: 01:00PM End: 04:50PM Bldg/Room: MM 312 Location: Pittsburgh, Pennsylvania Instructor(s): Damiani '),\n",
       " Document(page_content='Spring offering: Course: 48410 Title: Advanced Synthesis Options Studio II Units: 18.0 Lec/Sec: Section E Days: Tuesday, Thursday Begin: 01:00PM End: 04:50PM Bldg/Room: CFA 200 Location: Pittsburgh, Pennsylvania Instructor(s): Garofalo '),\n",
       " Document(page_content='Spring offering: Course: 48410 Title: Advanced Synthesis Options Studio II Units: 18.0 Lec/Sec: Section F Days: Tuesday, Thursday Begin: 01:00PM End: 04:50PM Bldg/Room: CFA 200 Location: Pittsburgh, Pennsylvania Instructor(s): Hayes '),\n",
       " Document(page_content='Spring offering: Course: 48410 Title: Advanced Synthesis Options Studio II Units: 18.0 Lec/Sec: Section G Days: Tuesday, Thursday Begin: 01:00PM End: 04:50PM Bldg/Room: CFA 200 Location: Pittsburgh, Pennsylvania Instructor(s): Lee '),\n",
       " Document(page_content='Spring offering: Course: 48425 Title: EX-CHANGE: Exhibition & Publication in Practice Units: 3.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 09:00AM End: 09:50AM Bldg/Room: MM 307 Location: Pittsburgh, Pennsylvania Instructor(s): Rafson '),\n",
       " Document(page_content='Spring offering: Course: 48435 Title: Modern Mexico & Guatemala: 19th-21st Century Architecture Units: 9.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 03:30PM End: 04:50PM Bldg/Room: CFA 102 Location: Pittsburgh, Pennsylvania Instructor(s): Shaw '),\n",
       " Document(page_content='Spring offering: Course: 48442 Title: History of Asian Architecture Units: 9.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 11:00AM End: 12:20PM Bldg/Room: CMU REMOTE Location: Pittsburgh, Pennsylvania Instructor(s): Chen '),\n",
       " Document(page_content='Spring offering: Course: 48467 Title: Design Build Elective Units: VAR Lec/Sec: Section A Days: Tuesday, Thursday Begin: 01:00PM End: 02:20PM Bldg/Room: CFA 200 Location: Pittsburgh, Pennsylvania Instructor(s): Lee '),\n",
       " Document(page_content='Spring offering: Course: 48485 Title: Design and Documentation in Revit Units: 3.0 Lec/Sec: Section A Days: Monday Begin: 09:30AM End: 10:50AM Bldg/Room: MM A11 Location: Pittsburgh, Pennsylvania Instructor(s): Sawyer '),\n",
       " Document(page_content='Spring offering: Course: 48497 Title: Pre-Thesis Units: 3.0 Lec/Sec: Section A Days: Wednesday Begin: 10:00AM End: 10:50AM Bldg/Room: CFA 211 Location: Pittsburgh, Pennsylvania Instructor(s): Arscott '),\n",
       " Document(page_content='Spring offering: Course: 48497 Title: Pre-Thesis Units: 3.0 Lec/Sec: Section B Days: Wednesday Begin: 09:00AM End: 09:50AM Bldg/Room: CFA 211 Location: Pittsburgh, Pennsylvania Instructor(s): Arscott '),\n",
       " Document(page_content='Spring offering: Course: 48510 Title: Advanced Synthesis Options Studio IV Units: 18.0 Lec/Sec: Lecture Days: Tuesday, Thursday Begin: 01:00PM End: 04:50PM Bldg/Room: MM 103 Location: Pittsburgh, Pennsylvania Instructor(s): Arscott '),\n",
       " Document(page_content='Spring offering: Course: 48510 Title: Advanced Synthesis Options Studio IV Units: 18.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 01:00PM End: 04:50PM Bldg/Room: MM 312 Location: Pittsburgh, Pennsylvania Instructor(s): Bista '),\n",
       " Document(page_content='Spring offering: Course: 48510 Title: Advanced Synthesis Options Studio IV Units: 18.0 Lec/Sec: Section B Days: Tuesday, Thursday Begin: 01:00PM End: 04:50PM Bldg/Room: MM 312 Location: Pittsburgh, Pennsylvania Instructor(s): Bizon '),\n",
       " Document(page_content='Spring offering: Course: 48510 Title: Advanced Synthesis Options Studio IV Units: 18.0 Lec/Sec: Section C Days: Tuesday, Thursday Begin: 01:00PM End: 04:50PM Bldg/Room: MM 312 Location: Pittsburgh, Pennsylvania Instructor(s): Cupkova '),\n",
       " Document(page_content='Spring offering: Course: 48510 Title: Advanced Synthesis Options Studio IV Units: 18.0 Lec/Sec: Section D Days: Tuesday, Thursday Begin: 01:00PM End: 04:50PM Bldg/Room: MM 312 Location: Pittsburgh, Pennsylvania Instructor(s): Damiani '),\n",
       " Document(page_content='Spring offering: Course: 48510 Title: Advanced Synthesis Options Studio IV Units: 18.0 Lec/Sec: Section E Days: Tuesday, Thursday Begin: 01:00PM End: 04:50PM Bldg/Room: CFA 200 Location: Pittsburgh, Pennsylvania Instructor(s): Garofalo '),\n",
       " Document(page_content='Spring offering: Course: 48510 Title: Advanced Synthesis Options Studio IV Units: 18.0 Lec/Sec: Section F Days: Tuesday, Thursday Begin: 01:00PM End: 04:50PM Bldg/Room: CFA 200 Location: Pittsburgh, Pennsylvania Instructor(s): Hayes '),\n",
       " Document(page_content='Spring offering: Course: 48510 Title: Advanced Synthesis Options Studio IV Units: 18.0 Lec/Sec: Section G Days: Tuesday, Thursday Begin: 01:00PM End: 04:50PM Bldg/Room: CFA 200 Location: Pittsburgh, Pennsylvania Instructor(s): Lee '),\n",
       " Document(page_content='Spring offering: Course: 48516 Title: Carnival Gateway Special Project Units: VAR Lec/Sec: Section A4 Days: Monday Begin: 06:30PM End: 07:50PM Bldg/Room: CFA 206A Location: Pittsburgh, Pennsylvania Instructor(s): Khan '),\n",
       " Document(page_content='Spring offering: Course: 48517 Title: Carnival Gateway Project Management: Units: VAR Lec/Sec: Section A4 Days: Monday Begin: 06:30PM End: 07:50PM Bldg/Room: CFA 206A Location: Pittsburgh, Pennsylvania Instructor(s): Khan '),\n",
       " Document(page_content='Spring offering: Course: 48517 Title: Carnival Gateway Project Management Units: VAR Lec/Sec: Section A Days: Monday Begin: 06:30PM End: 07:50PM Bldg/Room: CFA 206A Location: Pittsburgh, Pennsylvania Instructor(s): Khan '),\n",
       " Document(page_content='Spring offering: Course: 48517 Title: Carnival Gateway Project Management Units: VAR Lec/Sec: Section B Days: Monday Begin: 06:30PM End: 07:50PM Bldg/Room: CFA 206A Location: Pittsburgh, Pennsylvania Instructor(s): Achnani '),\n",
       " Document(page_content='Spring offering: Course: 48519 Title: Architecture Design Studio: Thesis II/ Independent Project Units: VAR Lec/Sec: Section A Days: Tuesday, Thursday Begin: 01:00PM End: 04:50PM Bldg/Room: MM 312 Location: Pittsburgh, Pennsylvania Instructor(s): Arscott, Rafson '),\n",
       " Document(page_content='Spring offering: Course: 48524 Title: Building Performance Modeling Units: 9.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 02:00PM End: 03:20PM Bldg/Room: MM 415IW Location: Pittsburgh, Pennsylvania Instructor(s): Liang '),\n",
       " Document(page_content='Spring offering: Course: 48545 Title: Design Fabrication Units: 9.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 09:30AM End: 10:50AM Bldg/Room: MM C4 Location: Pittsburgh, Pennsylvania Instructor(s): Patel '),\n",
       " Document(page_content='Spring offering: Course: 48557 Title: Formless as an Operation Units: 9.0 Lec/Sec: Section A Days: Friday Begin: 09:00AM End: 11:50AM Bldg/Room: MM 409 Location: Pittsburgh, Pennsylvania Instructor(s): Bizon '),\n",
       " Document(page_content='Spring offering: Course: 48569 Title: GIS/CAFM Units: VAR Lec/Sec: Section A Days: To be announced Begin: 09:00AM End: 11:50AM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Kurland '),\n",
       " Document(page_content='Spring offering: Course: 48599 Title: Undergraduate Independent Study Units: 3-9 Lec/Sec: Section A Days: To be announced Begin: 09:00AM End: 11:50AM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Yang '),\n",
       " Document(page_content='Spring offering: Course: 48599 Title: Undergraduate Independent Study Units: 3-9 Lec/Sec: Section B Days: To be announced Begin: 09:00AM End: 11:50AM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Ficca '),\n",
       " Document(page_content='Spring offering: Course: 48599 Title: Undergraduate Independent Study Units: 3-9 Lec/Sec: Section C Days: To be announced Begin: 09:00AM End: 11:50AM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Arscott '),\n",
       " Document(page_content='Spring offering: Course: 48599 Title: Undergraduate Independent Study Units: 3-9 Lec/Sec: Section D Days: To be announced Begin: 09:00AM End: 11:50AM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Kurland '),\n",
       " Document(page_content='Spring offering: Course: 48599 Title: Undergraduate Independent Study Units: 3-9 Lec/Sec: Section E Days: To be announced Begin: 09:00AM End: 11:50AM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Lee '),\n",
       " Document(page_content='Spring offering: Course: 48614 Title: New Pedagogies: Units: 9.0 Lec/Sec: Section E Days: To be announced Begin: 09:00AM End: 11:50AM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Lee '),\n",
       " Document(page_content='Spring offering: Course: 48614 Title: Unsettling Ground- Retiring the God View Units: 9.0 Lec/Sec: Section A Days: Monday Begin: 10:00AM End: 12:50PM Bldg/Room: MM 303 Location: Pittsburgh, Pennsylvania Instructor(s): Sindi '),\n",
       " Document(page_content='Spring offering: Course: 48614 Title: New Pedagogies: Units: 9.0 Lec/Sec: Section A Days: Monday Begin: 10:00AM End: 12:50PM Bldg/Room: MM 303 Location: Pittsburgh, Pennsylvania Instructor(s): Sindi '),\n",
       " Document(page_content='Spring offering: Course: 48614 Title: Material Regeneration Units: 9.0 Lec/Sec: Section B Days: Wednesday Begin: 09:00AM End: 11:50AM Bldg/Room: MM 321 Location: Pittsburgh, Pennsylvania Instructor(s): Kwon '),\n",
       " Document(page_content='Spring offering: Course: 48614 Title: New Pedagogies: Units: 9.0 Lec/Sec: Section B Days: Wednesday Begin: 09:00AM End: 11:50AM Bldg/Room: MM 321 Location: Pittsburgh, Pennsylvania Instructor(s): Kwon '),\n",
       " Document(page_content='Spring offering: Course: 48614 Title: A Multiple-Making Approach to Inquiry in Craft + Computation Units: 9.0 Lec/Sec: Section C Days: Friday Begin: 09:00AM End: 12:20PM Bldg/Room: MM C4 Location: Pittsburgh, Pennsylvania Instructor(s): Noel '),\n",
       " Document(page_content='Spring offering: Course: 48616 Title: Carnival Gateway Special Project (CARNIVAL GATEWAY) Units: VAR Lec/Sec: Section A4 Days: Monday Begin: 06:30PM End: 07:50PM Bldg/Room: CFA 206A Location: Pittsburgh, Pennsylvania Instructor(s): Khan '),\n",
       " Document(page_content='Spring offering: Course: 48617 Title: Carnival Gateway Project Management Units: VAR Lec/Sec: Section A Days: Monday Begin: 06:30PM End: 07:50PM Bldg/Room: CFA 206A Location: Pittsburgh, Pennsylvania Instructor(s): Khan '),\n",
       " Document(page_content='Spring offering: Course: 48617 Title: Carnival Gateway Project Management Units: VAR Lec/Sec: Section B Days: Monday Begin: 06:30PM End: 07:50PM Bldg/Room: CFA 206A Location: Pittsburgh, Pennsylvania Instructor(s): Achnani '),\n",
       " Document(page_content='Spring offering: Course: 48618 Title: Discourse and Praxis in the Climate Emergency Units: VAR Lec/Sec: Section A3 Days: Monday Begin: 11:00AM End: 12:20PM Bldg/Room: MM 307 Location: Pittsburgh, Pennsylvania Instructor(s): Anklesaria '),\n",
       " Document(page_content='Spring offering: Course: 48619 Title: Machine Intelligence, Cybernetics and Design Units: 9.0 Lec/Sec: Section A Days: Wednesday, Friday Begin: 02:00PM End: 03:20PM Bldg/Room: MM 307 Location: Pittsburgh, Pennsylvania Instructor(s): Temizel, Pangaro '),\n",
       " Document(page_content='Spring offering: Course: 48638 Title: Structural Design 2: Materials and Analysis Units: 9.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 02:00PM End: 03:20PM Bldg/Room: WEH 4707 Location: Pittsburgh, Pennsylvania Instructor(s): Lee '),\n",
       " Document(page_content='Spring offering: Course: 48640 Title: M.Arch Studio: Praxis II: Units: 18.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 02:00PM End: 03:20PM Bldg/Room: WEH 4707 Location: Pittsburgh, Pennsylvania Instructor(s): Lee '),\n",
       " Document(page_content='Spring offering: Course: 48640 Title: Praxis 2: Units: 18.0 Lec/Sec: Section A Days: Monday, Wednesday, Friday Begin: 02:00PM End: 04:50PM Bldg/Room: MM 320 Location: Pittsburgh, Pennsylvania Instructor(s): Davis, Huber, Sawyer '),\n",
       " Document(page_content='Spring offering: Course: 48641 Title: History of Modern Architecture Units: VAR Lec/Sec: Section A Days: Monday, Wednesday, Friday Begin: 10:00AM End: 10:50AM Bldg/Room: MM A14 Location: Pittsburgh, Pennsylvania Instructor(s): Gutschow '),\n",
       " Document(page_content='Spring offering: Course: 48642 Title: History of Asian Architecture Units: 9.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 11:00AM End: 12:20PM Bldg/Room: CMU REMOTE Location: Pittsburgh, Pennsylvania Instructor(s): Chen '),\n",
       " Document(page_content='Spring offering: Course: 48644 Title: M.Arch Pre-Thesis Units: 3.0 Lec/Sec: Section A Days: Friday Begin: 09:00AM End: 09:50AM Bldg/Room: MM 321 Location: Pittsburgh, Pennsylvania Instructor(s): Goral '),\n",
       " Document(page_content='Spring offering: Course: 48647 Title: Materiality and Construction Systems Units: 9.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 09:30AM End: 10:50AM Bldg/Room: POS 146 Location: Pittsburgh, Pennsylvania Instructor(s): Ficca '),\n",
       " Document(page_content='Spring offering: Course: 48648 Title: Ethics and Decision Making in Architecture Units: 6.0 Lec/Sec: Section A Days: Tuesday Begin: 04:00PM End: 05:50PM Bldg/Room: WEH 5403 Location: Pittsburgh, Pennsylvania Instructor(s): Vavasis '),\n",
       " Document(page_content='Spring offering: Course: 48649 Title: Design Leadership Units: 6.0 Lec/Sec: Section A Days: Thursday Begin: 09:00AM End: 10:50AM Bldg/Room: CFA 206A Location: Pittsburgh, Pennsylvania Instructor(s): Krissel '),\n",
       " Document(page_content='Spring offering: Course: 48658 Title: Constructing Value(s): Economies of Design Units: VAR Lec/Sec: Section A Days: Tuesday Begin: 06:00PM End: 07:50PM Bldg/Room: CFA 214 Location: Pittsburgh, Pennsylvania Instructor(s): Darga Ozutemiz '),\n",
       " Document(page_content='Spring offering: Course: 48660 Title: Advanced Synthesis Options Studio II: M.Arch Units: 18.0 Lec/Sec: Lecture Days: Tuesday, Thursday Begin: 01:00PM End: 04:50PM Bldg/Room: MM 312 Location: Pittsburgh, Pennsylvania Instructor(s): Anklesaria '),\n",
       " Document(page_content='Spring offering: Course: 48660 Title: Advanced Synthesis Options Studio II: M.Arch Units: 18.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 01:00PM End: 04:50PM Bldg/Room: MM 312 Location: Pittsburgh, Pennsylvania Instructor(s): Bista '),\n",
       " Document(page_content='Spring offering: Course: 48660 Title: Advanced Synthesis Options Studio II: M.Arch Units: 18.0 Lec/Sec: Section B Days: Tuesday, Thursday Begin: 01:00PM End: 04:50PM Bldg/Room: MM 312 Location: Pittsburgh, Pennsylvania Instructor(s): Bizon '),\n",
       " Document(page_content='Spring offering: Course: 48660 Title: Advanced Synthesis Options Studio II: M.Arch Units: 18.0 Lec/Sec: Section C Days: Tuesday, Thursday Begin: 01:00PM End: 04:50PM Bldg/Room: MM 312 Location: Pittsburgh, Pennsylvania Instructor(s): Cupkova '),\n",
       " Document(page_content='Spring offering: Course: 48660 Title: Advanced Synthesis Options Studio II: M.Arch Units: 18.0 Lec/Sec: Section D Days: Tuesday, Thursday Begin: 01:00PM End: 04:50PM Bldg/Room: MM 312 Location: Pittsburgh, Pennsylvania Instructor(s): Damiani '),\n",
       " Document(page_content='Spring offering: Course: 48660 Title: Advanced Synthesis Options Studio II: M.Arch Units: 18.0 Lec/Sec: Section E Days: Tuesday, Thursday Begin: 01:00PM End: 04:50PM Bldg/Room: CFA 200 Location: Pittsburgh, Pennsylvania Instructor(s): Garofalo '),\n",
       " Document(page_content='Spring offering: Course: 48660 Title: Advanced Synthesis Options Studio II: M.Arch Units: 18.0 Lec/Sec: Section F Days: Tuesday, Thursday Begin: 01:00PM End: 04:50PM Bldg/Room: MM 312 Location: Pittsburgh, Pennsylvania Instructor(s): Hayes '),\n",
       " Document(page_content='Spring offering: Course: 48660 Title: Advanced Synthesis Options Studio II: M.Arch Units: 18.0 Lec/Sec: Section G Days: Tuesday, Thursday Begin: 01:00PM End: 04:50PM Bldg/Room: CFA 200 Location: Pittsburgh, Pennsylvania Instructor(s): Lee '),\n",
       " Document(page_content='Spring offering: Course: 48667 Title: Material Histories Units: 9.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 11:00AM End: 12:20PM Bldg/Room: MM 409 Location: Pittsburgh, Pennsylvania Instructor(s): Torello '),\n",
       " Document(page_content='Spring offering: Course: 48670 Title: MArch Thesis Units: 18.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 01:00PM End: 04:50PM Bldg/Room: MM 320 Location: Pittsburgh, Pennsylvania Instructor(s): Rafson '),\n",
       " Document(page_content='Spring offering: Course: 48677 Title: Hines Competition Units: 3.0 Lec/Sec: Section A3 Days: To be announced Begin: 01:00PM End: 04:50PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Danes '),\n",
       " Document(page_content='Spring offering: Course: 48685 Title: Design and Documentation in Revit Units: VAR Lec/Sec: Section A Days: Monday Begin: 09:30AM End: 10:50AM Bldg/Room: MM A11 Location: Pittsburgh, Pennsylvania Instructor(s): Sawyer '),\n",
       " Document(page_content='Spring offering: Course: 48692 Title: Shaping Daylight Through Simulation and Virtual Reality Units: 9.0 Lec/Sec: Section A Days: Wednesday Begin: 10:00AM End: 12:20PM Bldg/Room: MM 415IW Location: Pittsburgh, Pennsylvania Instructor(s): Sawyer '),\n",
       " Document(page_content='Spring offering: Course: 48700 Title: Practicum Units: 0-36 Lec/Sec: Section A Days: To be announced Begin: 10:00AM End: 12:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Cupkova '),\n",
       " Document(page_content='Spring offering: Course: 48700 Title: Practicum: Units: 0-36 Lec/Sec: Section A Days: To be announced Begin: 10:00AM End: 12:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Cupkova '),\n",
       " Document(page_content='Spring offering: Course: 48700 Title: TBA Units: 0-36 Lec/Sec: Section B Days: To be announced Begin: 10:00AM End: 12:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Instructor TBA '),\n",
       " Document(page_content='Spring offering: Course: 48700 Title: TBA Units: 0-36 Lec/Sec: Section C Days: To be announced Begin: 10:00AM End: 12:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Instructor TBA '),\n",
       " Document(page_content='Spring offering: Course: 48700 Title: TBA Units: 0-36 Lec/Sec: Section D Days: To be announced Begin: 10:00AM End: 12:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Cupkova '),\n",
       " Document(page_content='Spring offering: Course: 48700 Title: TBA Units: 0-36 Lec/Sec: Section E Days: To be announced Begin: 10:00AM End: 12:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Instructor TBA '),\n",
       " Document(page_content='Spring offering: Course: 48700 Title: TBA Units: 0-36 Lec/Sec: Section F Days: To be announced Begin: 10:00AM End: 12:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Instructor TBA '),\n",
       " Document(page_content=\"Spring offering: Course: 48703 Title: Master's Project Units: 18-36 Lec/Sec: Section A Days: To be announced Begin: 10:00AM End: 12:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Cardoso Llach \"),\n",
       " Document(page_content=\"Spring offering: Course: 48703 Title: Master's Project Units: 18-36 Lec/Sec: Section B Days: To be announced Begin: 10:00AM End: 12:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Cochran \"),\n",
       " Document(page_content=\"Spring offering: Course: 48703 Title: Master's Project Units: 18-36 Lec/Sec: Section C Days: To be announced Begin: 10:00AM End: 12:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Kim \"),\n",
       " Document(page_content=\"Spring offering: Course: 48703 Title: Master's Project Units: 18-36 Lec/Sec: Section D Days: To be announced Begin: 10:00AM End: 12:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Krishnamurti \"),\n",
       " Document(page_content=\"Spring offering: Course: 48703 Title: Master's Project Units: 18-36 Lec/Sec: Section E Days: To be announced Begin: 10:00AM End: 12:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Loftness \"),\n",
       " Document(page_content=\"Spring offering: Course: 48703 Title: Master's Project Units: 18-36 Lec/Sec: Section F Days: To be announced Begin: 10:00AM End: 12:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Instructor TBA \"),\n",
       " Document(page_content=\"Spring offering: Course: 48703 Title: Master's Project Units: 18-36 Lec/Sec: Section G Days: To be announced Begin: 10:00AM End: 12:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Instructor TBA \"),\n",
       " Document(page_content=\"Spring offering: Course: 48703 Title: Master's Project Units: 18-36 Lec/Sec: Section H Days: To be announced Begin: 10:00AM End: 12:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Instructor TBA \"),\n",
       " Document(page_content='Spring offering: Course: 48704 Title: Internship (MS & PhD) Units: 0-18 Lec/Sec: Section A Days: To be announced Begin: 10:00AM End: 12:20PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Instructor TBA '),\n",
       " Document(page_content='Spring offering: Course: 48704 Title: Internship (MS & PhD) Units: 0-18 Lec/Sec: Section I Days: To be announced Begin: 10:00AM End: 12:20PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Loftness '),\n",
       " Document(page_content='Spring offering: Course: 48704 Title: Internship (MS & PhD) Units: 0-18 Lec/Sec: Section R Days: To be announced Begin: 10:00AM End: 12:20PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Lee '),\n",
       " Document(page_content='Spring offering: Course: 48706 Title: Urban Design Studio II: Urban Systems Units: 18.0 Lec/Sec: Section A Days: Monday, Wednesday, Friday Begin: 02:00PM End: 04:50PM Bldg/Room: MM 320 Location: Pittsburgh, Pennsylvania Instructor(s): Rehman, Brooks Takahashi '),\n",
       " Document(page_content='Spring offering: Course: 48708 Title: Urban Design Studio IV: Units: 18.0 Lec/Sec: Section A Days: Monday, Wednesday, Friday Begin: 02:00PM End: 04:50PM Bldg/Room: MM 320 Location: Pittsburgh, Pennsylvania Instructor(s): Rehman, Brooks Takahashi '),\n",
       " Document(page_content='Spring offering: Course: 48708 Title: Commoning the City Units: 18.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 01:00PM End: 04:50PM Bldg/Room: MM 312 Location: Pittsburgh, Pennsylvania Instructor(s): Kline '),\n",
       " Document(page_content='Spring offering: Course: 48711 Title: Paradigms of Research in Architecture Units: VAR Lec/Sec: Section A Days: Monday, Wednesday Begin: 08:30AM End: 09:50AM Bldg/Room: MM 415IW Location: Pittsburgh, Pennsylvania Instructor(s): Lee '),\n",
       " Document(page_content='Spring offering: Course: 48712 Title: Graduate Seminar 2: Issue of Global Urbanization Units: 3-9 Lec/Sec: Section A Days: Wednesday Begin: 07:00PM End: 08:20PM Bldg/Room: MM 415IW Location: Pittsburgh, Pennsylvania Instructor(s): Nisa '),\n",
       " Document(page_content='Spring offering: Course: 48713 Title: MUD Urban Ecology Units: 9.0 Lec/Sec: Section A Days: Tuesday Begin: 06:00PM End: 08:50PM Bldg/Room: MM 415IW Location: Pittsburgh, Pennsylvania Instructor(s): Mondor '),\n",
       " Document(page_content='Spring offering: Course: 48715 Title: MSCD Pre-Thesis 1 Units: 6.0 Lec/Sec: Section A Days: Wednesday Begin: 10:00AM End: 11:50AM Bldg/Room: MM 307 Location: Pittsburgh, Pennsylvania Instructor(s): Byrne, Erdolu '),\n",
       " Document(page_content='Spring offering: Course: 48721 Title: Building Controls and Diagnostics Units: 12.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 11:00AM End: 12:20PM Bldg/Room: GHC 4101 Location: Pittsburgh, Pennsylvania Instructor(s): Zhao '),\n",
       " Document(page_content='Spring offering: Course: 48722 Title: Building Performance Modeling Units: VAR Lec/Sec: Section A Days: Tuesday, Thursday Begin: 02:00PM End: 03:20PM Bldg/Room: MM 415IW Location: Pittsburgh, Pennsylvania Instructor(s): Liang '),\n",
       " Document(page_content='Spring offering: Course: 48731 Title: Sustainable Design Synthesis Prep Units: 1-18 Lec/Sec: Section A Days: To be announced Begin: 02:00PM End: 03:20PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Cupkova '),\n",
       " Document(page_content='Spring offering: Course: 48732 Title: Sustainable Design Synthesis Units: 01224 Lec/Sec: Section A Days: Friday Begin: 10:00AM End: 12:20PM Bldg/Room: MM 415IW Location: Pittsburgh, Pennsylvania Instructor(s): Cupkova, Suarez '),\n",
       " Document(page_content='Spring offering: Course: 48735 Title: Modern Mexico & Guatemala: 19th-21st Century Architecture (MOD MEX) Units: 9.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 03:30PM End: 04:50PM Bldg/Room: CFA 102 Location: Pittsburgh, Pennsylvania Instructor(s): Shaw '),\n",
       " Document(page_content=\"Spring offering: Course: 48736 Title: Master's Independent Study Units: 0-99 Lec/Sec: Section A Days: To be announced Begin: 03:30PM End: 04:50PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Sawyer \"),\n",
       " Document(page_content=\"Spring offering: Course: 48736 Title: Master's Independent Study: Units: 0-99 Lec/Sec: Section A Days: To be announced Begin: 03:30PM End: 04:50PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Sawyer \"),\n",
       " Document(page_content='Spring offering: Course: 48736 Title: TBA Units: 0-99 Lec/Sec: Section B Days: To be announced Begin: 03:30PM End: 04:50PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Baird '),\n",
       " Document(page_content='Spring offering: Course: 48736 Title: TBA Units: 0-99 Lec/Sec: Section D Days: To be announced Begin: 03:30PM End: 04:50PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Kim '),\n",
       " Document(page_content=\"Spring offering: Course: 48736 Title: Master's Independent Study: Units: 0-99 Lec/Sec: Section D Days: To be announced Begin: 03:30PM End: 04:50PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Kim \"),\n",
       " Document(page_content='Spring offering: Course: 48736 Title: TBA Units: 0-99 Lec/Sec: Section E Days: To be announced Begin: 03:30PM End: 04:50PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Cochran '),\n",
       " Document(page_content='Spring offering: Course: 48736 Title: TBA Units: 0-99 Lec/Sec: Section F Days: To be announced Begin: 03:30PM End: 04:50PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Cupkova '),\n",
       " Document(page_content='Spring offering: Course: 48736 Title: TBA Units: 0-99 Lec/Sec: Section G Days: To be announced Begin: 03:30PM End: 04:50PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Gruber '),\n",
       " Document(page_content='Spring offering: Course: 48736 Title: TBA Units: 0-99 Lec/Sec: Section H Days: To be announced Begin: 03:30PM End: 04:50PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Cardoso Llach '),\n",
       " Document(page_content='Spring offering: Course: 48736 Title: TBA Units: 0-99 Lec/Sec: Section I Days: To be announced Begin: 03:30PM End: 04:50PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Huber '),\n",
       " Document(page_content='Spring offering: Course: 48736 Title: TBA Units: 0-99 Lec/Sec: Section J Days: To be announced Begin: 03:30PM End: 04:50PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Instructor TBA '),\n",
       " Document(page_content='Spring offering: Course: 48736 Title: TBA Units: 0-99 Lec/Sec: Section K Days: To be announced Begin: 03:30PM End: 04:50PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Kurland '),\n",
       " Document(page_content='Spring offering: Course: 48736 Title: TBA Units: 0-99 Lec/Sec: Section L Days: To be announced Begin: 03:30PM End: 04:50PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Byrne '),\n",
       " Document(page_content='Spring offering: Course: 48736 Title: TBA Units: 0-99 Lec/Sec: Section M Days: To be announced Begin: 03:30PM End: 04:50PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Ficca '),\n",
       " Document(page_content='Spring offering: Course: 48736 Title: TBA Units: 0-99 Lec/Sec: Section N Days: To be announced Begin: 03:30PM End: 04:50PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Instructor TBA '),\n",
       " Document(page_content='Spring offering: Course: 48736 Title: TBA Units: 0-99 Lec/Sec: Section O Days: To be announced Begin: 03:30PM End: 04:50PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Bard '),\n",
       " Document(page_content='Spring offering: Course: 48736 Title: TBA Units: 0-99 Lec/Sec: Section R Days: To be announced Begin: 03:30PM End: 04:50PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Pangaro '),\n",
       " Document(page_content='Spring offering: Course: 48736 Title: TBA Units: 0-99 Lec/Sec: Section S Days: To be announced Begin: 03:30PM End: 04:50PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Instructor TBA '),\n",
       " Document(page_content='Spring offering: Course: 48736 Title: TBA Units: 0-99 Lec/Sec: Section V Days: To be announced Begin: 03:30PM End: 04:50PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Loftness '),\n",
       " Document(page_content='Spring offering: Course: 48736 Title: TBA Units: 0-99 Lec/Sec: Section X Days: To be announced Begin: 03:30PM End: 04:50PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Kim '),\n",
       " Document(page_content='Spring offering: Course: 48737 Title: Detailing Architecture Units: 9.0 Lec/Sec: Section A Days: Friday Begin: 09:30AM End: 12:20PM Bldg/Room: MM 307 Location: Pittsburgh, Pennsylvania Instructor(s): Damiani '),\n",
       " Document(page_content='Spring offering: Course: 48745 Title: Design Fabrication Units: 9.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 09:30AM End: 10:50AM Bldg/Room: MM C4 Location: Pittsburgh, Pennsylvania Instructor(s): Patel '),\n",
       " Document(page_content='Spring offering: Course: 48752 Title: Zero Energy Housing Units: 9.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 03:30PM End: 04:50PM Bldg/Room: MM 415IW Location: Pittsburgh, Pennsylvania Instructor(s): Baird '),\n",
       " Document(page_content='Spring offering: Course: 48756 Title: Project Planning and Reporting Units: VAR Lec/Sec: Section A Days: Friday Begin: 02:00PM End: 04:50PM Bldg/Room: PH 226A Location: Pittsburgh, Pennsylvania Instructor(s): Hameen '),\n",
       " Document(page_content='Spring offering: Course: 48759 Title: Value Based Design Introduction Units: VAR Lec/Sec: Section A Days: Monday, Wednesday Begin: 02:00PM End: 03:20PM Bldg/Room: MM 409 Location: Pittsburgh, Pennsylvania Instructor(s): Bates '),\n",
       " Document(page_content='Spring offering: Course: 48769 Title: Thesis/Project Units: VAR Lec/Sec: Section A Days: To be announced Begin: 02:00PM End: 03:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Sawyer '),\n",
       " Document(page_content='Spring offering: Course: 48769 Title: Thesis/Project Units: VAR Lec/Sec: Section B Days: To be announced Begin: 02:00PM End: 03:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Byrne '),\n",
       " Document(page_content='Spring offering: Course: 48769 Title: Thesis/Project Units: VAR Lec/Sec: Section C Days: To be announced Begin: 02:00PM End: 03:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Cardoso Llach '),\n",
       " Document(page_content='Spring offering: Course: 48769 Title: Thesis/Project Units: VAR Lec/Sec: Section D Days: To be announced Begin: 02:00PM End: 03:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Cupkova '),\n",
       " Document(page_content='Spring offering: Course: 48769 Title: Thesis/Project Units: VAR Lec/Sec: Section E Days: To be announced Begin: 02:00PM End: 03:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Cochran '),\n",
       " Document(page_content='Spring offering: Course: 48769 Title: Thesis/Project Units: VAR Lec/Sec: Section F Days: To be announced Begin: 02:00PM End: 03:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Bard '),\n",
       " Document(page_content='Spring offering: Course: 48769 Title: Thesis/Project Units: VAR Lec/Sec: Section G Days: To be announced Begin: 02:00PM End: 03:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Gruber '),\n",
       " Document(page_content='Spring offering: Course: 48769 Title: Thesis/Project Units: VAR Lec/Sec: Section M Days: To be announced Begin: 02:00PM End: 03:20PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Noel '),\n",
       " Document(page_content='Spring offering: Course: 48769 Title: Thesis/Project Units: VAR Lec/Sec: Section V Days: To be announced Begin: 02:00PM End: 03:20PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Loftness '),\n",
       " Document(page_content='Spring offering: Course: 48773 Title: Urban Design Media: Emerging Media Units: 9.0 Lec/Sec: Section A Days: Tuesday Begin: 11:00AM End: 12:20PM Bldg/Room: PH A19C Location: Pittsburgh, Pennsylvania Instructor(s): Li '),\n",
       " Document(page_content='Spring offering: Course: 48776 Title: MAAD Advanced Synthesis Options Studio II Units: 18.0 Lec/Sec: Lecture Days: Tuesday, Thursday Begin: 01:00PM End: 04:20PM Bldg/Room: MM 312 Location: Pittsburgh, Pennsylvania Instructor(s): Ficca '),\n",
       " Document(page_content='Spring offering: Course: 48776 Title: MAAD Advanced Synthesis Options Studio II Units: 18.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 01:00PM End: 04:50PM Bldg/Room: MM 312 Location: Pittsburgh, Pennsylvania Instructor(s): Bista '),\n",
       " Document(page_content='Spring offering: Course: 48776 Title: MAAD Advanced Synthesis Options Studio II Units: 18.0 Lec/Sec: Section B Days: Tuesday, Thursday Begin: 01:00PM End: 04:50PM Bldg/Room: MM 312 Location: Pittsburgh, Pennsylvania Instructor(s): Bizon '),\n",
       " Document(page_content='Spring offering: Course: 48776 Title: MAAD Advanced Synthesis Options Studio II Units: 18.0 Lec/Sec: Section C Days: Tuesday, Thursday Begin: 01:00PM End: 04:50PM Bldg/Room: MM 312 Location: Pittsburgh, Pennsylvania Instructor(s): Cupkova '),\n",
       " Document(page_content='Spring offering: Course: 48776 Title: MAAD Advanced Synthesis Options Studio II Units: 18.0 Lec/Sec: Section D Days: Tuesday, Thursday Begin: 01:00PM End: 04:50PM Bldg/Room: MM 312 Location: Pittsburgh, Pennsylvania Instructor(s): Damiani '),\n",
       " Document(page_content='Spring offering: Course: 48776 Title: MAAD Advanced Synthesis Options Studio II Units: 18.0 Lec/Sec: Section E Days: Tuesday, Thursday Begin: 01:00PM End: 04:50PM Bldg/Room: CFA 200 Location: Pittsburgh, Pennsylvania Instructor(s): Garofalo '),\n",
       " Document(page_content='Spring offering: Course: 48776 Title: MAAD Advanced Synthesis Options Studio II Units: 18.0 Lec/Sec: Section F Days: Tuesday, Thursday Begin: 01:00PM End: 04:50PM Bldg/Room: CFA 200 Location: Pittsburgh, Pennsylvania Instructor(s): Hayes '),\n",
       " Document(page_content='Spring offering: Course: 48776 Title: MAAD Advanced Synthesis Options Studio II Units: 18.0 Lec/Sec: Section G Days: Tuesday, Thursday Begin: 01:00PM End: 04:50PM Bldg/Room: CFA 200 Location: Pittsburgh, Pennsylvania Instructor(s): Lee '),\n",
       " Document(page_content='Spring offering: Course: 48781 Title: Spatial Analysis in Infrastructure Planning Units: VAR Lec/Sec: Section A Days: To be announced Begin: 01:00PM End: 04:50PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Kurland '),\n",
       " Document(page_content='Spring offering: Course: 48785 Title: MAAD Research by Design Project Units: 18.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 01:00PM End: 04:50PM Bldg/Room: MM 312 Location: Pittsburgh, Pennsylvania Instructor(s): Rafson '),\n",
       " Document(page_content='Spring offering: Course: 48787 Title: LEED, Green Infrastructure and Community Rating in Global Context Units: 6.0 Lec/Sec: Section A4 Days: Monday, Wednesday Begin: 11:00AM End: 12:20PM Bldg/Room: CFA 102 Location: Pittsburgh, Pennsylvania Instructor(s): Baird '),\n",
       " Document(page_content='Spring offering: Course: 48791 Title: M.S. Project Units: 0-99 Lec/Sec: Section A Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Instructor TBA '),\n",
       " Document(page_content='Spring offering: Course: 48791 Title: M.S. Project Units: 0-99 Lec/Sec: Section B Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Instructor TBA '),\n",
       " Document(page_content='Spring offering: Course: 48791 Title: M.S. Project Units: 0-99 Lec/Sec: Section C Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Instructor TBA '),\n",
       " Document(page_content='Spring offering: Course: 48791 Title: M.S. Project Units: 0-99 Lec/Sec: Section D Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Instructor TBA '),\n",
       " Document(page_content='Spring offering: Course: 48792 Title: Ph.D. Independent Study Units: 0-99 Lec/Sec: Section A Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Sawyer '),\n",
       " Document(page_content='Spring offering: Course: 48792 Title: Ph.D. Independent Study Units: 0-99 Lec/Sec: Section C Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Cardoso Llach '),\n",
       " Document(page_content='Spring offering: Course: 48792 Title: Ph.D. Independent Study Units: 0-99 Lec/Sec: Section D Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Cupkova '),\n",
       " Document(page_content='Spring offering: Course: 48792 Title: Ph.D. Independent Study Units: 0-99 Lec/Sec: Section E Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Cochran '),\n",
       " Document(page_content='Spring offering: Course: 48792 Title: Ph.D. Independent Study Units: 0-99 Lec/Sec: Section G Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Gruber '),\n",
       " Document(page_content='Spring offering: Course: 48792 Title: Ph.D. Independent Study Units: 0-99 Lec/Sec: Section H Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Damiani '),\n",
       " Document(page_content='Spring offering: Course: 48792 Title: Ph.D. Independent Study Units: 0-99 Lec/Sec: Section I Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Baird '),\n",
       " Document(page_content='Spring offering: Course: 48792 Title: Ph.D. Independent Study Units: 0-99 Lec/Sec: Section J Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Bard '),\n",
       " Document(page_content='Spring offering: Course: 48792 Title: Ph.D. Independent Study Units: 0-99 Lec/Sec: Section K Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Gutschow '),\n",
       " Document(page_content='Spring offering: Course: 48792 Title: Ph.D. Independent Study Units: 0-99 Lec/Sec: Section L Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Lee '),\n",
       " Document(page_content='Spring offering: Course: 48792 Title: Ph.D. Independent Study Units: 0-99 Lec/Sec: Section M Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Kim '),\n",
       " Document(page_content='Spring offering: Course: 48792 Title: Ph.D. Independent Study Units: 0-99 Lec/Sec: Section N Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Rehman '),\n",
       " Document(page_content='Spring offering: Course: 48792 Title: Ph.D. Independent Study Units: 0-99 Lec/Sec: Section O Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Byrne '),\n",
       " Document(page_content='Spring offering: Course: 48792 Title: Ph.D. Independent Study Units: 0-99 Lec/Sec: Section P Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Instructor TBA '),\n",
       " Document(page_content='Spring offering: Course: 48792 Title: Ph.D. Independent Study Units: 0-99 Lec/Sec: Section R Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Cooper '),\n",
       " Document(page_content='Spring offering: Course: 48792 Title: Ph.D. Independent Study Units: 0-99 Lec/Sec: Section S Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Shaw '),\n",
       " Document(page_content='Spring offering: Course: 48792 Title: Ph.D. Independent Study Units: 0-99 Lec/Sec: Section V Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Loftness '),\n",
       " Document(page_content='Spring offering: Course: 48792 Title: Ph.D. Independent Study Units: 0-99 Lec/Sec: Section X Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Krishnamurti '),\n",
       " Document(page_content='Spring offering: Course: 48793 Title: Ph.D. Thesis Units: 0-99 Lec/Sec: Section A Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Sawyer '),\n",
       " Document(page_content='Spring offering: Course: 48793 Title: Ph.D. Thesis Units: 0-99 Lec/Sec: Section B Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Byrne '),\n",
       " Document(page_content='Spring offering: Course: 48793 Title: Ph.D. Thesis Units: 0-99 Lec/Sec: Section C Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Cardoso Llach '),\n",
       " Document(page_content='Spring offering: Course: 48793 Title: Ph.D. Thesis Units: 0-99 Lec/Sec: Section D Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Cupkova '),\n",
       " Document(page_content='Spring offering: Course: 48793 Title: Ph.D. Thesis Units: 0-99 Lec/Sec: Section E Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Cochran '),\n",
       " Document(page_content='Spring offering: Course: 48793 Title: Ph.D. Thesis Units: 0-99 Lec/Sec: Section F Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Ficca '),\n",
       " Document(page_content='Spring offering: Course: 48793 Title: Ph.D. Thesis Units: 0-99 Lec/Sec: Section G Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Gruber '),\n",
       " Document(page_content='Spring offering: Course: 48793 Title: Ph.D. Thesis Units: 0-99 Lec/Sec: Section H Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Damiani '),\n",
       " Document(page_content='Spring offering: Course: 48793 Title: Ph.D. Thesis Units: 0-99 Lec/Sec: Section I Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Baird '),\n",
       " Document(page_content='Spring offering: Course: 48793 Title: Ph.D. Thesis Units: 0-99 Lec/Sec: Section J Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Bard '),\n",
       " Document(page_content='Spring offering: Course: 48793 Title: Ph.D. Thesis Units: 0-99 Lec/Sec: Section K Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Gutschow '),\n",
       " Document(page_content='Spring offering: Course: 48793 Title: Ph.D. Thesis Units: 0-99 Lec/Sec: Section V Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Loftness '),\n",
       " Document(page_content='Spring offering: Course: 48793 Title: Ph.D. Thesis Units: 0-99 Lec/Sec: Section X Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Krishnamurti '),\n",
       " Document(page_content='Spring offering: Course: 48795 Title: LEED: Units: 6.0 Lec/Sec: Section X Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Krishnamurti '),\n",
       " Document(page_content='Spring offering: Course: 48795 Title: LEED, Green Design and Building Ratings Units: 6.0 Lec/Sec: Section A3 Days: Monday, Wednesday Begin: 11:00AM End: 12:20PM Bldg/Room: CFA 102 Location: Pittsburgh, Pennsylvania Instructor(s): Baird '),\n",
       " Document(page_content='Spring offering: Course: 48797 Title: PhD Dissertation Defense Units: 5.0,36 Lec/Sec: Section A Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Sawyer '),\n",
       " Document(page_content='Spring offering: Course: 48797 Title: PhD Dissertation Defense Units: 5.0,36 Lec/Sec: Section B Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Byrne '),\n",
       " Document(page_content='Spring offering: Course: 48797 Title: PhD Dissertation Defense Units: 5.0,36 Lec/Sec: Section C Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Cardoso Llach '),\n",
       " Document(page_content='Spring offering: Course: 48797 Title: PhD Dissertation Defense Units: 5.0,36 Lec/Sec: Section D Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Cupkova '),\n",
       " Document(page_content='Spring offering: Course: 48797 Title: PhD Dissertation Defense Units: 5.0,36 Lec/Sec: Section E Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Cochran '),\n",
       " Document(page_content='Spring offering: Course: 48797 Title: PhD Dissertation Defense Units: 5.0,36 Lec/Sec: Section F Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Baird '),\n",
       " Document(page_content='Spring offering: Course: 48797 Title: PhD Dissertation Defense Units: 5.0,36 Lec/Sec: Section G Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Gruber '),\n",
       " Document(page_content='Spring offering: Course: 48797 Title: PhD Dissertation Defense Units: 5.0,36 Lec/Sec: Section X Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Krishnamurti '),\n",
       " Document(page_content='Spring offering: Course: 60101 Title: Foundations: Risk, Agency, Failure Units: 10.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 08:00AM End: 10:50AM Bldg/Room: DH B309 Location: Pittsburgh, Pennsylvania Instructor(s): Ray '),\n",
       " Document(page_content='Spring offering: Course: 60101 Title: Foundations: Risk, Agency, Failure Units: 10.0 Lec/Sec: Section B Days: Monday, Wednesday Begin: 02:00PM End: 04:50PM Bldg/Room: DH B309 Location: Pittsburgh, Pennsylvania Instructor(s): Andrew '),\n",
       " Document(page_content='Spring offering: Course: 60101 Title: Foundations: Risk, Agency, Failure Units: 10.0 Lec/Sec: Section C Days: Tuesday, Thursday Begin: 02:00PM End: 04:50PM Bldg/Room: DH B309 Location: Pittsburgh, Pennsylvania Instructor(s): Washko '),\n",
       " Document(page_content='Spring offering: Course: 60106 Title: Cultural History of the Visual Arts - the Modern Period Units: 9.0 Lec/Sec: Section A Days: Monday Begin: 07:00PM End: 09:20PM Bldg/Room: PH A19 Location: Pittsburgh, Pennsylvania Instructor(s): Versari '),\n",
       " Document(page_content='Spring offering: Course: 60106 Title: Cultural History of the Visual Arts - the Modern Period Units: 9.0 Lec/Sec: Section B Days: Wednesday Begin: 07:00PM End: 09:20PM Bldg/Room: PH A19 Location: Pittsburgh, Pennsylvania Instructor(s): Versari '),\n",
       " Document(page_content='Spring offering: Course: 60107 Title: Foundations: Critical Studies Units: 9.0 Lec/Sec: Section A Days: Tuesday Begin: 07:00PM End: 09:20PM Bldg/Room: WEH 5403 Location: Pittsburgh, Pennsylvania Instructor(s): Kim '),\n",
       " Document(page_content='Spring offering: Course: 60107 Title: Foundations: Critical Studies Units: 9.0 Lec/Sec: Section B Days: Wednesday Begin: 07:00PM End: 09:20PM Bldg/Room: WEH 5310 Location: Pittsburgh, Pennsylvania Instructor(s): Kim '),\n",
       " Document(page_content='Spring offering: Course: 60120 Title: Foundations: Digital Media Units: 10.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 02:00PM End: 04:50PM Bldg/Room: CFA 318 Location: Pittsburgh, Pennsylvania Instructor(s): Pedercini '),\n",
       " Document(page_content='Spring offering: Course: 60120 Title: Foundations: Digital Media Units: 10.0 Lec/Sec: Section B Days: Tuesday, Thursday Begin: 02:00PM End: 04:50PM Bldg/Room: WEH 5202 Location: Pittsburgh, Pennsylvania Instructor(s): Levin '),\n",
       " Document(page_content='Spring offering: Course: 60125 Title: IDeATe: Introduction to 3D Animation Pipeline Units: 12.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 12:30PM End: 01:50PM Bldg/Room: CFA 317 Location: Pittsburgh, Pennsylvania Instructor(s): Mahler '),\n",
       " Document(page_content='Spring offering: Course: 60135 Title: Foundations: Sculpture II Units: 10.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 08:00AM End: 10:50AM Bldg/Room: DH C200 Location: Pittsburgh, Pennsylvania Instructor(s): Ku '),\n",
       " Document(page_content='Spring offering: Course: 60135 Title: Foundations: Sculpture II Units: 10.0 Lec/Sec: Section B Days: Monday, Wednesday Begin: 02:00PM End: 04:50PM Bldg/Room: DH C200 Location: Pittsburgh, Pennsylvania Instructor(s): Ku '),\n",
       " Document(page_content='Spring offering: Course: 60135 Title: Foundations: Sculpture II Units: 10.0 Lec/Sec: Section C Days: Tuesday, Thursday Begin: 08:00AM End: 10:50AM Bldg/Room: DH C200 Location: Pittsburgh, Pennsylvania Instructor(s): Ransom '),\n",
       " Document(page_content='Spring offering: Course: 60136 Title: Ceramics for Non-Majors Units: 10.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 02:00PM End: 04:50PM Bldg/Room: DH B301 Location: Pittsburgh, Pennsylvania Instructor(s): Sekino-Bove '),\n",
       " Document(page_content='Spring offering: Course: 60141 Title: Black and White Photography I Units: 10.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 08:00AM End: 10:50AM Bldg/Room: MM B10D Location: Pittsburgh, Pennsylvania Instructor(s): Antonelli '),\n",
       " Document(page_content='Spring offering: Course: 60141 Title: Black and White Photography I Units: 10.0 Lec/Sec: Section B Days: Monday, Wednesday Begin: 02:00PM End: 04:50PM Bldg/Room: MM B10D Location: Pittsburgh, Pennsylvania Instructor(s): Carroll '),\n",
       " Document(page_content='Spring offering: Course: 60141 Title: Black and White Photography I Units: 10.0 Lec/Sec: Section C Days: Monday, Wednesday Begin: 07:00PM End: 09:50PM Bldg/Room: MM B10D Location: Pittsburgh, Pennsylvania Instructor(s): Carroll '),\n",
       " Document(page_content='Spring offering: Course: 60142 Title: Digital Photography I Units: 10.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 08:00AM End: 10:50AM Bldg/Room: MM B2 Location: Pittsburgh, Pennsylvania Instructor(s): Mantle '),\n",
       " Document(page_content='Spring offering: Course: 60142 Title: Digital Photography I Units: 10.0 Lec/Sec: Section B Days: Tuesday, Thursday Begin: 02:00PM End: 04:50PM Bldg/Room: MM B2 Location: Pittsburgh, Pennsylvania Instructor(s): Blum '),\n",
       " Document(page_content='Spring offering: Course: 60157 Title: Drawing for Non-Majors Units: 10.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 08:00AM End: 10:50AM Bldg/Room: CFA 313 Location: Pittsburgh, Pennsylvania Instructor(s): Mullins '),\n",
       " Document(page_content='Spring offering: Course: 60157 Title: Drawing for Non-Majors Units: 10.0 Lec/Sec: Section B Days: Tuesday, Thursday Begin: 02:00PM End: 04:50PM Bldg/Room: CFA 313 Location: Pittsburgh, Pennsylvania Instructor(s): Mullins '),\n",
       " Document(page_content='Spring offering: Course: 60170 Title: Foundations: Paint/Print Units: 10.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 08:00AM End: 10:50AM Bldg/Room: CFA 308 Location: Pittsburgh, Pennsylvania Instructor(s): Henderson '),\n",
       " Document(page_content='Spring offering: Course: 60170 Title: Foundations: Paint/Print Units: 10.0 Lec/Sec: Section B Days: Tuesday, Thursday Begin: 08:00AM End: 10:50AM Bldg/Room: CFA 308 Location: Pittsburgh, Pennsylvania Instructor(s): Ferrell '),\n",
       " Document(page_content='Spring offering: Course: 60170 Title: Foundations: Paint/Print Units: 10.0 Lec/Sec: Section C Days: Tuesday, Thursday Begin: 02:00PM End: 04:50PM Bldg/Room: CFA 308 Location: Pittsburgh, Pennsylvania Instructor(s): Barrois '),\n",
       " Document(page_content='Spring offering: Course: 60200 Title: Sophomore Review Units: 0.0 Lec/Sec: Section A Days: To be announced Begin: 02:00PM End: 04:50PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Jefferson, Cato '),\n",
       " Document(page_content='Spring offering: Course: 60203 Title: Intermediate Studio: Museum as Resource Units: 10.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 02:00PM End: 04:50PM Bldg/Room: OFF CAMPUS Location: Pittsburgh, Pennsylvania Instructor(s): Beauford '),\n",
       " Document(page_content='Spring offering: Course: 60211 Title: Intermediate Studio: Sound + Vision - Intro to Audiovisual Art Units: 10.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 02:00PM End: 04:50PM Bldg/Room: CFA 318 Location: Pittsburgh, Pennsylvania Instructor(s): Silver '),\n",
       " Document(page_content='Spring offering: Course: 60219 Title: Intermediate Studio: Stop-Motion Animation Units: 10.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 08:00AM End: 10:50AM Bldg/Room: CFA 313 Location: Pittsburgh, Pennsylvania Instructor(s): DeYoung '),\n",
       " Document(page_content='Spring offering: Course: 60221 Title: Intermediate Studio: Animation Workshop Units: 10.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 08:00AM End: 10:50AM Bldg/Room: CFA 317 Location: Pittsburgh, Pennsylvania Instructor(s): Duesing '),\n",
       " Document(page_content='Spring offering: Course: 60223 Title: IDeATe: Introduction to Physical Computing Units: 10.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 10:00AM End: 11:50AM Bldg/Room: HL A10 Location: Pittsburgh, Pennsylvania Instructor(s): Zacharias '),\n",
       " Document(page_content='Spring offering: Course: 60224 Title: Intermediate Game Studio: Interactivity Units: 10.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 07:00PM End: 09:50PM Bldg/Room: CFA 303 Location: Pittsburgh, Pennsylvania Instructor(s): Pedercini '),\n",
       " Document(page_content='Spring offering: Course: 60225 Title: Intermediate Studio: Drawing with Machines Units: 10.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 07:00PM End: 09:50PM Bldg/Room: CFA 303 Location: Pittsburgh, Pennsylvania Instructor(s): Levin '),\n",
       " Document(page_content='Spring offering: Course: 60233 Title: Intermediate Studio: Soft Sculpture Units: 10.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 08:00AM End: 10:50AM Bldg/Room: DH B302 Location: Pittsburgh, Pennsylvania Instructor(s): Hansen '),\n",
       " Document(page_content='Spring offering: Course: 60234 Title: Intermediate Studio: Ceramics Units: 10.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 10:00AM End: 12:50PM Bldg/Room: DH B301 Location: Pittsburgh, Pennsylvania Instructor(s): Sekino-Bove '),\n",
       " Document(page_content='Spring offering: Course: 60237 Title: Intermediate Studio: Digital Fabrication Units: 10.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 02:00PM End: 04:50PM Bldg/Room: DH B302 Location: Pittsburgh, Pennsylvania Instructor(s): Ransom '),\n",
       " Document(page_content='Spring offering: Course: 60241 Title: Black and White Photography II Units: 10.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 02:00PM End: 04:50PM Bldg/Room: MM B10 Location: Pittsburgh, Pennsylvania Instructor(s): Gruzska '),\n",
       " Document(page_content='Spring offering: Course: 60250 Title: Intermediate Studio: Painting Units: 10.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 02:00PM End: 04:50PM Bldg/Room: CFA 304 Location: Pittsburgh, Pennsylvania Instructor(s): Ray '),\n",
       " Document(page_content='Spring offering: Course: 60251 Title: Intermediate Studio: Print Media Units: 10.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 02:00PM End: 04:50PM Bldg/Room: DH C301 Location: Pittsburgh, Pennsylvania Instructor(s): Ebersole '),\n",
       " Document(page_content='Spring offering: Course: 60253 Title: Intermediate Studio: Fundamentals of Figuration Units: 10.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 07:00PM End: 09:50PM Bldg/Room: CFA 304 Location: Pittsburgh, Pennsylvania Instructor(s): Ferrell '),\n",
       " Document(page_content='Spring offering: Course: 60255 Title: Intermediate Drawing Studio: Trails of Touch Units: 10.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 02:00PM End: 04:50PM Bldg/Room: CFA 308 Location: Pittsburgh, Pennsylvania Instructor(s): Johnson '),\n",
       " Document(page_content='Spring offering: Course: 60258 Title: Intermediate Studio: Screenprint Units: 10.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 08:00AM End: 10:50AM Bldg/Room: DH C300 Location: Pittsburgh, Pennsylvania Instructor(s): Yeh '),\n",
       " Document(page_content='Spring offering: Course: 60263 Title: Intermediate Studio: Graphic Novel Units: 10.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 07:00PM End: 09:50PM Bldg/Room: CFA 313 Location: Pittsburgh, Pennsylvania Instructor(s): Pena '),\n",
       " Document(page_content='Spring offering: Course: 60276 Title: Photography and the Ephemeral Units: 10.0 Lec/Sec: Section A Days: Friday Begin: 08:00AM End: 10:50AM Bldg/Room: MM B14 Location: Pittsburgh, Pennsylvania Instructor(s): Martello '),\n",
       " Document(page_content='Spring offering: Course: 60276 Title: Photography and the Ephemeral Units: 10.0 Lec/Sec: Section A Days: Friday Begin: 02:00PM End: 04:50PM Bldg/Room: MM B14 Location: Pittsburgh, Pennsylvania Instructor(s): Martello '),\n",
       " Document(page_content='Spring offering: Course: 60337 Title: IDeATe Special Topics in Animation: Storyboarding Units: 6.0 Lec/Sec: Section A4 Days: Monday, Wednesday Begin: 07:00PM End: 08:50PM Bldg/Room: CMU REMOTE Location: Pittsburgh, Pennsylvania Instructor(s): Lim Haas '),\n",
       " Document(page_content='Spring offering: Course: 60356 Title: Critical Studies: Once Upon A Time: A Survey of International Fairy Tale Film Units: 9.0 Lec/Sec: Section A Days: Tuesday Begin: 07:00PM End: 09:50PM Bldg/Room: CFA 307 Location: Pittsburgh, Pennsylvania Instructor(s): Silver '),\n",
       " Document(page_content='Spring offering: Course: 60362 Title: Critical Studies: Art Writer: Writing as Object, Criticism, and Experiment Units: 9.0 Lec/Sec: Section A Days: Tuesday Begin: 07:00PM End: 09:50PM Bldg/Room: CFA 310 Location: Pittsburgh, Pennsylvania Instructor(s): Ragona '),\n",
       " Document(page_content='Spring offering: Course: 60367 Title: Critical Studies: Wanderlust: Art and Nature Units: 9.0 Lec/Sec: Section A Days: Monday Begin: 12:00PM End: 01:50PM Bldg/Room: CFA 303 Location: Pittsburgh, Pennsylvania Instructor(s): Beck '),\n",
       " Document(page_content='Spring offering: Course: 60369 Title: Critical Studies: DEEP FAKE, AI and Beyond: Posthumanism & Contemporary Art Units: 9.0 Lec/Sec: Section A Days: Wednesday Begin: 07:00PM End: 09:20PM Bldg/Room: CFA 307 Location: Pittsburgh, Pennsylvania Instructor(s): Ragona '),\n",
       " Document(page_content='Spring offering: Course: 60375 Title: Large Format Photography: The Antiquarian Avant-Garde Units: 10.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 08:00AM End: 10:50AM Bldg/Room: MM B10 Location: Pittsburgh, Pennsylvania Instructor(s): Blum '),\n",
       " Document(page_content='Spring offering: Course: 60398 Title: Critical Studies: Social History of Animation Units: 9.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 12:30PM End: 01:50PM Bldg/Room: CFA 303 Location: Pittsburgh, Pennsylvania Instructor(s): Duesing '),\n",
       " Document(page_content='Spring offering: Course: 60399 Title: Critical Studies Independent Study Units: 9.0 Lec/Sec: Section A Days: To be announced Begin: 12:30PM End: 01:50PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Barrois '),\n",
       " Document(page_content='Spring offering: Course: 60399 Title: Critical Studies Independent Study Units: 9.0 Lec/Sec: Section B Days: To be announced Begin: 12:30PM End: 01:50PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Ragona '),\n",
       " Document(page_content='Spring offering: Course: 60399 Title: Critical Studies Independent Study Units: 9.0 Lec/Sec: Section C Days: To be announced Begin: 12:30PM End: 01:50PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Chodos '),\n",
       " Document(page_content='Spring offering: Course: 60402 Title: Senior Studio Units: 10.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 02:00PM End: 04:50PM Bldg/Room: CFA 303 Location: Pittsburgh, Pennsylvania Instructor(s): Hansen, Merrell '),\n",
       " Document(page_content='Spring offering: Course: 60404 Title: Advanced ETB: Digitally Mediated Performance Units: 10.0 Lec/Sec: Section A Days: Friday Begin: 10:00AM End: 12:50PM Bldg/Room: HL A10A Location: Pittsburgh, Pennsylvania Instructor(s): Yoo '),\n",
       " Document(page_content='Spring offering: Course: 60404 Title: Advanced ETB: Digitally Mediated Performance Units: 10.0 Lec/Sec: Section A Days: Friday Begin: 02:00PM End: 04:50PM Bldg/Room: HL A10A Location: Pittsburgh, Pennsylvania Instructor(s): Yoo '),\n",
       " Document(page_content='Spring offering: Course: 60407 Title: IDeATe: Experimental Sound Synthesis Units: 9.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 12:00PM End: 01:50PM Bldg/Room: HL A10A Location: Pittsburgh, Pennsylvania Instructor(s): Hsieh '),\n",
       " Document(page_content='Spring offering: Course: 60409 Title: Advanced ETB: Decolonizing Data Beyond Metaphor Units: 10.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 07:00PM End: 09:50PM Bldg/Room: CMU REMOTE Location: Pittsburgh, Pennsylvania Instructor(s): Morehshin '),\n",
       " Document(page_content='Spring offering: Course: 60415 Title: Advanced ETB: Animation Studio: Units: 10.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 07:00PM End: 09:50PM Bldg/Room: CMU REMOTE Location: Pittsburgh, Pennsylvania Instructor(s): Morehshin '),\n",
       " Document(page_content='Spring offering: Course: 60415 Title: Chinese Mythology and Animation Units: 10.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 02:00PM End: 03:20PM Bldg/Room: CFA 303 Location: Pittsburgh, Pennsylvania Instructor(s): Liu, DeYoung '),\n",
       " Document(page_content='Spring offering: Course: 60415 Title: Chinese Mythology and Animation Units: 10.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 02:00PM End: 03:20PM Bldg/Room: CFA 317 Location: Pittsburgh, Pennsylvania Instructor(s): Liu, DeYoung '),\n",
       " Document(page_content='Spring offering: Course: 60424 Title: Advanced ETB: Animation Studio:: Units: 10.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 02:00PM End: 03:20PM Bldg/Room: CFA 317 Location: Pittsburgh, Pennsylvania Instructor(s): Liu, DeYoung '),\n",
       " Document(page_content='Spring offering: Course: 60424 Title: AI Generated Animation Units: 10.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 07:00PM End: 09:50PM Bldg/Room: CFA 317 Location: Pittsburgh, Pennsylvania Instructor(s): Andrew '),\n",
       " Document(page_content='Spring offering: Course: 60430 Title: Advanced SIS: Open Sculpture Units: 10.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 02:00PM End: 04:50PM Bldg/Room: DH B302 Location: Pittsburgh, Pennsylvania Instructor(s): Toure '),\n",
       " Document(page_content='Spring offering: Course: 60433 Title: Advanced SIS: Ceramics Units: 10.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 02:00PM End: 04:50PM Bldg/Room: DH B301 Location: Pittsburgh, Pennsylvania Instructor(s): Sekino-Bove '),\n",
       " Document(page_content='Spring offering: Course: 60453 Title: Advanced DP3: Painting: Units: 10.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 02:00PM End: 04:50PM Bldg/Room: DH B301 Location: Pittsburgh, Pennsylvania Instructor(s): Sekino-Bove '),\n",
       " Document(page_content='Spring offering: Course: 60453 Title: Advanced DP3: Painting - Lump, Clump, Nestle, Merge: Drawing Complexity Units: 10.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 07:00PM End: 09:50PM Bldg/Room: CFA 415 Location: Pittsburgh, Pennsylvania Instructor(s): Johnson '),\n",
       " Document(page_content='Spring offering: Course: 60457 Title: Advanced DP3: Painting - Reverence And Representation Units: 10.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 02:00PM End: 04:50PM Bldg/Room: CFA 415 Location: Pittsburgh, Pennsylvania Instructor(s): Henderson '),\n",
       " Document(page_content='Spring offering: Course: 60462 Title: Advanced DP3: Painting Matter & Substance Units: 10.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 08:00AM End: 10:50AM Bldg/Room: CFA 304 Location: Pittsburgh, Pennsylvania Instructor(s): Merrell '),\n",
       " Document(page_content='Spring offering: Course: 60469 Title: Advanced DP3: Special Topic:: Units: 10.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 08:00AM End: 10:50AM Bldg/Room: CFA 304 Location: Pittsburgh, Pennsylvania Instructor(s): Merrell '),\n",
       " Document(page_content='Spring offering: Course: 60469 Title: Print and Resistance Units: 10.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 08:00AM End: 10:50AM Bldg/Room: DH C300 Location: Pittsburgh, Pennsylvania Instructor(s): Ebersole '),\n",
       " Document(page_content='Spring offering: Course: 60474 Title: Advanced DP3: Photobook Units: 10.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 02:00PM End: 04:50PM Bldg/Room: CFA 310 Location: Pittsburgh, Pennsylvania Instructor(s): Beck '),\n",
       " Document(page_content='Spring offering: Course: 60499 Title: Studio Independent Study Units: 5-10 Lec/Sec: Section A Days: To be announced Begin: 02:00PM End: 04:50PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Pedercini '),\n",
       " Document(page_content='Spring offering: Course: 60499 Title: Studio Independent Study Units: 5-10 Lec/Sec: Section B Days: To be announced Begin: 02:00PM End: 04:50PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Washko '),\n",
       " Document(page_content='Spring offering: Course: 60499 Title: Studio Independent Study Units: 5-10 Lec/Sec: Section C Days: To be announced Begin: 02:00PM End: 04:50PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Barrois '),\n",
       " Document(page_content='Spring offering: Course: 60499 Title: Studio Independent Study Units: 5-10 Lec/Sec: Section D Days: To be announced Begin: 02:00PM End: 04:50PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Hansen, Merrell, Henderson '),\n",
       " Document(page_content='Spring offering: Course: 60499 Title: Studio Independent Study Units: 5-10 Lec/Sec: Section E Days: To be announced Begin: 02:00PM End: 04:50PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Johnson '),\n",
       " Document(page_content='Spring offering: Course: 60499 Title: Studio Independent Study Units: 5-10 Lec/Sec: Section F Days: To be announced Begin: 02:00PM End: 04:50PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Merrell '),\n",
       " Document(page_content='Spring offering: Course: 60499 Title: Studio Independent Study Units: 5-10 Lec/Sec: Section G Days: To be announced Begin: 02:00PM End: 04:50PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): DeYoung '),\n",
       " Document(page_content='Spring offering: Course: 60499 Title: Studio Independent Study Units: 5-10 Lec/Sec: Section H Days: To be announced Begin: 02:00PM End: 04:50PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Levin '),\n",
       " Document(page_content='Spring offering: Course: 60499 Title: Studio Independent Study Units: 5-10 Lec/Sec: Section I Days: To be announced Begin: 02:00PM End: 04:50PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Instructor TBA '),\n",
       " Document(page_content='Spring offering: Course: 60499 Title: Studio Independent Study Units: 5-10 Lec/Sec: Section J Days: To be announced Begin: 02:00PM End: 04:50PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Instructor TBA '),\n",
       " Document(page_content='Spring offering: Course: 60499 Title: Studio Independent Study Units: 5-10 Lec/Sec: Section K Days: To be announced Begin: 02:00PM End: 04:50PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Instructor TBA '),\n",
       " Document(page_content='Spring offering: Course: 60499 Title: Studio Independent Study Units: 5-10 Lec/Sec: Section L Days: To be announced Begin: 02:00PM End: 04:50PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Instructor TBA '),\n",
       " Document(page_content='Spring offering: Course: 60499 Title: Studio Independent Study Units: 5-10 Lec/Sec: Section M Days: To be announced Begin: 02:00PM End: 04:50PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Instructor TBA '),\n",
       " Document(page_content='Spring offering: Course: 60499 Title: Studio Independent Study Units: 5-10 Lec/Sec: Section N Days: To be announced Begin: 02:00PM End: 04:50PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Instructor TBA '),\n",
       " Document(page_content='Spring offering: Course: 60590 Title: Internship Units: 1-9 Lec/Sec: Section A Days: To be announced Begin: 02:00PM End: 04:50PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Cato '),\n",
       " Document(page_content='Spring offering: Course: 60703 Title: M.F.A. Studio Units: 12-24 Lec/Sec: Section A Days: Thursday Begin: 05:30PM End: 09:20PM Bldg/Room: HOA 322 Location: Pittsburgh, Pennsylvania Instructor(s): Barrois, Hubbard, Washko '),\n",
       " Document(page_content='Spring offering: Course: 60706 Title: M.F.A. Contextual Practice Units: 12.0 Lec/Sec: Section A Days: Monday Begin: 06:30PM End: 09:20PM Bldg/Room: HOA 322 Location: Pittsburgh, Pennsylvania Instructor(s): Brooks Takahashi '),\n",
       " Document(page_content='Spring offering: Course: 60711 Title: M.F.A. Thesis Writing Seminar I Units: 9.0 Lec/Sec: Section A Days: Monday Begin: 06:30PM End: 09:20PM Bldg/Room: HOA 323 Location: Pittsburgh, Pennsylvania Instructor(s): Dunn '),\n",
       " Document(page_content='Spring offering: Course: 60714 Title: MFA Thesis Exhibition Units: 12.0 Lec/Sec: Section A Days: Wednesday Begin: 12:00PM End: 01:50PM Bldg/Room: HOA 323 Location: Pittsburgh, Pennsylvania Instructor(s): Chodos '),\n",
       " Document(page_content='Spring offering: Course: 60791 Title: M.F.A. Integrative Seminar Units: 12.0 Lec/Sec: Section A Days: Wednesday Begin: 06:30PM End: 09:20PM Bldg/Room: HOA 322 Location: Pittsburgh, Pennsylvania Instructor(s): Wormsley '),\n",
       " Document(page_content='Spring offering: Course: 60799 Title: Graduate Independent Study Units: VAR Lec/Sec: Section A Days: To be announced Begin: 06:30PM End: 09:20PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Ragona '),\n",
       " Document(page_content='Spring offering: Course: 60799 Title: Graduate Independent Study Units: VAR Lec/Sec: Section B Days: To be announced Begin: 06:30PM End: 09:20PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Instructor TBA '),\n",
       " Document(page_content='Spring offering: Course: 93812 Title: Presenting Performing Arts & Festivals Units: 6.0 Lec/Sec: Section A4 Days: Tuesday Begin: 06:30PM End: 09:20PM Bldg/Room: HBH 2008 Location: Pittsburgh, Pennsylvania Instructor(s): Miller '),\n",
       " Document(page_content='Spring offering: Course: 93815 Title: Audience Engagement and User-Experience Design Units: 6.0 Lec/Sec: Section A4 Days: Tuesday, Thursday Begin: 02:00PM End: 03:20PM Bldg/Room: HBH 1202 Location: Pittsburgh, Pennsylvania Instructor(s): Crawford '),\n",
       " Document(page_content='Spring offering: Course: 93821 Title: External Relations: Arts Marketing and PR Units: 6.0 Lec/Sec: Section A3 Days: Tuesday, Thursday Begin: 02:00PM End: 03:20PM Bldg/Room: HBH 1202 Location: Pittsburgh, Pennsylvania Instructor(s): Crawford '),\n",
       " Document(page_content='Spring offering: Course: 93826 Title: External Relations: Fundraising & Individual Giving Units: 6.0 Lec/Sec: Section A3 Days: Tuesday Begin: 06:30PM End: 09:20PM Bldg/Room: HBH 1202 Location: Pittsburgh, Pennsylvania Instructor(s): McMahon '),\n",
       " Document(page_content='Spring offering: Course: 93827 Title: External Relations: Sponsorships & Institutional Giving Units: 6.0 Lec/Sec: Section A4 Days: Tuesday Begin: 06:30PM End: 09:20PM Bldg/Room: HBH 1202 Location: Pittsburgh, Pennsylvania Instructor(s): Benvenuti '),\n",
       " Document(page_content='Spring offering: Course: 93844 Title: Live Music: Touring and Revenue Streams Units: 6.0 Lec/Sec: Section A3 Days: Thursday Begin: 06:30PM End: 09:20PM Bldg/Room: HBH 2008 Location: Pittsburgh, Pennsylvania Instructor(s): Baab '),\n",
       " Document(page_content='Spring offering: Course: 93846 Title: Cultural Policy & Advocacy in the US Units: 6.0 Lec/Sec: Section A3 Days: Thursday Begin: 06:30PM End: 09:20PM Bldg/Room: HBH 1006 Location: Pittsburgh, Pennsylvania Instructor(s): Poulin '),\n",
       " Document(page_content='Spring offering: Course: 93849 Title: Arts Management Professional Seminar II Units: 0.0 Lec/Sec: Section A Days: Thursday Begin: 12:30PM End: 01:50PM Bldg/Room: HBH 1204 Location: Pittsburgh, Pennsylvania Instructor(s): Spangler, Bowser '),\n",
       " Document(page_content='Spring offering: Course: 93854 Title: Film Acquisition Units: 3.0 Lec/Sec: Section L Days: To be announced Begin: 12:30PM End: 01:50PM Bldg/Room: DNM DNM Location: Los Angeles, California Instructor(s): Iyengar '),\n",
       " Document(page_content='Spring offering: Course: 93855 Title: Intellectual Property Units: 6.0 Lec/Sec: Section L Days: To be announced Begin: 12:30PM End: 01:50PM Bldg/Room: DNM DNM Location: Los Angeles, California Instructor(s): Hime '),\n",
       " Document(page_content='Spring offering: Course: 93856 Title: Business Development Units: 6.0 Lec/Sec: Section L Days: To be announced Begin: 12:30PM End: 01:50PM Bldg/Room: DNM DNM Location: Los Angeles, California Instructor(s): Smith '),\n",
       " Document(page_content='Spring offering: Course: 93858 Title: Market Research in the Entertainment Industry Units: 3.0 Lec/Sec: Section L Days: To be announced Begin: 12:30PM End: 01:50PM Bldg/Room: DNM DNM Location: Los Angeles, California Instructor(s): Yoder '),\n",
       " Document(page_content='Spring offering: Course: 93859 Title: Fandom in Music and Media Units: 3.0 Lec/Sec: Section L Days: To be announced Begin: 12:30PM End: 01:50PM Bldg/Room: DNM DNM Location: Los Angeles, California Instructor(s): Ben '),\n",
       " Document(page_content='Spring offering: Course: 93860 Title: What Makes Us Play: The Craft of the Video Game Units: 4.0 Lec/Sec: Section L Days: To be announced Begin: 12:30PM End: 01:50PM Bldg/Room: DNM DNM Location: Los Angeles, California Instructor(s): Platz '),\n",
       " Document(page_content='Spring offering: Course: 93866 Title: Creative Producing Units: 6.0 Lec/Sec: Section L Days: To be announced Begin: 12:30PM End: 01:50PM Bldg/Room: DNM DNM Location: Los Angeles, California Instructor(s): Christopher '),\n",
       " Document(page_content='Spring offering: Course: 93869 Title: Television Economics Units: 6.0 Lec/Sec: Section L Days: To be announced Begin: 12:30PM End: 01:50PM Bldg/Room: TBA Location: Los Angeles, California Instructor(s): Quinn, Vallerian '),\n",
       " Document(page_content='Spring offering: Course: 93870 Title: Introduction to Writing for Television Units: 9.0 Lec/Sec: Section A Days: Tuesday Begin: 06:30PM End: 09:20PM Bldg/Room: HBH 2003 Location: Pittsburgh, Pennsylvania Instructor(s): Skopov '),\n",
       " Document(page_content='Spring offering: Course: 93873 Title: Film Exhibition Units: 3.0 Lec/Sec: Section L Days: To be announced Begin: 06:30PM End: 09:20PM Bldg/Room: DNM DNM Location: Los Angeles, California Instructor(s): Green, Alcaraz Aguilar '),\n",
       " Document(page_content='Spring offering: Course: 93878 Title: Branded Entertainment Units: 3.0 Lec/Sec: Section L Days: To be announced Begin: 06:30PM End: 09:20PM Bldg/Room: DNM DNM Location: Los Angeles, California Instructor(s): Ong '),\n",
       " Document(page_content='Spring offering: Course: 93880 Title: Production Finance Units: 6.0 Lec/Sec: Section A4 Days: Monday, Wednesday Begin: 11:00AM End: 12:20PM Bldg/Room: CMU REMOTE Location: Pittsburgh, Pennsylvania Instructor(s): Saul '),\n",
       " Document(page_content='Spring offering: Course: 93888 Title: Practicum II Units: 1.0 Lec/Sec: Section L Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: DNM DNM Location: Los Angeles, California Instructor(s): Green '),\n",
       " Document(page_content='Spring offering: Course: 93890 Title: Capstone Management Project II Units: 9.0 Lec/Sec: Section L Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: DNM DNM Location: Los Angeles, California Instructor(s): Green '),\n",
       " Document(page_content='Spring offering: Course: 93890 Title: Capstone Management Project II Units: 9.0 Lec/Sec: Section LA Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: DNM DNM Location: Los Angeles, California Instructor(s): Green, Garner '),\n",
       " Document(page_content='Spring offering: Course: 93890 Title: Capstone Management Project II Units: 9.0 Lec/Sec: Section LB Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: DNM DNM Location: Los Angeles, California Instructor(s): Green, Eich '),\n",
       " Document(page_content='Spring offering: Course: 93890 Title: Capstone Management Project II Units: 9.0 Lec/Sec: Section LC Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: TBA Location: Los Angeles, California Instructor(s): Green, Emilio '),\n",
       " Document(page_content='Spring offering: Course: 93890 Title: Capstone Management Project II Units: 9.0 Lec/Sec: Section LD Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: DNM DNM Location: Los Angeles, California Instructor(s): Green, Lewgood '),\n",
       " Document(page_content='Spring offering: Course: 93890 Title: Capstone Management Project II Units: 9.0 Lec/Sec: Section LE Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: DNM DNM Location: Los Angeles, California Instructor(s): Green '),\n",
       " Document(page_content='Spring offering: Course: 93890 Title: Capstone Management Project II Units: 9.0 Lec/Sec: Section LF Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: TBA Location: Los Angeles, California Instructor(s): Stein, Green '),\n",
       " Document(page_content='Spring offering: Course: 93890 Title: Capstone Management Project II Units: 9.0 Lec/Sec: Section LG Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: DNM DNM Location: Los Angeles, California Instructor(s): Green '),\n",
       " Document(page_content='Spring offering: Course: 52291 Title: BXA Seminar II: Transferring Knowledge Units: 4.5 Lec/Sec: Section A4 Days: Friday Begin: 12:00PM End: 01:50PM Bldg/Room: HOA 211F Location: Pittsburgh, Pennsylvania Instructor(s): Murray '),\n",
       " Document(page_content='Spring offering: Course: 52292 Title: BXA Student Advisory Council Units: 3.0 Lec/Sec: Section A Days: To be announced Begin: 12:00PM End: 01:50PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Hagan '),\n",
       " Document(page_content='Spring offering: Course: 52390 Title: BXA Undergraduate Research Project Units: 3-12 Lec/Sec: Section A Days: To be announced Begin: 12:00PM End: 01:50PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Murray, Hagan '),\n",
       " Document(page_content='Spring offering: Course: 52392 Title: BXA Seminar III: Deconstructing Disciplines Units: 9.0 Lec/Sec: Section A Days: Monday Begin: 06:00PM End: 06:50PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Marcum, Hagan '),\n",
       " Document(page_content='Spring offering: Course: 52401 Title: BXA Seminar IV: Capstone Project Research Units: VAR Lec/Sec: Section A Days: To be announced Begin: 06:00PM End: 06:50PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Murray, Hagan '),\n",
       " Document(page_content='Spring offering: Course: 52402 Title: BXA Seminar V: Capstone Project Production Units: 9.0 Lec/Sec: Section A Days: To be announced Begin: 06:00PM End: 06:50PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Murray, Hagan '),\n",
       " Document(page_content='Spring offering: Course: 52590 Title: BXA Internship Units: 3-12 Lec/Sec: Section A Days: To be announced Begin: 06:00PM End: 06:50PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Hagan, Murray '),\n",
       " Document(page_content='Spring offering: Course: 03117 Title: Frontiers, Analysis, and Discovery in Biological Sciences Units: 6.0 Lec/Sec: Lecture Days: Tuesday, Thursday Begin: 01:00PM End: 01:50PM Bldg/Room: MI SOCIAL Location: Pittsburgh, Pennsylvania Instructor(s): Doonan, Willard, Laperuta '),\n",
       " Document(page_content='Spring offering: Course: 03117 Title: Frontiers, Analysis, and Discovery in Biological Sciences Units: 6.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 01:00PM End: 04:00PM Bldg/Room: MI 357 Location: Pittsburgh, Pennsylvania Instructor(s): Doonan, Willard, Laperuta '),\n",
       " Document(page_content='Spring offering: Course: 03117 Title: Frontiers, Analysis, and Discovery in Biological Sciences Units: 6.0 Lec/Sec: Section W Days: Monday, Wednesday Begin: 02:30PM End: 05:20PM Bldg/Room: CMB 3025 Location: Doha, Qatar Instructor(s): Younis, Inayat '),\n",
       " Document(page_content='Spring offering: Course: 03119 Title: Biology for Life Special Topics Micro: Units: 3.0 Lec/Sec: Section W Days: Monday, Wednesday Begin: 02:30PM End: 05:20PM Bldg/Room: CMB 3025 Location: Doha, Qatar Instructor(s): Younis, Inayat '),\n",
       " Document(page_content='Spring offering: Course: 03119 Title: Genome Editing: Past, Present, and Future Units: 3.0 Lec/Sec: Section W4 Days: Sunday, Tuesday, Thursday Begin: 06:00PM End: 07:30PM Bldg/Room: CMB 1030 Location: Doha, Qatar Instructor(s): Rule '),\n",
       " Document(page_content='Spring offering: Course: 03119 Title: Biology for Life Special Topics Micro: Units: 3.0 Lec/Sec: Section W4 Days: Sunday, Tuesday, Thursday Begin: 06:00PM End: 07:30PM Bldg/Room: CMB 1030 Location: Doha, Qatar Instructor(s): Rule '),\n",
       " Document(page_content='Spring offering: Course: 03119 Title: Biopharma Protein-Based Pharmaceuticals Units: 3.0 Lec/Sec: Section X4 Days: Monday, Wednesday Begin: 06:00PM End: 07:30PM Bldg/Room: CMB 1031 Location: Doha, Qatar Instructor(s): Rule '),\n",
       " Document(page_content='Spring offering: Course: 03121 Title: Modern Biology Units: 9.0 Lec/Sec: Section B Days: Monday, Wednesday, Friday Begin: 09:00AM End: 09:50AM Bldg/Room: DH 2315 Location: Pittsburgh, Pennsylvania Instructor(s): Wong-Noonan, Wisniewski '),\n",
       " Document(page_content='Spring offering: Course: 03124 Title: Modern Biology Laboratory Units: 9.0 Lec/Sec: Lecture Days: Wednesday Begin: 11:00AM End: 11:50AM Bldg/Room: WEH 5421 Location: Pittsburgh, Pennsylvania Instructor(s): Laperuta, Doonan '),\n",
       " Document(page_content='Spring offering: Course: 03124 Title: Modern Biology Laboratory Units: 9.0 Lec/Sec: Section A Days: Wednesday Begin: 06:00PM End: 09:00PM Bldg/Room: DH 2303 Location: Pittsburgh, Pennsylvania Instructor(s): Laperuta, Doonan '),\n",
       " Document(page_content='Spring offering: Course: 03124 Title: Modern Biology Laboratory Units: 9.0 Lec/Sec: Section B Days: Wednesday Begin: 01:00PM End: 03:50PM Bldg/Room: DH 2303 Location: Pittsburgh, Pennsylvania Instructor(s): Doonan, Laperuta '),\n",
       " Document(page_content='Spring offering: Course: 03133 Title: Neurobiology of Disease Units: 9.0 Lec/Sec: Section A Days: Monday, Wednesday, Friday Begin: 09:00AM End: 09:50AM Bldg/Room: SH 105 Location: Pittsburgh, Pennsylvania Instructor(s): Brasier '),\n",
       " Document(page_content='Spring offering: Course: 03161 Title: Molecules to Mind Units: 9.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 09:30AM End: 10:50AM Bldg/Room: POS 153 Location: Pittsburgh, Pennsylvania Instructor(s): Barth '),\n",
       " Document(page_content='Spring offering: Course: 03202 Title: Undergraduate Colloquium for Sophomores Units: 1-3 Lec/Sec: Section W Days: Monday Begin: 11:30AM End: 12:20PM Bldg/Room: CMB 3046 Location: Doha, Qatar Instructor(s): Younis '),\n",
       " Document(page_content='Spring offering: Course: 03206 Title: Biomedical Engineering Laboratory Units: 9.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 02:00PM End: 03:20PM Bldg/Room: EDS 125 Location: Pittsburgh, Pennsylvania Instructor(s): Ren '),\n",
       " Document(page_content='Spring offering: Course: 03206 Title: Biomedical Engineering Laboratory Units: 9.0 Lec/Sec: Section B Days: Tuesday, Thursday Begin: 11:00AM End: 12:20PM Bldg/Room: EDS 125 Location: Pittsburgh, Pennsylvania Instructor(s): Ren '),\n",
       " Document(page_content='Spring offering: Course: 03210 Title: Independent Study Units: 1-12 Lec/Sec: Section A3 Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Willard '),\n",
       " Document(page_content='Spring offering: Course: 03210 Title: Independent Study Units: 1-12 Lec/Sec: Section B4 Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Willard '),\n",
       " Document(page_content='Spring offering: Course: 03210 Title: Independent Study Units: 1-12 Lec/Sec: Section W3 Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: DNM DNM Location: Doha, Qatar Instructor(s): Bouaouina '),\n",
       " Document(page_content='Spring offering: Course: 03210 Title: Independent Study Units: 1-12 Lec/Sec: Section W4 Days: To be announced Begin: 11:00AM End: 12:20PM Bldg/Room: DNM DNM Location: Doha, Qatar Instructor(s): Younis '),\n",
       " Document(page_content='Spring offering: Course: 03221 Title: Genomes, Evolution, and Disease: Introduction to Quantitative Genetic Analysis Units: 9.0 Lec/Sec: Lecture Days: Monday, Wednesday, Friday Begin: 11:00AM End: 11:50AM Bldg/Room: GHC 4307 Location: Pittsburgh, Pennsylvania Instructor(s): Kuang, Wong-Noonan '),\n",
       " Document(page_content='Spring offering: Course: 03221 Title: Genomes, Evolution, and Disease: Introduction to Quantitative Genetic Analysis Units: 9.0 Lec/Sec: Section A Days: Friday Begin: 03:30PM End: 04:50PM Bldg/Room: DH A302 Location: Pittsburgh, Pennsylvania Instructor(s): Kuang, Wong-Noonan '),\n",
       " Document(page_content='Spring offering: Course: 03230 Title: Intro to Mammalian Physiology Units: 9.0 Lec/Sec: Section Lec 2 Days: Sunday, Tuesday, Thursday Begin: 08:30AM End: 09:20AM Bldg/Room: CMB 3178 Location: Doha, Qatar Instructor(s): Affara '),\n",
       " Document(page_content='Spring offering: Course: 03230 Title: Intro to Mammalian Physiology Units: 9.0 Lec/Sec: Section W Days: Thursday Begin: 02:30PM End: 03:30PM Bldg/Room: CMB 3178 Location: Doha, Qatar Instructor(s): Affara '),\n",
       " Document(page_content='Spring offering: Course: 03231 Title: Honors Biochemistry Units: 9.0 Lec/Sec: Lecture Days: Monday, Wednesday, Friday Begin: 10:00AM End: 10:50AM Bldg/Room: GHC 4215 Location: Pittsburgh, Pennsylvania Instructor(s): Lee '),\n",
       " Document(page_content='Spring offering: Course: 03231 Title: Honors Biochemistry Units: 9.0 Lec/Sec: Section A Days: Tuesday Begin: 04:00PM End: 04:50PM Bldg/Room: DH 1212 Location: Pittsburgh, Pennsylvania Instructor(s): Lee '),\n",
       " Document(page_content='Spring offering: Course: 03232 Title: Biochemistry I Units: 9.0 Lec/Sec: Section Lec 1 Days: Monday, Wednesday, Friday Begin: 09:00AM End: 09:50AM Bldg/Room: POS 151 Location: Pittsburgh, Pennsylvania Instructor(s): Rule '),\n",
       " Document(page_content='Spring offering: Course: 03232 Title: Biochemistry I Units: 9.0 Lec/Sec: Section Lec 2 Days: Sunday, Tuesday, Thursday Begin: 10:00AM End: 10:50AM Bldg/Room: CMB 1031 Location: Doha, Qatar Instructor(s): Naik, Vincent '),\n",
       " Document(page_content='Spring offering: Course: 03232 Title: Biochemistry I Units: 9.0 Lec/Sec: Section A Days: Wednesday Begin: 02:00PM End: 02:50PM Bldg/Room: PH 226A Location: Pittsburgh, Pennsylvania Instructor(s): Rule '),\n",
       " Document(page_content='Spring offering: Course: 03232 Title: Biochemistry I Units: 9.0 Lec/Sec: Section B Days: Thursday Begin: 04:00PM End: 04:50PM Bldg/Room: PH A22 Location: Pittsburgh, Pennsylvania Instructor(s): Rule '),\n",
       " Document(page_content='Spring offering: Course: 03232 Title: Biochemistry I Units: 9.0 Lec/Sec: Section C Days: Friday Begin: 02:00PM End: 02:50PM Bldg/Room: WEH 4709 Location: Pittsburgh, Pennsylvania Instructor(s): Rule '),\n",
       " Document(page_content='Spring offering: Course: 03232 Title: Biochemistry I Units: 9.0 Lec/Sec: Section W Days: Wednesday Begin: 02:30PM End: 03:45PM Bldg/Room: CMB 1031 Location: Doha, Qatar Instructor(s): Vincent, Naik '),\n",
       " Document(page_content='Spring offering: Course: 03240 Title: Cell Biology Units: 9.0 Lec/Sec: Section Lec 2 Days: Wednesday Begin: 11:30AM End: 12:20PM Bldg/Room: CMB 3046 Location: Doha, Qatar Instructor(s): Bouaouina '),\n",
       " Document(page_content='Spring offering: Course: 03240 Title: Cell Biology Units: 9.0 Lec/Sec: Section Lec 2 Days: Sunday, Tuesday Begin: 11:30AM End: 12:20PM Bldg/Room: CMB 3046 Location: Doha, Qatar Instructor(s): Bouaouina '),\n",
       " Document(page_content='Spring offering: Course: 03240 Title: Cell Biology Units: 9.0 Lec/Sec: Section W Days: Thursday Begin: 11:30AM End: 12:20PM Bldg/Room: CMB 3046 Location: Doha, Qatar Instructor(s): Bouaouina '),\n",
       " Document(page_content='Spring offering: Course: 03290 Title: Applied Cell Biology Units: 3.0 Lec/Sec: Section W4 Days: Thursday Begin: 02:30PM End: 05:20PM Bldg/Room: CMB 3025 Location: Doha, Qatar Instructor(s): Bouaouina '),\n",
       " Document(page_content='Spring offering: Course: 03344 Title: Experimental Biochemistry Units: 12.0 Lec/Sec: Section Lec 1 Days: Wednesday Begin: 11:00AM End: 11:50AM Bldg/Room: MI 328 Location: Pittsburgh, Pennsylvania Instructor(s): Doonan '),\n",
       " Document(page_content='Spring offering: Course: 03344 Title: Experimental Biochemistry Units: 12.0 Lec/Sec: Section Lec 2 Days: Sunday Begin: 04:00PM End: 04:50PM Bldg/Room: CMB 3044 Location: Doha, Qatar Instructor(s): Vincent, Naik '),\n",
       " Document(page_content='Spring offering: Course: 03344 Title: Experimental Biochemistry Units: 12.0 Lec/Sec: Section B Days: Tuesday, Thursday Begin: 12:00PM End: 12:50PM Bldg/Room: MI 328 Location: Pittsburgh, Pennsylvania Instructor(s): Doonan '),\n",
       " Document(page_content='Spring offering: Course: 03344 Title: Experimental Biochemistry Units: 12.0 Lec/Sec: Section B Days: Tuesday, Thursday Begin: 12:00PM End: 04:00PM Bldg/Room: MI WETLAB Location: Pittsburgh, Pennsylvania Instructor(s): Doonan '),\n",
       " Document(page_content='Spring offering: Course: 03344 Title: Experimental Biochemistry Units: 12.0 Lec/Sec: Section W Days: Monday, Wednesday Begin: 09:00AM End: 01:00PM Bldg/Room: CMB 3025 Location: Doha, Qatar Instructor(s): Vincent, Naik '),\n",
       " Document(page_content='Spring offering: Course: 03345 Title: Experimental Cell and Developmental Biology Units: 12.0 Lec/Sec: Lecture Days: Wednesday Begin: 02:00PM End: 02:50PM Bldg/Room: MI 355 Location: Pittsburgh, Pennsylvania Instructor(s): Drill, Wisniewski '),\n",
       " Document(page_content='Spring offering: Course: 03345 Title: Experimental Cell and Developmental Biology Units: 12.0 Lec/Sec: Section A Days: Monday, Friday Begin: 12:00PM End: 04:00PM Bldg/Room: MI 304 Location: Pittsburgh, Pennsylvania Instructor(s): Wisniewski, Drill '),\n",
       " Document(page_content='Spring offering: Course: 03345 Title: Experimental Cell and Developmental Biology Units: 12.0 Lec/Sec: Section B Days: Tuesday, Thursday Begin: 12:00PM End: 04:00PM Bldg/Room: MI 304 Location: Pittsburgh, Pennsylvania Instructor(s): Drill, Wisniewski '),\n",
       " Document(page_content='Spring offering: Course: 03350 Title: Developmental Biology Units: 9.0 Lec/Sec: Section A Days: Monday, Wednesday, Friday Begin: 10:00AM End: 10:50AM Bldg/Room: WEH 5415 Location: Pittsburgh, Pennsylvania Instructor(s): Ettensohn '),\n",
       " Document(page_content='Spring offering: Course: 03363 Title: Systems Neuroscience Units: 9.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 12:30PM End: 01:50PM Bldg/Room: DH 1212 Location: Pittsburgh, Pennsylvania Instructor(s): Yttri '),\n",
       " Document(page_content='Spring offering: Course: 03363 Title: Systems Neuroscience Units: 9.0 Lec/Sec: Section W Days: Monday, Wednesday Begin: 02:30PM End: 03:45PM Bldg/Room: CMB 3048 Location: Doha, Qatar Instructor(s): Affara '),\n",
       " Document(page_content='Spring offering: Course: 03365 Title: Neural Correlates of Learning and Memory Units: 9.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 11:00AM End: 12:20PM Bldg/Room: DH 1211 Location: Pittsburgh, Pennsylvania Instructor(s): McGuier '),\n",
       " Document(page_content='Spring offering: Course: 03366 Title: Neuropharmacology: Drugs, Brain and Behavior Units: 9.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 09:30AM End: 10:50AM Bldg/Room: WEH 5415 Location: Pittsburgh, Pennsylvania Instructor(s): Hong '),\n",
       " Document(page_content='Spring offering: Course: 03390 Title: Molecular and Cellular Immunology Units: 9.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 09:30AM End: 10:50AM Bldg/Room: GHC 4101 Location: Pittsburgh, Pennsylvania Instructor(s): Giannoukakis '),\n",
       " Document(page_content='Spring offering: Course: 03391 Title: Microbiology Units: 9.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 09:30AM End: 10:50AM Bldg/Room: POS 151 Location: Pittsburgh, Pennsylvania Instructor(s): Hiller '),\n",
       " Document(page_content='Spring offering: Course: 03412 Title: Topics in Research Units: 1-2 Lec/Sec: Section A Days: Wednesday Begin: 12:30PM End: 01:50PM Bldg/Room: MI 348 Location: Pittsburgh, Pennsylvania Instructor(s): Linstedt '),\n",
       " Document(page_content='Spring offering: Course: 03412 Title: Topics in Research Units: 1-2 Lec/Sec: Section W Days: Sunday Begin: 10:00AM End: 11:15AM Bldg/Room: CMB 3046 Location: Doha, Qatar Instructor(s): Bouaouina '),\n",
       " Document(page_content='Spring offering: Course: 03435 Title: Cancer Biology Units: 9.0 Lec/Sec: Section A Days: Monday, Wednesday, Friday Begin: 09:00AM End: 09:50AM Bldg/Room: PH A22 Location: Pittsburgh, Pennsylvania Instructor(s): Zhao '),\n",
       " Document(page_content='Spring offering: Course: 03442 Title: Molecular Biology Units: 9.0 Lec/Sec: Section W Days: Tuesday, Thursday Begin: 10:00AM End: 11:15AM Bldg/Room: CMB 3046 Location: Doha, Qatar Instructor(s): Younis '),\n",
       " Document(page_content='Spring offering: Course: 03445 Title: Undergraduate Research Units: 1-18 Lec/Sec: Section Lec 1 Days: To be announced Begin: 10:00AM End: 11:15AM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Willard '),\n",
       " Document(page_content='Spring offering: Course: 03445 Title: Undergraduate Research Units: 1-18 Lec/Sec: Section Lec 2 Days: To be announced Begin: 10:00AM End: 11:15AM Bldg/Room: DNM DNM Location: Doha, Qatar Instructor(s): Younis '),\n",
       " Document(page_content='Spring offering: Course: 03445 Title: Undergraduate Research Units: 1-18 Lec/Sec: Section A Days: To be announced Begin: 10:00AM End: 11:15AM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Willard '),\n",
       " Document(page_content='Spring offering: Course: 03445 Title: Undergraduate Research Units: 1-18 Lec/Sec: Section AA Days: To be announced Begin: 10:00AM End: 11:15AM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Minden, Willard '),\n",
       " Document(page_content='Spring offering: Course: 03445 Title: Undergraduate Research Units: 1-18 Lec/Sec: Section B Days: To be announced Begin: 10:00AM End: 11:15AM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Barth, Willard '),\n",
       " Document(page_content='Spring offering: Course: 03445 Title: Undergraduate Research Units: 1-18 Lec/Sec: Section BB Days: To be announced Begin: 10:00AM End: 11:15AM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Willard, Durand '),\n",
       " Document(page_content='Spring offering: Course: 03445 Title: Undergraduate Research Units: 1-18 Lec/Sec: Section C Days: To be announced Begin: 10:00AM End: 11:15AM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Bridges, Willard '),\n",
       " Document(page_content='Spring offering: Course: 03445 Title: Undergraduate Research Units: 1-18 Lec/Sec: Section CC Days: To be announced Begin: 10:00AM End: 11:15AM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Willard, Cai '),\n",
       " Document(page_content='Spring offering: Course: 03445 Title: Undergraduate Research Units: 1-18 Lec/Sec: Section D Days: To be announced Begin: 10:00AM End: 11:15AM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Willard, Mccartney '),\n",
       " Document(page_content='Spring offering: Course: 03445 Title: Undergraduate Research Units: 1-18 Lec/Sec: Section DD Days: To be announced Begin: 10:00AM End: 11:15AM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Willard, Yttri '),\n",
       " Document(page_content='Spring offering: Course: 03445 Title: Undergraduate Research Units: 1-18 Lec/Sec: Section E Days: To be announced Begin: 10:00AM End: 11:15AM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Willard, Ettensohn '),\n",
       " Document(page_content='Spring offering: Course: 03445 Title: Undergraduate Research Units: 1-18 Lec/Sec: Section EE Days: To be announced Begin: 10:00AM End: 11:15AM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Willard, Rule '),\n",
       " Document(page_content='Spring offering: Course: 03445 Title: Undergraduate Research Units: 1-18 Lec/Sec: Section F Days: To be announced Begin: 10:00AM End: 11:15AM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Gittis, Willard '),\n",
       " Document(page_content='Spring offering: Course: 03445 Title: Undergraduate Research Units: 1-18 Lec/Sec: Section FF Days: To be announced Begin: 10:00AM End: 11:15AM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Willard, Schwartz '),\n",
       " Document(page_content='Spring offering: Course: 03445 Title: Undergraduate Research Units: 1-18 Lec/Sec: Section G Days: To be announced Begin: 10:00AM End: 11:15AM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Willard, Zhao '),\n",
       " Document(page_content='Spring offering: Course: 03445 Title: Undergraduate Research Units: 1-18 Lec/Sec: Section GG Days: To be announced Begin: 10:00AM End: 11:15AM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Hiller, Willard '),\n",
       " Document(page_content='Spring offering: Course: 03445 Title: Undergraduate Research Units: 1-18 Lec/Sec: Section H Days: To be announced Begin: 10:00AM End: 11:15AM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Hinman, Willard '),\n",
       " Document(page_content='Spring offering: Course: 03445 Title: Undergraduate Research Units: 1-18 Lec/Sec: Section HH Days: To be announced Begin: 10:00AM End: 11:15AM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Kuang, Willard '),\n",
       " ...]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_documents('/zfsauton2/home/yifuc/11711-RAG/data/cmu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluation metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/zfsauton2/home/yifuc/.conda/envs/llama_hw/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading builder script: 100%|██████████| 6.77k/6.77k [00:00<00:00, 16.8MB/s]\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "\n",
    "f = evaluate.load('f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "invalid literal for int() with base 10: 'shihihit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m prediction \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhihihi\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhe is bad\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhaha no sir\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      2\u001b[0m answer \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mshihihit\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhe is good\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mheyhey yessir\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 3\u001b[0m \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreferences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43manswer\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/llama_hw/lib/python3.11/site-packages/evaluate/module.py:450\u001b[0m, in \u001b[0;36mEvaluationModule.compute\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    447\u001b[0m compute_kwargs \u001b[38;5;241m=\u001b[39m {k: kwargs[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_names()}\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(v \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mvalues()):\n\u001b[0;32m--> 450\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_finalize()\n\u001b[1;32m    453\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcache_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.conda/envs/llama_hw/lib/python3.11/site-packages/evaluate/module.py:515\u001b[0m, in \u001b[0;36mEvaluationModule.add_batch\u001b[0;34m(self, predictions, references, **kwargs)\u001b[0m\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(column) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    514\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enforce_nested_string_type(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselected_feature_format[key], column[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 515\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselected_feature_format\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter\u001b[38;5;241m.\u001b[39mwrite_batch(batch)\n\u001b[1;32m    517\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (pa\u001b[38;5;241m.\u001b[39mArrowInvalid, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n",
      "File \u001b[0;32m~/.conda/envs/llama_hw/lib/python3.11/site-packages/datasets/features/features.py:1922\u001b[0m, in \u001b[0;36mFeatures.encode_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, column \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   1921\u001b[0m     column \u001b[38;5;241m=\u001b[39m cast_to_python_objects(column)\n\u001b[0;32m-> 1922\u001b[0m     encoded_batch[key] \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mencode_nested_example\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mcolumn\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m encoded_batch\n",
      "File \u001b[0;32m~/.conda/envs/llama_hw/lib/python3.11/site-packages/datasets/features/features.py:1922\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1920\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m key, column \u001b[38;5;129;01min\u001b[39;00m batch\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   1921\u001b[0m     column \u001b[38;5;241m=\u001b[39m cast_to_python_objects(column)\n\u001b[0;32m-> 1922\u001b[0m     encoded_batch[key] \u001b[38;5;241m=\u001b[39m [\u001b[43mencode_nested_example\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m column]\n\u001b[1;32m   1923\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m encoded_batch\n",
      "File \u001b[0;32m~/.conda/envs/llama_hw/lib/python3.11/site-packages/datasets/features/features.py:1299\u001b[0m, in \u001b[0;36mencode_nested_example\u001b[0;34m(schema, obj, level)\u001b[0m\n\u001b[1;32m   1296\u001b[0m \u001b[38;5;66;03m# Object with special encoding:\u001b[39;00m\n\u001b[1;32m   1297\u001b[0m \u001b[38;5;66;03m# ClassLabel will convert from string to int, TranslationVariableLanguages does some checks\u001b[39;00m\n\u001b[1;32m   1298\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(schema, (Audio, Image, ClassLabel, TranslationVariableLanguages, Value, _ArrayXD)):\n\u001b[0;32m-> 1299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mschema\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_example\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1300\u001b[0m \u001b[38;5;66;03m# Other object should be directly convertible to a native Arrow type (like Translation and Translation)\u001b[39;00m\n\u001b[1;32m   1301\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m obj\n",
      "File \u001b[0;32m~/.conda/envs/llama_hw/lib/python3.11/site-packages/datasets/features/features.py:514\u001b[0m, in \u001b[0;36mValue.encode_example\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(value)\n\u001b[1;32m    513\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mis_integer(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpa_type):\n\u001b[0;32m--> 514\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    515\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m pa\u001b[38;5;241m.\u001b[39mtypes\u001b[38;5;241m.\u001b[39mis_floating(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpa_type):\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(value)\n",
      "\u001b[0;31mValueError\u001b[0m: invalid literal for int() with base 10: 'shihihit'"
     ]
    }
   ],
   "source": [
    "prediction = ['hihihi', 'he is bad', 'haha no sir']\n",
    "answer = ['shihihit', 'he is good', 'heyhey yessir']\n",
    "f.compute(references=prediction, predictions=answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import collections\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import sys\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "ARTICLES_REGEX = re.compile(r\"\\b(a|an|the)\\b\", re.UNICODE)\n",
    "\n",
    "\n",
    "def normalize_answer(s):\n",
    "    \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "\n",
    "    def remove_articles(text):\n",
    "        return ARTICLES_REGEX.sub(\" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def get_tokens(s):\n",
    "    if not s:\n",
    "        return []\n",
    "    return normalize_answer(s).split()\n",
    "\n",
    "def compute_exact(a_gold, a_pred):\n",
    "    return int(normalize_answer(a_gold) == normalize_answer(a_pred))\n",
    "\n",
    "\n",
    "def compute_f1(a_gold, a_pred):\n",
    "    gold_toks = get_tokens(a_gold)\n",
    "    pred_toks = get_tokens(a_pred)\n",
    "    common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
    "    num_same = sum(common.values())\n",
    "    if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
    "        # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
    "        return int(gold_toks == pred_toks)\n",
    "    if num_same == 0:\n",
    "        return 0\n",
    "    precision = 1.0 * num_same / len(pred_toks)\n",
    "    recall = 1.0 * num_same / len(gold_toks)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1, recall\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4000000000000001, 0.4)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gold = 'hihihi money 123 haha oij'\n",
    "pred = 'hihihi moey 123 hahk aok'\n",
    "\n",
    "compute_exact(gold, pred)\n",
    "\n",
    "compute_f1(gold, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(prediction, truth):\n",
    "    '''\n",
    "    prediction: string\n",
    "    truth: string\n",
    "    return:\n",
    "    f1 and recall between prediction and string\n",
    "    '''\n",
    "    pred_tokens = prediction.split()\n",
    "    truth_tokens = truth.split()\n",
    "    \n",
    "    # if either the prediction or the truth is no-answer then f1 = 1 if they agree, 0 otherwise\n",
    "    if len(pred_tokens) == 0 or len(truth_tokens) == 0:\n",
    "        return int(pred_tokens == truth_tokens), int(pred_tokens == truth_tokens)\n",
    "    \n",
    "    common_tokens = set(pred_tokens) & set(truth_tokens)\n",
    "    \n",
    "    # if there are no common tokens then f1 = 0\n",
    "    if len(common_tokens) == 0:\n",
    "        return 0, 0\n",
    "    \n",
    "    prec = len(common_tokens) / len(pred_tokens)\n",
    "    rec = len(common_tokens) / len(truth_tokens)\n",
    "    f1 = 2 * (prec * rec) / (prec + rec)\n",
    "    return f1, rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4000000000000001, 0.4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_metrics(pred, gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder: GIST-large-Embedding-v0\n",
      "vectordb: FAISS\n",
      "decoder: phi-2\n",
      "items\n",
      "encoder: e5-mistral-7b-instruct\n",
      "vectordb: Chroma\n",
      "decoder: phi-2\n",
      "items\n",
      "encoder: contriever\n",
      "vectordb: FAISS\n",
      "decoder: phi-2\n",
      "items\n",
      "encoder: e5-mistral-7b-instruct\n",
      "vectordb: Chroma\n",
      "decoder: phi-2\n",
      "items\n",
      "encoder: GIST-large-Embedding-v0\n",
      "vectordb: FAISS\n",
      "decoder: phi-2\n",
      "items\n",
      "encoder: e5-mistral-7b-instruct\n",
      "vectordb: FAISS\n",
      "decoder: phi-2\n",
      "items\n",
      "encoder: e5-mistral-7b-instruct\n",
      "vectordb: FAISS\n",
      "decoder: phi-2\n",
      "items\n",
      "encoder: contriever\n",
      "vectordb: FAISS\n",
      "decoder: phi-2\n",
      "items\n",
      "encoder: SFR-Embedding-Mistral\n",
      "vectordb: Chroma\n",
      "decoder: phi-2\n",
      "items\n",
      "encoder: GIST-large-Embedding-v0\n",
      "vectordb: Chroma\n",
      "decoder: phi-2\n",
      "items\n",
      "encoder: contriever\n",
      "vectordb: Chroma\n",
      "decoder: phi-2\n",
      "items\n",
      "encoder: SFR-Embedding-Mistral\n",
      "vectordb: FAISS\n",
      "decoder: phi-2\n",
      "items\n",
      "encoder: GIST-large-Embedding-v0\n",
      "vectordb: Chroma\n",
      "decoder: phi-2\n",
      "items\n",
      "encoder: SFR-Embedding-Mistral\n",
      "vectordb: Chroma\n",
      "decoder: phi-2\n",
      "items\n",
      "encoder: contriever\n",
      "vectordb: Chroma\n",
      "decoder: phi-2\n",
      "items\n",
      "encoder: SFR-Embedding-Mistral\n",
      "vectordb: FAISS\n",
      "decoder: phi-2\n",
      "items\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "result_dir = '/zfsauton2/home/yifuc/11711-RAG/result/cmu'\n",
    "df = {'Dataset': [], 'Encoder': [], 'VectorDB': [], 'Decoder': [], 'F1': [], 'Recall': [], 'Retrieval Accuracy': []}\n",
    "for file in os.listdir(result_dir):\n",
    "    if file.endswith('.json'):\n",
    "        items = file.split('_')\n",
    "        encoder = items[-4]\n",
    "        vectordb = items[-5]\n",
    "        decoder = items[-6]\n",
    "        print(f'encoder: {encoder}')\n",
    "        print(f'vectordb: {vectordb}')\n",
    "        print(f'decoder: {decoder}')\n",
    "        print('items')\n",
    "        \n",
    "        if 'automate' in file:\n",
    "            ds_name = 'automated dataset'\n",
    "        else:\n",
    "            ds_name = 'manual dataset'\n",
    "        \n",
    "        result = json.load(open(result_dir + '/' + file))[-1]\n",
    "        f1, recall, retrieval_acc = result['f1'], result['recall'], result['retrieval_acc']\n",
    "        df['Dataset'].append(ds_name)\n",
    "        df['Encoder'].append(encoder)\n",
    "        df['VectorDB'].append(vectordb)\n",
    "        df['Decoder'].append(decoder)\n",
    "        df['F1'].append(f1)\n",
    "        df['Recall'].append(recall)\n",
    "        df['Retrieval Accuracy'].append(retrieval_acc)\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(df)\n",
    "df.to_csv('/zfsauton2/home/yifuc/11711-RAG/result/cmu.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ColBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/zfsauton2/home/yifuc/.conda/envs/llama_hw/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from ragatouille import RAGTrainer\n",
    "import json\n",
    "import os\n",
    "import io\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.docstore.document import Document\n",
    "import random, numpy as np, torch\n",
    "from os import listdir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _make_w_io_base(f, mode: str):\n",
    "    if not isinstance(f, io.IOBase):\n",
    "        f_dirname = os.path.dirname(f)\n",
    "        if f_dirname != \"\":\n",
    "            os.makedirs(f_dirname, exist_ok=True)\n",
    "        f = open(f, mode=mode)\n",
    "    return f\n",
    "\n",
    "\n",
    "def _make_r_io_base(f, mode: str):\n",
    "    if not isinstance(f, io.IOBase):\n",
    "        f = open(f, mode=mode)\n",
    "    return f\n",
    "\n",
    "def jdump(obj, f, mode=\"w\", indent=4, default=str):\n",
    "    \"\"\"Dump a str or dictionary to a file in json format.\n",
    "\n",
    "    Args:\n",
    "        obj: An object to be written.\n",
    "        f: A string path to the location on disk.\n",
    "        mode: Mode for opening the file.\n",
    "        indent: Indent for storing json dictionaries.\n",
    "        default: A function to handle non-serializable entries; defaults to `str`.\n",
    "    \"\"\"\n",
    "    f = _make_w_io_base(f, mode)\n",
    "    if isinstance(obj, (dict, list)):\n",
    "        json.dump(obj, f, indent=indent, default=default)\n",
    "    elif isinstance(obj, str):\n",
    "        f.write(obj)\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected type: {type(obj)}\")\n",
    "    f.close()\n",
    "\n",
    "def jload(f, mode=\"r\"):\n",
    "    \"\"\"Load a .json file into a dictionary.\"\"\"\n",
    "    f = _make_r_io_base(f, mode)\n",
    "    jdict = json.load(f)\n",
    "    f.close()\n",
    "    return jdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def load_documents(text_path, chunk_size=1000, chunk_overlap=150):\n",
    "\n",
    "    '''\n",
    "    text_path: str, a directory of .txt files\n",
    "    return: \n",
    "    langchain document format (a list of trucated document)\n",
    "    '''\n",
    "    if 'databrick' in text_path:\n",
    "        loader = DirectoryLoader(text_path, glob=\"**/*.txt\")\n",
    "        data = loader.load()\n",
    "        text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        docs = text_splitter.split_documents(data)\n",
    "        return docs\n",
    "    elif 'cmu' in text_path:\n",
    "        docs = []\n",
    "        #files that need to be truncated\n",
    "        truncate_files = ['buggy_history.json', 'cmu_history.json', 'handbook_text.json', 'kiltie_band_fact.json',\n",
    "                          'scotty_fact.json', 'tartan_fact.json']\n",
    "        files = listdir(text_path)\n",
    "        docs = []\n",
    "        for file in files:\n",
    "            doc = jload(text_path + '/' + file)\n",
    "            doc = [Document(page_content = s) for s in doc]\n",
    "            if file not in truncate_files:\n",
    "                docs.extend(doc)\n",
    "            else:\n",
    "                text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "                doc = text_splitter.split_documents(doc)\n",
    "                docs.extend(doc)\n",
    "        return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = load_documents('/zfsauton2/home/yifuc/11711-RAG/data/cmu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [d.page_content for d in documents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluate zero shot retrieval \n",
    "from ragatouille import RAGPretrainedModel\n",
    "from ragatouille.utils import get_wikipedia_page\n",
    "\n",
    "RAG = RAGPretrainedModel.from_pretrained(\"colbert-ir/colbertv2.0\", n_gpu=1)\n",
    "index_path = RAG.index(index_name=\"/home/scratch/yifuc/checkpoints/colbert\", collection=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading searcher for index /home/scratch/yifuc/checkpoints/colbert for the first time... This may take a few seconds\n",
      "[Feb 24, 17:46:15] #> Loading codec...\n",
      "[Feb 24, 17:46:15] #> Loading IVF...\n",
      "[Feb 24, 17:46:16] #> Loading doclens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 2130.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 24, 17:46:16] #> Loading codes and residuals...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 72.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searcher loaded!\n",
      "\n",
      "#> QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "#> Input: . Where can a mural celebrating Buggy be found within CMU, and what does it depict?, \t\t True, \t\t None\n",
      "#> Output IDs: torch.Size([32]), tensor([  101,     1,  2073,  2064,  1037, 15533, 12964, 11829,  6292,  2022,\n",
      "         2179,  2306,  4642,  2226,  1010,  1998,  2054,  2515,  2009, 17120,\n",
      "         1029,   102,   103,   103,   103,   103,   103,   103,   103,   103,\n",
      "          103,   103], device='cuda:0')\n",
      "#> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manual accu: 0.3333333333333333\n",
      "automate accu: 0.5540540540540541\n"
     ]
    }
   ],
   "source": [
    "#evaluate zero shot retrieval \n",
    "from ragatouille import RAGPretrainedModel\n",
    "\n",
    "RAG = RAGPretrainedModel.from_index(\"/home/scratch/yifuc/checkpoints/colbert\")\n",
    "manual_data = jload('/zfsauton2/home/yifuc/11711-RAG/data/questions.json')\n",
    "automate_data = jload('/zfsauton2/home/yifuc/11711-RAG/data/automated_questions.json')\n",
    "\n",
    "manual_correct, manual_total = 0, 0\n",
    "for data in manual_data:\n",
    "    manual_total += 1\n",
    "    question = data['question']\n",
    "    results = RAG.search(question)\n",
    "    for result in results:\n",
    "        if data['answer'] in result['content']:\n",
    "            manual_correct += 1\n",
    "            break\n",
    "print(f'manual accu: {manual_correct/manual_total}')\n",
    "\n",
    "automate_correct, automate_total = 0, 0\n",
    "for data in automate_data:\n",
    "    automate_total += 1\n",
    "    question = data['question']\n",
    "    results = RAG.search(question)\n",
    "    for result in results:\n",
    "        if data['answer'] in result['content']:\n",
    "            automate_correct += 1\n",
    "            break\n",
    "print(f'automate accu: {automate_correct/automate_total}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = []\n",
    "manual_data = jload('/zfsauton2/home/yifuc/11711-RAG/data/questions.json')\n",
    "automate_data = jload('/zfsauton2/home/yifuc/11711-RAG/data/automated_questions.json')\n",
    "automate_manual_data = jload('/zfsauton2/home/yifuc/11711-RAG/data/augmented_data/generated_dataset_with_facts_5000.json')\n",
    "\n",
    "for data in automate_manual_data:\n",
    "    pairs.append((data['question'], data['context']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Hard Negative SimpleMiner dense embedding model BAAI/bge-small-en-v1.5...\n",
      "Building hard negative index for 20173 documents...\n",
      "All documents embedded, now adding to index...\n",
      "save_index set to False, skipping saving hard negative index\n",
      "Hard negative index generated\n",
      "#> Starting...\n",
      "nranks = 1 \t num_gpus = 8 \t device=0\n",
      "{\n",
      "    \"query_token_id\": \"[unused0]\",\n",
      "    \"doc_token_id\": \"[unused1]\",\n",
      "    \"query_token\": \"[Q]\",\n",
      "    \"doc_token\": \"[D]\",\n",
      "    \"ncells\": null,\n",
      "    \"centroid_score_threshold\": null,\n",
      "    \"ndocs\": null,\n",
      "    \"load_index_with_mmap\": false,\n",
      "    \"index_path\": null,\n",
      "    \"index_bsize\": 64,\n",
      "    \"nbits\": 2,\n",
      "    \"kmeans_niters\": 20,\n",
      "    \"resume\": false,\n",
      "    \"similarity\": \"cosine\",\n",
      "    \"bsize\": 32,\n",
      "    \"accumsteps\": 1,\n",
      "    \"lr\": 5e-6,\n",
      "    \"maxsteps\": 500000,\n",
      "    \"save_every\": 122,\n",
      "    \"warmup\": 122,\n",
      "    \"warmup_bert\": null,\n",
      "    \"relu\": false,\n",
      "    \"nway\": 2,\n",
      "    \"use_ib_negatives\": true,\n",
      "    \"reranker\": false,\n",
      "    \"distillation_alpha\": 1.0,\n",
      "    \"ignore_scores\": false,\n",
      "    \"model_name\": \"fintunedonautomate\",\n",
      "    \"query_maxlen\": 32,\n",
      "    \"attend_to_mask_tokens\": false,\n",
      "    \"interaction\": \"colbert\",\n",
      "    \"dim\": 128,\n",
      "    \"doc_maxlen\": 256,\n",
      "    \"mask_punctuation\": true,\n",
      "    \"checkpoint\": \"colbert-ir\\/colbertv2.0\",\n",
      "    \"triples\": \"\\/home\\/scratch\\/yifuc\\/data\\/colbert\\/triples.train.colbert.jsonl\",\n",
      "    \"collection\": \"\\/home\\/scratch\\/yifuc\\/data\\/colbert\\/corpus.train.colbert.tsv\",\n",
      "    \"queries\": \"\\/home\\/scratch\\/yifuc\\/data\\/colbert\\/queries.train.colbert.tsv\",\n",
      "    \"index_name\": null,\n",
      "    \"overwrite\": false,\n",
      "    \"root\": \".ragatouille\\/\",\n",
      "    \"experiment\": \"colbert\",\n",
      "    \"index_root\": null,\n",
      "    \"name\": \"2024-02\\/25\\/08.57.38\",\n",
      "    \"rank\": 0,\n",
      "    \"nranks\": 1,\n",
      "    \"amp\": true,\n",
      "    \"gpus\": 8,\n",
      "    \"avoid_fork_if_possible\": false\n",
      "}\n",
      "Using config.bsize = 32 (per process) and config.accumsteps = 1\n",
      "[Feb 25, 09:01:09] #> Loading the queries from /home/scratch/yifuc/data/colbert/queries.train.colbert.tsv ...\n",
      "[Feb 25, 09:01:09] #> Got 3826 queries. All QIDs are unique.\n",
      "\n",
      "[Feb 25, 09:01:09] #> Loading collection...\n",
      "0M \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/zfsauton2/home/yifuc/.conda/envs/llama_hw/lib/python3.11/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#> LR will use 122 warmup steps and linear decay over 500000 steps.\n",
      "\n",
      "#> QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "#> Input: . What is the proposed solution for enhancing mental health care for refugees?, \t\t True, \t\t None\n",
      "#> Output IDs: torch.Size([32]), tensor([  101,     1,  2054,  2003,  1996,  3818,  5576,  2005, 20226,  5177,\n",
      "         2740,  2729,  2005,  8711,  1029,   102,   103,   103,   103,   103,\n",
      "          103,   103,   103,   103,   103,   103,   103,   103,   103,   103,\n",
      "          103,   103], device='cuda:0')\n",
      "#> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "\n",
      "\t\t\t\t 0.4289298355579376 0.3925057351589203\n",
      "#>>>    20.81 13.01 \t\t|\t\t 7.799999999999999\n",
      "[Feb 25, 09:01:11] 0 0.8214355707168579\n",
      "\t\t\t\t 0.28376349806785583 0.6383675932884216\n",
      "#>>>    20.83 13.51 \t\t|\t\t 7.3199999999999985\n",
      "[Feb 25, 09:01:11] 1 0.8215362662076949\n",
      "\t\t\t\t 0.09349390119314194 0.34306272864341736\n",
      "#>>>    21.12 13.95 \t\t|\t\t 7.170000000000002\n",
      "[Feb 25, 09:01:11] 2 0.8211512865787743\n",
      "\t\t\t\t 0.29426902532577515 0.5331268906593323\n",
      "#>>>    20.27 13.67 \t\t|\t\t 6.6\n",
      "[Feb 25, 09:01:12] 3 0.8211575312081807\n",
      "\t\t\t\t 0.298247754573822 0.5973193049430847\n",
      "#>>>    20.11 14.53 \t\t|\t\t 5.58\n",
      "[Feb 25, 09:01:12] 4 0.8212319407364894\n",
      "\t\t\t\t 0.461195707321167 0.849053144454956\n",
      "#>>>    20.62 13.4 \t\t|\t\t 7.220000000000001\n",
      "[Feb 25, 09:01:12] 5 0.8217209576475291\n",
      "\t\t\t\t 0.2610423266887665 0.6363449096679688\n",
      "#>>>    20.22 13.92 \t\t|\t\t 6.299999999999999\n",
      "[Feb 25, 09:01:13] 6 0.8217966239560406\n",
      "\t\t\t\t 0.3064231872558594 0.4957442283630371\n",
      "#>>>    21.22 14.02 \t\t|\t\t 7.199999999999999\n",
      "[Feb 25, 09:01:13] 7 0.8217769947477034\n",
      "\t\t\t\t 0.1088729277253151 0.28564560413360596\n",
      "#>>>    21.47 13.39 \t\t|\t\t 8.079999999999998\n",
      "[Feb 25, 09:01:13] 8 0.821349736277364\n",
      "\t\t\t\t 0.11773142963647842 0.3792911469936371\n",
      "#>>>    20.95 13.63 \t\t|\t\t 7.3199999999999985\n",
      "[Feb 25, 09:01:14] 9 0.8210254091102661\n",
      "\t\t\t\t 0.2633712887763977 0.6585776209831238\n",
      "#>>>    20.04 12.29 \t\t|\t\t 7.75\n",
      "[Feb 25, 09:01:14] 10 0.8211263326109154\n",
      "\t\t\t\t 0.2587669789791107 0.6000325083732605\n",
      "#>>>    20.98 15.95 \t\t|\t\t 5.030000000000001\n",
      "[Feb 25, 09:01:14] 11 0.8211640057358546\n",
      "\t\t\t\t 0.18880344927310944 0.625951886177063\n",
      "#>>>    19.85 12.83 \t\t|\t\t 7.020000000000001\n",
      "[Feb 25, 09:01:14] 12 0.8211575970506677\n",
      "\t\t\t\t 0.8563551306724548 1.2506532669067383\n",
      "#>>>    19.68 14.42 \t\t|\t\t 5.26\n",
      "[Feb 25, 09:01:15] 13 0.8224434479108008\n",
      "\t\t\t\t 0.15795718133449554 0.7470335960388184\n",
      "#>>>    20.79 13.13 \t\t|\t\t 7.659999999999998\n",
      "[Feb 25, 09:01:15] 14 0.8225259952551645\n",
      "\t\t\t\t 0.27301132678985596 0.6473758816719055\n",
      "#>>>    20.57 14.34 \t\t|\t\t 6.23\n",
      "[Feb 25, 09:01:15] 15 0.8226238564683711\n",
      "\t\t\t\t 0.9568436741828918 1.8840525150299072\n",
      "#>>>    18.8 14.84 \t\t|\t\t 3.960000000000001\n",
      "[Feb 25, 09:01:16] 16 0.8246421287415109\n",
      "\t\t\t\t 0.3594393730163574 0.7774786353111267\n",
      "#>>>    19.81 13.28 \t\t|\t\t 6.529999999999999\n",
      "[Feb 25, 09:01:16] 17 0.8249544046807015\n",
      "\t\t\t\t 0.5859770774841309 0.9442682862281799\n",
      "#>>>    19.51 14.04 \t\t|\t\t 5.470000000000002\n",
      "[Feb 25, 09:01:16] 18 0.8256596955801284\n",
      "\t\t\t\t 0.2928653359413147 0.6504900455474854\n",
      "#>>>    19.67 14.49 \t\t|\t\t 5.1800000000000015\n",
      "[Feb 25, 09:01:17] 19 0.8257773912660371\n",
      "\t\t\t\t 0.2792559862136841 0.5989229083061218\n",
      "#>>>    20.71 15.24 \t\t|\t\t 5.470000000000001\n",
      "[Feb 25, 09:01:17] 20 0.8258297927692909\n",
      "\t\t\t\t 0.4024913012981415 0.8381031155586243\n",
      "#>>>    20.23 13.77 \t\t|\t\t 6.460000000000001\n",
      "[Feb 25, 09:01:17] 21 0.8262445573635762\n",
      "\t\t\t\t 0.23330573737621307 0.7540204524993896\n",
      "#>>>    20.37 14.36 \t\t|\t\t 6.010000000000002\n",
      "[Feb 25, 09:01:17] 22 0.8264056390109894\n",
      "\t\t\t\t 0.11148177087306976 0.3674355447292328\n",
      "#>>>    20.78 12.64 \t\t|\t\t 8.14\n",
      "[Feb 25, 09:01:18] 23 0.8260581506726795\n",
      "\t\t\t\t 0.40675830841064453 1.1528339385986328\n",
      "#>>>    19.88 13.04 \t\t|\t\t 6.84\n",
      "[Feb 25, 09:01:18] 24 0.8267916847690161\n",
      "\t\t\t\t 0.4808835983276367 0.7902430891990662\n",
      "#>>>    21.23 13.94 \t\t|\t\t 7.290000000000001\n",
      "[Feb 25, 09:01:18] 25 0.8272360198313784\n",
      "\t\t\t\t 0.28950127959251404 0.583734393119812\n",
      "#>>>    20.53 13.44 \t\t|\t\t 7.090000000000002\n",
      "[Feb 25, 09:01:19] 26 0.8272820195140617\n",
      "\t\t\t\t 0.13615666329860687 0.7966258525848389\n",
      "#>>>    20.35 13.06 \t\t|\t\t 7.290000000000001\n",
      "[Feb 25, 09:01:19] 27 0.8273875200253322\n",
      "\t\t\t\t 0.21380075812339783 0.4536300599575043\n",
      "#>>>    21.15 13.86 \t\t|\t\t 7.289999999999999\n",
      "[Feb 25, 09:01:19] 28 0.8272275633233878\n",
      "\t\t\t\t 0.9053465723991394 1.0582071542739868\n",
      "#>>>    18.93 14.58 \t\t|\t\t 4.35\n",
      "[Feb 25, 09:01:20] 29 0.8283638894271328\n",
      "\t\t\t\t 0.1905660778284073 0.7248572707176208\n",
      "#>>>    20.56 14.1 \t\t|\t\t 6.459999999999999\n",
      "[Feb 25, 09:01:20] 30 0.8284509488713506\n",
      "\t\t\t\t 0.7078426480293274 1.1882117986679077\n",
      "#>>>    18.99 13.47 \t\t|\t\t 5.519999999999998\n",
      "[Feb 25, 09:01:20] 31 0.8295185524287811\n",
      "\t\t\t\t 0.4383522868156433 0.44837892055511475\n",
      "#>>>    21.06 14.79 \t\t|\t\t 6.27\n",
      "[Feb 25, 09:01:21] 32 0.8295757650837231\n",
      "\t\t\t\t 0.27488845586776733 0.7017074823379517\n",
      "#>>>    19.82 13.68 \t\t|\t\t 6.140000000000001\n",
      "[Feb 25, 09:01:21] 33 0.8297227852568452\n",
      "\t\t\t\t 0.23210054636001587 0.9886909127235413\n",
      "#>>>    19.73 13.69 \t\t|\t\t 6.040000000000001\n",
      "[Feb 25, 09:01:21] 34 0.8301138539306719\n",
      "\t\t\t\t 0.5412083268165588 0.6960870027542114\n",
      "#>>>    20.32 14.32 \t\t|\t\t 6.0\n",
      "[Feb 25, 09:01:21] 35 0.8305210354659166\n",
      "\t\t\t\t 0.023460879921913147 0.3663260340690613\n",
      "#>>>    21.68 14.03 \t\t|\t\t 7.65\n",
      "[Feb 25, 09:01:22] 36 0.8300803013295406\n",
      "\t\t\t\t 0.189335435628891 0.905563473701477\n",
      "#>>>    20.73 14.6 \t\t|\t\t 6.130000000000001\n",
      "[Feb 25, 09:01:22] 37 0.8303451199673437\n",
      "\t\t\t\t 0.16488231718540192 0.5901625156402588\n",
      "#>>>    21.01 13.62 \t\t|\t\t 7.390000000000002\n",
      "[Feb 25, 09:01:22] 38 0.8302698196653009\n",
      "\t\t\t\t 0.8740459084510803 1.1772572994232178\n",
      "#>>>    20.74 14.12 \t\t|\t\t 6.619999999999999\n",
      "[Feb 25, 09:01:23] 39 0.8314908529939052\n",
      "\t\t\t\t 0.20848532021045685 0.3623289465904236\n",
      "#>>>    19.98 12.2 \t\t|\t\t 7.780000000000001\n",
      "[Feb 25, 09:01:23] 40 0.831230176392811\n",
      "\t\t\t\t 0.25042498111724854 0.7202893495559692\n",
      "#>>>    20.68 14.88 \t\t|\t\t 5.799999999999999\n",
      "[Feb 25, 09:01:23] 41 0.8313696605470914\n",
      "\t\t\t\t 0.49742311239242554 0.7178328633308411\n",
      "#>>>    20.18 13.31 \t\t|\t\t 6.869999999999999\n",
      "[Feb 25, 09:01:24] 42 0.8317535468622675\n",
      "\t\t\t\t 0.08374721556901932 0.11276766657829285\n",
      "#>>>    21.72 13.27 \t\t|\t\t 8.45\n",
      "[Feb 25, 09:01:24] 43 0.831118308190102\n",
      "\t\t\t\t 0.1212068498134613 0.4138730466365814\n",
      "#>>>    20.39 14.58 \t\t|\t\t 5.8100000000000005\n",
      "[Feb 25, 09:01:24] 44 0.830822269778362\n",
      "\t\t\t\t 0.08510193228721619 0.3463599383831024\n",
      "#>>>    21.46 12.98 \t\t|\t\t 8.48\n",
      "[Feb 25, 09:01:24] 45 0.830422909379254\n",
      "\t\t\t\t 0.6815672516822815 1.8071660995483398\n",
      "#>>>    18.62 14.03 \t\t|\t\t 4.590000000000002\n",
      "[Feb 25, 09:01:25] 46 0.8320812197615007\n",
      "\t\t\t\t 0.3985566198825836 1.0403155088424683\n",
      "#>>>    19.19 12.44 \t\t|\t\t 6.750000000000002\n",
      "[Feb 25, 09:01:25] 47 0.8326880106406619\n",
      "\t\t\t\t 0.3178475499153137 0.40241944789886475\n",
      "#>>>    20.11 14.11 \t\t|\t\t 6.0\n",
      "[Feb 25, 09:01:25] 48 0.8325755896278354\n",
      "\t\t\t\t 0.7318359017372131 1.4543834924697876\n",
      "#>>>    19.03 13.81 \t\t|\t\t 5.220000000000001\n",
      "[Feb 25, 09:01:26] 49 0.8339292334920193\n",
      "\t\t\t\t 0.35527727007865906 1.5466145277023315\n",
      "#>>>    18.69 13.51 \t\t|\t\t 5.1800000000000015\n",
      "[Feb 25, 09:01:26] 50 0.8349971960861106\n",
      "\t\t\t\t 0.6564813852310181 1.3401607275009155\n",
      "#>>>    19.9 13.89 \t\t|\t\t 6.009999999999998\n",
      "[Feb 25, 09:01:26] 51 0.8361588410027564\n",
      "\t\t\t\t 0.13985110819339752 0.46107620000839233\n",
      "#>>>    20.71 13.05 \t\t|\t\t 7.66\n",
      "[Feb 25, 09:01:27] 52 0.8359236094550543\n",
      "\t\t\t\t 0.1482572853565216 0.6082614660263062\n",
      "#>>>    20.07 12.76 \t\t|\t\t 7.3100000000000005\n",
      "[Feb 25, 09:01:27] 53 0.8358442045671798\n",
      "\t\t\t\t 0.21506616473197937 0.5492023229598999\n",
      "#>>>    20.32 13.82 \t\t|\t\t 6.5\n",
      "[Feb 25, 09:01:27] 54 0.8357726288801068\n",
      "\t\t\t\t 0.2280561923980713 0.49813663959503174\n",
      "#>>>    20.82 13.82 \t\t|\t\t 7.0\n",
      "[Feb 25, 09:01:27] 55 0.8356630490832199\n",
      "\t\t\t\t 0.12150588631629944 0.3318917155265808\n",
      "#>>>    20.77 11.21 \t\t|\t\t 9.559999999999999\n",
      "[Feb 25, 09:01:28] 56 0.8352807836359796\n",
      "\t\t\t\t 0.40174177289009094 0.2565077245235443\n",
      "#>>>    20.96 13.84 \t\t|\t\t 7.120000000000001\n",
      "[Feb 25, 09:01:28] 57 0.8351037523497573\n",
      "\t\t\t\t 0.15310993790626526 0.5288532376289368\n",
      "#>>>    20.73 13.69 \t\t|\t\t 7.040000000000001\n",
      "[Feb 25, 09:01:28] 58 0.8349506118027451\n",
      "\t\t\t\t 0.5099619626998901 1.2060047388076782\n",
      "#>>>    19.78 12.93 \t\t|\t\t 6.850000000000001\n",
      "[Feb 25, 09:01:29] 59 0.8358316278924499\n",
      "\t\t\t\t 0.21978063881397247 1.056118130683899\n",
      "#>>>    20.31 14.11 \t\t|\t\t 6.199999999999999\n",
      "[Feb 25, 09:01:29] 60 0.8362716950787588\n",
      "\t\t\t\t 0.6338357329368591 0.8053693771362305\n",
      "#>>>    19.94 13.96 \t\t|\t\t 5.98\n",
      "[Feb 25, 09:01:29] 61 0.8368746285533578\n",
      "\t\t\t\t 0.05555417761206627 0.4061755836009979\n",
      "#>>>    20.94 14.09 \t\t|\t\t 6.850000000000001\n",
      "[Feb 25, 09:01:30] 62 0.8364994836897428\n",
      "\t\t\t\t 0.11162823438644409 0.30489885807037354\n",
      "#>>>    20.35 12.66 \t\t|\t\t 7.690000000000001\n",
      "[Feb 25, 09:01:30] 63 0.8360795112985099\n",
      "\t\t\t\t 0.38604965806007385 0.9822635054588318\n",
      "#>>>    19.75 14.17 \t\t|\t\t 5.58\n",
      "[Feb 25, 09:01:30] 64 0.8366117449805326\n",
      "\t\t\t\t 0.9033956527709961 1.6870646476745605\n",
      "#>>>    19.1 14.59 \t\t|\t\t 4.510000000000002\n",
      "[Feb 25, 09:01:30] 65 0.8383655935359976\n",
      "\t\t\t\t 0.27483439445495605 0.5696039199829102\n",
      "#>>>    20.59 13.94 \t\t|\t\t 6.65\n",
      "[Feb 25, 09:01:31] 66 0.8383716662568994\n",
      "\t\t\t\t 0.5220290422439575 0.9064019322395325\n",
      "#>>>    19.84 12.53 \t\t|\t\t 7.3100000000000005\n",
      "[Feb 25, 09:01:31] 67 0.8389617256247307\n",
      "\t\t\t\t 0.05590413883328438 0.23079025745391846\n",
      "#>>>    20.84 12.62 \t\t|\t\t 8.22\n",
      "[Feb 25, 09:01:31] 68 0.8384094583065691\n",
      "\t\t\t\t 0.47347724437713623 0.8472355604171753\n",
      "#>>>    19.41 13.0 \t\t|\t\t 6.41\n",
      "[Feb 25, 09:01:32] 69 0.8388917616530568\n",
      "\t\t\t\t 0.09632470458745956 0.4752621650695801\n",
      "#>>>    19.88 12.36 \t\t|\t\t 7.52\n",
      "[Feb 25, 09:01:32] 70 0.838624456738709\n",
      "\t\t\t\t 0.2610374391078949 0.44557005167007446\n",
      "#>>>    19.81 12.22 \t\t|\t\t 7.589999999999998\n",
      "[Feb 25, 09:01:32] 71 0.838492439742946\n",
      "\t\t\t\t 0.23312072455883026 0.7145324349403381\n",
      "#>>>    19.41 13.25 \t\t|\t\t 6.16\n",
      "[Feb 25, 09:01:33] 72 0.8386016004776033\n",
      "\t\t\t\t 0.2449110746383667 0.6343311667442322\n",
      "#>>>    19.93 12.73 \t\t|\t\t 7.199999999999999\n",
      "[Feb 25, 09:01:33] 73 0.8386422411185084\n",
      "\t\t\t\t 0.41569504141807556 0.8326911330223083\n",
      "#>>>    19.42 12.3 \t\t|\t\t 7.120000000000001\n",
      "[Feb 25, 09:01:33] 74 0.8390519850220279\n",
      "\t\t\t\t 0.17439720034599304 0.775078535079956\n",
      "#>>>    19.79 13.13 \t\t|\t\t 6.659999999999998\n",
      "[Feb 25, 09:01:33] 75 0.8391624088022342\n",
      "\t\t\t\t 0.6359971761703491 1.1594831943511963\n",
      "#>>>    19.78 13.06 \t\t|\t\t 6.720000000000001\n",
      "[Feb 25, 09:01:34] 76 0.8401187267639536\n",
      "\t\t\t\t 0.37658369541168213 0.594672679901123\n",
      "#>>>    20.23 14.51 \t\t|\t\t 5.720000000000001\n",
      "[Feb 25, 09:01:34] 77 0.8402498644125025\n",
      "\t\t\t\t 0.14183305203914642 0.41164928674697876\n",
      "#>>>    20.76 12.48 \t\t|\t\t 8.280000000000001\n",
      "[Feb 25, 09:01:34] 78 0.8399630969017772\n",
      "\t\t\t\t 0.16613523662090302 0.38332900404930115\n",
      "#>>>    19.81 12.27 \t\t|\t\t 7.539999999999999\n",
      "[Feb 25, 09:01:35] 79 0.8396725980306444\n",
      "\t\t\t\t 0.19804571568965912 0.7495438456535339\n",
      "#>>>    20.48 13.38 \t\t|\t\t 7.1\n",
      "[Feb 25, 09:01:35] 80 0.8397805150088581\n",
      "\t\t\t\t 0.8462848663330078 1.548818588256836\n",
      "#>>>    19.26 14.04 \t\t|\t\t 5.220000000000002\n",
      "[Feb 25, 09:01:35] 81 0.841335837948439\n",
      "\t\t\t\t 0.8178623914718628 1.0065442323684692\n",
      "#>>>    19.67 13.22 \t\t|\t\t 6.450000000000001\n",
      "[Feb 25, 09:01:36] 82 0.8423189087343309\n",
      "\t\t\t\t 0.0905802994966507 0.4437917470932007\n",
      "#>>>    20.3 13.12 \t\t|\t\t 7.1800000000000015\n",
      "[Feb 25, 09:01:36] 83 0.8420109618572853\n",
      "\t\t\t\t 0.22529025375843048 0.625139057636261\n",
      "#>>>    20.84 13.07 \t\t|\t\t 7.77\n",
      "[Feb 25, 09:01:36] 84 0.8420193801919215\n",
      "\t\t\t\t 0.1405985802412033 0.38966667652130127\n",
      "#>>>    20.24 13.42 \t\t|\t\t 6.8199999999999985\n",
      "[Feb 25, 09:01:37] 85 0.8417076260833932\n",
      "\t\t\t\t 0.46365886926651 0.863369882106781\n",
      "#>>>    19.52 13.71 \t\t|\t\t 5.809999999999999\n",
      "[Feb 25, 09:01:37] 86 0.8421929472086831\n",
      "\t\t\t\t 0.3549782931804657 0.6410447955131531\n",
      "#>>>    20.09 13.63 \t\t|\t\t 6.459999999999999\n",
      "[Feb 25, 09:01:37] 87 0.8423467773203658\n",
      "\t\t\t\t 0.23872242867946625 0.7253848910331726\n",
      "#>>>    19.2 13.59 \t\t|\t\t 5.609999999999999\n",
      "[Feb 25, 09:01:37] 88 0.8424685378776593\n",
      "\t\t\t\t 0.1222202479839325 0.23863385617733002\n",
      "#>>>    21.17 13.23 \t\t|\t\t 7.940000000000001\n",
      "[Feb 25, 09:01:38] 89 0.8419869234290418\n",
      "\t\t\t\t 0.26967906951904297 1.0523629188537598\n",
      "#>>>    20.01 14.38 \t\t|\t\t 5.630000000000001\n",
      "[Feb 25, 09:01:38] 90 0.8424669784939856\n",
      "\t\t\t\t 0.46785858273506165 0.6934716105461121\n",
      "#>>>    20.14 13.5 \t\t|\t\t 6.640000000000001\n",
      "[Feb 25, 09:01:38] 91 0.8427858417385751\n",
      "\t\t\t\t 0.1170184314250946 0.3343598544597626\n",
      "#>>>    20.51 13.91 \t\t|\t\t 6.600000000000001\n",
      "[Feb 25, 09:01:39] 92 0.8423944341827213\n",
      "\t\t\t\t 0.4402356743812561 0.7316099405288696\n",
      "#>>>    19.64 14.12 \t\t|\t\t 5.520000000000001\n",
      "[Feb 25, 09:01:39] 93 0.8427238854230533\n",
      "\t\t\t\t 0.1881648153066635 0.6983842253684998\n",
      "#>>>    20.0 14.37 \t\t|\t\t 5.630000000000001\n",
      "[Feb 25, 09:01:39] 94 0.8427677105932065\n",
      "\t\t\t\t 0.09015470743179321 0.45912832021713257\n",
      "#>>>    20.01 12.77 \t\t|\t\t 7.240000000000002\n",
      "[Feb 25, 09:01:40] 95 0.8424742259102622\n",
      "\t\t\t\t 0.5695483684539795 1.084687352180481\n",
      "#>>>    19.39 14.21 \t\t|\t\t 5.18\n",
      "[Feb 25, 09:01:40] 96 0.8432859874049865\n",
      "\t\t\t\t 0.25384968519210815 0.6576593518257141\n",
      "#>>>    20.8 14.44 \t\t|\t\t 6.360000000000001\n",
      "[Feb 25, 09:01:40] 97 0.8433542104545994\n",
      "\t\t\t\t 0.23038367927074432 0.5947138667106628\n",
      "#>>>    21.39 14.3 \t\t|\t\t 7.09\n",
      "[Feb 25, 09:01:40] 98 0.8433359538050273\n",
      "\t\t\t\t 0.5201840400695801 0.6194028258323669\n",
      "#>>>    19.94 13.08 \t\t|\t\t 6.860000000000001\n",
      "[Feb 25, 09:01:41] 99 0.843632204776729\n",
      "\t\t\t\t 0.4521752595901489 0.8414428234100342\n",
      "#>>>    19.57 11.84 \t\t|\t\t 7.73\n",
      "[Feb 25, 09:01:41] 100 0.8440821906549525\n",
      "\t\t\t\t 0.2958306074142456 0.6405377984046936\n",
      "#>>>    19.81 13.53 \t\t|\t\t 6.279999999999999\n",
      "[Feb 25, 09:01:41] 101 0.8441744768701165\n",
      "\t\t\t\t 0.4817924201488495 0.8286840915679932\n",
      "#>>>    18.39 12.49 \t\t|\t\t 5.9\n",
      "[Feb 25, 09:01:42] 102 0.8446407789347655\n",
      "\t\t\t\t 0.2915005385875702 0.3455957770347595\n",
      "#>>>    20.66 11.8 \t\t|\t\t 8.86\n",
      "[Feb 25, 09:01:42] 103 0.8444332344416507\n",
      "\t\t\t\t 0.3728378117084503 1.0247913599014282\n",
      "#>>>    19.42 12.77 \t\t|\t\t 6.650000000000002\n",
      "[Feb 25, 09:01:42] 104 0.8449864303490167\n",
      "\t\t\t\t 0.24251320958137512 0.7784369587898254\n",
      "#>>>    19.35 13.32 \t\t|\t\t 6.030000000000001\n",
      "[Feb 25, 09:01:43] 105 0.8451623941168411\n",
      "\t\t\t\t 0.22863291203975677 0.49709850549697876\n",
      "#>>>    20.75 12.81 \t\t|\t\t 7.9399999999999995\n",
      "[Feb 25, 09:01:43] 106 0.8450429631551623\n",
      "\t\t\t\t 0.09881515055894852 0.18301589787006378\n",
      "#>>>    21.09 13.31 \t\t|\t\t 7.779999999999999\n",
      "[Feb 25, 09:01:43] 107 0.8444797512478868\n",
      "\t\t\t\t 0.07498469948768616 0.669735312461853\n",
      "#>>>    19.05 12.26 \t\t|\t\t 6.790000000000001\n",
      "[Feb 25, 09:01:43] 108 0.8443799914787861\n",
      "\t\t\t\t 0.22498297691345215 0.5888079404830933\n",
      "#>>>    19.91 12.61 \t\t|\t\t 7.300000000000001\n",
      "[Feb 25, 09:01:44] 109 0.8443494024047038\n",
      "\t\t\t\t 0.12379882484674454 0.4791419506072998\n",
      "#>>>    19.72 11.84 \t\t|\t\t 7.879999999999999\n",
      "[Feb 25, 09:01:44] 110 0.8441079938001048\n",
      "\t\t\t\t 0.14228686690330505 0.35551509261131287\n",
      "#>>>    20.83 12.71 \t\t|\t\t 8.119999999999997\n",
      "[Feb 25, 09:01:44] 111 0.8437616877658194\n",
      "\t\t\t\t 0.17020930349826813 0.34905484318733215\n",
      "#>>>    20.63 12.58 \t\t|\t\t 8.049999999999999\n",
      "[Feb 25, 09:01:45] 112 0.8434371902396404\n",
      "\t\t\t\t 0.3300802707672119 0.6644335985183716\n",
      "#>>>    20.44 13.06 \t\t|\t\t 7.380000000000001\n",
      "[Feb 25, 09:01:45] 113 0.8435882669186864\n",
      "\t\t\t\t 0.19732674956321716 0.5714259743690491\n",
      "#>>>    19.99 13.07 \t\t|\t\t 6.919999999999998\n",
      "[Feb 25, 09:01:45] 114 0.8435134313458976\n",
      "\t\t\t\t 0.7185933589935303 1.0137797594070435\n",
      "#>>>    19.37 12.31 \t\t|\t\t 7.0600000000000005\n",
      "[Feb 25, 09:01:46] 115 0.8444022910329523\n",
      "\t\t\t\t 0.5239852666854858 0.8024318814277649\n",
      "#>>>    19.36 13.05 \t\t|\t\t 6.309999999999999\n",
      "[Feb 25, 09:01:46] 116 0.8448843059496371\n",
      "\t\t\t\t 0.1830468326807022 0.38730666041374207\n",
      "#>>>    20.46 13.35 \t\t|\t\t 7.110000000000001\n",
      "[Feb 25, 09:01:46] 117 0.8446097751516831\n",
      "\t\t\t\t 0.10139363259077072 0.304723858833313\n",
      "#>>>    19.89 12.68 \t\t|\t\t 7.210000000000001\n",
      "[Feb 25, 09:01:46] 118 0.8441712828754061\n",
      "\t\t\t\t 0.17382603883743286 0.4969905912876129\n",
      "#>>>    18.94 12.46 \t\t|\t\t 6.48\n",
      "[Feb 25, 09:01:47] 119 0.8439979282524581\n",
      "\t\t\t\t 0.33119794726371765 1.0353991985321045\n",
      "#>>>    18.41 12.96 \t\t|\t\t 5.449999999999999\n",
      "[Feb 25, 09:01:47] 120 0.8445205274998039\n",
      "\t\t\t\t 0.46310552954673767 0.9415345191955566\n",
      "#>>>    19.73 13.38 \t\t|\t\t 6.35\n",
      "[Feb 25, 09:01:47] 121 0.8450806470508486\n",
      "\t\t\t\t 0.10878460854291916 0.4362472593784332\n",
      "#>>>    20.45 12.67 \t\t|\t\t 7.779999999999999\n",
      "[Feb 25, 09:01:48] 122 0.8447805982493675\n",
      "\t\t\t\t 0.06435424089431763 0.40450435876846313\n",
      "#>>>    19.24 12.74 \t\t|\t\t 6.499999999999998\n",
      "[Feb 25, 09:01:48] 123 0.844404676250781\n",
      "\t\t\t\t 0.8407890796661377 0.9788553714752197\n",
      "#>>>    18.83 14.16 \t\t|\t\t 4.669999999999998\n",
      "[Feb 25, 09:01:48] 124 0.8453799160256715\n",
      "\t\t\t\t 0.12238504737615585 0.4065386950969696\n",
      "#>>>    19.78 12.56 \t\t|\t\t 7.220000000000001\n",
      "[Feb 25, 09:01:49] 125 0.8450634598595697\n",
      "\t\t\t\t 0.30444690585136414 0.6883756518363953\n",
      "#>>>    19.05 12.78 \t\t|\t\t 6.270000000000001\n",
      "[Feb 25, 09:01:49] 126 0.8452112189275955\n",
      "\t\t\t\t 0.5169340372085571 0.8110077381134033\n",
      "#>>>    19.11 12.52 \t\t|\t\t 6.59\n",
      "[Feb 25, 09:01:49] 127 0.8456939494839899\n",
      "\t\t\t\t 0.2101615071296692 0.41831207275390625\n",
      "#>>>    19.78 12.84 \t\t|\t\t 6.940000000000001\n",
      "[Feb 25, 09:01:49] 128 0.8454767291143895\n",
      "\t\t\t\t 0.14493633806705475 0.40913939476013184\n",
      "#>>>    19.64 12.7 \t\t|\t\t 6.940000000000001\n",
      "[Feb 25, 09:01:50] 129 0.8451853281032011\n",
      "\t\t\t\t 0.4104366600513458 0.8034712672233582\n",
      "#>>>    19.36 12.75 \t\t|\t\t 6.609999999999999\n",
      "[Feb 25, 09:01:50] 130 0.8455540507321749\n",
      "\t\t\t\t 0.3065335154533386 0.6623465418815613\n",
      "#>>>    20.18 13.36 \t\t|\t\t 6.82\n",
      "[Feb 25, 09:01:50] 131 0.8456773767387776\n",
      "\t\t\t\t 0.25511255860328674 0.8680779337882996\n",
      "#>>>    19.57 13.28 \t\t|\t\t 6.290000000000001\n",
      "[Feb 25, 09:01:51] 132 0.8459548898842327\n",
      "\t\t\t\t 0.2668399512767792 0.666849672794342\n",
      "#>>>    18.87 12.47 \t\t|\t\t 6.4\n",
      "[Feb 25, 09:01:51] 133 0.8460426245886172\n",
      "\t\t\t\t 0.09842412173748016 0.3066687285900116\n",
      "#>>>    20.15 12.72 \t\t|\t\t 7.429999999999998\n",
      "[Feb 25, 09:01:51] 134 0.8456016747994549\n",
      "\t\t\t\t 0.11809077858924866 0.29834121465682983\n",
      "#>>>    20.26 11.97 \t\t|\t\t 8.290000000000001\n",
      "[Feb 25, 09:01:52] 135 0.8451725051179015\n",
      "\t\t\t\t 0.09162741154432297 0.41799572110176086\n",
      "#>>>    20.0 12.75 \t\t|\t\t 7.25\n",
      "[Feb 25, 09:01:52] 136 0.8448369557230779\n",
      "\t\t\t\t 0.5940424799919128 1.2476836442947388\n",
      "#>>>    19.05 13.64 \t\t|\t\t 5.41\n",
      "[Feb 25, 09:01:52] 137 0.8458338448320368\n",
      "\t\t\t\t 0.05675964802503586 0.10136015713214874\n",
      "#>>>    20.29 12.66 \t\t|\t\t 7.629999999999999\n",
      "[Feb 25, 09:01:52] 138 0.8451461307849114\n",
      "\t\t\t\t 0.1896849125623703 0.6685628294944763\n",
      "#>>>    19.44 12.78 \t\t|\t\t 6.660000000000002\n",
      "[Feb 25, 09:01:53] 139 0.8451592324110845\n",
      "\t\t\t\t 0.093301922082901 0.3153189718723297\n",
      "#>>>    20.51 12.56 \t\t|\t\t 7.950000000000001\n",
      "[Feb 25, 09:01:53] 140 0.8447226940726286\n",
      "\t\t\t\t 0.455890029668808 0.896708071231842\n",
      "#>>>    19.19 13.36 \t\t|\t\t 5.830000000000002\n",
      "[Feb 25, 09:01:53] 141 0.8452305694496542\n",
      "\t\t\t\t 0.15646368265151978 0.39569780230522156\n",
      "#>>>    19.71 13.18 \t\t|\t\t 6.530000000000001\n",
      "[Feb 25, 09:01:54] 142 0.8449375003353591\n",
      "\t\t\t\t 0.1867208182811737 0.3685235381126404\n",
      "#>>>    19.18 13.69 \t\t|\t\t 5.49\n",
      "[Feb 25, 09:01:54] 143 0.8446478071616152\n",
      "\t\t\t\t 0.2179068624973297 0.5111696720123291\n",
      "#>>>    19.21 12.77 \t\t|\t\t 6.440000000000001\n",
      "[Feb 25, 09:01:54] 144 0.8445322358591609\n",
      "\t\t\t\t 0.19640663266181946 0.5178109407424927\n",
      "#>>>    18.51 13.26 \t\t|\t\t 5.250000000000002\n",
      "[Feb 25, 09:01:55] 145 0.8444019211669038\n",
      "\t\t\t\t 0.31988054513931274 0.9750105142593384\n",
      "#>>>    18.88 12.22 \t\t|\t\t 6.659999999999998\n",
      "[Feb 25, 09:01:55] 146 0.8448524103647402\n",
      "\t\t\t\t 0.05959276482462883 0.33373376727104187\n",
      "#>>>    19.76 12.35 \t\t|\t\t 7.410000000000002\n",
      "[Feb 25, 09:01:55] 147 0.8444008844752954\n",
      "\t\t\t\t 0.2324589192867279 0.3767573833465576\n",
      "#>>>    19.86 12.22 \t\t|\t\t 7.639999999999999\n",
      "[Feb 25, 09:01:55] 148 0.8441656999232556\n",
      "\t\t\t\t 0.15540917217731476 0.6487368941307068\n",
      "#>>>    19.44 12.76 \t\t|\t\t 6.6800000000000015\n",
      "[Feb 25, 09:01:56] 149 0.8441256802747392\n",
      "\t\t\t\t 0.16601163148880005 0.5476789474487305\n",
      "#>>>    18.71 12.83 \t\t|\t\t 5.880000000000001\n",
      "[Feb 25, 09:01:56] 150 0.8439952451734021\n",
      "\t\t\t\t 0.20738476514816284 0.35670918226242065\n",
      "#>>>    20.43 12.9 \t\t|\t\t 7.529999999999999\n",
      "[Feb 25, 09:01:56] 151 0.8437153438756393\n",
      "\t\t\t\t 0.3394215404987335 0.7952488660812378\n",
      "#>>>    19.22 12.05 \t\t|\t\t 7.169999999999998\n",
      "[Feb 25, 09:01:57] 152 0.8440062989085413\n",
      "\t\t\t\t 0.3598249554634094 0.7756153345108032\n",
      "#>>>    19.42 13.78 \t\t|\t\t 5.640000000000002\n",
      "[Feb 25, 09:01:57] 153 0.8442977329592116\n",
      "\t\t\t\t 0.14927908778190613 0.5000921487808228\n",
      "#>>>    19.48 11.8 \t\t|\t\t 7.68\n",
      "[Feb 25, 09:01:57] 154 0.8441028064926175\n",
      "\t\t\t\t 0.17587801814079285 0.19476436078548431\n",
      "#>>>    19.86 11.57 \t\t|\t\t 8.29\n",
      "[Feb 25, 09:01:58] 155 0.8436293460501499\n",
      "\t\t\t\t 0.4943638741970062 0.7957577705383301\n",
      "#>>>    18.95 12.63 \t\t|\t\t 6.3199999999999985\n",
      "[Feb 25, 09:01:58] 156 0.8440758383786374\n",
      "\t\t\t\t 0.35202881693840027 0.8729000687599182\n",
      "#>>>    19.42 13.0 \t\t|\t\t 6.420000000000002\n",
      "[Feb 25, 09:01:58] 157 0.8444566913961548\n",
      "\t\t\t\t 0.23658141493797302 0.41466742753982544\n",
      "#>>>    20.1 13.51 \t\t|\t\t 6.590000000000002\n",
      "[Feb 25, 09:01:59] 158 0.8442634835174342\n",
      "\t\t\t\t 0.18446862697601318 0.8008280992507935\n",
      "#>>>    18.38 13.06 \t\t|\t\t 5.3199999999999985\n",
      "[Feb 25, 09:01:59] 159 0.8444045167601436\n",
      "\t\t\t\t 0.3536342680454254 0.506723165512085\n",
      "#>>>    20.06 13.37 \t\t|\t\t 6.6899999999999995\n",
      "[Feb 25, 09:01:59] 160 0.8444204696471387\n",
      "\t\t\t\t 0.28835102915763855 0.9342914819717407\n",
      "#>>>    18.77 13.15 \t\t|\t\t 5.619999999999999\n",
      "[Feb 25, 09:01:59] 161 0.8447986917184233\n",
      "\t\t\t\t 0.26481807231903076 0.8195537328720093\n",
      "#>>>    19.93 12.99 \t\t|\t\t 6.9399999999999995\n",
      "[Feb 25, 09:02:00] 162 0.8450382648318959\n",
      "\t\t\t\t 0.2308914214372635 0.6938523054122925\n",
      "#>>>    18.94 12.92 \t\t|\t\t 6.020000000000001\n",
      "[Feb 25, 09:02:00] 163 0.8451179702790125\n",
      "\t\t\t\t 0.19462911784648895 0.8841546773910522\n",
      "#>>>    18.3 11.73 \t\t|\t\t 6.57\n",
      "[Feb 25, 09:02:00] 164 0.8453516360592676\n",
      "\t\t\t\t 0.0740615576505661 0.3264164924621582\n",
      "#>>>    19.85 12.8 \t\t|\t\t 7.050000000000001\n",
      "[Feb 25, 09:02:01] 165 0.8449067624882222\n",
      "\t\t\t\t 0.24581557512283325 0.570677638053894\n",
      "#>>>    19.2 12.69 \t\t|\t\t 6.51\n",
      "[Feb 25, 09:02:01] 166 0.8448783489389107\n",
      "\t\t\t\t 0.10308779776096344 0.5246543288230896\n",
      "#>>>    19.24 12.19 \t\t|\t\t 7.049999999999999\n",
      "[Feb 25, 09:02:01] 167 0.8446612127016547\n",
      "\t\t\t\t 0.08937515318393707 0.6593678593635559\n",
      "#>>>    19.46 13.66 \t\t|\t\t 5.800000000000001\n",
      "[Feb 25, 09:02:02] 168 0.8445652944865994\n",
      "\t\t\t\t 0.2516789734363556 0.7457551956176758\n",
      "#>>>    18.79 12.84 \t\t|\t\t 5.949999999999999\n",
      "[Feb 25, 09:02:02] 169 0.8447181633313645\n",
      "\t\t\t\t 0.22575005888938904 0.5527100563049316\n",
      "#>>>    20.82 13.79 \t\t|\t\t 7.030000000000001\n",
      "[Feb 25, 09:02:02] 170 0.8446519053130298\n",
      "\t\t\t\t 0.14542707800865173 0.46425721049308777\n",
      "#>>>    20.14 11.95 \t\t|\t\t 8.190000000000001\n",
      "[Feb 25, 09:02:02] 171 0.8444169376962185\n",
      "\t\t\t\t 0.08326636254787445 0.28345978260040283\n",
      "#>>>    20.33 13.38 \t\t|\t\t 6.9499999999999975\n",
      "[Feb 25, 09:02:03] 172 0.8439392469185717\n",
      "\t\t\t\t 0.26791492104530334 0.817885160446167\n",
      "#>>>    19.47 12.67 \t\t|\t\t 6.799999999999999\n",
      "[Feb 25, 09:02:03] 173 0.8441811077233422\n",
      "\t\t\t\t 0.2172977477312088 0.6388976573944092\n",
      "#>>>    20.0 13.91 \t\t|\t\t 6.09\n",
      "[Feb 25, 09:02:03] 174 0.8441931220058433\n",
      "\t\t\t\t 0.34756702184677124 0.6152471303939819\n",
      "#>>>    19.87 14.22 \t\t|\t\t 5.65\n",
      "[Feb 25, 09:02:04] 175 0.8443117430360781\n",
      "\t\t\t\t 0.21580901741981506 0.6488121151924133\n",
      "#>>>    19.14 13.84 \t\t|\t\t 5.300000000000001\n",
      "[Feb 25, 09:02:04] 176 0.8443320524554566\n",
      "\t\t\t\t 0.17149235308170319 0.49700191617012024\n",
      "#>>>    20.21 13.05 \t\t|\t\t 7.16\n",
      "[Feb 25, 09:02:04] 177 0.8441562146871541\n",
      "\t\t\t\t 0.1402357816696167 0.5359473824501038\n",
      "#>>>    19.81 13.59 \t\t|\t\t 6.219999999999999\n",
      "[Feb 25, 09:02:05] 178 0.8439882416365867\n",
      "\t\t\t\t 0.14124013483524323 0.4282919764518738\n",
      "#>>>    19.39 12.68 \t\t|\t\t 6.710000000000001\n",
      "[Feb 25, 09:02:05] 179 0.8437137854913361\n",
      "\t\t\t\t 0.05065194144845009 0.34119850397109985\n",
      "#>>>    19.73 12.27 \t\t|\t\t 7.460000000000001\n",
      "[Feb 25, 09:02:05] 180 0.843261922147539\n",
      "\t\t\t\t 0.09980916976928711 0.32122981548309326\n",
      "#>>>    20.82 13.72 \t\t|\t\t 7.1\n",
      "[Feb 25, 09:02:06] 181 0.8428396992106438\n",
      "\t\t\t\t 0.0480760894715786 0.3219490051269531\n",
      "#>>>    20.35 12.44 \t\t|\t\t 7.910000000000002\n",
      "[Feb 25, 09:02:06] 182 0.8423668846097571\n",
      "\t\t\t\t 0.1933225393295288 1.05293607711792\n",
      "#>>>    18.88 12.88 \t\t|\t\t 5.999999999999998\n",
      "[Feb 25, 09:02:06] 183 0.8427707763415947\n",
      "\t\t\t\t 0.19636408984661102 0.5639573931694031\n",
      "#>>>    19.57 13.92 \t\t|\t\t 5.65\n",
      "[Feb 25, 09:02:06] 184 0.8426883270631702\n",
      "\t\t\t\t 0.12846186757087708 0.3566485643386841\n",
      "#>>>    20.52 12.91 \t\t|\t\t 7.609999999999999\n",
      "[Feb 25, 09:02:07] 185 0.8423307491680166\n",
      "\t\t\t\t 0.31113553047180176 0.4833661913871765\n",
      "#>>>    20.34 13.44 \t\t|\t\t 6.9\n",
      "[Feb 25, 09:02:07] 186 0.8422829201407076\n",
      "\t\t\t\t 0.2969643473625183 0.4728405475616455\n",
      "#>>>    19.66 12.92 \t\t|\t\t 6.74\n",
      "[Feb 25, 09:02:07] 187 0.8422104421154911\n",
      "\t\t\t\t 0.20907440781593323 0.8233367204666138\n",
      "#>>>    19.36 12.81 \t\t|\t\t 6.549999999999999\n",
      "[Feb 25, 09:02:08] 188 0.8424006427718558\n",
      "\t\t\t\t 0.36798444390296936 0.6237426400184631\n",
      "#>>>    19.84 13.21 \t\t|\t\t 6.629999999999999\n",
      "[Feb 25, 09:02:08] 189 0.8425499692428077\n",
      "\t\t\t\t 0.07000825554132462 0.3543431758880615\n",
      "#>>>    20.58 12.85 \t\t|\t\t 7.729999999999999\n",
      "[Feb 25, 09:02:08] 190 0.8421317706975436\n",
      "\t\t\t\t 0.0481906421482563 0.48215579986572266\n",
      "#>>>    19.57 11.71 \t\t|\t\t 7.859999999999999\n",
      "[Feb 25, 09:02:09] 191 0.8418199853800359\n",
      "\t\t\t\t 0.28647932410240173 0.5498543381690979\n",
      "#>>>    19.72 12.69 \t\t|\t\t 7.029999999999999\n",
      "[Feb 25, 09:02:09] 192 0.841814499027125\n",
      "\t\t\t\t 0.16099752485752106 0.19645193219184875\n",
      "#>>>    20.78 13.25 \t\t|\t\t 7.530000000000001\n",
      "[Feb 25, 09:02:09] 193 0.8413301340000484\n",
      "\t\t\t\t 0.13131612539291382 0.49327656626701355\n",
      "#>>>    19.63 12.83 \t\t|\t\t 6.799999999999999\n",
      "[Feb 25, 09:02:09] 194 0.8411133965279061\n",
      "\t\t\t\t 0.12193993479013443 0.38471901416778564\n",
      "#>>>    20.08 13.77 \t\t|\t\t 6.309999999999999\n",
      "[Feb 25, 09:02:10] 195 0.8407789421026878\n",
      "\t\t\t\t 0.3228338062763214 0.4639805555343628\n",
      "#>>>    19.85 14.79 \t\t|\t\t 5.060000000000002\n",
      "[Feb 25, 09:02:10] 196 0.8407249774925935\n",
      "\t\t\t\t 0.11146339774131775 0.5495584011077881\n",
      "#>>>    19.3 12.61 \t\t|\t\t 6.690000000000001\n",
      "[Feb 25, 09:02:10] 197 0.8405452743437524\n",
      "\t\t\t\t 0.22493082284927368 0.6641120314598083\n",
      "#>>>    19.9 11.18 \t\t|\t\t 8.719999999999999\n",
      "[Feb 25, 09:02:11] 198 0.8405937719237178\n",
      "\t\t\t\t 0.16639941930770874 0.43936869502067566\n",
      "#>>>    19.6 13.26 \t\t|\t\t 6.340000000000002\n",
      "[Feb 25, 09:02:11] 199 0.8403589462363201\n",
      "\t\t\t\t 0.17571842670440674 0.42040565609931946\n",
      "#>>>    19.85 13.04 \t\t|\t\t 6.810000000000002\n",
      "[Feb 25, 09:02:11] 200 0.8401147113430851\n",
      "\t\t\t\t 0.09987017512321472 0.36491429805755615\n",
      "#>>>    20.34 12.92 \t\t|\t\t 7.42\n",
      "[Feb 25, 09:02:12] 201 0.8397393811049229\n",
      "\t\t\t\t 0.21747373044490814 0.778816282749176\n",
      "#>>>    19.16 14.24 \t\t|\t\t 4.92\n",
      "[Feb 25, 09:02:12] 202 0.8398959317519131\n",
      "\t\t\t\t 0.15271781384944916 0.41614261269569397\n",
      "#>>>    20.64 13.71 \t\t|\t\t 6.93\n",
      "[Feb 25, 09:02:12] 203 0.8396248962318051\n",
      "\t\t\t\t 0.09068048745393753 0.3386712670326233\n",
      "#>>>    20.91 13.37 \t\t|\t\t 7.540000000000001\n",
      "[Feb 25, 09:02:12] 204 0.8392146230826093\n",
      "\t\t\t\t 0.18871940672397614 0.40328386425971985\n",
      "#>>>    20.34 13.2 \t\t|\t\t 7.140000000000001\n",
      "[Feb 25, 09:02:13] 205 0.8389674117454116\n",
      "\t\t\t\t 0.2418631613254547 0.6403458714485168\n",
      "#>>>    19.21 11.69 \t\t|\t\t 7.520000000000001\n",
      "[Feb 25, 09:02:13] 206 0.8390106533962425\n",
      "\t\t\t\t 0.03493642061948776 0.382766455411911\n",
      "#>>>    19.76 12.96 \t\t|\t\t 6.800000000000001\n",
      "[Feb 25, 09:02:13] 207 0.8385893456263283\n",
      "\t\t\t\t 0.1809035837650299 0.6171694993972778\n",
      "#>>>    19.31 13.83 \t\t|\t\t 5.479999999999999\n",
      "[Feb 25, 09:02:14] 208 0.8385488293340619\n",
      "\t\t\t\t 0.10524551570415497 0.35070493817329407\n",
      "#>>>    20.25 13.09 \t\t|\t\t 7.16\n",
      "[Feb 25, 09:02:14] 209 0.8381662309437041\n",
      "\t\t\t\t 0.3555079400539398 0.5733349323272705\n",
      "#>>>    19.36 13.89 \t\t|\t\t 5.469999999999999\n",
      "[Feb 25, 09:02:14] 210 0.838256907614944\n",
      "\t\t\t\t 0.07172094285488129 0.4079405665397644\n",
      "#>>>    20.06 12.6 \t\t|\t\t 7.459999999999999\n",
      "[Feb 25, 09:02:15] 211 0.8378983122316248\n",
      "\t\t\t\t 0.09255645424127579 0.49986547231674194\n",
      "#>>>    19.52 11.96 \t\t|\t\t 7.559999999999999\n",
      "[Feb 25, 09:02:15] 212 0.837652835868303\n",
      "\t\t\t\t 0.12162929028272629 0.6468992233276367\n",
      "#>>>    19.82 13.0 \t\t|\t\t 6.82\n",
      "[Feb 25, 09:02:15] 213 0.8375837115534955\n",
      "\t\t\t\t 0.12601010501384735 0.3820124864578247\n",
      "#>>>    20.19 13.39 \t\t|\t\t 6.800000000000001\n",
      "[Feb 25, 09:02:16] 214 0.8372541504483149\n",
      "\t\t\t\t 0.30031633377075195 0.5701761841773987\n",
      "#>>>    19.27 11.94 \t\t|\t\t 7.33\n",
      "[Feb 25, 09:02:16] 215 0.8372873888158147\n",
      "\t\t\t\t 0.09273460507392883 0.5478153824806213\n",
      "#>>>    20.04 13.15 \t\t|\t\t 6.889999999999999\n",
      "[Feb 25, 09:02:16] 216 0.8370906514443557\n",
      "\t\t\t\t 0.19955888390541077 0.7354533076286316\n",
      "#>>>    19.38 11.85 \t\t|\t\t 7.529999999999999\n",
      "[Feb 25, 09:02:16] 217 0.8371885730142478\n",
      "\t\t\t\t 0.13295793533325195 0.23059360682964325\n",
      "#>>>    20.42 12.65 \t\t|\t\t 7.770000000000001\n",
      "[Feb 25, 09:02:17] 218 0.8367149359982975\n",
      "\t\t\t\t 0.17848339676856995 0.3944619297981262\n",
      "#>>>    20.24 13.28 \t\t|\t\t 6.959999999999999\n",
      "[Feb 25, 09:02:17] 219 0.8364511664186681\n",
      "\t\t\t\t 0.2519907057285309 0.5733398795127869\n",
      "#>>>    20.01 13.33 \t\t|\t\t 6.6800000000000015\n",
      "[Feb 25, 09:02:17] 220 0.8364400458672931\n",
      "\t\t\t\t 0.2135010063648224 0.7675173878669739\n",
      "#>>>    19.16 12.69 \t\t|\t\t 6.470000000000001\n",
      "[Feb 25, 09:02:18] 221 0.83658462424546\n",
      "\t\t\t\t 0.19065961241722107 0.5458325743675232\n",
      "#>>>    20.54 14.61 \t\t|\t\t 5.93\n",
      "[Feb 25, 09:02:18] 222 0.836484531778197\n",
      "\t\t\t\t 0.11160966008901596 0.4538557529449463\n",
      "#>>>    20.26 11.95 \t\t|\t\t 8.310000000000002\n",
      "[Feb 25, 09:02:18] 223 0.8362135126371011\n",
      "\t\t\t\t 0.15886801481246948 0.14591960608959198\n",
      "#>>>    20.65 12.75 \t\t|\t\t 7.899999999999999\n",
      "[Feb 25, 09:02:19] 224 0.8356820867602672\n",
      "\t\t\t\t 0.27896150946617126 0.5851379632949829\n",
      "#>>>    19.51 13.65 \t\t|\t\t 5.860000000000001\n",
      "[Feb 25, 09:02:19] 225 0.8357105041760705\n",
      "\t\t\t\t 0.12717269361019135 0.6838332414627075\n",
      "#>>>    20.34 14.19 \t\t|\t\t 6.15\n",
      "[Feb 25, 09:02:19] 226 0.8356857996218685\n",
      "\t\t\t\t 0.33568716049194336 0.29109224677085876\n",
      "#>>>    20.21 13.91 \t\t|\t\t 6.300000000000001\n",
      "[Feb 25, 09:02:19] 227 0.8354768932593117\n",
      "\t\t\t\t 0.06942863762378693 0.37555792927742004\n",
      "#>>>    20.11 12.88 \t\t|\t\t 7.229999999999999\n",
      "[Feb 25, 09:02:20] 228 0.8350864029478547\n",
      "\t\t\t\t 0.250040739774704 0.5325580835342407\n",
      "#>>>    20.36 14.27 \t\t|\t\t 6.09\n",
      "[Feb 25, 09:02:20] 229 0.8350339153980182\n",
      "\t\t\t\t 0.03164534270763397 0.5431311130523682\n",
      "#>>>    19.14 12.07 \t\t|\t\t 7.07\n",
      "[Feb 25, 09:02:20] 230 0.8347736579532814\n",
      "\t\t\t\t 0.26231056451797485 0.5394078493118286\n",
      "#>>>    20.2 14.95 \t\t|\t\t 5.25\n",
      "[Feb 25, 09:02:21] 231 0.834740602709158\n",
      "\t\t\t\t 0.13430669903755188 0.7550605535507202\n",
      "#>>>    19.22 13.05 \t\t|\t\t 6.169999999999998\n",
      "[Feb 25, 09:02:21] 232 0.8347952293292348\n",
      "\t\t\t\t 0.2914086878299713 0.5951284766197205\n",
      "#>>>    19.48 11.87 \t\t|\t\t 7.610000000000001\n",
      "[Feb 25, 09:02:21] 233 0.8348469712941575\n",
      "\t\t\t\t 0.2023296058177948 0.5673744082450867\n",
      "#>>>    20.01 14.05 \t\t|\t\t 5.960000000000001\n",
      "[Feb 25, 09:02:22] 234 0.8347818283071239\n",
      "\t\t\t\t 0.15812572836875916 0.27079105377197266\n",
      "#>>>    19.4 12.38 \t\t|\t\t 7.019999999999998\n",
      "[Feb 25, 09:02:22] 235 0.8343759632609575\n",
      "\t\t\t\t 0.058433473110198975 0.43695876002311707\n",
      "#>>>    21.01 13.29 \t\t|\t\t 7.720000000000002\n",
      "[Feb 25, 09:02:22] 236 0.8340369795308299\n",
      "\t\t\t\t 0.17134009301662445 0.5231866836547852\n",
      "#>>>    19.93 12.76 \t\t|\t\t 7.17\n",
      "[Feb 25, 09:02:22] 237 0.8338974693428716\n",
      "\t\t\t\t 0.24904468655586243 0.5943183302879333\n",
      "#>>>    19.11 12.76 \t\t|\t\t 6.35\n",
      "[Feb 25, 09:02:23] 238 0.8339069349201748\n",
      "\t\t\t\t 0.08629757165908813 0.3299294114112854\n",
      "#>>>    19.82 12.16 \t\t|\t\t 7.66\n",
      "[Feb 25, 09:02:23] 239 0.833489254968325\n",
      "\t\t\t\t 0.12343781441450119 0.3204819858074188\n",
      "#>>>    20.31 12.55 \t\t|\t\t 7.759999999999998\n",
      "[Feb 25, 09:02:23] 240 0.8330996855210293\n",
      "\t\t\t\t 0.11366964131593704 0.5907359719276428\n",
      "#>>>    19.84 13.09 \t\t|\t\t 6.75\n",
      "[Feb 25, 09:02:24] 241 0.8329709914413013\n",
      "\t\t\t\t 0.09536696970462799 0.6660042405128479\n",
      "#>>>    18.94 13.47 \t\t|\t\t 5.470000000000001\n",
      "[Feb 25, 09:02:24] 242 0.8328993916451762\n",
      "\t\t\t\t 0.09051565080881119 0.513043999671936\n",
      "#>>>    20.14 13.05 \t\t|\t\t 7.09\n",
      "[Feb 25, 09:02:24] 243 0.8326700519263636\n",
      "\t\t\t\t 0.05067425221204758 0.27792632579803467\n",
      "#>>>    20.07 12.7 \t\t|\t\t 7.370000000000001\n",
      "[Feb 25, 09:02:25] 244 0.8321659824598979\n",
      "\t\t\t\t 0.10783720761537552 0.3293333351612091\n",
      "#>>>    20.17 12.96 \t\t|\t\t 7.210000000000001\n",
      "[Feb 25, 09:02:25] 245 0.831770987012764\n",
      "\t\t\t\t 0.3158942461013794 0.3730188012123108\n",
      "#>>>    19.79 12.35 \t\t|\t\t 7.4399999999999995\n",
      "[Feb 25, 09:02:25] 246 0.8316281290730648\n",
      "\t\t\t\t 0.277835488319397 0.9075921773910522\n",
      "#>>>    18.72 13.28 \t\t|\t\t 5.4399999999999995\n",
      "[Feb 25, 09:02:26] 247 0.8319819286097022\n",
      "\t\t\t\t 0.1814558058977127 0.5647047758102417\n",
      "#>>>    19.1 13.68 \t\t|\t\t 5.420000000000002\n",
      "[Feb 25, 09:02:26] 248 0.8318961072478993\n",
      "\t\t\t\t 0.23990529775619507 0.4075363576412201\n",
      "#>>>    20.17 12.1 \t\t|\t\t 8.070000000000002\n",
      "[Feb 25, 09:02:26] 249 0.8317116527662465\n",
      "\t\t\t\t 0.19429156184196472 0.41809993982315063\n",
      "#>>>    18.93 12.97 \t\t|\t\t 5.959999999999999\n",
      "[Feb 25, 09:02:26] 250 0.8314923325853429\n",
      "\t\t\t\t 0.1811768263578415 0.5209231972694397\n",
      "#>>>    19.73 12.23 \t\t|\t\t 7.5\n",
      "[Feb 25, 09:02:27] 251 0.831362940291286\n",
      "\t\t\t\t 0.22659854590892792 0.4927592873573303\n",
      "#>>>    19.56 12.54 \t\t|\t\t 7.02\n",
      "[Feb 25, 09:02:27] 252 0.8312509351991622\n",
      "\t\t\t\t 0.0790332704782486 0.21735700964927673\n",
      "#>>>    20.01 12.97 \t\t|\t\t 7.040000000000001\n",
      "[Feb 25, 09:02:27] 253 0.8307160745589917\n",
      "\t\t\t\t 0.4544571042060852 0.9187905788421631\n",
      "#>>>    19.2 12.57 \t\t|\t\t 6.629999999999999\n",
      "[Feb 25, 09:02:28] 254 0.8312586061078763\n",
      "\t\t\t\t 0.3216232657432556 0.506687581539154\n",
      "#>>>    19.57 13.99 \t\t|\t\t 5.58\n",
      "[Feb 25, 09:02:28] 255 0.8312556583490508\n",
      "\t\t\t\t 0.043606944382190704 0.24932704865932465\n",
      "#>>>    20.22 11.4 \t\t|\t\t 8.819999999999999\n",
      "[Feb 25, 09:02:28] 256 0.8307173366911939\n",
      "\t\t\t\t 0.14342372119426727 0.5730103254318237\n",
      "#>>>    19.96 13.47 \t\t|\t\t 6.49\n",
      "[Feb 25, 09:02:29] 257 0.8306030534160299\n",
      "\t\t\t\t 0.1009448915719986 0.4378700256347656\n",
      "#>>>    19.95 13.48 \t\t|\t\t 6.469999999999999\n",
      "[Feb 25, 09:02:29] 258 0.8303112652649195\n",
      "\t\t\t\t 0.14616955816745758 0.7977129817008972\n",
      "#>>>    18.93 12.84 \t\t|\t\t 6.09\n",
      "[Feb 25, 09:02:29] 259 0.8304248365246218\n",
      "\t\t\t\t 0.12193199247121811 0.5088387131690979\n",
      "#>>>    20.43 13.09 \t\t|\t\t 7.34\n",
      "[Feb 25, 09:02:29] 260 0.8302251823713858\n",
      "\t\t\t\t 0.14059360325336456 0.433123379945755\n",
      "#>>>    19.81 13.83 \t\t|\t\t 5.979999999999999\n",
      "[Feb 25, 09:02:30] 261 0.8299686741871146\n",
      "\t\t\t\t 0.05260371044278145 0.5380998849868774\n",
      "#>>>    19.28 12.73 \t\t|\t\t 6.550000000000001\n",
      "[Feb 25, 09:02:30] 262 0.829729409119533\n",
      "\t\t\t\t 0.14934615790843964 0.5062561631202698\n",
      "#>>>    19.43 13.5 \t\t|\t\t 5.93\n",
      "[Feb 25, 09:02:30] 263 0.8295552820463433\n",
      "\t\t\t\t 0.022467466071248055 0.3351787030696869\n",
      "#>>>    20.31 12.29 \t\t|\t\t 8.02\n",
      "[Feb 25, 09:02:31] 264 0.8290833729315753\n",
      "\t\t\t\t 0.05380311608314514 0.1879337877035141\n",
      "#>>>    19.9 11.42 \t\t|\t\t 8.479999999999999\n",
      "[Feb 25, 09:02:31] 265 0.8284960264624305\n",
      "\t\t\t\t 0.23270480334758759 0.611463725566864\n",
      "#>>>    19.87 12.89 \t\t|\t\t 6.98\n",
      "[Feb 25, 09:02:31] 266 0.8285116989797837\n",
      "\t\t\t\t 0.1149752214550972 0.4142860174179077\n",
      "#>>>    20.06 13.26 \t\t|\t\t 6.799999999999999\n",
      "[Feb 25, 09:02:32] 267 0.8282124485122263\n",
      "\t\t\t\t 0.06789428740739822 0.36362385749816895\n",
      "#>>>    20.22 12.3 \t\t|\t\t 7.919999999999998\n",
      "[Feb 25, 09:02:32] 268 0.827815754201169\n",
      "\t\t\t\t 0.08951906114816666 0.5105644464492798\n",
      "#>>>    20.29 13.2 \t\t|\t\t 7.09\n",
      "[Feb 25, 09:02:32] 269 0.827588021976917\n",
      "\t\t\t\t 0.14972905814647675 0.3695012331008911\n",
      "#>>>    19.86 11.71 \t\t|\t\t 8.149999999999999\n",
      "[Feb 25, 09:02:33] 270 0.8272796642610887\n",
      "\t\t\t\t 0.1829972118139267 0.6392442584037781\n",
      "#>>>    20.05 14.47 \t\t|\t\t 5.58\n",
      "[Feb 25, 09:02:33] 271 0.8272746260819465\n",
      "\t\t\t\t 0.06654244661331177 0.28867393732070923\n",
      "#>>>    20.09 12.02 \t\t|\t\t 8.07\n",
      "[Feb 25, 09:02:33] 272 0.8268025678397986\n",
      "\t\t\t\t 0.057370997965335846 0.17974475026130676\n",
      "#>>>    20.53 13.56 \t\t|\t\t 6.970000000000001\n",
      "[Feb 25, 09:02:33] 273 0.8262128810127348\n",
      "\t\t\t\t 0.08656343817710876 0.4025537371635437\n",
      "#>>>    20.04 12.26 \t\t|\t\t 7.779999999999999\n",
      "[Feb 25, 09:02:34] 274 0.8258757853070627\n",
      "\t\t\t\t 0.09653553366661072 0.2240997552871704\n",
      "#>>>    20.63 13.93 \t\t|\t\t 6.699999999999999\n",
      "[Feb 25, 09:02:34] 275 0.8253705448107095\n",
      "\t\t\t\t 0.09656275063753128 0.18546909093856812\n",
      "#>>>    21.17 13.43 \t\t|\t\t 7.740000000000002\n",
      "[Feb 25, 09:02:34] 276 0.8248272061000244\n",
      "\t\t\t\t 0.15270017087459564 0.7689406871795654\n",
      "#>>>    19.11 12.02 \t\t|\t\t 7.09\n",
      "[Feb 25, 09:02:35] 277 0.8249240197668797\n",
      "\t\t\t\t 0.12425992637872696 0.8374121189117432\n",
      "#>>>    19.35 13.17 \t\t|\t\t 6.1800000000000015\n",
      "[Feb 25, 09:02:35] 278 0.825060767814755\n",
      "\t\t\t\t 0.19674764573574066 0.5192122459411621\n",
      "#>>>    19.46 12.36 \t\t|\t\t 7.100000000000001\n",
      "[Feb 25, 09:02:35] 279 0.8249516669535183\n",
      "\t\t\t\t 0.09373476356267929 0.3679980933666229\n",
      "#>>>    20.3 12.91 \t\t|\t\t 7.390000000000001\n",
      "[Feb 25, 09:02:36] 280 0.8245884481509447\n",
      "\t\t\t\t 0.20218662917613983 0.8027657270431519\n",
      "#>>>    19.11 12.62 \t\t|\t\t 6.49\n",
      "[Feb 25, 09:02:36] 281 0.8247688120143095\n",
      "\t\t\t\t 0.30463308095932007 0.5363624095916748\n",
      "#>>>    19.89 13.61 \t\t|\t\t 6.280000000000001\n",
      "[Feb 25, 09:02:36] 282 0.8247850386928461\n",
      "\t\t\t\t 0.2518406808376312 0.47829562425613403\n",
      "#>>>    19.63 13.85 \t\t|\t\t 5.779999999999999\n",
      "[Feb 25, 09:02:36] 283 0.8246903899294447\n",
      "\t\t\t\t 0.1479262262582779 0.37478041648864746\n",
      "#>>>    19.32 12.25 \t\t|\t\t 7.07\n",
      "[Feb 25, 09:02:37] 284 0.824388406167361\n",
      "\t\t\t\t 0.11504839360713959 0.5424816608428955\n",
      "#>>>    19.83 12.05 \t\t|\t\t 7.779999999999998\n",
      "[Feb 25, 09:02:37] 285 0.8242215478305448\n",
      "\t\t\t\t 0.05545175075531006 0.3651628792285919\n",
      "#>>>    20.76 12.89 \t\t|\t\t 7.870000000000001\n",
      "[Feb 25, 09:02:37] 286 0.8238179409126981\n",
      "\t\t\t\t 0.12407411634922028 0.4988649785518646\n",
      "#>>>    19.78 13.02 \t\t|\t\t 6.760000000000002\n",
      "[Feb 25, 09:02:38] 287 0.8236170620815877\n",
      "\t\t\t\t 0.06537584960460663 0.3126485347747803\n",
      "#>>>    20.14 12.11 \t\t|\t\t 8.030000000000001\n",
      "[Feb 25, 09:02:38] 288 0.8231714694187867\n",
      "\t\t\t\t 0.22390951216220856 0.7765378952026367\n",
      "#>>>    18.63 13.34 \t\t|\t\t 5.289999999999999\n",
      "[Feb 25, 09:02:38] 289 0.8233487453418316\n",
      "\t\t\t\t 0.2414318174123764 0.5476081371307373\n",
      "#>>>    19.53 12.2 \t\t|\t\t 7.330000000000002\n",
      "[Feb 25, 09:02:39] 290 0.823314436565934\n",
      "\t\t\t\t 0.10894647985696793 0.44721320271492004\n",
      "#>>>    19.88 13.61 \t\t|\t\t 6.27\n",
      "[Feb 25, 09:02:39] 291 0.8230472818044894\n",
      "\t\t\t\t 0.06157004460692406 0.18845130503177643\n",
      "#>>>    20.96 12.18 \t\t|\t\t 8.780000000000001\n",
      "[Feb 25, 09:02:39] 292 0.8224742558611478\n",
      "\t\t\t\t 0.09597910195589066 0.2857189476490021\n",
      "#>>>    20.39 12.8 \t\t|\t\t 7.59\n",
      "[Feb 25, 09:02:39] 293 0.8220334796474409\n",
      "\t\t\t\t 0.16714195907115936 0.2513386011123657\n",
      "#>>>    20.26 13.29 \t\t|\t\t 6.970000000000002\n",
      "[Feb 25, 09:02:40] 294 0.821629926742878\n",
      "\t\t\t\t 0.08299607783555984 0.3264444172382355\n",
      "#>>>    20.18 11.97 \t\t|\t\t 8.209999999999999\n",
      "[Feb 25, 09:02:40] 295 0.8212177373037585\n",
      "\t\t\t\t 0.0713372603058815 0.31832990050315857\n",
      "#>>>    20.28 13.74 \t\t|\t\t 6.540000000000001\n",
      "[Feb 25, 09:02:40] 296 0.8207861867198131\n",
      "\t\t\t\t 0.13309094309806824 0.18816518783569336\n",
      "#>>>    21.15 12.96 \t\t|\t\t 8.189999999999998\n",
      "[Feb 25, 09:02:41] 297 0.8202866566640271\n",
      "\t\t\t\t 0.14692413806915283 0.5560525059700012\n",
      "#>>>    20.39 14.66 \t\t|\t\t 5.73\n",
      "[Feb 25, 09:02:41] 298 0.8201693466514023\n",
      "\t\t\t\t 0.06339657306671143 0.2258400022983551\n",
      "#>>>    20.5 13.83 \t\t|\t\t 6.67\n",
      "[Feb 25, 09:02:41] 299 0.8196384138801159\n",
      "\t\t\t\t 0.1472369283437729 0.36567744612693787\n",
      "#>>>    20.25 13.46 \t\t|\t\t 6.789999999999999\n",
      "[Feb 25, 09:02:42] 300 0.8193316898258054\n",
      "\t\t\t\t 0.09732385724782944 0.44912955164909363\n",
      "#>>>    20.02 12.42 \t\t|\t\t 7.6\n",
      "[Feb 25, 09:02:42] 301 0.8190588115523271\n",
      "\t\t\t\t 0.1345287412405014 0.4242514967918396\n",
      "#>>>    20.11 13.85 \t\t|\t\t 6.26\n",
      "[Feb 25, 09:02:42] 302 0.8187985329937083\n",
      "\t\t\t\t 0.12376931309700012 0.6184890866279602\n",
      "#>>>    19.89 13.66 \t\t|\t\t 6.23\n",
      "[Feb 25, 09:02:43] 303 0.8187219928902418\n",
      "\t\t\t\t 0.16238543391227722 0.47806426882743835\n",
      "#>>>    19.65 12.8 \t\t|\t\t 6.849999999999998\n",
      "[Feb 25, 09:02:43] 304 0.8185437206000913\n",
      "\t\t\t\t 0.10166668891906738 0.40052223205566406\n",
      "#>>>    20.57 14.09 \t\t|\t\t 6.48\n",
      "[Feb 25, 09:02:43] 305 0.818227365800466\n",
      "\t\t\t\t 0.21032646298408508 0.6011943817138672\n",
      "#>>>    19.73 12.51 \t\t|\t\t 7.220000000000001\n",
      "[Feb 25, 09:02:43] 306 0.8182206592495611\n",
      "\t\t\t\t 0.17450201511383057 0.35947421193122864\n",
      "#>>>    20.39 12.83 \t\t|\t\t 7.5600000000000005\n",
      "[Feb 25, 09:02:44] 307 0.8179364147875543\n",
      "\t\t\t\t 0.2149137705564499 0.5303131341934204\n",
      "#>>>    19.61 13.33 \t\t|\t\t 6.279999999999999\n",
      "[Feb 25, 09:02:44] 308 0.8178637052924178\n",
      "\t\t\t\t 0.12062317878007889 0.7142258286476135\n",
      "#>>>    19.24 12.75 \t\t|\t\t 6.489999999999998\n",
      "[Feb 25, 09:02:44] 309 0.8178806905871024\n",
      "\t\t\t\t 0.17521549761295319 0.4391533434391022\n",
      "#>>>    20.26 13.3 \t\t|\t\t 6.960000000000001\n",
      "[Feb 25, 09:02:45] 310 0.8176771787524686\n",
      "\t\t\t\t 0.14857546985149384 0.4787929654121399\n",
      "#>>>    19.86 13.48 \t\t|\t\t 6.379999999999999\n",
      "[Feb 25, 09:02:45] 311 0.8174868700238809\n",
      "\t\t\t\t 0.059804677963256836 0.33005884289741516\n",
      "#>>>    20.37 12.12 \t\t|\t\t 8.250000000000002\n",
      "[Feb 25, 09:02:45] 312 0.8170592466747176\n",
      "\t\t\t\t 0.05753951892256737 0.25572511553764343\n",
      "#>>>    20.27 12.94 \t\t|\t\t 7.33\n",
      "[Feb 25, 09:02:46] 313 0.8165554520662284\n",
      "\t\t\t\t 0.13262933492660522 0.5710394978523254\n",
      "#>>>    19.43 12.51 \t\t|\t\t 6.92\n",
      "[Feb 25, 09:02:46] 314 0.816442565446941\n",
      "\t\t\t\t 0.1439386010169983 0.5079944133758545\n",
      "#>>>    20.12 13.84 \t\t|\t\t 6.280000000000001\n",
      "[Feb 25, 09:02:46] 315 0.8162780558958869\n",
      "\t\t\t\t 0.10954834520816803 0.2573152780532837\n",
      "#>>>    21.35 13.21 \t\t|\t\t 8.14\n",
      "[Feb 25, 09:02:46] 316 0.8158286414483512\n",
      "\t\t\t\t 0.25265592336654663 0.4508206248283386\n",
      "#>>>    19.48 13.89 \t\t|\t\t 5.59\n",
      "[Feb 25, 09:02:47] 317 0.8157162893550978\n",
      "\t\t\t\t 0.14503353834152222 0.6735018491744995\n",
      "#>>>    19.96 12.48 \t\t|\t\t 7.48\n",
      "[Feb 25, 09:02:47] 318 0.8157191084532587\n",
      "\t\t\t\t 0.13157784938812256 0.5196278095245361\n",
      "#>>>    19.8 13.23 \t\t|\t\t 6.57\n",
      "[Feb 25, 09:02:47] 319 0.8155545950037181\n",
      "\t\t\t\t 0.1743636429309845 0.6295198798179626\n",
      "#>>>    19.93 12.43 \t\t|\t\t 7.5\n",
      "[Feb 25, 09:02:48] 320 0.8155429239612656\n",
      "\t\t\t\t 0.040109965950250626 0.2622447609901428\n",
      "#>>>    20.52 13.61 \t\t|\t\t 6.91\n",
      "[Feb 25, 09:02:48] 321 0.8150297357605194\n",
      "\t\t\t\t 0.16472792625427246 0.3664492964744568\n",
      "#>>>    20.44 13.0 \t\t|\t\t 7.440000000000001\n",
      "[Feb 25, 09:02:48] 322 0.8147458832474876\n",
      "\t\t\t\t 0.07690800726413727 0.4551272392272949\n",
      "#>>>    20.12 12.87 \t\t|\t\t 7.250000000000002\n",
      "[Feb 25, 09:02:49] 323 0.8144631725958305\n",
      "\t\t\t\t 0.10055064409971237 0.5173636674880981\n",
      "#>>>    19.89 12.19 \t\t|\t\t 7.700000000000001\n",
      "[Feb 25, 09:02:49] 324 0.814266623742273\n",
      "\t\t\t\t 0.04041428864002228 0.11840454488992691\n",
      "#>>>    21.24 12.79 \t\t|\t\t 8.45\n",
      "[Feb 25, 09:02:49] 325 0.8136111759595113\n",
      "\t\t\t\t 0.15857543051242828 0.5618094801902771\n",
      "#>>>    20.03 13.25 \t\t|\t\t 6.780000000000001\n",
      "[Feb 25, 09:02:50] 326 0.8135179496793533\n",
      "\t\t\t\t 0.056415967643260956 0.46804022789001465\n",
      "#>>>    19.5 12.17 \t\t|\t\t 7.33\n",
      "[Feb 25, 09:02:50] 327 0.8132288879326578\n",
      "\t\t\t\t 0.1032235324382782 0.34708553552627563\n",
      "#>>>    20.37 13.82 \t\t|\t\t 6.550000000000001\n",
      "[Feb 25, 09:02:50] 328 0.8128659681126896\n",
      "\t\t\t\t 0.08683142811059952 0.40437403321266174\n",
      "#>>>    19.53 13.17 \t\t|\t\t 6.360000000000001\n",
      "[Feb 25, 09:02:50] 329 0.8125443075984496\n",
      "\t\t\t\t 0.2623368203639984 0.8074952363967896\n",
      "#>>>    19.57 13.75 \t\t|\t\t 5.82\n",
      "[Feb 25, 09:02:51] 330 0.8128015953774143\n",
      "\t\t\t\t 0.06170979142189026 0.36942166090011597\n",
      "#>>>    20.77 13.2 \t\t|\t\t 7.57\n",
      "[Feb 25, 09:02:51] 331 0.8124199252343588\n",
      "\t\t\t\t 0.09792902320623398 0.5717632174491882\n",
      "#>>>    19.7 12.53 \t\t|\t\t 7.17\n",
      "[Feb 25, 09:02:51] 332 0.8122771975274281\n",
      "\t\t\t\t 0.14941009879112244 0.34586238861083984\n",
      "#>>>    20.58 13.15 \t\t|\t\t 7.429999999999998\n",
      "[Feb 25, 09:02:52] 333 0.8119601928173027\n",
      "\t\t\t\t 0.1390145868062973 0.658920407295227\n",
      "#>>>    20.19 13.83 \t\t|\t\t 6.360000000000001\n",
      "[Feb 25, 09:02:52] 334 0.8119461676334881\n",
      "\t\t\t\t 0.11536777019500732 0.30682533979415894\n",
      "#>>>    19.79 12.61 \t\t|\t\t 7.18\n",
      "[Feb 25, 09:02:52] 335 0.8115564145758438\n",
      "\t\t\t\t 0.059305042028427124 0.252478688955307\n",
      "#>>>    20.29 12.53 \t\t|\t\t 7.76\n",
      "[Feb 25, 09:02:53] 336 0.8110566418922517\n",
      "\t\t\t\t 0.16328223049640656 0.508452296257019\n",
      "#>>>    19.8 13.16 \t\t|\t\t 6.640000000000001\n",
      "[Feb 25, 09:02:53] 337 0.8109173197622117\n",
      "\t\t\t\t 0.10485617816448212 0.3859902620315552\n",
      "#>>>    20.27 13.39 \t\t|\t\t 6.879999999999999\n",
      "[Feb 25, 09:02:53] 338 0.8105972488975467\n",
      "\t\t\t\t 0.04072023928165436 0.2402942180633545\n",
      "#>>>    20.11 11.48 \t\t|\t\t 8.629999999999999\n",
      "[Feb 25, 09:02:53] 339 0.810067666091093\n",
      "\t\t\t\t 0.0934777557849884 0.3823850154876709\n",
      "#>>>    18.97 11.46 \t\t|\t\t 7.509999999999998\n",
      "[Feb 25, 09:02:54] 340 0.8097334611962747\n",
      "\t\t\t\t 0.18774141371250153 0.7135773301124573\n",
      "#>>>    18.97 13.63 \t\t|\t\t 5.339999999999998\n",
      "[Feb 25, 09:02:54] 341 0.8098250464640022\n",
      "\t\t\t\t 0.15715037286281586 0.4257645905017853\n",
      "#>>>    19.17 12.0 \t\t|\t\t 7.170000000000002\n",
      "[Feb 25, 09:02:54] 342 0.8095981363660016\n",
      "\t\t\t\t 0.1709607094526291 0.5645838975906372\n",
      "#>>>    19.26 12.46 \t\t|\t\t 6.800000000000001\n",
      "[Feb 25, 09:02:55] 343 0.8095240828515801\n",
      "\t\t\t\t 0.08040811121463776 0.4119194746017456\n",
      "#>>>    19.72 12.55 \t\t|\t\t 7.169999999999998\n",
      "[Feb 25, 09:02:55] 344 0.8092068863396437\n",
      "\t\t\t\t 0.06544692814350128 0.19524717330932617\n",
      "#>>>    20.52 12.38 \t\t|\t\t 8.139999999999999\n",
      "[Feb 25, 09:02:55] 345 0.8086583735398557\n",
      "\t\t\t\t 0.07995975762605667 0.3859630823135376\n",
      "#>>>    19.92 12.74 \t\t|\t\t 7.1800000000000015\n",
      "[Feb 25, 09:02:56] 346 0.8083156379988049\n",
      "\t\t\t\t 0.122479647397995 0.47993940114974976\n",
      "#>>>    19.68 12.97 \t\t|\t\t 6.709999999999999\n",
      "[Feb 25, 09:02:56] 347 0.8081097413795516\n",
      "\t\t\t\t 0.040425240993499756 0.35575637221336365\n",
      "#>>>    19.98 13.18 \t\t|\t\t 6.800000000000001\n",
      "[Feb 25, 09:02:56] 348 0.8076978132513789\n",
      "\t\t\t\t 0.08516030758619308 0.2919784486293793\n",
      "#>>>    20.45 13.59 \t\t|\t\t 6.859999999999999\n",
      "[Feb 25, 09:02:56] 349 0.8072672542017937\n",
      "\t\t\t\t 0.016869042068719864 0.16478312015533447\n",
      "#>>>    20.52 11.42 \t\t|\t\t 9.1\n",
      "[Feb 25, 09:02:57] 350 0.8066416391060907\n",
      "\t\t\t\t 0.09600462764501572 0.49955761432647705\n",
      "#>>>    19.69 12.31 \t\t|\t\t 7.380000000000001\n",
      "[Feb 25, 09:02:57] 351 0.8064305596866044\n",
      "\t\t\t\t 0.047381240874528885 0.238192617893219\n",
      "#>>>    20.48 12.03 \t\t|\t\t 8.450000000000001\n",
      "[Feb 25, 09:02:57] 352 0.8059097029968614\n",
      "\t\t\t\t 0.09311553090810776 0.2773977220058441\n",
      "#>>>    20.37 12.44 \t\t|\t\t 7.9300000000000015\n",
      "[Feb 25, 09:02:58] 353 0.805474306554229\n",
      "\t\t\t\t 0.2835902273654938 0.7526624202728271\n",
      "#>>>    19.15 12.63 \t\t|\t\t 6.519999999999998\n",
      "[Feb 25, 09:02:58] 354 0.8057050848655108\n",
      "\t\t\t\t 0.05287107452750206 0.25573572516441345\n",
      "#>>>    20.58 12.77 \t\t|\t\t 7.809999999999999\n",
      "[Feb 25, 09:02:58] 355 0.8052079865840625\n",
      "\t\t\t\t 0.1302369385957718 0.49247315526008606\n",
      "#>>>    19.42 12.64 \t\t|\t\t 6.780000000000001\n",
      "[Feb 25, 09:02:59] 356 0.8050254887062355\n",
      "\t\t\t\t 0.14453643560409546 0.3525410294532776\n",
      "#>>>    19.77 11.83 \t\t|\t\t 7.9399999999999995\n",
      "[Feb 25, 09:02:59] 357 0.8047175406825866\n",
      "\t\t\t\t 0.09522144496440887 0.5781909823417664\n",
      "#>>>    20.18 12.87 \t\t|\t\t 7.3100000000000005\n",
      "[Feb 25, 09:02:59] 358 0.8045862355841114\n",
      "\t\t\t\t 0.10914485156536102 0.3911778926849365\n",
      "#>>>    19.86 12.67 \t\t|\t\t 7.1899999999999995\n",
      "[Feb 25, 09:03:00] 359 0.8042819721076787\n",
      "\t\t\t\t 0.07701978832483292 0.6092039942741394\n",
      "#>>>    19.97 11.84 \t\t|\t\t 8.129999999999999\n",
      "[Feb 25, 09:03:00] 360 0.8041639139405218\n",
      "\t\t\t\t 0.5824598670005798 0.8735530972480774\n",
      "#>>>    19.32 12.84 \t\t|\t\t 6.48\n",
      "[Feb 25, 09:03:00] 361 0.8048157629908299\n",
      "\t\t\t\t 0.21641936898231506 0.7307102680206299\n",
      "#>>>    19.08 12.51 \t\t|\t\t 6.5699999999999985\n",
      "[Feb 25, 09:03:00] 362 0.8049580768350397\n",
      "\t\t\t\t 0.0501018650829792 0.42907071113586426\n",
      "#>>>    20.13 11.06 \t\t|\t\t 9.069999999999999\n",
      "[Feb 25, 09:03:01] 363 0.8046322913455994\n",
      "\t\t\t\t 0.027387255802750587 0.5560100078582764\n",
      "#>>>    19.83 11.96 \t\t|\t\t 7.869999999999997\n",
      "[Feb 25, 09:03:01] 364 0.8044110563235027\n",
      "\t\t\t\t 0.12033192068338394 0.5666619539260864\n",
      "#>>>    19.38 11.89 \t\t|\t\t 7.489999999999998\n",
      "[Feb 25, 09:03:01] 365 0.8042936391641405\n",
      "\t\t\t\t 0.141348734498024 0.4699398875236511\n",
      "#>>>    19.56 13.38 \t\t|\t\t 6.179999999999998\n",
      "[Feb 25, 09:03:02] 366 0.804100634132097\n",
      "\t\t\t\t 0.06786978989839554 0.23296421766281128\n",
      "#>>>    20.08 11.56 \t\t|\t\t 8.519999999999998\n",
      "[Feb 25, 09:03:02] 367 0.8035973674980755\n",
      "\t\t\t\t 0.08883100003004074 0.3697197139263153\n",
      "#>>>    19.17 12.46 \t\t|\t\t 6.710000000000001\n",
      "[Feb 25, 09:03:02] 368 0.8032523208519845\n",
      "\t\t\t\t 0.12163963168859482 0.3780663013458252\n",
      "#>>>    19.96 12.03 \t\t|\t\t 7.9300000000000015\n",
      "[Feb 25, 09:03:03] 369 0.8029487744716175\n",
      "\t\t\t\t 0.03945362567901611 0.4318538010120392\n",
      "#>>>    19.95 11.79 \t\t|\t\t 8.16\n",
      "[Feb 25, 09:03:03] 370 0.802617133123837\n",
      "\t\t\t\t 0.10052275657653809 0.36889511346817017\n",
      "#>>>    20.43 12.75 \t\t|\t\t 7.68\n",
      "[Feb 25, 09:03:03] 371 0.8022839338607579\n",
      "\t\t\t\t 0.19623610377311707 0.6811875104904175\n",
      "#>>>    19.2 12.74 \t\t|\t\t 6.459999999999999\n",
      "[Feb 25, 09:03:03] 372 0.802359073570963\n",
      "\t\t\t\t 0.29030105471611023 0.3338647782802582\n",
      "#>>>    20.14 12.8 \t\t|\t\t 7.34\n",
      "[Feb 25, 09:03:04] 373 0.8021808803303885\n",
      "\t\t\t\t 0.15406619012355804 0.533733606338501\n",
      "#>>>    19.68 12.38 \t\t|\t\t 7.299999999999999\n",
      "[Feb 25, 09:03:04] 374 0.8020664992614214\n",
      "\t\t\t\t 0.0876363143324852 0.5140638947486877\n",
      "#>>>    19.71 12.61 \t\t|\t\t 7.100000000000001\n",
      "[Feb 25, 09:03:04] 375 0.8018661329488894\n",
      "\t\t\t\t 0.12557624280452728 0.39407655596733093\n",
      "#>>>    20.03 13.16 \t\t|\t\t 6.870000000000001\n",
      "[Feb 25, 09:03:05] 376 0.8015839195998112\n",
      "\t\t\t\t 0.09448792785406113 0.38744479417800903\n",
      "#>>>    20.49 12.01 \t\t|\t\t 8.479999999999999\n",
      "[Feb 25, 09:03:05] 377 0.801264268409694\n",
      "\t\t\t\t 0.17999428510665894 0.3646194338798523\n",
      "#>>>    19.94 12.7 \t\t|\t\t 7.240000000000002\n",
      "[Feb 25, 09:03:05] 378 0.8010076178602709\n",
      "\t\t\t\t 0.032585229724645615 0.10538024455308914\n",
      "#>>>    19.64 11.34 \t\t|\t\t 8.3\n",
      "[Feb 25, 09:03:06] 379 0.800344575712963\n",
      "\t\t\t\t 0.08201377093791962 0.27371546626091003\n",
      "#>>>    20.49 12.25 \t\t|\t\t 8.239999999999998\n",
      "[Feb 25, 09:03:06] 380 0.7998999603595477\n",
      "\t\t\t\t 0.11783336102962494 0.4014184772968292\n",
      "#>>>    19.62 13.42 \t\t|\t\t 6.200000000000001\n",
      "[Feb 25, 09:03:06] 381 0.7996193122226134\n",
      "\t\t\t\t 0.07017002254724503 0.7929015755653381\n",
      "#>>>    18.92 11.96 \t\t|\t\t 6.960000000000001\n",
      "[Feb 25, 09:03:07] 382 0.7996827645308551\n",
      "\t\t\t\t 0.10037769377231598 0.196271151304245\n",
      "#>>>    20.16 12.19 \t\t|\t\t 7.970000000000001\n",
      "[Feb 25, 09:03:07] 383 0.7991797306263019\n",
      "\t\t\t\t 0.04045672342181206 0.18961530923843384\n",
      "#>>>    19.66 12.13 \t\t|\t\t 7.529999999999999\n",
      "[Feb 25, 09:03:07] 384 0.7986106229320612\n",
      "\t\t\t\t 0.09711416810750961 0.40562331676483154\n",
      "#>>>    20.24 13.14 \t\t|\t\t 7.099999999999998\n",
      "[Feb 25, 09:03:07] 385 0.7983147497716497\n",
      "\t\t\t\t 0.07476799935102463 0.29799866676330566\n",
      "#>>>    20.38 12.65 \t\t|\t\t 7.729999999999999\n",
      "[Feb 25, 09:03:08] 386 0.797889201695443\n",
      "\t\t\t\t 0.14130829274654388 0.531656801700592\n",
      "#>>>    19.13 12.44 \t\t|\t\t 6.6899999999999995\n",
      "[Feb 25, 09:03:08] 387 0.7977642776030959\n",
      "\t\t\t\t 0.06002527475357056 0.2584465742111206\n",
      "#>>>    19.34 11.94 \t\t|\t\t 7.4\n",
      "[Feb 25, 09:03:08] 388 0.7972849851744575\n",
      "\t\t\t\t 0.07761626690626144 0.3000262975692749\n",
      "#>>>    19.74 12.54 \t\t|\t\t 7.199999999999999\n",
      "[Feb 25, 09:03:09] 389 0.796865342761209\n",
      "\t\t\t\t 0.04470657929778099 0.04239046946167946\n",
      "#>>>    20.43 12.13 \t\t|\t\t 8.299999999999999\n",
      "[Feb 25, 09:03:09] 390 0.7961555744672072\n",
      "\t\t\t\t 0.11415746808052063 0.70671546459198\n",
      "#>>>    19.75 12.37 \t\t|\t\t 7.380000000000001\n",
      "[Feb 25, 09:03:09] 391 0.7961802917956101\n",
      "\t\t\t\t 0.15699157118797302 0.789581298828125\n",
      "#>>>    19.42 12.85 \t\t|\t\t 6.570000000000002\n",
      "[Feb 25, 09:03:10] 392 0.796330684403633\n",
      "\t\t\t\t 0.08863110840320587 0.373615562915802\n",
      "#>>>    20.77 12.5 \t\t|\t\t 8.27\n",
      "[Feb 25, 09:03:10] 393 0.7959966003756471\n",
      "\t\t\t\t 0.07291907072067261 0.4046395719051361\n",
      "#>>>    19.91 12.07 \t\t|\t\t 7.84\n",
      "[Feb 25, 09:03:10] 394 0.7956781624178972\n",
      "\t\t\t\t 0.07281584292650223 0.5046530365943909\n",
      "#>>>    19.59 11.97 \t\t|\t\t 7.619999999999999\n",
      "[Feb 25, 09:03:10] 395 0.7954599531275497\n",
      "\t\t\t\t 0.09521055221557617 0.2883705198764801\n",
      "#>>>    20.09 12.41 \t\t|\t\t 7.68\n",
      "[Feb 25, 09:03:11] 396 0.7950480742465141\n",
      "\t\t\t\t 0.059848569333553314 0.1902613639831543\n",
      "#>>>    20.01 12.26 \t\t|\t\t 7.750000000000002\n",
      "[Feb 25, 09:03:11] 397 0.7945031361130349\n",
      "\t\t\t\t 0.1328570544719696 0.30165010690689087\n",
      "#>>>    20.58 12.33 \t\t|\t\t 8.249999999999998\n",
      "[Feb 25, 09:03:11] 398 0.7941431401383007\n",
      "\t\t\t\t 0.4516206979751587 0.6607570052146912\n",
      "#>>>    19.39 12.72 \t\t|\t\t 6.67\n",
      "[Feb 25, 09:03:12] 399 0.7944613746417476\n",
      "\t\t\t\t 0.13927848637104034 0.3468341529369354\n",
      "#>>>    19.57 12.02 \t\t|\t\t 7.550000000000001\n",
      "[Feb 25, 09:03:12] 400 0.794153025921315\n",
      "\t\t\t\t 0.16483542323112488 0.5601805448532104\n",
      "#>>>    19.16 12.82 \t\t|\t\t 6.34\n",
      "[Feb 25, 09:03:12] 401 0.7940838888932803\n",
      "\t\t\t\t 0.14657457172870636 0.49077150225639343\n",
      "#>>>    19.34 12.45 \t\t|\t\t 6.890000000000001\n",
      "[Feb 25, 09:03:13] 402 0.7939271510932733\n",
      "\t\t\t\t 0.15445442497730255 0.5968201160430908\n",
      "#>>>    19.16 12.96 \t\t|\t\t 6.199999999999999\n",
      "[Feb 25, 09:03:13] 403 0.7938844984682992\n",
      "\t\t\t\t 0.13728183507919312 0.5649223327636719\n",
      "#>>>    18.92 12.46 \t\t|\t\t 6.460000000000001\n",
      "[Feb 25, 09:03:13] 404 0.7937928181376738\n",
      "\t\t\t\t 0.08305829018354416 0.2465871274471283\n",
      "#>>>    20.43 10.98 \t\t|\t\t 9.45\n",
      "[Feb 25, 09:03:14] 405 0.7933286707446174\n",
      "\t\t\t\t 0.5461099147796631 0.8614272475242615\n",
      "#>>>    19.33 11.15 \t\t|\t\t 8.179999999999998\n",
      "[Feb 25, 09:03:14] 406 0.7939428792957813\n",
      "\t\t\t\t 0.18549099564552307 0.5301462411880493\n",
      "#>>>    19.15 11.92 \t\t|\t\t 7.229999999999999\n",
      "[Feb 25, 09:03:14] 407 0.7938645736235168\n",
      "\t\t\t\t 0.0687791034579277 0.42270204424858093\n",
      "#>>>    19.6 12.39 \t\t|\t\t 7.210000000000001\n",
      "[Feb 25, 09:03:14] 408 0.7935621902050504\n",
      "\t\t\t\t 0.05992317199707031 0.40733692049980164\n",
      "#>>>    19.74 12.62 \t\t|\t\t 7.119999999999999\n",
      "[Feb 25, 09:03:15] 409 0.7932358881073421\n",
      "\t\t\t\t 0.12437495589256287 0.298734188079834\n",
      "#>>>    19.63 12.52 \t\t|\t\t 7.109999999999999\n",
      "[Feb 25, 09:03:15] 410 0.7928657613632072\n",
      "\t\t\t\t 0.1651584506034851 0.8834104537963867\n",
      "#>>>    17.96 11.74 \t\t|\t\t 6.220000000000001\n",
      "[Feb 25, 09:03:15] 411 0.7931214645658485\n",
      "\t\t\t\t 0.13231387734413147 0.6289666891098022\n",
      "#>>>    19.1 12.57 \t\t|\t\t 6.530000000000001\n",
      "[Feb 25, 09:03:16] 412 0.7930896236379342\n",
      "\t\t\t\t 0.06043199449777603 0.4338235855102539\n",
      "#>>>    19.79 12.33 \t\t|\t\t 7.459999999999999\n",
      "[Feb 25, 09:03:16] 413 0.7927907895868538\n",
      "\t\t\t\t 0.05885010212659836 0.4660278856754303\n",
      "#>>>    19.23 11.68 \t\t|\t\t 7.550000000000001\n",
      "[Feb 25, 09:03:16] 414 0.7925228767627173\n",
      "\t\t\t\t 0.10174635052680969 0.45792850852012634\n",
      "#>>>    19.04 12.58 \t\t|\t\t 6.459999999999999\n",
      "[Feb 25, 09:03:17] 415 0.7922900287450015\n",
      "\t\t\t\t 0.07590296119451523 0.2988211214542389\n",
      "#>>>    20.13 12.66 \t\t|\t\t 7.469999999999999\n",
      "[Feb 25, 09:03:17] 416 0.7918724628063558\n",
      "\t\t\t\t 0.14813221991062164 0.31521889567375183\n",
      "#>>>    19.62 12.24 \t\t|\t\t 7.380000000000001\n",
      "[Feb 25, 09:03:17] 417 0.7915439414740351\n",
      "\t\t\t\t 0.1149100512266159 0.2419428825378418\n",
      "#>>>    19.17 11.36 \t\t|\t\t 7.810000000000002\n",
      "[Feb 25, 09:03:17] 418 0.7911092504812266\n",
      "\t\t\t\t 0.20081637799739838 0.6803998947143555\n",
      "#>>>    18.89 12.97 \t\t|\t\t 5.92\n",
      "[Feb 25, 09:03:18] 419 0.7911993575183583\n",
      "\t\t\t\t 0.12612901628017426 0.7460638880729675\n",
      "#>>>    18.91 12.76 \t\t|\t\t 6.15\n",
      "[Feb 25, 09:03:18] 420 0.7912803510800943\n",
      "\t\t\t\t 0.12786993384361267 0.4176231324672699\n",
      "#>>>    19.54 12.08 \t\t|\t\t 7.459999999999999\n",
      "[Feb 25, 09:03:18] 421 0.7910345637953251\n",
      "\t\t\t\t 0.17069965600967407 0.41540077328681946\n",
      "#>>>    19.32 12.35 \t\t|\t\t 6.970000000000001\n",
      "[Feb 25, 09:03:19] 422 0.7908296296906285\n",
      "\t\t\t\t 0.08603071421384811 0.41775888204574585\n",
      "#>>>    20.02 12.19 \t\t|\t\t 7.83\n",
      "[Feb 25, 09:03:19] 423 0.7905425896646481\n",
      "\t\t\t\t 0.07668790221214294 0.4780413508415222\n",
      "#>>>    19.25 12.39 \t\t|\t\t 6.859999999999999\n",
      "[Feb 25, 09:03:19] 424 0.7903067762982349\n",
      "\t\t\t\t 0.08016621321439743 0.4527055025100708\n",
      "#>>>    19.75 11.33 \t\t|\t\t 8.42\n",
      "[Feb 25, 09:03:20] 425 0.7900493412451117\n",
      "\t\t\t\t 0.12054882198572159 0.43735188245773315\n",
      "#>>>    19.49 12.65 \t\t|\t\t 6.839999999999998\n",
      "[Feb 25, 09:03:20] 426 0.7898171926306617\n",
      "\t\t\t\t 0.05296395719051361 0.44006049633026123\n",
      "#>>>    19.07 10.35 \t\t|\t\t 8.72\n",
      "[Feb 25, 09:03:20] 427 0.789520399906453\n",
      "\t\t\t\t 0.15631456673145294 0.3540336489677429\n",
      "#>>>    19.07 12.31 \t\t|\t\t 6.76\n",
      "[Feb 25, 09:03:20] 428 0.7892412277073445\n",
      "\t\t\t\t 0.0513840913772583 0.2681233286857605\n",
      "#>>>    19.04 12.57 \t\t|\t\t 6.469999999999999\n",
      "[Feb 25, 09:03:21] 429 0.7887714938997002\n",
      "\t\t\t\t 0.09962372481822968 0.22516675293445587\n",
      "#>>>    19.53 12.34 \t\t|\t\t 7.190000000000001\n",
      "[Feb 25, 09:03:21] 430 0.7883075128835533\n",
      "\t\t\t\t 0.18390361964702606 0.5007784366607666\n",
      "#>>>    19.32 12.81 \t\t|\t\t 6.51\n",
      "[Feb 25, 09:03:21] 431 0.7882038874418787\n",
      "\t\t\t\t 0.1096390038728714 0.4238654375076294\n",
      "#>>>    19.81 12.5 \t\t|\t\t 7.309999999999999\n",
      "[Feb 25, 09:03:22] 432 0.7879491879809163\n",
      "\t\t\t\t 0.21442779898643494 0.9326313734054565\n",
      "#>>>    18.55 11.81 \t\t|\t\t 6.74\n",
      "[Feb 25, 09:03:22] 433 0.7883082979951297\n",
      "\t\t\t\t 0.06009260192513466 0.3382202684879303\n",
      "#>>>    20.02 11.01 \t\t|\t\t 9.01\n",
      "[Feb 25, 09:03:22] 434 0.7879183025638223\n",
      "\t\t\t\t 0.14896593987941742 0.48623210191726685\n",
      "#>>>    19.45 11.74 \t\t|\t\t 7.709999999999999\n",
      "[Feb 25, 09:03:23] 435 0.7877655823179563\n",
      "\t\t\t\t 0.039252374321222305 0.45979541540145874\n",
      "#>>>    19.33 11.92 \t\t|\t\t 7.409999999999998\n",
      "[Feb 25, 09:03:23] 436 0.7874768645216358\n",
      "\t\t\t\t 0.10854513943195343 0.19820177555084229\n",
      "#>>>    19.81 11.88 \t\t|\t\t 7.929999999999998\n",
      "[Feb 25, 09:03:23] 437 0.7869961345571957\n",
      "\t\t\t\t 0.1347501575946808 0.6505504250526428\n",
      "#>>>    18.6 11.72 \t\t|\t\t 6.880000000000001\n",
      "[Feb 25, 09:03:24] 438 0.7869944390350881\n",
      "\t\t\t\t 0.1973969042301178 0.4913703501224518\n",
      "#>>>    19.43 12.65 \t\t|\t\t 6.779999999999999\n",
      "[Feb 25, 09:03:24] 439 0.7868962118504056\n",
      "\t\t\t\t 0.5346152186393738 0.5653183460235596\n",
      "#>>>    19.31 12.24 \t\t|\t\t 7.0699999999999985\n",
      "[Feb 25, 09:03:24] 440 0.7872092492628228\n",
      "\t\t\t\t 0.13161298632621765 0.24836915731430054\n",
      "#>>>    19.37 13.21 \t\t|\t\t 6.16\n",
      "[Feb 25, 09:03:24] 441 0.7868020221572005\n",
      "\t\t\t\t 0.15368176996707916 0.3864251375198364\n",
      "#>>>    19.42 12.31 \t\t|\t\t 7.110000000000001\n",
      "[Feb 25, 09:03:25] 442 0.7865553270276291\n",
      "\t\t\t\t 0.19799861311912537 0.46086177229881287\n",
      "#>>>    18.63 12.5 \t\t|\t\t 6.129999999999999\n",
      "[Feb 25, 09:03:25] 443 0.7864276320860194\n",
      "\t\t\t\t 0.053777772933244705 0.5468435287475586\n",
      "#>>>    19.17 11.62 \t\t|\t\t 7.5500000000000025\n",
      "[Feb 25, 09:03:25] 444 0.7862418257369878\n",
      "\t\t\t\t 0.023555880412459373 0.1120777502655983\n",
      "#>>>    20.73 11.34 \t\t|\t\t 9.39\n",
      "[Feb 25, 09:03:26] 445 0.7855912175437915\n",
      "\t\t\t\t 0.038765497505664825 0.23246251046657562\n",
      "#>>>    20.07 12.09 \t\t|\t\t 7.98\n",
      "[Feb 25, 09:03:26] 446 0.7850768543416704\n",
      "\t\t\t\t 0.09420706331729889 0.5373052358627319\n",
      "#>>>    19.35 11.67 \t\t|\t\t 7.6800000000000015\n",
      "[Feb 25, 09:03:26] 447 0.7849232897716075\n",
      "\t\t\t\t 0.16024914383888245 0.41522911190986633\n",
      "#>>>    19.86 12.08 \t\t|\t\t 7.779999999999999\n",
      "[Feb 25, 09:03:27] 448 0.7847138447375847\n",
      "\t\t\t\t 0.3314497768878937 0.5021975040435791\n",
      "#>>>    19.2 12.04 \t\t|\t\t 7.16\n",
      "[Feb 25, 09:03:27] 449 0.7847627781439762\n",
      "\t\t\t\t 0.1401389241218567 0.6219053268432617\n",
      "#>>>    19.86 11.89 \t\t|\t\t 7.969999999999999\n",
      "[Feb 25, 09:03:27] 450 0.7847400596167974\n",
      "\t\t\t\t 0.06860075891017914 0.458183616399765\n",
      "#>>>    19.81 11.96 \t\t|\t\t 7.849999999999998\n",
      "[Feb 25, 09:03:27] 451 0.7844821039175893\n",
      "\t\t\t\t 0.10629278421401978 0.15196505188941956\n",
      "#>>>    19.9 12.05 \t\t|\t\t 7.849999999999998\n",
      "[Feb 25, 09:03:28] 452 0.7839558796497752\n",
      "\t\t\t\t 0.29207831621170044 0.8310040235519409\n",
      "#>>>    19.08 13.2 \t\t|\t\t 5.879999999999999\n",
      "[Feb 25, 09:03:28] 453 0.7842950061694938\n",
      "\t\t\t\t 0.04397996887564659 0.22871984541416168\n",
      "#>>>    19.34 10.5 \t\t|\t\t 8.84\n",
      "[Feb 25, 09:03:28] 454 0.7837834109664382\n",
      "\t\t\t\t 0.18842804431915283 0.39818453788757324\n",
      "#>>>    19.14 11.41 \t\t|\t\t 7.73\n",
      "[Feb 25, 09:03:29] 455 0.7835862401376785\n",
      "\t\t\t\t 0.1547202467918396 0.48667046427726746\n",
      "#>>>    18.88 12.35 \t\t|\t\t 6.529999999999999\n",
      "[Feb 25, 09:03:29] 456 0.7834440445788077\n",
      "\t\t\t\t 0.09602091461420059 0.3228565454483032\n",
      "#>>>    19.28 10.85 \t\t|\t\t 8.430000000000001\n",
      "[Feb 25, 09:03:29] 457 0.7830794779868407\n",
      "\t\t\t\t 0.18293660879135132 0.47952011227607727\n",
      "#>>>    18.85 12.28 \t\t|\t\t 6.570000000000002\n",
      "[Feb 25, 09:03:30] 458 0.7829588552597236\n",
      "\t\t\t\t 0.0917096957564354 0.3068876564502716\n",
      "#>>>    19.72 11.34 \t\t|\t\t 8.379999999999999\n",
      "[Feb 25, 09:03:30] 459 0.7825744937641212\n",
      "\t\t\t\t 0.1399431824684143 0.5267580151557922\n",
      "#>>>    19.42 11.99 \t\t|\t\t 7.4300000000000015\n",
      "[Feb 25, 09:03:30] 460 0.7824586204679813\n",
      "\t\t\t\t 0.04947260394692421 0.3228256404399872\n",
      "#>>>    19.69 13.02 \t\t|\t\t 6.670000000000002\n",
      "[Feb 25, 09:03:31] 461 0.782048460088175\n",
      "\t\t\t\t 0.13868750631809235 0.49188846349716187\n",
      "#>>>    19.4 11.44 \t\t|\t\t 7.959999999999999\n",
      "[Feb 25, 09:03:31] 462 0.7818969875830009\n",
      "\t\t\t\t 0.16917884349822998 0.4999489486217499\n",
      "#>>>    19.37 12.4 \t\t|\t\t 6.970000000000001\n",
      "[Feb 25, 09:03:31] 463 0.7817842184173401\n",
      "\t\t\t\t 0.10035429149866104 0.8490440249443054\n",
      "#>>>    19.0 11.88 \t\t|\t\t 7.119999999999999\n",
      "[Feb 25, 09:03:31] 464 0.7819518325377175\n",
      "\t\t\t\t 0.17373186349868774 0.3410337567329407\n",
      "#>>>    20.22 12.68 \t\t|\t\t 7.539999999999999\n",
      "[Feb 25, 09:03:32] 465 0.7816846463254115\n",
      "\t\t\t\t 0.08222545683383942 0.38818424940109253\n",
      "#>>>    19.65 12.3 \t\t|\t\t 7.349999999999998\n",
      "[Feb 25, 09:03:32] 466 0.7813733713704197\n",
      "\t\t\t\t 0.19237327575683594 0.20872537791728973\n",
      "#>>>    20.26 13.54 \t\t|\t\t 6.720000000000002\n",
      "[Feb 25, 09:03:32] 467 0.7809930966676246\n",
      "\t\t\t\t 0.04983217269182205 0.17998208105564117\n",
      "#>>>    19.45 11.9 \t\t|\t\t 7.549999999999999\n",
      "[Feb 25, 09:03:33] 468 0.7804419178321551\n",
      "\t\t\t\t 0.0703396126627922 0.23446868360042572\n",
      "#>>>    19.71 11.53 \t\t|\t\t 8.180000000000001\n",
      "[Feb 25, 09:03:33] 469 0.7799662842031356\n",
      "\t\t\t\t 0.16923530399799347 0.5377836227416992\n",
      "#>>>    19.19 11.62 \t\t|\t\t 7.570000000000002\n",
      "[Feb 25, 09:03:33] 470 0.779893336830771\n",
      "\t\t\t\t 0.0631977841258049 0.21981778740882874\n",
      "#>>>    20.19 12.36 \t\t|\t\t 7.830000000000002\n",
      "[Feb 25, 09:03:34] 471 0.7793964590729254\n",
      "\t\t\t\t 0.11750108003616333 0.3371996581554413\n",
      "#>>>    19.92 11.47 \t\t|\t\t 8.450000000000001\n",
      "[Feb 25, 09:03:34] 472 0.7790717633520442\n",
      "\t\t\t\t 0.13813605904579163 0.2963533401489258\n",
      "#>>>    20.32 12.93 \t\t|\t\t 7.390000000000001\n",
      "[Feb 25, 09:03:34] 473 0.778727180987887\n",
      "\t\t\t\t 0.0967254489660263 0.19517315924167633\n",
      "#>>>    19.72 11.1 \t\t|\t\t 8.62\n",
      "[Feb 25, 09:03:34] 474 0.7782403524151067\n",
      "\t\t\t\t 0.12128808349370956 0.2510172724723816\n",
      "#>>>    19.46 12.22 \t\t|\t\t 7.24\n",
      "[Feb 25, 09:03:35] 475 0.7778344174261083\n",
      "\t\t\t\t 0.08821188658475876 0.21282443404197693\n",
      "#>>>    20.31 11.16 \t\t|\t\t 9.149999999999999\n",
      "[Feb 25, 09:03:35] 476 0.7773576193367595\n",
      "\t\t\t\t 0.02265464887022972 0.25326216220855713\n",
      "#>>>    20.28 11.09 \t\t|\t\t 9.190000000000001\n",
      "[Feb 25, 09:03:35] 477 0.7768561785322269\n",
      "\t\t\t\t 0.053007736802101135 0.4078693389892578\n",
      "#>>>    19.36 12.27 \t\t|\t\t 7.09\n",
      "[Feb 25, 09:03:36] 478 0.7765401994145849\n",
      "\t\t\t\t 0.1725408136844635 0.8174141645431519\n",
      "#>>>    18.63 12.65 \t\t|\t\t 5.979999999999999\n",
      "[Feb 25, 09:03:36] 479 0.7767536141635956\n",
      "\t\t\t\t 0.13816432654857635 0.3846514821052551\n",
      "#>>>    19.5 12.0 \t\t|\t\t 7.5\n",
      "[Feb 25, 09:03:36] 480 0.7764996763729869\n",
      "\t\t\t\t 0.16597303748130798 0.5841938257217407\n",
      "#>>>    19.57 12.08 \t\t|\t\t 7.49\n",
      "[Feb 25, 09:03:37] 481 0.7764733435896193\n",
      "\t\t\t\t 0.11552509665489197 0.36420419812202454\n",
      "#>>>    19.11 12.55 \t\t|\t\t 6.559999999999999\n",
      "[Feb 25, 09:03:37] 482 0.7761765995408065\n",
      "\t\t\t\t 0.10156875103712082 0.5794336795806885\n",
      "#>>>    18.59 11.73 \t\t|\t\t 6.859999999999999\n",
      "[Feb 25, 09:03:37] 483 0.7760814253793341\n",
      "\t\t\t\t 0.08272945880889893 0.3600408136844635\n",
      "#>>>    19.69 12.09 \t\t|\t\t 7.600000000000001\n",
      "[Feb 25, 09:03:38] 484 0.7757481142264482\n",
      "\t\t\t\t 0.07500038295984268 0.3890937566757202\n",
      "#>>>    19.64 12.85 \t\t|\t\t 6.790000000000001\n",
      "[Feb 25, 09:03:38] 485 0.7754364602444067\n",
      "\t\t\t\t 0.04888710752129555 0.30856797099113464\n",
      "#>>>    19.37 11.66 \t\t|\t\t 7.710000000000001\n",
      "[Feb 25, 09:03:38] 486 0.7750184788589495\n",
      "\t\t\t\t 0.1869160532951355 0.5368648767471313\n",
      "#>>>    19.57 11.36 \t\t|\t\t 8.21\n",
      "[Feb 25, 09:03:38] 487 0.7749672413101327\n",
      "\t\t\t\t 0.14850662648677826 0.19322292506694794\n",
      "#>>>    20.16 12.68 \t\t|\t\t 7.48\n",
      "[Feb 25, 09:03:39] 488 0.7745340036203764\n",
      "\t\t\t\t 0.057490136474370956 0.18002283573150635\n",
      "#>>>    19.78 11.23 \t\t|\t\t 8.55\n",
      "[Feb 25, 09:03:39] 489 0.7739969825926872\n",
      "\t\t\t\t 0.05298088118433952 0.2411172091960907\n",
      "#>>>    20.03 11.64 \t\t|\t\t 8.39\n",
      "[Feb 25, 09:03:39] 490 0.7735170836892991\n",
      "\t\t\t\t 0.1872299313545227 0.28724974393844604\n",
      "#>>>    20.01 13.45 \t\t|\t\t 6.560000000000002\n",
      "[Feb 25, 09:03:40] 491 0.7732180462809027\n",
      "\t\t\t\t 0.09877623617649078 0.6634381413459778\n",
      "#>>>    19.11 12.65 \t\t|\t\t 6.459999999999999\n",
      "[Feb 25, 09:03:40] 492 0.7732070425972432\n",
      "\t\t\t\t 0.12454657256603241 0.5516676306724548\n",
      "#>>>    19.29 12.06 \t\t|\t\t 7.229999999999999\n",
      "[Feb 25, 09:03:40] 493 0.7731100497727856\n",
      "\t\t\t\t 0.2587631046772003 0.5358192920684814\n",
      "#>>>    19.77 12.46 \t\t|\t\t 7.309999999999999\n",
      "[Feb 25, 09:03:41] 494 0.7731315220899563\n",
      "\t\t\t\t 0.08474510908126831 0.19688190519809723\n",
      "#>>>    20.09 12.35 \t\t|\t\t 7.74\n",
      "[Feb 25, 09:03:41] 495 0.7726400175672445\n",
      "\t\t\t\t 0.12256423383951187 0.39052799344062805\n",
      "#>>>    19.3 11.63 \t\t|\t\t 7.67\n",
      "[Feb 25, 09:03:41] 496 0.7723804697695068\n",
      "\t\t\t\t 0.37085655331611633 0.20661665499210358\n",
      "#>>>    20.77 13.58 \t\t|\t\t 7.1899999999999995\n",
      "[Feb 25, 09:03:41] 497 0.7721855625229467\n",
      "\t\t\t\t 0.24615132808685303 0.6291735768318176\n",
      "#>>>    18.86 13.29 \t\t|\t\t 5.57\n",
      "[Feb 25, 09:03:42] 498 0.7722887018653425\n",
      "\t\t\t\t 0.05929110571742058 0.26411810517311096\n",
      "#>>>    19.34 11.47 \t\t|\t\t 7.869999999999999\n",
      "[Feb 25, 09:03:42] 499 0.7718398223631918\n",
      "\t\t\t\t 0.10448524355888367 0.3968372941017151\n",
      "#>>>    18.6 11.72 \t\t|\t\t 6.880000000000001\n",
      "[Feb 25, 09:03:42] 500 0.7715693050486869\n",
      "\t\t\t\t 0.09846600145101547 0.2282693237066269\n",
      "#>>>    19.65 13.04 \t\t|\t\t 6.609999999999999\n",
      "[Feb 25, 09:03:43] 501 0.7711244710613453\n",
      "\t\t\t\t 0.10515528172254562 0.2191089391708374\n",
      "#>>>    20.27 12.63 \t\t|\t\t 7.639999999999999\n",
      "[Feb 25, 09:03:43] 502 0.770677610818628\n",
      "\t\t\t\t 0.08466202020645142 0.3632923364639282\n",
      "#>>>    20.3 13.65 \t\t|\t\t 6.65\n",
      "[Feb 25, 09:03:43] 503 0.7703548875644797\n",
      "\t\t\t\t 0.15596815943717957 0.601798415184021\n",
      "#>>>    19.32 13.13 \t\t|\t\t 6.1899999999999995\n",
      "[Feb 25, 09:03:44] 504 0.7703422992813387\n",
      "\t\t\t\t 0.1593158096075058 0.5256372094154358\n",
      "#>>>    19.89 11.69 \t\t|\t\t 8.200000000000001\n",
      "[Feb 25, 09:03:44] 505 0.7702569100159815\n",
      "\t\t\t\t 0.12067245692014694 0.4773081839084625\n",
      "#>>>    18.92 12.49 \t\t|\t\t 6.4300000000000015\n",
      "[Feb 25, 09:03:44] 506 0.7700846337244424\n",
      "\t\t\t\t 0.1331736147403717 0.32014212012290955\n",
      "#>>>    19.11 11.86 \t\t|\t\t 7.25\n",
      "[Feb 25, 09:03:45] 507 0.7697678648255812\n",
      "\t\t\t\t 0.13477742671966553 0.5165533423423767\n",
      "#>>>    19.72 11.01 \t\t|\t\t 8.709999999999999\n",
      "[Feb 25, 09:03:45] 508 0.7696494277298177\n",
      "\t\t\t\t 0.15826308727264404 0.2874927222728729\n",
      "#>>>    19.11 11.9 \t\t|\t\t 7.209999999999999\n",
      "[Feb 25, 09:03:45] 509 0.7693255341116334\n",
      "\t\t\t\t 0.08601532131433487 0.2532675266265869\n",
      "#>>>    19.34 12.2 \t\t|\t\t 7.140000000000001\n",
      "[Feb 25, 09:03:45] 510 0.7688954914180121\n",
      "\t\t\t\t 0.03876250982284546 0.25088390707969666\n",
      "#>>>    19.6 11.0 \t\t|\t\t 8.600000000000001\n",
      "[Feb 25, 09:03:46] 511 0.7684162423434966\n",
      "\t\t\t\t 0.15740180015563965 0.4537772834300995\n",
      "#>>>    18.96 13.05 \t\t|\t\t 5.91\n",
      "[Feb 25, 09:03:46] 512 0.7682590052145412\n",
      "\t\t\t\t 0.21477839350700378 0.6373188495635986\n",
      "#>>>    19.31 13.68 \t\t|\t\t 5.629999999999999\n",
      "[Feb 25, 09:03:46] 513 0.7683428434821996\n",
      "\t\t\t\t 0.09274449944496155 0.251668781042099\n",
      "#>>>    19.92 13.13 \t\t|\t\t 6.790000000000001\n",
      "[Feb 25, 09:03:47] 514 0.7679189139192045\n",
      "\t\t\t\t 0.14088982343673706 0.46242913603782654\n",
      "#>>>    19.48 12.32 \t\t|\t\t 7.16\n",
      "[Feb 25, 09:03:47] 515 0.7677543139349575\n",
      "\t\t\t\t 0.06654925644397736 0.22008968889713287\n",
      "#>>>    19.33 10.74 \t\t|\t\t 8.589999999999998\n",
      "[Feb 25, 09:03:47] 516 0.7672731985663637\n",
      "\t\t\t\t 0.04221194609999657 0.3218921720981598\n",
      "#>>>    18.91 10.77 \t\t|\t\t 8.14\n",
      "[Feb 25, 09:03:48] 517 0.7668700294897207\n",
      "\t\t\t\t 0.06095736846327782 0.28236374258995056\n",
      "#>>>    20.14 12.19 \t\t|\t\t 7.950000000000001\n",
      "[Feb 25, 09:03:48] 518 0.7664464805750095\n",
      "\t\t\t\t 0.13228996098041534 0.4095411002635956\n",
      "#>>>    20.07 11.56 \t\t|\t\t 8.51\n",
      "[Feb 25, 09:03:48] 519 0.7662218651705797\n",
      "\t\t\t\t 0.1448165327310562 0.5722972750663757\n",
      "#>>>    19.63 12.05 \t\t|\t\t 7.579999999999998\n",
      "[Feb 25, 09:03:48] 520 0.7661727570983053\n",
      "\t\t\t\t 0.18027494847774506 0.40072065591812134\n",
      "#>>>    19.49 12.84 \t\t|\t\t 6.649999999999999\n",
      "[Feb 25, 09:03:49] 521 0.765987579960504\n",
      "\t\t\t\t 0.013494757004082203 0.10296886414289474\n",
      "#>>>    20.16 11.18 \t\t|\t\t 8.98\n",
      "[Feb 25, 09:03:49] 522 0.7653380560044845\n",
      "\t\t\t\t 0.07556026428937912 0.43219631910324097\n",
      "#>>>    19.68 12.76 \t\t|\t\t 6.92\n",
      "[Feb 25, 09:03:49] 523 0.7650804745393232\n",
      "\t\t\t\t 0.07328315824270248 0.5306339263916016\n",
      "#>>>    19.64 12.1 \t\t|\t\t 7.540000000000001\n",
      "[Feb 25, 09:03:50] 524 0.7649193111270665\n",
      "\t\t\t\t 0.2612032890319824 0.6269525289535522\n",
      "#>>>    19.29 12.75 \t\t|\t\t 6.539999999999999\n",
      "[Feb 25, 09:03:50] 525 0.7650425476339249\n",
      "\t\t\t\t 0.05711742490530014 0.254025399684906\n",
      "#>>>    20.38 12.04 \t\t|\t\t 8.34\n",
      "[Feb 25, 09:03:50] 526 0.7645886479183318\n",
      "\t\t\t\t 0.0024930271320044994 0.14258742332458496\n",
      "#>>>    19.62 9.9 \t\t|\t\t 9.72\n",
      "[Feb 25, 09:03:51] 527 0.7639691397176105\n",
      "\t\t\t\t 0.12412935495376587 0.3301471769809723\n",
      "#>>>    18.77 11.97 \t\t|\t\t 6.799999999999999\n",
      "[Feb 25, 09:03:51] 528 0.7636594471098276\n",
      "\t\t\t\t 0.08020491898059845 0.21932756900787354\n",
      "#>>>    19.03 11.92 \t\t|\t\t 7.110000000000001\n",
      "[Feb 25, 09:03:51] 529 0.763195320135805\n",
      "\t\t\t\t 0.1001596450805664 0.15354479849338531\n",
      "#>>>    19.84 11.09 \t\t|\t\t 8.75\n",
      "[Feb 25, 09:03:52] 530 0.762685829244342\n",
      "\t\t\t\t 0.0484209842979908 0.23913495242595673\n",
      "#>>>    20.97 11.19 \t\t|\t\t 9.78\n",
      "[Feb 25, 09:03:52] 531 0.7622106993480964\n",
      "\t\t\t\t 0.10159628093242645 0.330232709646225\n",
      "#>>>    19.67 11.46 \t\t|\t\t 8.21\n",
      "[Feb 25, 09:03:52] 532 0.7618803176244258\n",
      "\t\t\t\t 0.1606135368347168 0.4633687138557434\n",
      "#>>>    19.3 12.84 \t\t|\t\t 6.460000000000001\n",
      "[Feb 25, 09:03:52] 533 0.7617424195574918\n",
      "\t\t\t\t 0.20569446682929993 0.24072401225566864\n",
      "#>>>    19.88 12.01 \t\t|\t\t 7.869999999999999\n",
      "[Feb 25, 09:03:53] 534 0.7614270956021181\n",
      "\t\t\t\t 0.13049259781837463 0.7505861520767212\n",
      "#>>>    18.73 12.33 \t\t|\t\t 6.4\n",
      "[Feb 25, 09:03:53] 535 0.7615467472266088\n",
      "\t\t\t\t 0.07532467693090439 0.30399999022483826\n",
      "#>>>    19.01 11.77 \t\t|\t\t 7.240000000000002\n",
      "[Feb 25, 09:03:53] 536 0.7611645251539885\n",
      "\t\t\t\t 0.13853012025356293 0.46992820501327515\n",
      "#>>>    18.77 11.25 \t\t|\t\t 7.52\n",
      "[Feb 25, 09:03:54] 537 0.7610118189690025\n",
      "\t\t\t\t 0.058438532054424286 0.3248947560787201\n",
      "#>>>    19.4 10.75 \t\t|\t\t 8.649999999999999\n",
      "[Feb 25, 09:03:54] 538 0.7606341404456172\n",
      "\t\t\t\t 0.08016682416200638 0.4270890951156616\n",
      "#>>>    19.14 10.13 \t\t|\t\t 9.01\n",
      "[Feb 25, 09:03:54] 539 0.7603807622169987\n",
      "\t\t\t\t 0.09625732153654099 0.1381790041923523\n",
      "#>>>    18.76 10.95 \t\t|\t\t 7.810000000000002\n",
      "[Feb 25, 09:03:55] 540 0.7598548177879612\n",
      "\t\t\t\t 0.06475638598203659 0.39896491169929504\n",
      "#>>>    19.88 11.52 \t\t|\t\t 8.36\n",
      "[Feb 25, 09:03:55] 541 0.7595586842753052\n",
      "\t\t\t\t 0.129531592130661 0.5733548402786255\n",
      "#>>>    18.65 11.54 \t\t|\t\t 7.109999999999999\n",
      "[Feb 25, 09:03:55] 542 0.7595020120532414\n",
      "\t\t\t\t 0.179011270403862 0.6301195621490479\n",
      "#>>>    19.4 12.98 \t\t|\t\t 6.419999999999998\n",
      "[Feb 25, 09:03:56] 543 0.7595516408886422\n",
      "\t\t\t\t 0.039609480649232864 0.19793230295181274\n",
      "#>>>    19.8 11.37 \t\t|\t\t 8.430000000000001\n",
      "[Feb 25, 09:03:56] 544 0.7590296310276294\n",
      "\t\t\t\t 0.10837403684854507 0.38170939683914185\n",
      "#>>>    18.64 11.68 \t\t|\t\t 6.960000000000001\n",
      "[Feb 25, 09:03:56] 545 0.758760684822839\n",
      "\t\t\t\t 0.0743657723069191 0.36535152792930603\n",
      "#>>>    18.63 10.3 \t\t|\t\t 8.329999999999998\n",
      "[Feb 25, 09:03:56] 546 0.7584416414308018\n",
      "\t\t\t\t 0.15555104613304138 0.3060831129550934\n",
      "#>>>    19.11 12.55 \t\t|\t\t 6.559999999999999\n",
      "[Feb 25, 09:03:57] 547 0.7581448339484592\n",
      "\t\t\t\t 0.08434402942657471 0.28521978855133057\n",
      "#>>>    19.74 12.89 \t\t|\t\t 6.849999999999998\n",
      "[Feb 25, 09:03:57] 548 0.7577562529324887\n",
      "\t\t\t\t 0.031315118074417114 0.1703169047832489\n",
      "#>>>    19.12 10.63 \t\t|\t\t 8.49\n",
      "[Feb 25, 09:03:57] 549 0.7572001287024138\n",
      "\t\t\t\t 0.06342340260744095 0.3358781933784485\n",
      "#>>>    19.04 11.25 \t\t|\t\t 7.789999999999999\n",
      "[Feb 25, 09:03:58] 550 0.7568422301622467\n",
      "\t\t\t\t 0.06571020185947418 0.15122663974761963\n",
      "#>>>    18.9 11.18 \t\t|\t\t 7.719999999999999\n",
      "[Feb 25, 09:03:58] 551 0.7563023247736915\n",
      "\t\t\t\t 0.06224449723958969 0.06799767911434174\n",
      "#>>>    19.06 10.95 \t\t|\t\t 8.11\n",
      "[Feb 25, 09:03:58] 552 0.7556762646178212\n",
      "\t\t\t\t 0.0938594788312912 0.3526453673839569\n",
      "#>>>    18.14 11.0 \t\t|\t\t 7.140000000000001\n",
      "[Feb 25, 09:03:59] 553 0.7553670931845174\n",
      "\t\t\t\t 0.05696463957428932 0.20740044116973877\n",
      "#>>>    19.48 11.23 \t\t|\t\t 8.25\n",
      "[Feb 25, 09:03:59] 554 0.7548760911683517\n",
      "\t\t\t\t 0.14949697256088257 0.4852292537689209\n",
      "#>>>    18.88 11.73 \t\t|\t\t 7.149999999999999\n",
      "[Feb 25, 09:03:59] 555 0.7547559413035132\n",
      "\t\t\t\t 0.07984289526939392 0.20058897137641907\n",
      "#>>>    19.3 11.96 \t\t|\t\t 7.34\n",
      "[Feb 25, 09:03:59] 556 0.7542816172288556\n",
      "\t\t\t\t 0.1557832658290863 0.5220161080360413\n",
      "#>>>    18.97 12.19 \t\t|\t\t 6.779999999999999\n",
      "[Feb 25, 09:04:00] 557 0.7542051349556894\n",
      "\t\t\t\t 0.08425186574459076 0.3284459412097931\n",
      "#>>>    19.41 11.57 \t\t|\t\t 7.84\n",
      "[Feb 25, 09:04:00] 558 0.7538636276127869\n",
      "\t\t\t\t 0.07390209287405014 0.37845298647880554\n",
      "#>>>    19.23 11.28 \t\t|\t\t 7.950000000000001\n",
      "[Feb 25, 09:04:00] 559 0.7535621190719776\n",
      "\t\t\t\t 0.12076503038406372 0.3326411545276642\n",
      "#>>>    19.5 10.68 \t\t|\t\t 8.82\n",
      "[Feb 25, 09:04:01] 560 0.7532619631378173\n",
      "\t\t\t\t 0.14608313143253326 0.2814424932003021\n",
      "#>>>    18.97 11.42 \t\t|\t\t 7.549999999999999\n",
      "[Feb 25, 09:04:01] 561 0.7529362268142135\n",
      "\t\t\t\t 0.1036602333188057 0.2481241077184677\n",
      "#>>>    19.76 11.31 \t\t|\t\t 8.450000000000001\n",
      "[Feb 25, 09:04:01] 562 0.7525350749358871\n",
      "\t\t\t\t 0.08404798060655594 0.28078731894493103\n",
      "#>>>    18.82 10.97 \t\t|\t\t 7.85\n",
      "[Feb 25, 09:04:02] 563 0.7521473751530521\n",
      "\t\t\t\t 0.07709616422653198 0.6087691187858582\n",
      "#>>>    18.46 11.05 \t\t|\t\t 7.41\n",
      "[Feb 25, 09:04:02] 564 0.7520810930609114\n",
      "\t\t\t\t 0.1149209663271904 0.3836766481399536\n",
      "#>>>    18.72 11.14 \t\t|\t\t 7.579999999999998\n",
      "[Feb 25, 09:04:02] 565 0.7518276095897682\n",
      "\t\t\t\t 0.03151199221611023 0.30285951495170593\n",
      "#>>>    18.96 10.4 \t\t|\t\t 8.56\n",
      "[Feb 25, 09:04:03] 566 0.7514101534873463\n",
      "\t\t\t\t 0.10634735226631165 0.46288684010505676\n",
      "#>>>    19.55 11.32 \t\t|\t\t 8.23\n",
      "[Feb 25, 09:04:03] 567 0.7512279775262303\n",
      "\t\t\t\t 0.010578506626188755 0.06623858958482742\n",
      "#>>>    19.68 10.01 \t\t|\t\t 9.67\n",
      "[Feb 25, 09:04:03] 568 0.7505535666439838\n",
      "\t\t\t\t 0.028771206736564636 0.22303181886672974\n",
      "#>>>    19.02 9.75 \t\t|\t\t 9.27\n",
      "[Feb 25, 09:04:03] 569 0.7500548161178442\n",
      "\t\t\t\t 0.07614242285490036 0.22473743557929993\n",
      "#>>>    18.84 11.22 \t\t|\t\t 7.619999999999999\n",
      "[Feb 25, 09:04:04] 570 0.7496056411676111\n",
      "\t\t\t\t 0.051856424659490585 0.26791369915008545\n",
      "#>>>    19.68 11.69 \t\t|\t\t 7.99\n",
      "[Feb 25, 09:04:04] 571 0.7491758056539782\n",
      "\t\t\t\t 0.04121341183781624 0.20890289545059204\n",
      "#>>>    19.64 11.54 \t\t|\t\t 8.100000000000001\n",
      "[Feb 25, 09:04:04] 572 0.7486767461667885\n",
      "\t\t\t\t 0.061219364404678345 0.28520286083221436\n",
      "#>>>    19.14 11.29 \t\t|\t\t 7.850000000000001\n",
      "[Feb 25, 09:04:05] 573 0.7482744916458586\n",
      "\t\t\t\t 0.030095474794507027 0.3654911518096924\n",
      "#>>>    19.6 10.37 \t\t|\t\t 9.230000000000002\n",
      "[Feb 25, 09:04:05] 574 0.7479218037938554\n",
      "\t\t\t\t 0.08058042079210281 0.2883353531360626\n",
      "#>>>    19.11 10.97 \t\t|\t\t 8.139999999999999\n",
      "[Feb 25, 09:04:05] 575 0.7475427977565391\n",
      "\t\t\t\t 0.07264699041843414 0.11457338184118271\n",
      "#>>>    19.41 9.51 \t\t|\t\t 9.9\n",
      "[Feb 25, 09:04:06] 576 0.7469824753235915\n",
      "\t\t\t\t 0.1309240609407425 0.6456955075263977\n",
      "#>>>    18.37 11.07 \t\t|\t\t 7.300000000000001\n",
      "[Feb 25, 09:04:06] 577 0.747012112401834\n",
      "\t\t\t\t 0.07028818130493164 0.26734575629234314\n",
      "#>>>    18.98 10.76 \t\t|\t\t 8.22\n",
      "[Feb 25, 09:04:06] 578 0.7466027342270294\n",
      "\t\t\t\t 0.08084584027528763 0.3020283281803131\n",
      "#>>>    18.98 10.43 \t\t|\t\t 8.55\n",
      "[Feb 25, 09:04:06] 579 0.7462390056538074\n",
      "\t\t\t\t 0.0755261704325676 0.378572553396225\n",
      "#>>>    18.17 9.01 \t\t|\t\t 9.160000000000002\n",
      "[Feb 25, 09:04:07] 580 0.7459468653794329\n",
      "\t\t\t\t 0.03470649942755699 0.26620522141456604\n",
      "#>>>    19.11 10.35 \t\t|\t\t 8.76\n",
      "[Feb 25, 09:04:07] 581 0.7455018302386208\n",
      "\t\t\t\t 0.059515658766031265 0.17411382496356964\n",
      "#>>>    19.58 11.62 \t\t|\t\t 7.959999999999999\n",
      "[Feb 25, 09:04:07] 582 0.7449899578883865\n",
      "\t\t\t\t 0.07821109145879745 0.29225876927375793\n",
      "#>>>    18.87 10.79 \t\t|\t\t 8.080000000000002\n",
      "[Feb 25, 09:04:08] 583 0.7446154377986812\n",
      "\t\t\t\t 0.07184229791164398 0.3227817416191101\n",
      "#>>>    19.61 11.22 \t\t|\t\t 8.389999999999999\n",
      "[Feb 25, 09:04:08] 584 0.7442654464153144\n",
      "\t\t\t\t 0.07617373019456863 0.12817296385765076\n",
      "#>>>    18.95 11.15 \t\t|\t\t 7.799999999999999\n",
      "[Feb 25, 09:04:08] 585 0.7437255276555008\n",
      "\t\t\t\t 0.13190454244613647 0.3704916536808014\n",
      "#>>>    19.02 9.48 \t\t|\t\t 9.54\n",
      "[Feb 25, 09:04:09] 586 0.7434841983537747\n",
      "\t\t\t\t 0.015048402361571789 0.21124310791492462\n",
      "#>>>    19.06 10.5 \t\t|\t\t 8.559999999999999\n",
      "[Feb 25, 09:04:09] 587 0.7429670056629034\n",
      "\t\t\t\t 0.1413775235414505 0.3095262944698334\n",
      "#>>>    18.38 10.79 \t\t|\t\t 7.59\n",
      "[Feb 25, 09:04:09] 588 0.742674942490153\n",
      "\t\t\t\t 0.020042410120368004 0.5144368410110474\n",
      "#>>>    18.8 9.71 \t\t|\t\t 9.09\n",
      "[Feb 25, 09:04:09] 589 0.7424667468081074\n",
      "\t\t\t\t 0.06189081817865372 0.6878196001052856\n",
      "#>>>    18.33 10.17 \t\t|\t\t 8.159999999999998\n",
      "[Feb 25, 09:04:10] 590 0.742473990501935\n",
      "\t\t\t\t 0.2136562168598175 0.39720484614372253\n",
      "#>>>    18.39 11.41 \t\t|\t\t 6.98\n",
      "[Feb 25, 09:04:10] 591 0.7423423775744366\n",
      "\t\t\t\t 0.04514056071639061 0.21032005548477173\n",
      "#>>>    19.02 11.5 \t\t|\t\t 7.52\n",
      "[Feb 25, 09:04:10] 592 0.7418554958167886\n",
      "\t\t\t\t 0.1981351375579834 0.6083269715309143\n",
      "#>>>    18.73 10.73 \t\t|\t\t 8.0\n",
      "[Feb 25, 09:04:11] 593 0.7419201024300607\n",
      "\t\t\t\t 0.32125499844551086 0.20299392938613892\n",
      "#>>>    19.12 12.65 \t\t|\t\t 6.470000000000001\n",
      "[Feb 25, 09:04:11] 594 0.7417024312852646\n",
      "\t\t\t\t 0.030069148167967796 0.1668480634689331\n",
      "#>>>    19.32 10.15 \t\t|\t\t 9.17\n",
      "[Feb 25, 09:04:11] 595 0.7411576460600283\n",
      "\t\t\t\t 0.11625722795724869 0.5344617962837219\n",
      "#>>>    18.33 10.67 \t\t|\t\t 7.659999999999998\n",
      "[Feb 25, 09:04:12] 596 0.7410672074605611\n",
      "\t\t\t\t 0.20990659296512604 0.5083274841308594\n",
      "#>>>    18.55 11.63 \t\t|\t\t 6.92\n",
      "[Feb 25, 09:04:12] 597 0.7410443743152954\n",
      "\t\t\t\t 0.09630350768566132 0.3247867226600647\n",
      "#>>>    18.55 9.97 \t\t|\t\t 8.58\n",
      "[Feb 25, 09:04:12] 598 0.7407244201862269\n",
      "\t\t\t\t 0.08093127608299255 0.28438621759414673\n",
      "#>>>    18.58 10.12 \t\t|\t\t 8.459999999999999\n",
      "[Feb 25, 09:04:13] 599 0.7403490132597178\n",
      "\t\t\t\t 0.11537240445613861 0.3713824450969696\n",
      "#>>>    19.31 10.26 \t\t|\t\t 9.049999999999999\n",
      "[Feb 25, 09:04:13] 600 0.7400954190811101\n",
      "\t\t\t\t 0.04829682409763336 0.610681414604187\n",
      "#>>>    18.79 10.93 \t\t|\t\t 7.859999999999999\n",
      "[Feb 25, 09:04:13] 601 0.7400143018858296\n",
      "\t\t\t\t 0.06492354720830917 0.619046151638031\n",
      "#>>>    19.22 10.45 \t\t|\t\t 8.77\n",
      "[Feb 25, 09:04:13] 602 0.7399582572604384\n",
      "\t\t\t\t 0.035365257412195206 0.45821720361709595\n",
      "#>>>    18.52 10.66 \t\t|\t\t 7.859999999999999\n",
      "[Feb 25, 09:04:14] 603 0.739711881460482\n",
      "\t\t\t\t 0.13305038213729858 0.4365867078304291\n",
      "#>>>    18.33 11.67 \t\t|\t\t 6.659999999999998\n",
      "[Feb 25, 09:04:14] 604 0.7395418066391869\n",
      "\t\t\t\t 0.04808713123202324 0.18297447264194489\n",
      "#>>>    18.84 10.72 \t\t|\t\t 8.12\n",
      "[Feb 25, 09:04:14] 605 0.739033326440147\n",
      "\t\t\t\t 0.03812721371650696 0.3082473874092102\n",
      "#>>>    17.97 9.22 \t\t|\t\t 8.749999999999998\n",
      "[Feb 25, 09:04:15] 606 0.7386406677148326\n",
      "\t\t\t\t 0.05526457726955414 0.587925136089325\n",
      "#>>>    18.78 10.3 \t\t|\t\t 8.48\n",
      "[Feb 25, 09:04:15] 607 0.7385452167753779\n",
      "\t\t\t\t 0.03187650814652443 0.18405890464782715\n",
      "#>>>    19.43 9.54 \t\t|\t\t 9.89\n",
      "[Feb 25, 09:04:15] 608 0.7380226069676715\n",
      "\t\t\t\t 0.05305973067879677 0.5436520576477051\n",
      "#>>>    18.38 11.47 \t\t|\t\t 6.909999999999998\n",
      "[Feb 25, 09:04:16] 609 0.7378812961751074\n",
      "\t\t\t\t 0.0385449193418026 0.3423117697238922\n",
      "#>>>    19.06 10.68 \t\t|\t\t 8.379999999999999\n",
      "[Feb 25, 09:04:16] 610 0.7375242715717233\n",
      "\t\t\t\t 0.10490085929632187 0.18949203193187714\n",
      "#>>>    19.51 11.76 \t\t|\t\t 7.750000000000002\n",
      "[Feb 25, 09:04:16] 611 0.7370811401839292\n",
      "\t\t\t\t 0.08273945748806 0.23461399972438812\n",
      "#>>>    19.34 11.45 \t\t|\t\t 7.890000000000001\n",
      "[Feb 25, 09:04:17] 612 0.7366614125009577\n",
      "\t\t\t\t 0.055385030806064606 0.29674288630485535\n",
      "#>>>    19.65 12.78 \t\t|\t\t 6.869999999999999\n",
      "[Feb 25, 09:04:17] 613 0.7362768789981171\n",
      "\t\t\t\t 0.07307573407888412 0.31841760873794556\n",
      "#>>>    19.69 11.45 \t\t|\t\t 8.240000000000002\n",
      "[Feb 25, 09:04:17] 614 0.7359320954693864\n",
      "\t\t\t\t 0.2638424336910248 0.6922401785850525\n",
      "#>>>    18.76 11.32 \t\t|\t\t 7.440000000000001\n",
      "[Feb 25, 09:04:17] 615 0.7361522459563907\n",
      "\t\t\t\t 0.04739570990204811 0.4911844730377197\n",
      "#>>>    19.21 12.06 \t\t|\t\t 7.15\n",
      "[Feb 25, 09:04:18] 616 0.7359546738896489\n",
      "\t\t\t\t 0.16461783647537231 0.4122338593006134\n",
      "#>>>    18.55 11.43 \t\t|\t\t 7.120000000000001\n",
      "[Feb 25, 09:04:18] 617 0.7357955709413375\n",
      "\t\t\t\t 0.01213433500379324 0.21486519277095795\n",
      "#>>>    19.22 9.47 \t\t|\t\t 9.749999999999998\n",
      "[Feb 25, 09:04:18] 618 0.7352867748916516\n",
      "\t\t\t\t 0.07067350298166275 0.38721147179603577\n",
      "#>>>    18.98 10.76 \t\t|\t\t 8.22\n",
      "[Feb 25, 09:04:19] 619 0.7350093730840871\n",
      "\t\t\t\t 0.06680312752723694 0.5483514666557312\n",
      "#>>>    18.51 9.66 \t\t|\t\t 8.850000000000001\n",
      "[Feb 25, 09:04:19] 620 0.7348895183349883\n",
      "\t\t\t\t 0.0255455169826746 0.1601940095424652\n",
      "#>>>    19.14 10.61 \t\t|\t\t 8.530000000000001\n",
      "[Feb 25, 09:04:19] 621 0.7343403683487664\n",
      "\t\t\t\t 0.1043585017323494 0.41122007369995117\n",
      "#>>>    19.17 11.75 \t\t|\t\t 7.420000000000002\n",
      "[Feb 25, 09:04:20] 622 0.7341216065483993\n",
      "\t\t\t\t 0.034802649170160294 0.4563200771808624\n",
      "#>>>    19.22 10.41 \t\t|\t\t 8.809999999999999\n",
      "[Feb 25, 09:04:20] 623 0.7338786076644767\n",
      "\t\t\t\t 0.16192251443862915 0.3041555881500244\n",
      "#>>>    18.94 12.13 \t\t|\t\t 6.8100000000000005\n",
      "[Feb 25, 09:04:20] 624 0.7336108071594009\n",
      "\t\t\t\t 0.036789968609809875 0.6073987483978271\n",
      "#>>>    18.83 9.99 \t\t|\t\t 8.839999999999998\n",
      "[Feb 25, 09:04:20] 625 0.7335213850543479\n",
      "\t\t\t\t 0.12570765614509583 0.5921019911766052\n",
      "#>>>    18.46 10.61 \t\t|\t\t 7.850000000000001\n",
      "[Feb 25, 09:04:21] 626 0.7335056733464176\n",
      "\t\t\t\t 0.14511798322200775 0.45260533690452576\n",
      "#>>>    18.44 11.24 \t\t|\t\t 7.200000000000001\n",
      "[Feb 25, 09:04:21] 627 0.7333698909782965\n",
      "\t\t\t\t 0.029654551297426224 0.33235371112823486\n",
      "#>>>    19.16 10.92 \t\t|\t\t 8.24\n",
      "[Feb 25, 09:04:21] 628 0.7329985293609198\n",
      "\t\t\t\t 0.22810189425945282 0.47868281602859497\n",
      "#>>>    18.55 11.14 \t\t|\t\t 7.41\n",
      "[Feb 25, 09:04:22] 629 0.7329723155567481\n",
      "\t\t\t\t 0.07627697288990021 0.19204844534397125\n",
      "#>>>    19.29 11.32 \t\t|\t\t 7.969999999999999\n",
      "[Feb 25, 09:04:22] 630 0.7325076686594252\n",
      "\t\t\t\t 0.0278200451284647 0.24918177723884583\n",
      "#>>>    19.57 10.98 \t\t|\t\t 8.59\n",
      "[Feb 25, 09:04:22] 631 0.732052162818721\n",
      "\t\t\t\t 0.022652890533208847 0.20461948215961456\n",
      "#>>>    19.22 10.19 \t\t|\t\t 9.03\n",
      "[Feb 25, 09:04:23] 632 0.7315473830323204\n",
      "\t\t\t\t 0.01886081136763096 0.22917038202285767\n",
      "#>>>    20.01 10.88 \t\t|\t\t 9.13\n",
      "[Feb 25, 09:04:23] 633 0.7310638668482665\n",
      "\t\t\t\t 0.05007219314575195 0.14909027516841888\n",
      "#>>>    19.39 10.52 \t\t|\t\t 8.870000000000001\n",
      "[Feb 25, 09:04:23] 634 0.7305319654497324\n",
      "\t\t\t\t 0.15724848210811615 0.3017725944519043\n",
      "#>>>    19.49 11.15 \t\t|\t\t 8.339999999999998\n",
      "[Feb 25, 09:04:24] 635 0.730260454575744\n",
      "\t\t\t\t 0.07702996581792831 0.4173666536808014\n",
      "#>>>    19.1 11.39 \t\t|\t\t 7.710000000000001\n",
      "[Feb 25, 09:04:24] 636 0.7300245907481175\n",
      "\t\t\t\t 0.12307649105787277 0.19770464301109314\n",
      "#>>>    19.31 10.73 \t\t|\t\t 8.579999999999998\n",
      "[Feb 25, 09:04:24] 637 0.729615347298889\n",
      "\t\t\t\t 0.09870875626802444 0.16918440163135529\n",
      "#>>>    19.6 11.1 \t\t|\t\t 8.500000000000002\n",
      "[Feb 25, 09:04:24] 638 0.72915362511694\n",
      "\t\t\t\t 0.09905047714710236 0.5596418976783752\n",
      "#>>>    18.1 11.33 \t\t|\t\t 6.770000000000001\n",
      "[Feb 25, 09:04:25] 639 0.7290831638517474\n",
      "\t\t\t\t 0.1342323124408722 0.3962678015232086\n",
      "#>>>    18.6 12.63 \t\t|\t\t 5.970000000000001\n",
      "[Feb 25, 09:04:25] 640 0.7288845808018597\n",
      "\t\t\t\t 0.07897330820560455 0.3346337080001831\n",
      "#>>>    19.78 11.32 \t\t|\t\t 8.46\n",
      "[Feb 25, 09:04:25] 641 0.7285693032223625\n",
      "\t\t\t\t 0.09486741572618484 0.17166662216186523\n",
      "#>>>    19.21 11.89 \t\t|\t\t 7.32\n",
      "[Feb 25, 09:04:26] 642 0.7281072679495775\n",
      "\t\t\t\t 0.11836981773376465 0.632042407989502\n",
      "#>>>    18.37 10.94 \t\t|\t\t 7.4300000000000015\n",
      "[Feb 25, 09:04:26] 643 0.7281295729073511\n",
      "\t\t\t\t 0.127180278301239 0.5814616084098816\n",
      "#>>>    19.27 11.62 \t\t|\t\t 7.65\n",
      "[Feb 25, 09:04:26] 644 0.728110085221155\n",
      "\t\t\t\t 0.1256651133298874 0.42282989621162415\n",
      "#>>>    18.91 12.04 \t\t|\t\t 6.870000000000001\n",
      "[Feb 25, 09:04:27] 645 0.7279304701305742\n",
      "\t\t\t\t 0.1257605403661728 0.44006046652793884\n",
      "#>>>    18.65 11.21 \t\t|\t\t 7.439999999999998\n",
      "[Feb 25, 09:04:27] 646 0.7277683606524367\n",
      "\t\t\t\t 0.03657250478863716 0.3184383809566498\n",
      "#>>>    19.69 11.25 \t\t|\t\t 8.440000000000001\n",
      "[Feb 25, 09:04:27] 647 0.7273956031887053\n",
      "\t\t\t\t 0.07748086750507355 0.5805693864822388\n",
      "#>>>    18.47 11.04 \t\t|\t\t 7.43\n",
      "[Feb 25, 09:04:27] 648 0.7273262578246028\n",
      "\t\t\t\t 0.11998123675584793 0.4906793236732483\n",
      "#>>>    18.27 10.6 \t\t|\t\t 7.67\n",
      "[Feb 25, 09:04:28] 649 0.7272095921197568\n",
      "\t\t\t\t 0.11329444497823715 0.18632148206233978\n",
      "#>>>    18.56 11.88 \t\t|\t\t 6.679999999999998\n",
      "[Feb 25, 09:04:28] 650 0.726781998447227\n",
      "\t\t\t\t 0.03636004030704498 0.42866259813308716\n",
      "#>>>    18.85 10.36 \t\t|\t\t 8.490000000000002\n",
      "[Feb 25, 09:04:28] 651 0.7265202390723188\n",
      "\t\t\t\t 0.1012423112988472 0.37132322788238525\n",
      "#>>>    19.06 11.61 \t\t|\t\t 7.449999999999999\n",
      "[Feb 25, 09:04:29] 652 0.7262662843649772\n",
      "\t\t\t\t 0.10473093390464783 0.24997927248477936\n",
      "#>>>    19.21 12.13 \t\t|\t\t 7.08\n",
      "[Feb 25, 09:04:29] 653 0.7258947283019028\n",
      "\t\t\t\t 0.059579119086265564 0.33582746982574463\n",
      "#>>>    19.45 10.77 \t\t|\t\t 8.68\n",
      "[Feb 25, 09:04:29] 654 0.7255642401774142\n",
      "\t\t\t\t 0.05238047614693642 0.08315328508615494\n",
      "#>>>    19.24 10.79 \t\t|\t\t 8.45\n",
      "[Feb 25, 09:04:30] 655 0.7249742097021951\n",
      "\t\t\t\t 0.13077782094478607 0.3604286313056946\n",
      "#>>>    18.44 10.35 \t\t|\t\t 8.090000000000002\n",
      "[Feb 25, 09:04:30] 656 0.7247404419596445\n",
      "\t\t\t\t 0.0657346099615097 0.26946505904197693\n",
      "#>>>    19.02 10.85 \t\t|\t\t 8.17\n",
      "[Feb 25, 09:04:30] 657 0.7243509011717871\n",
      "\t\t\t\t 0.08754783123731613 0.2845836877822876\n",
      "#>>>    19.43 11.09 \t\t|\t\t 8.34\n",
      "[Feb 25, 09:04:30] 658 0.7239986817970856\n",
      "\t\t\t\t 0.039444103837013245 0.39139315485954285\n",
      "#>>>    18.54 11.77 \t\t|\t\t 6.77\n",
      "[Feb 25, 09:04:31] 659 0.7237055203888862\n",
      "\t\t\t\t 0.08375385403633118 0.22113260626792908\n",
      "#>>>    18.5 10.05 \t\t|\t\t 8.45\n",
      "[Feb 25, 09:04:31] 660 0.7232867013288016\n",
      "\t\t\t\t 0.01906302012503147 0.18966980278491974\n",
      "#>>>    18.72 10.88 \t\t|\t\t 7.839999999999998\n",
      "[Feb 25, 09:04:31] 661 0.7227721474559706\n",
      "\t\t\t\t 0.15291528403759003 0.4587882459163666\n",
      "#>>>    18.54 12.11 \t\t|\t\t 6.43\n",
      "[Feb 25, 09:04:32] 662 0.7226610788235674\n",
      "\t\t\t\t 0.018197888508439064 0.15436075627803802\n",
      "#>>>    18.7 10.09 \t\t|\t\t 8.61\n",
      "[Feb 25, 09:04:32] 663 0.7221109763951183\n",
      "\t\t\t\t 0.0799637958407402 0.1222638413310051\n",
      "#>>>    18.96 11.12 \t\t|\t\t 7.840000000000002\n",
      "[Feb 25, 09:04:32] 664 0.721591093055895\n",
      "\t\t\t\t 0.08036129176616669 0.3479001820087433\n",
      "#>>>    20.01 10.53 \t\t|\t\t 9.480000000000002\n",
      "[Feb 25, 09:04:33] 665 0.7212977634217128\n",
      "\t\t\t\t 0.2443006932735443 0.44320055842399597\n",
      "#>>>    18.22 11.0 \t\t|\t\t 7.219999999999999\n",
      "[Feb 25, 09:04:33] 666 0.7212639669099887\n",
      "\t\t\t\t 0.020962350070476532 0.30654099583625793\n",
      "#>>>    18.67 9.81 \t\t|\t\t 8.860000000000001\n",
      "[Feb 25, 09:04:33] 667 0.7208702062964361\n",
      "\t\t\t\t 0.05420200526714325 0.36441174149513245\n",
      "#>>>    18.85 11.12 \t\t|\t\t 7.730000000000002\n",
      "[Feb 25, 09:04:34] 668 0.7205679498220008\n",
      "\t\t\t\t 0.1274494081735611 0.23268334567546844\n",
      "#>>>    19.08 12.03 \t\t|\t\t 7.049999999999999\n",
      "[Feb 25, 09:04:34] 669 0.7202075146260278\n",
      "\t\t\t\t 0.055353257805109024 0.7399983406066895\n",
      "#>>>    18.55 10.99 \t\t|\t\t 7.5600000000000005\n",
      "[Feb 25, 09:04:34] 670 0.7202826587358907\n",
      "\t\t\t\t 0.01787688210606575 0.14110037684440613\n",
      "#>>>    19.41 9.51 \t\t|\t\t 9.9\n",
      "[Feb 25, 09:04:34] 671 0.71972135333238\n",
      "\t\t\t\t 0.011698497459292412 0.14181852340698242\n",
      "#>>>    19.59 11.29 \t\t|\t\t 8.3\n",
      "[Feb 25, 09:04:35] 672 0.7191551490017765\n",
      "\t\t\t\t 0.09058450162410736 0.3243851959705353\n",
      "#>>>    18.44 11.32 \t\t|\t\t 7.120000000000001\n",
      "[Feb 25, 09:04:35] 673 0.7188509635354682\n",
      "\t\t\t\t 0.10218774527311325 0.239938423037529\n",
      "#>>>    18.47 11.7 \t\t|\t\t 6.77\n",
      "[Feb 25, 09:04:35] 674 0.7184742387327927\n",
      "\t\t\t\t 0.10954361408948898 0.32002320885658264\n",
      "#>>>    18.79 10.29 \t\t|\t\t 8.5\n",
      "[Feb 25, 09:04:36] 675 0.7181853313244565\n",
      "\t\t\t\t 0.05201432481408119 0.2746431231498718\n",
      "#>>>    18.39 10.46 \t\t|\t\t 7.93\n",
      "[Feb 25, 09:04:36] 676 0.7177938034373706\n",
      "\t\t\t\t 0.05852121859788895 0.7956368923187256\n",
      "#>>>    17.86 10.86 \t\t|\t\t 7.0\n",
      "[Feb 25, 09:04:36] 677 0.7179301677373993\n",
      "\t\t\t\t 0.08964276313781738 0.23875658214092255\n",
      "#>>>    19.58 11.63 \t\t|\t\t 7.9499999999999975\n",
      "[Feb 25, 09:04:37] 678 0.7175406369298418\n",
      "\t\t\t\t 0.15190663933753967 0.5132439732551575\n",
      "#>>>    19.3 11.28 \t\t|\t\t 8.020000000000001\n",
      "[Feb 25, 09:04:37] 679 0.717488246935307\n",
      "\t\t\t\t 0.04083890840411186 0.18074266612529755\n",
      "#>>>    19.48 10.95 \t\t|\t\t 8.530000000000001\n",
      "[Feb 25, 09:04:37] 680 0.7169923402666264\n",
      "\t\t\t\t 0.08307360112667084 0.12622064352035522\n",
      "#>>>    19.63 10.88 \t\t|\t\t 8.749999999999998\n",
      "[Feb 25, 09:04:37] 681 0.7164846421710067\n",
      "\t\t\t\t 0.31221407651901245 0.4411906599998474\n",
      "#>>>    18.69 11.21 \t\t|\t\t 7.48\n",
      "[Feb 25, 09:04:38] 682 0.7165215622653546\n",
      "\t\t\t\t 0.06166711449623108 0.07455193996429443\n",
      "#>>>    19.27 10.52 \t\t|\t\t 8.75\n",
      "[Feb 25, 09:04:38] 683 0.7159412597575497\n",
      "\t\t\t\t 0.09013818204402924 0.5456717610359192\n",
      "#>>>    18.54 11.74 \t\t|\t\t 6.799999999999999\n",
      "[Feb 25, 09:04:38] 684 0.7158611284557732\n",
      "\t\t\t\t 0.04374238848686218 0.3942547142505646\n",
      "#>>>    18.83 10.12 \t\t|\t\t 8.709999999999999\n",
      "[Feb 25, 09:04:39] 685 0.7155832644300548\n",
      "\t\t\t\t 0.04378433898091316 0.10114634782075882\n",
      "#>>>    18.85 11.23 \t\t|\t\t 7.620000000000001\n",
      "[Feb 25, 09:04:39] 686 0.7150126118561517\n",
      "\t\t\t\t 0.12405586242675781 0.35827872157096863\n",
      "#>>>    18.73 11.3 \t\t|\t\t 7.43\n",
      "[Feb 25, 09:04:39] 687 0.7147799338282933\n",
      "\t\t\t\t 0.3528534173965454 0.818168044090271\n",
      "#>>>    18.52 11.26 \t\t|\t\t 7.26\n",
      "[Feb 25, 09:04:40] 688 0.7152361753559519\n",
      "\t\t\t\t 0.08784466236829758 0.40193068981170654\n",
      "#>>>    18.5 10.94 \t\t|\t\t 7.5600000000000005\n",
      "[Feb 25, 09:04:40] 689 0.7150107145402265\n",
      "\t\t\t\t 0.052382174879312515 0.208634153008461\n",
      "#>>>    18.97 12.28 \t\t|\t\t 6.6899999999999995\n",
      "[Feb 25, 09:04:40] 690 0.7145567201647498\n",
      "\t\t\t\t 0.07857997715473175 0.25804951786994934\n",
      "#>>>    19.05 10.83 \t\t|\t\t 8.22\n",
      "[Feb 25, 09:04:41] 691 0.7141787929545109\n",
      "\t\t\t\t 0.09321287274360657 0.39856117963790894\n",
      "#>>>    19.34 10.8 \t\t|\t\t 8.54\n",
      "[Feb 25, 09:04:41] 692 0.7139563882139379\n",
      "\t\t\t\t 0.05994190648198128 0.2904295027256012\n",
      "#>>>    18.7 9.35 \t\t|\t\t 9.35\n",
      "[Feb 25, 09:04:41] 693 0.7135928032461074\n",
      "\t\t\t\t 0.057466208934783936 0.22855804860591888\n",
      "#>>>    19.27 10.42 \t\t|\t\t 8.85\n",
      "[Feb 25, 09:04:41] 694 0.7131652347153031\n",
      "\t\t\t\t 0.06231861934065819 0.3455316722393036\n",
      "#>>>    19.28 11.14 \t\t|\t\t 8.14\n",
      "[Feb 25, 09:04:42] 695 0.712859919775893\n",
      "\t\t\t\t 0.14181016385555267 0.4485473930835724\n",
      "#>>>    18.63 11.17 \t\t|\t\t 7.459999999999999\n",
      "[Feb 25, 09:04:42] 696 0.7127374173981551\n",
      "\t\t\t\t 0.07979858666658401 0.2581920623779297\n",
      "#>>>    18.77 10.57 \t\t|\t\t 8.2\n",
      "[Feb 25, 09:04:42] 697 0.7123626706223508\n",
      "\t\t\t\t 0.06744542717933655 0.5442249774932861\n",
      "#>>>    18.41 9.87 \t\t|\t\t 8.540000000000001\n",
      "[Feb 25, 09:04:43] 698 0.7122619783265988\n",
      "\t\t\t\t 0.1527147740125656 0.5941392779350281\n",
      "#>>>    17.97 10.81 \t\t|\t\t 7.159999999999998\n",
      "[Feb 25, 09:04:43] 699 0.7122965704151208\n",
      "\t\t\t\t 0.10156615078449249 0.21321892738342285\n",
      "#>>>    18.58 10.83 \t\t|\t\t 7.749999999999998\n",
      "[Feb 25, 09:04:43] 700 0.7118990589079724\n",
      "\t\t\t\t 0.0697869062423706 0.24408899247646332\n",
      "#>>>    18.56 10.13 \t\t|\t\t 8.429999999999998\n",
      "[Feb 25, 09:04:44] 701 0.7115010357626844\n",
      "\t\t\t\t 0.005821838043630123 0.11874914914369583\n",
      "#>>>    19.24 9.66 \t\t|\t\t 9.579999999999998\n",
      "[Feb 25, 09:04:44] 702 0.7109141057150404\n",
      "\t\t\t\t 0.04135226830840111 0.3020097315311432\n",
      "#>>>    18.88 9.93 \t\t|\t\t 8.95\n",
      "[Feb 25, 09:04:44] 703 0.7105465536128903\n",
      "\t\t\t\t 0.071256622672081 0.42173001170158386\n",
      "#>>>    17.98 10.36 \t\t|\t\t 7.620000000000001\n",
      "[Feb 25, 09:04:44] 704 0.7103289936787499\n",
      "\t\t\t\t 0.05127536132931709 0.2045685350894928\n",
      "#>>>    19.11 10.65 \t\t|\t\t 8.459999999999999\n",
      "[Feb 25, 09:04:45] 705 0.7098745085926659\n",
      "\t\t\t\t 0.12885594367980957 0.4388572871685028\n",
      "#>>>    18.39 11.0 \t\t|\t\t 7.390000000000001\n",
      "[Feb 25, 09:04:45] 706 0.7097323473447239\n",
      "\t\t\t\t 0.07526520639657974 0.157612606883049\n",
      "#>>>    19.46 11.13 \t\t|\t\t 8.33\n",
      "[Feb 25, 09:04:45] 707 0.7092554928181094\n",
      "\t\t\t\t 0.10717319697141647 0.17524240911006927\n",
      "#>>>    19.28 11.04 \t\t|\t\t 8.240000000000002\n",
      "[Feb 25, 09:04:46] 708 0.7088286529239223\n",
      "\t\t\t\t 0.18271231651306152 0.3790627419948578\n",
      "#>>>    18.46 11.56 \t\t|\t\t 6.9\n",
      "[Feb 25, 09:04:46] 709 0.7086815993593086\n",
      "\t\t\t\t 0.10435126721858978 0.5479148030281067\n",
      "#>>>    18.58 10.46 \t\t|\t\t 8.119999999999997\n",
      "[Feb 25, 09:04:46] 710 0.7086251838450972\n",
      "\t\t\t\t 0.12265288084745407 0.3717671036720276\n",
      "#>>>    18.63 10.07 \t\t|\t\t 8.559999999999999\n",
      "[Feb 25, 09:04:47] 711 0.7084109786532222\n",
      "\t\t\t\t 0.030986065044999123 0.2967234253883362\n",
      "#>>>    18.13 9.74 \t\t|\t\t 8.389999999999999\n",
      "[Feb 25, 09:04:47] 712 0.7080302771705902\n",
      "\t\t\t\t 0.05630219355225563 0.3079598546028137\n",
      "#>>>    19.32 9.86 \t\t|\t\t 9.46\n",
      "[Feb 25, 09:04:47] 713 0.7076865089378493\n",
      "\t\t\t\t 0.11046792566776276 0.1816803514957428\n",
      "#>>>    18.79 10.86 \t\t|\t\t 7.93\n",
      "[Feb 25, 09:04:48] 714 0.7072709707209762\n",
      "\t\t\t\t 0.03579596430063248 0.4198482632637024\n",
      "#>>>    18.61 10.63 \t\t|\t\t 7.979999999999999\n",
      "[Feb 25, 09:04:48] 715 0.707019343970369\n",
      "\t\t\t\t 0.04184751212596893 0.21882793307304382\n",
      "#>>>    19.18 11.32 \t\t|\t\t 7.859999999999999\n",
      "[Feb 25, 09:04:48] 716 0.7065730000566965\n",
      "\t\t\t\t 0.04776708781719208 0.14743481576442719\n",
      "#>>>    19.21 11.12 \t\t|\t\t 8.090000000000002\n",
      "[Feb 25, 09:04:48] 717 0.7060616289602214\n",
      "\t\t\t\t 0.05974965915083885 0.32306668162345886\n",
      "#>>>    19.05 11.05 \t\t|\t\t 8.0\n",
      "[Feb 25, 09:04:49] 718 0.7057383836757607\n",
      "\t\t\t\t 0.12091337144374847 0.5278703570365906\n",
      "#>>>    18.55 10.55 \t\t|\t\t 8.0\n",
      "[Feb 25, 09:04:49] 719 0.7056814290354665\n",
      "\t\t\t\t 0.10658763349056244 0.41281580924987793\n",
      "#>>>    19.06 10.05 \t\t|\t\t 9.009999999999998\n",
      "[Feb 25, 09:04:49] 720 0.7054951510640726\n",
      "\t\t\t\t 0.06559451669454575 0.3020496964454651\n",
      "#>>>    18.63 10.9 \t\t|\t\t 7.729999999999999\n",
      "[Feb 25, 09:04:50] 721 0.705157300133599\n",
      "\t\t\t\t 0.1130182072520256 0.40739578008651733\n",
      "#>>>    18.73 11.54 \t\t|\t\t 7.190000000000001\n",
      "[Feb 25, 09:04:50] 722 0.7049725568282544\n",
      "\t\t\t\t 0.021560993045568466 0.1988988071680069\n",
      "#>>>    19.25 9.39 \t\t|\t\t 9.86\n",
      "[Feb 25, 09:04:50] 723 0.704488044075365\n",
      "\t\t\t\t 0.015155988745391369 0.26447707414627075\n",
      "#>>>    19.4 11.28 \t\t|\t\t 8.12\n",
      "[Feb 25, 09:04:51] 724 0.7040631891062885\n",
      "\t\t\t\t 0.12387416511774063 0.4271402657032013\n",
      "#>>>    19.16 10.7 \t\t|\t\t 8.46\n",
      "[Feb 25, 09:04:51] 725 0.7039101403405525\n",
      "\t\t\t\t 0.06661561131477356 0.5146397352218628\n",
      "#>>>    19.07 11.54 \t\t|\t\t 7.530000000000001\n",
      "[Feb 25, 09:04:51] 726 0.7037874855169464\n",
      "\t\t\t\t 0.023836150765419006 0.2364853173494339\n",
      "#>>>    18.69 10.06 \t\t|\t\t 8.63\n",
      "[Feb 25, 09:04:51] 727 0.7033440194995443\n",
      "\t\t\t\t 0.14393748342990875 0.3442719280719757\n",
      "#>>>    18.95 10.96 \t\t|\t\t 7.989999999999998\n",
      "[Feb 25, 09:04:52] 728 0.7031288849064478\n",
      "\t\t\t\t 0.14993926882743835 0.3292759954929352\n",
      "#>>>    18.72 11.16 \t\t|\t\t 7.559999999999999\n",
      "[Feb 25, 09:04:52] 729 0.7029049712858617\n",
      "\t\t\t\t 0.17989680171012878 0.29019227623939514\n",
      "#>>>    19.81 12.75 \t\t|\t\t 7.059999999999999\n",
      "[Feb 25, 09:04:52] 730 0.7026721553925254\n",
      "\t\t\t\t 0.03482351452112198 0.17228759825229645\n",
      "#>>>    19.24 11.3 \t\t|\t\t 7.939999999999998\n",
      "[Feb 25, 09:04:53] 731 0.7021765943573568\n",
      "\t\t\t\t 0.07176980376243591 0.2789844870567322\n",
      "#>>>    19.44 11.15 \t\t|\t\t 8.290000000000001\n",
      "[Feb 25, 09:04:53] 732 0.7018251720538187\n",
      "\t\t\t\t 0.2991275489330292 0.45292460918426514\n",
      "#>>>    18.84 11.77 \t\t|\t\t 7.07\n",
      "[Feb 25, 09:04:53] 733 0.7018753990696844\n",
      "\t\t\t\t 0.043235473334789276 0.5098779201507568\n",
      "#>>>    19.4 11.66 \t\t|\t\t 7.739999999999998\n",
      "[Feb 25, 09:04:54] 734 0.7017266370715508\n",
      "\t\t\t\t 0.055104225873947144 0.26213327050209045\n",
      "#>>>    19.56 10.28 \t\t|\t\t 9.28\n",
      "[Feb 25, 09:04:54] 735 0.7013421479308553\n",
      "\t\t\t\t 0.057539720088243484 0.4040679633617401\n",
      "#>>>    20.23 9.19 \t\t|\t\t 11.040000000000001\n",
      "[Feb 25, 09:04:54] 736 0.7011024134775503\n",
      "\t\t\t\t 0.01577192358672619 0.1679699569940567\n",
      "#>>>    19.81 11.19 \t\t|\t\t 8.62\n",
      "[Feb 25, 09:04:54] 737 0.7005850529465162\n",
      "\t\t\t\t 0.014826877042651176 0.13885511457920074\n",
      "#>>>    19.82 10.61 \t\t|\t\t 9.21\n",
      "[Feb 25, 09:04:55] 738 0.7000381498870542\n",
      "\t\t\t\t 0.06988910585641861 0.1838422268629074\n",
      "#>>>    19.51 10.21 \t\t|\t\t 9.3\n",
      "[Feb 25, 09:04:55] 739 0.699591843077337\n",
      "\t\t\t\t 0.08612953126430511 0.5390617847442627\n",
      "#>>>    19.35 11.94 \t\t|\t\t 7.410000000000002\n",
      "[Feb 25, 09:04:55] 740 0.6995174425651695\n",
      "\t\t\t\t 0.06983286887407303 0.42559313774108887\n",
      "#>>>    18.6 12.25 \t\t|\t\t 6.350000000000001\n",
      "[Feb 25, 09:04:56] 741 0.699313351121769\n",
      "\t\t\t\t 0.0611192025244236 0.35903292894363403\n",
      "#>>>    20.13 11.22 \t\t|\t\t 8.909999999999998\n",
      "[Feb 25, 09:04:56] 742 0.69903418989839\n",
      "\t\t\t\t 0.03503522649407387 0.17122992873191833\n",
      "#>>>    19.42 11.6 \t\t|\t\t 7.820000000000002\n",
      "[Feb 25, 09:04:56] 743 0.6985414208599923\n",
      "\t\t\t\t 0.0837792307138443 0.5910410284996033\n",
      "#>>>    18.76 10.69 \t\t|\t\t 8.070000000000002\n",
      "[Feb 25, 09:04:57] 744 0.6985176996834446\n",
      "\t\t\t\t 0.06660091876983643 0.5275120139122009\n",
      "#>>>    18.4 10.87 \t\t|\t\t 7.529999999999999\n",
      "[Feb 25, 09:04:57] 745 0.6984132949164432\n",
      "\t\t\t\t 0.036292508244514465 0.18376877903938293\n",
      "#>>>    19.26 10.6 \t\t|\t\t 8.660000000000002\n",
      "[Feb 25, 09:04:57] 746 0.6979349429088106\n",
      "\t\t\t\t 0.03929705545306206 0.24586758017539978\n",
      "#>>>    19.94 11.1 \t\t|\t\t 8.840000000000002\n",
      "[Feb 25, 09:04:58] 747 0.6975221725903544\n",
      "\t\t\t\t 0.08231665939092636 0.22225432097911835\n",
      "#>>>    19.92 11.21 \t\t|\t\t 8.71\n",
      "[Feb 25, 09:04:58] 748 0.6971292213906836\n",
      "\t\t\t\t 0.11041194945573807 0.3648128807544708\n",
      "#>>>    19.7 11.5 \t\t|\t\t 8.2\n",
      "[Feb 25, 09:04:58] 749 0.6969073169920526\n",
      "\t\t\t\t 0.05701780691742897 0.246864914894104\n",
      "#>>>    18.9 10.06 \t\t|\t\t 8.839999999999998\n",
      "[Feb 25, 09:04:58] 750 0.6965142923931468\n",
      "\t\t\t\t 0.10015063732862473 0.35716643929481506\n",
      "#>>>    19.02 10.99 \t\t|\t\t 8.03\n",
      "[Feb 25, 09:04:59] 751 0.6962750951848277\n",
      "\t\t\t\t 0.05788755789399147 0.479232519865036\n",
      "#>>>    18.46 10.88 \t\t|\t\t 7.58\n",
      "[Feb 25, 09:04:59] 752 0.696115940193479\n",
      "\t\t\t\t 0.07017587870359421 0.3457030951976776\n",
      "#>>>    19.26 11.73 \t\t|\t\t 7.530000000000001\n",
      "[Feb 25, 09:04:59] 753 0.6958357032346374\n",
      "\t\t\t\t 0.06738273054361343 0.17658628523349762\n",
      "#>>>    19.56 10.89 \t\t|\t\t 8.669999999999998\n",
      "[Feb 25, 09:05:00] 754 0.6953838365546304\n",
      "\t\t\t\t 0.07406982779502869 0.3933151662349701\n",
      "#>>>    19.49 11.72 \t\t|\t\t 7.769999999999998\n",
      "[Feb 25, 09:05:00] 755 0.6951558377121058\n",
      "\t\t\t\t 0.0695231705904007 0.46820682287216187\n",
      "#>>>    19.86 12.01 \t\t|\t\t 7.85\n",
      "[Feb 25, 09:05:00] 756 0.6949984118529551\n",
      "\t\t\t\t 0.11041636019945145 0.3811711370944977\n",
      "#>>>    18.09 10.64 \t\t|\t\t 7.449999999999999\n",
      "[Feb 25, 09:05:01] 757 0.6947950009309455\n",
      "\t\t\t\t 0.04704534262418747 0.09535985440015793\n",
      "#>>>    20.32 11.34 \t\t|\t\t 8.98\n",
      "[Feb 25, 09:05:01] 758 0.6942426111270389\n",
      "\t\t\t\t 0.07762540131807327 0.3712300658226013\n",
      "#>>>    19.11 11.42 \t\t|\t\t 7.6899999999999995\n",
      "[Feb 25, 09:05:01] 759 0.6939972239756019\n",
      "\t\t\t\t 0.11862222105264664 0.25804418325424194\n",
      "#>>>    19.64 11.89 \t\t|\t\t 7.75\n",
      "[Feb 25, 09:05:01] 760 0.6936798931484827\n",
      "\t\t\t\t 0.01994856633245945 0.17923840880393982\n",
      "#>>>    19.37 10.98 \t\t|\t\t 8.39\n",
      "[Feb 25, 09:05:02] 761 0.6931854002360585\n",
      "\t\t\t\t 0.1646382212638855 0.46761879324913025\n",
      "#>>>    18.8 11.36 \t\t|\t\t 7.440000000000001\n",
      "[Feb 25, 09:05:02] 762 0.693124471820533\n",
      "\t\t\t\t 0.031129540875554085 0.3022422790527344\n",
      "#>>>    18.5 10.97 \t\t|\t\t 7.529999999999999\n",
      "[Feb 25, 09:05:02] 763 0.6927647191667782\n",
      "\t\t\t\t 0.07798760384321213 0.11601343750953674\n",
      "#>>>    19.82 11.9 \t\t|\t\t 7.92\n",
      "[Feb 25, 09:05:03] 764 0.6922659554964147\n",
      "\t\t\t\t 0.07784851640462875 0.38071534037590027\n",
      "#>>>    18.79 11.68 \t\t|\t\t 7.109999999999999\n",
      "[Feb 25, 09:05:03] 765 0.6920322534051494\n",
      "\t\t\t\t 0.0624508336186409 0.24425433576107025\n",
      "#>>>    19.32 10.06 \t\t|\t\t 9.26\n",
      "[Feb 25, 09:05:03] 766 0.6916469263285745\n",
      "\t\t\t\t 0.07497155666351318 0.1716429889202118\n",
      "#>>>    19.84 10.74 \t\t|\t\t 9.1\n",
      "[Feb 25, 09:05:04] 767 0.6912018939478296\n",
      "\t\t\t\t 0.10943247377872467 0.38753828406333923\n",
      "#>>>    19.99 11.24 \t\t|\t\t 8.749999999999998\n",
      "[Feb 25, 09:05:04] 768 0.6910076628266251\n",
      "\t\t\t\t 0.034921471029520035 0.37487587332725525\n",
      "#>>>    18.96 10.0 \t\t|\t\t 8.96\n",
      "[Feb 25, 09:05:04] 769 0.69072645250443\n",
      "\t\t\t\t 0.13174918293952942 0.6146624684333801\n",
      "#>>>    18.43 11.07 \t\t|\t\t 7.359999999999999\n",
      "[Feb 25, 09:05:05] 770 0.6907821377331008\n",
      "\t\t\t\t 0.08208557963371277 0.3166157603263855\n",
      "#>>>    19.61 11.38 \t\t|\t\t 8.229999999999999\n",
      "[Feb 25, 09:05:05] 771 0.6904900569353278\n",
      "\t\t\t\t 0.07738365978002548 0.16453126072883606\n",
      "#>>>    18.9 11.13 \t\t|\t\t 7.769999999999998\n",
      "[Feb 25, 09:05:05] 772 0.6900414818063519\n",
      "\t\t\t\t 0.13142818212509155 0.41211703419685364\n",
      "#>>>    18.74 11.0 \t\t|\t\t 7.739999999999998\n",
      "[Feb 25, 09:05:05] 773 0.6898949855706699\n",
      "\t\t\t\t 0.044549066573381424 0.3907650411128998\n",
      "#>>>    19.08 11.21 \t\t|\t\t 7.869999999999997\n",
      "[Feb 25, 09:05:06] 774 0.6896404047039613\n",
      "\t\t\t\t 0.032562293112277985 0.2635965049266815\n",
      "#>>>    19.41 10.01 \t\t|\t\t 9.4\n",
      "[Feb 25, 09:05:06] 775 0.6892469230898457\n",
      "\t\t\t\t 0.08105383813381195 0.581695020198822\n",
      "#>>>    18.97 11.55 \t\t|\t\t 7.419999999999998\n",
      "[Feb 25, 09:05:06] 776 0.6892204250399897\n",
      "\t\t\t\t 0.1556939333677292 0.3491673767566681\n",
      "#>>>    18.32 10.72 \t\t|\t\t 7.6\n",
      "[Feb 25, 09:05:07] 777 0.689036065910173\n",
      "\t\t\t\t 0.029055986553430557 0.21679334342479706\n",
      "#>>>    20.23 10.97 \t\t|\t\t 9.26\n",
      "[Feb 25, 09:05:07] 778 0.6885928791705157\n",
      "\t\t\t\t 0.0956161692738533 0.24201637506484985\n",
      "#>>>    19.72 11.54 \t\t|\t\t 8.18\n",
      "[Feb 25, 09:05:07] 779 0.6882419188282333\n",
      "\t\t\t\t 0.054876863956451416 0.3263359069824219\n",
      "#>>>    19.05 11.7 \t\t|\t\t 7.350000000000001\n",
      "[Feb 25, 09:05:08] 780 0.6879348896803439\n",
      "\t\t\t\t 0.06555481255054474 0.2322995662689209\n",
      "#>>>    19.56 11.24 \t\t|\t\t 8.319999999999999\n",
      "[Feb 25, 09:05:08] 781 0.6875448091545818\n",
      "\t\t\t\t 0.03950710967183113 0.27603650093078613\n",
      "#>>>    18.6 10.06 \t\t|\t\t 8.540000000000001\n",
      "[Feb 25, 09:05:08] 782 0.6871728079672057\n",
      "\t\t\t\t 0.01758694462478161 0.2622676491737366\n",
      "#>>>    19.37 10.17 \t\t|\t\t 9.200000000000001\n",
      "[Feb 25, 09:05:08] 783 0.6867654897548997\n",
      "\t\t\t\t 0.032361578196287155 0.14644281566143036\n",
      "#>>>    19.25 10.27 \t\t|\t\t 8.98\n",
      "[Feb 25, 09:05:09] 784 0.6862575286627278\n",
      "\t\t\t\t 0.0474686399102211 0.17966602742671967\n",
      "#>>>    19.05 10.57 \t\t|\t\t 8.48\n",
      "[Feb 25, 09:05:09] 785 0.6857984058088527\n",
      "\t\t\t\t 0.09471006691455841 0.3168153464794159\n",
      "#>>>    18.99 10.97 \t\t|\t\t 8.019999999999998\n",
      "[Feb 25, 09:05:09] 786 0.685524132831339\n",
      "\t\t\t\t 0.11143515259027481 0.5607609748840332\n",
      "#>>>    18.77 11.01 \t\t|\t\t 7.76\n",
      "[Feb 25, 09:05:10] 787 0.6855108048483337\n",
      "\t\t\t\t 0.04484282061457634 0.37752795219421387\n",
      "#>>>    19.51 11.65 \t\t|\t\t 7.860000000000001\n",
      "[Feb 25, 09:05:10] 788 0.6852476648051182\n",
      "\t\t\t\t 0.08054954558610916 0.3376321792602539\n",
      "#>>>    19.05 11.63 \t\t|\t\t 7.42\n",
      "[Feb 25, 09:05:10] 789 0.6849805988577089\n",
      "\t\t\t\t 0.057310592383146286 0.16839534044265747\n",
      "#>>>    19.51 11.59 \t\t|\t\t 7.920000000000002\n",
      "[Feb 25, 09:05:11] 790 0.6845213241954023\n",
      "\t\t\t\t 0.16102363169193268 0.4632764160633087\n",
      "#>>>    18.87 12.7 \t\t|\t\t 6.170000000000002\n",
      "[Feb 25, 09:05:11] 791 0.6844611029338634\n",
      "\t\t\t\t 0.01577177084982395 0.39522168040275574\n",
      "#>>>    19.02 10.2 \t\t|\t\t 8.82\n",
      "[Feb 25, 09:05:11] 792 0.68418763528777\n",
      "\t\t\t\t 0.07749155163764954 0.4359970986843109\n",
      "#>>>    18.4 10.91 \t\t|\t\t 7.489999999999998\n",
      "[Feb 25, 09:05:12] 793 0.6840169363028042\n",
      "\t\t\t\t 0.09791824221611023 0.3496534526348114\n",
      "#>>>    19.56 10.8 \t\t|\t\t 8.759999999999998\n",
      "[Feb 25, 09:05:12] 794 0.6837804910613523\n",
      "\t\t\t\t 0.027977824211120605 0.15259051322937012\n",
      "#>>>    18.45 10.29 \t\t|\t\t 8.16\n",
      "[Feb 25, 09:05:12] 795 0.6832772789077315\n",
      "\t\t\t\t 0.035820428282022476 0.15818063914775848\n",
      "#>>>    19.4 10.28 \t\t|\t\t 9.12\n",
      "[Feb 25, 09:05:12] 796 0.6827880026925283\n",
      "\t\t\t\t 0.19435393810272217 0.4817255139350891\n",
      "#>>>    17.89 11.84 \t\t|\t\t 6.050000000000001\n",
      "[Feb 25, 09:05:13] 797 0.6827812941418736\n",
      "\t\t\t\t 0.023013178259134293 0.22738736867904663\n",
      "#>>>    19.35 10.87 \t\t|\t\t 8.480000000000002\n",
      "[Feb 25, 09:05:13] 798 0.6823489133909446\n",
      "\t\t\t\t 0.17531590163707733 0.14900366961956024\n",
      "#>>>    19.68 11.1 \t\t|\t\t 8.58\n",
      "[Feb 25, 09:05:13] 799 0.6819908840488104\n",
      "\t\t\t\t 0.06405990570783615 0.44636255502700806\n",
      "#>>>    18.96 10.0 \t\t|\t\t 8.96\n",
      "[Feb 25, 09:05:14] 800 0.681819315632947\n",
      "\t\t\t\t 0.08331534266471863 0.4618528485298157\n",
      "#>>>    19.43 10.43 \t\t|\t\t 9.0\n",
      "[Feb 25, 09:05:14] 801 0.6816826644787062\n",
      "\t\t\t\t 0.015340518206357956 0.1234847754240036\n",
      "#>>>    19.05 10.73 \t\t|\t\t 8.32\n",
      "[Feb 25, 09:05:14] 802 0.6811398071115832\n",
      "\t\t\t\t 0.05341145023703575 0.3372599482536316\n",
      "#>>>    18.83 11.56 \t\t|\t\t 7.269999999999998\n",
      "[Feb 25, 09:05:15] 803 0.6808493387066875\n",
      "\t\t\t\t 0.006797386333346367 0.25741422176361084\n",
      "#>>>    19.34 9.12 \t\t|\t\t 10.22\n",
      "[Feb 25, 09:05:15] 804 0.6804327009630392\n",
      "\t\t\t\t 0.051160212606191635 0.1341145783662796\n",
      "#>>>    19.33 11.19 \t\t|\t\t 8.139999999999999\n",
      "[Feb 25, 09:05:15] 805 0.6799375430567739\n",
      "\t\t\t\t 0.2550364136695862 0.45314598083496094\n",
      "#>>>    18.33 11.84 \t\t|\t\t 6.489999999999998\n",
      "[Feb 25, 09:05:16] 806 0.6799657879082217\n",
      "\t\t\t\t 0.013677042908966541 0.18477895855903625\n",
      "#>>>    19.16 10.36 \t\t|\t\t 8.8\n",
      "[Feb 25, 09:05:16] 807 0.6794842781245755\n",
      "\t\t\t\t 0.07190926373004913 0.4007367491722107\n",
      "#>>>    18.61 10.18 \t\t|\t\t 8.43\n",
      "[Feb 25, 09:05:16] 808 0.679277439844452\n",
      "\t\t\t\t 0.1103060320019722 0.37843894958496094\n",
      "#>>>    18.91 10.71 \t\t|\t\t 8.2\n",
      "[Feb 25, 09:05:16] 809 0.6790869073787439\n",
      "\t\t\t\t 0.18157385289669037 0.5025590658187866\n",
      "#>>>    18.86 12.02 \t\t|\t\t 6.84\n",
      "[Feb 25, 09:05:17] 810 0.6790919534049819\n",
      "\t\t\t\t 0.00943685881793499 0.14254456758499146\n",
      "#>>>    19.08 9.25 \t\t|\t\t 9.829999999999998\n",
      "[Feb 25, 09:05:17] 811 0.6785648428798424\n",
      "\t\t\t\t 0.07447599619626999 0.15225720405578613\n",
      "#>>>    19.32 9.97 \t\t|\t\t 9.35\n",
      "[Feb 25, 09:05:17] 812 0.6781130112446652\n",
      "\t\t\t\t 0.10864069312810898 0.1979176551103592\n",
      "#>>>    19.65 10.58 \t\t|\t\t 9.069999999999999\n",
      "[Feb 25, 09:05:18] 813 0.6777414565742084\n",
      "\t\t\t\t 0.027741877362132072 0.1192532405257225\n",
      "#>>>    18.55 8.94 \t\t|\t\t 9.610000000000001\n",
      "[Feb 25, 09:05:18] 814 0.6772107102299341\n",
      "\t\t\t\t 0.0517323799431324 0.1548265814781189\n",
      "#>>>    20.07 12.0 \t\t|\t\t 8.07\n",
      "[Feb 25, 09:05:18] 815 0.6767400584774\n",
      "\t\t\t\t 0.05718080699443817 0.12341450899839401\n",
      "#>>>    19.64 11.67 \t\t|\t\t 7.970000000000001\n",
      "[Feb 25, 09:05:19] 816 0.6762439137274648\n",
      "\t\t\t\t 0.07946906238794327 0.2548089325428009\n",
      "#>>>    19.92 11.88 \t\t|\t\t 8.040000000000001\n",
      "[Feb 25, 09:05:19] 817 0.6759019478012176\n",
      "\t\t\t\t 0.06742420047521591 0.4091721177101135\n",
      "#>>>    19.16 11.27 \t\t|\t\t 7.890000000000001\n",
      "[Feb 25, 09:05:19] 818 0.6757026421790523\n",
      "\t\t\t\t 0.056467216461896896 0.3053620457649231\n",
      "#>>>    18.2 10.11 \t\t|\t\t 8.09\n",
      "[Feb 25, 09:05:19] 819 0.6753887687879242\n",
      "\t\t\t\t 0.31678974628448486 0.4491511285305023\n",
      "#>>>    18.35 12.02 \t\t|\t\t 6.330000000000002\n",
      "[Feb 25, 09:05:20] 820 0.6754793209237536\n",
      "\t\t\t\t 0.01880260929465294 0.15873533487319946\n",
      "#>>>    19.61 10.27 \t\t|\t\t 9.34\n",
      "[Feb 25, 09:05:20] 821 0.674981379550723\n",
      "\t\t\t\t 0.05376773327589035 0.2601639926433563\n",
      "#>>>    19.6 10.16 \t\t|\t\t 9.440000000000001\n",
      "[Feb 25, 09:05:20] 822 0.6746203299045421\n",
      "\t\t\t\t 0.28435274958610535 0.2179366946220398\n",
      "#>>>    19.08 10.48 \t\t|\t\t 8.599999999999998\n",
      "[Feb 25, 09:05:21] 823 0.6744479989890434\n",
      "\t\t\t\t 0.031909629702568054 0.4635569453239441\n",
      "#>>>    17.84 9.8 \t\t|\t\t 8.04\n",
      "[Feb 25, 09:05:21] 824 0.674269017579982\n",
      "\t\t\t\t 0.04730132967233658 0.3394688367843628\n",
      "#>>>    19.42 9.03 \t\t|\t\t 10.390000000000002\n",
      "[Feb 25, 09:05:21] 825 0.6739815187214082\n",
      "\t\t\t\t 0.10150749236345291 0.2528238296508789\n",
      "#>>>    19.02 11.36 \t\t|\t\t 7.66\n",
      "[Feb 25, 09:05:22] 826 0.6736618685172505\n",
      "\t\t\t\t 0.0648394525051117 0.21680867671966553\n",
      "#>>>    19.92 11.95 \t\t|\t\t 7.970000000000002\n",
      "[Feb 25, 09:05:22] 827 0.6732698547779581\n",
      "\t\t\t\t 0.10861560702323914 0.2895820140838623\n",
      "#>>>    19.34 10.55 \t\t|\t\t 8.79\n",
      "[Feb 25, 09:05:22] 828 0.6729947825442872\n",
      "\t\t\t\t 0.10421503335237503 0.2767731547355652\n",
      "#>>>    18.45 10.31 \t\t|\t\t 8.139999999999999\n",
      "[Feb 25, 09:05:23] 829 0.6727027759423804\n",
      "\t\t\t\t 0.10024698823690414 0.3817429542541504\n",
      "#>>>    18.7 10.37 \t\t|\t\t 8.33\n",
      "[Feb 25, 09:05:23] 830 0.6725120631163797\n",
      "\t\t\t\t 0.1466749608516693 0.49663427472114563\n",
      "#>>>    18.6 9.59 \t\t|\t\t 9.010000000000002\n",
      "[Feb 25, 09:05:23] 831 0.6724828602888361\n",
      "\t\t\t\t 0.07928145676851273 0.359505295753479\n",
      "#>>>    18.63 10.46 \t\t|\t\t 8.169999999999998\n",
      "[Feb 25, 09:05:23] 832 0.6722491641736186\n",
      "\t\t\t\t 0.13885362446308136 0.3504904806613922\n",
      "#>>>    18.37 10.85 \t\t|\t\t 7.520000000000001\n",
      "[Feb 25, 09:05:24] 833 0.6720662591294706\n",
      "\t\t\t\t 0.10963329672813416 0.25269633531570435\n",
      "#>>>    18.82 11.98 \t\t|\t\t 6.84\n",
      "[Feb 25, 09:05:24] 834 0.6717565225023849\n",
      "\t\t\t\t 0.1688985526561737 0.48047128319740295\n",
      "#>>>    18.75 10.98 \t\t|\t\t 7.77\n",
      "[Feb 25, 09:05:24] 835 0.6717341358157362\n",
      "\t\t\t\t 0.04371343553066254 0.4572220742702484\n",
      "#>>>    18.7 10.55 \t\t|\t\t 8.149999999999999\n",
      "[Feb 25, 09:05:25] 836 0.6715633371748201\n",
      "\t\t\t\t 0.02462751977145672 0.272359699010849\n",
      "#>>>    19.28 8.72 \t\t|\t\t 10.56\n",
      "[Feb 25, 09:05:25] 837 0.6711887610433891\n",
      "\t\t\t\t 0.10634059458971024 0.13236792385578156\n",
      "#>>>    19.12 10.44 \t\t|\t\t 8.680000000000001\n",
      "[Feb 25, 09:05:25] 838 0.6707562808082417\n",
      "\t\t\t\t 0.04549473151564598 0.5690208077430725\n",
      "#>>>    18.55 10.43 \t\t|\t\t 8.120000000000001\n",
      "[Feb 25, 09:05:26] 839 0.6707000400704174\n",
      "\t\t\t\t 0.0469106063246727 0.11752403527498245\n",
      "#>>>    19.71 11.08 \t\t|\t\t 8.63\n",
      "[Feb 25, 09:05:26] 840 0.6701937746719466\n",
      "\t\t\t\t 0.03499528393149376 0.32816529273986816\n",
      "#>>>    19.28 10.4 \t\t|\t\t 8.88\n",
      "[Feb 25, 09:05:26] 841 0.6698867414776714\n",
      "\t\t\t\t 0.04209122434258461 0.15425348281860352\n",
      "#>>>    19.61 10.45 \t\t|\t\t 9.16\n",
      "[Feb 25, 09:05:26] 842 0.6694131994396295\n",
      "\t\t\t\t 0.03552382066845894 0.39626121520996094\n",
      "#>>>    18.87 11.14 \t\t|\t\t 7.73\n",
      "[Feb 25, 09:05:27] 843 0.6691755712872443\n",
      "\t\t\t\t 0.03978291526436806 0.3118889033794403\n",
      "#>>>    19.08 10.19 \t\t|\t\t 8.889999999999999\n",
      "[Feb 25, 09:05:27] 844 0.6688580675308755\n",
      "\t\t\t\t 0.1430148035287857 0.4132874310016632\n",
      "#>>>    19.98 12.85 \t\t|\t\t 7.130000000000001\n",
      "[Feb 25, 09:05:27] 845 0.6687455117127762\n",
      "\t\t\t\t 0.07173807919025421 0.14009130001068115\n",
      "#>>>    20.23 11.65 \t\t|\t\t 8.58\n",
      "[Feb 25, 09:05:28] 846 0.6682885955802643\n",
      "\t\t\t\t 0.12797561287879944 0.20374765992164612\n",
      "#>>>    18.81 10.2 \t\t|\t\t 8.61\n",
      "[Feb 25, 09:05:28] 847 0.6679520302574845\n",
      "\t\t\t\t 0.05427205562591553 0.15383855998516083\n",
      "#>>>    19.62 11.33 \t\t|\t\t 8.290000000000001\n",
      "[Feb 25, 09:05:28] 848 0.6674921888428381\n",
      "\t\t\t\t 0.0820421501994133 0.3364548087120056\n",
      "#>>>    18.77 11.07 \t\t|\t\t 7.699999999999999\n",
      "[Feb 25, 09:05:29] 849 0.6672431936203572\n",
      "\t\t\t\t 0.025658901780843735 0.3736279010772705\n",
      "#>>>    19.07 12.35 \t\t|\t\t 6.720000000000001\n",
      "[Feb 25, 09:05:29] 850 0.6669752372333203\n",
      "\t\t\t\t 0.12685610353946686 0.3044750988483429\n",
      "#>>>    19.76 12.02 \t\t|\t\t 7.740000000000002\n",
      "[Feb 25, 09:05:29] 851 0.666739593213376\n",
      "\t\t\t\t 0.0712411031126976 0.23846101760864258\n",
      "#>>>    19.85 10.91 \t\t|\t\t 8.940000000000001\n",
      "[Feb 25, 09:05:30] 852 0.6663825557483345\n",
      "\t\t\t\t 0.06544744968414307 0.25208157300949097\n",
      "#>>>    19.63 11.5 \t\t|\t\t 8.129999999999999\n",
      "[Feb 25, 09:05:30] 853 0.6660337022152798\n",
      "\t\t\t\t 0.023965122178196907 0.028874773532152176\n",
      "#>>>    20.38 10.94 \t\t|\t\t 9.44\n",
      "[Feb 25, 09:05:30] 854 0.6654205084106375\n",
      "\t\t\t\t 0.15908938646316528 0.48950979113578796\n",
      "#>>>    19.23 11.25 \t\t|\t\t 7.98\n",
      "[Feb 25, 09:05:30] 855 0.6654036870500234\n",
      "\t\t\t\t 0.07150156050920486 0.2786279022693634\n",
      "#>>>    20.07 10.69 \t\t|\t\t 9.38\n",
      "[Feb 25, 09:05:31] 856 0.6650884128183013\n",
      "\t\t\t\t 0.023799879476428032 0.5279983878135681\n",
      "#>>>    18.78 10.05 \t\t|\t\t 8.73\n",
      "[Feb 25, 09:05:31] 857 0.6649751226895368\n",
      "\t\t\t\t 0.054551053792238235 0.24710705876350403\n",
      "#>>>    19.55 11.46 \t\t|\t\t 8.09\n",
      "[Feb 25, 09:05:31] 858 0.6646118056905789\n",
      "\t\t\t\t 0.08173616975545883 0.1681366264820099\n",
      "#>>>    19.12 10.7 \t\t|\t\t 8.420000000000002\n",
      "[Feb 25, 09:05:32] 859 0.6641970666885764\n",
      "\t\t\t\t 0.0596839003264904 0.056230850517749786\n",
      "#>>>    19.85 10.06 \t\t|\t\t 9.790000000000001\n",
      "[Feb 25, 09:05:32] 860 0.6636487843690067\n",
      "\t\t\t\t 0.0699150413274765 0.2581649422645569\n",
      "#>>>    19.97 12.35 \t\t|\t\t 7.619999999999999\n",
      "[Feb 25, 09:05:32] 861 0.6633132155831308\n",
      "\t\t\t\t 0.04613756015896797 0.34860676527023315\n",
      "#>>>    19.37 11.64 \t\t|\t\t 7.73\n",
      "[Feb 25, 09:05:33] 862 0.6630446467041528\n",
      "\t\t\t\t 0.0659405067563057 0.3136923611164093\n",
      "#>>>    19.34 12.16 \t\t|\t\t 7.18\n",
      "[Feb 25, 09:05:33] 863 0.6627612349178706\n",
      "\t\t\t\t 0.11065477877855301 0.2589699923992157\n",
      "#>>>    20.34 11.42 \t\t|\t\t 8.92\n",
      "[Feb 25, 09:05:33] 864 0.66246809844668\n",
      "\t\t\t\t 0.15244463086128235 0.26339206099510193\n",
      "#>>>    19.8 11.56 \t\t|\t\t 8.24\n",
      "[Feb 25, 09:05:33] 865 0.6622214670400897\n",
      "\t\t\t\t 0.0666956901550293 0.46272704005241394\n",
      "#>>>    19.66 10.67 \t\t|\t\t 8.99\n",
      "[Feb 25, 09:05:34] 866 0.6620886683330593\n",
      "\t\t\t\t 0.02696475200355053 0.32569923996925354\n",
      "#>>>    20.85 10.87 \t\t|\t\t 9.980000000000002\n",
      "[Feb 25, 09:05:34] 867 0.6617792436585618\n",
      "\t\t\t\t 0.053593747317790985 0.06599049270153046\n",
      "#>>>    20.44 10.85 \t\t|\t\t 9.590000000000002\n",
      "[Feb 25, 09:05:34] 868 0.6612370486549226\n",
      "\t\t\t\t 0.05808418616652489 0.18229374289512634\n",
      "#>>>    20.57 10.87 \t\t|\t\t 9.700000000000001\n",
      "[Feb 25, 09:05:35] 869 0.6608161895390546\n",
      "\t\t\t\t 0.0495920367538929 0.3627326190471649\n",
      "#>>>    19.63 12.13 \t\t|\t\t 7.499999999999998\n",
      "[Feb 25, 09:05:35] 870 0.6605676980164925\n",
      "\t\t\t\t 0.07363373786211014 0.4074065387248993\n",
      "#>>>    19.46 11.24 \t\t|\t\t 8.22\n",
      "[Feb 25, 09:05:35] 871 0.6603881705876125\n",
      "\t\t\t\t 0.08878915011882782 0.3446269929409027\n",
      "#>>>    19.88 11.29 \t\t|\t\t 8.59\n",
      "[Feb 25, 09:05:36] 872 0.6601611985451834\n",
      "\t\t\t\t 0.03045971505343914 0.1299196034669876\n",
      "#>>>    18.95 10.62 \t\t|\t\t 8.33\n",
      "[Feb 25, 09:05:36] 873 0.6596614166670213\n",
      "\t\t\t\t 0.0659361183643341 0.5422826409339905\n",
      "#>>>    19.55 10.36 \t\t|\t\t 9.190000000000001\n",
      "[Feb 25, 09:05:36] 874 0.6596099740394549\n",
      "\t\t\t\t 0.17502865195274353 0.621020495891571\n",
      "#>>>    18.55 11.12 \t\t|\t\t 7.4300000000000015\n",
      "[Feb 25, 09:05:37] 875 0.6597464131834574\n",
      "\t\t\t\t 0.0774540826678276 0.678441047668457\n",
      "#>>>    18.84 11.51 \t\t|\t\t 7.33\n",
      "[Feb 25, 09:05:37] 876 0.6598425619080608\n",
      "\t\t\t\t 0.06521356850862503 0.21782651543617249\n",
      "#>>>    20.03 11.5 \t\t|\t\t 8.530000000000001\n",
      "[Feb 25, 09:05:37] 877 0.659465759422647\n",
      "\t\t\t\t 0.10170407593250275 0.25168007612228394\n",
      "#>>>    19.28 11.74 \t\t|\t\t 7.540000000000001\n",
      "[Feb 25, 09:05:37] 878 0.6591596778003779\n",
      "\t\t\t\t 0.06882932782173157 0.4013630449771881\n",
      "#>>>    19.46 12.14 \t\t|\t\t 7.32\n",
      "[Feb 25, 09:05:38] 879 0.6589707104953764\n",
      "\t\t\t\t 0.01087894942611456 0.11420106887817383\n",
      "#>>>    19.61 10.02 \t\t|\t\t 9.59\n",
      "[Feb 25, 09:05:38] 880 0.6584368198041166\n",
      "\t\t\t\t 0.05634394288063049 0.3081675171852112\n",
      "#>>>    19.16 10.85 \t\t|\t\t 8.31\n",
      "[Feb 25, 09:05:38] 881 0.6581428944443783\n",
      "\t\t\t\t 0.06423460692167282 0.30134767293930054\n",
      "#>>>    19.37 10.09 \t\t|\t\t 9.280000000000001\n",
      "[Feb 25, 09:05:39] 882 0.6578503338372456\n",
      "\t\t\t\t 0.014344334602355957 0.2487148642539978\n",
      "#>>>    19.76 11.86 \t\t|\t\t 7.900000000000002\n",
      "[Feb 25, 09:05:39] 883 0.6574555427022647\n",
      "\t\t\t\t 0.0616709366440773 0.25016024708747864\n",
      "#>>>    19.77 10.35 \t\t|\t\t 9.42\n",
      "[Feb 25, 09:05:39] 884 0.6571099183358434\n",
      "\t\t\t\t 0.0478474386036396 0.05776325240731239\n",
      "#>>>    18.82 10.01 \t\t|\t\t 8.81\n",
      "[Feb 25, 09:05:40] 885 0.6565584191085185\n",
      "\t\t\t\t 0.0612206794321537 0.5503666400909424\n",
      "#>>>    19.4 12.06 \t\t|\t\t 7.339999999999998\n",
      "[Feb 25, 09:05:40] 886 0.6565134480350102\n",
      "\t\t\t\t 0.025160307064652443 0.2571578025817871\n",
      "#>>>    20.35 10.58 \t\t|\t\t 9.770000000000001\n",
      "[Feb 25, 09:05:40] 887 0.6561392527022096\n",
      "\t\t\t\t 0.1379956603050232 0.3207995593547821\n",
      "#>>>    18.82 11.71 \t\t|\t\t 7.109999999999999\n",
      "[Feb 25, 09:05:40] 888 0.6559419086691672\n",
      "\t\t\t\t 0.004501441027969122 0.015375819057226181\n",
      "#>>>    19.5 10.01 \t\t|\t\t 9.49\n",
      "[Feb 25, 09:05:41] 889 0.655305844021049\n",
      "\t\t\t\t 0.05757080391049385 0.31057995557785034\n",
      "#>>>    18.91 11.04 \t\t|\t\t 7.870000000000001\n",
      "[Feb 25, 09:05:41] 890 0.6550186889476921\n",
      "\t\t\t\t 0.02295161597430706 0.3698970675468445\n",
      "#>>>    19.76 10.34 \t\t|\t\t 9.420000000000002\n",
      "[Feb 25, 09:05:41] 891 0.654756518929227\n",
      "\t\t\t\t 0.08640144020318985 0.3832210600376129\n",
      "#>>>    18.88 11.21 \t\t|\t\t 7.669999999999998\n",
      "[Feb 25, 09:05:42] 892 0.6545713849030881\n",
      "\t\t\t\t 0.14083337783813477 0.3948984742164612\n",
      "#>>>    18.85 11.13 \t\t|\t\t 7.720000000000001\n",
      "[Feb 25, 09:05:42] 893 0.6544525453702396\n",
      "\t\t\t\t 0.06811019778251648 0.3151872456073761\n",
      "#>>>    19.06 11.81 \t\t|\t\t 7.249999999999998\n",
      "[Feb 25, 09:05:42] 894 0.6541813902682593\n",
      "\t\t\t\t 0.1725718230009079 0.24572966992855072\n",
      "#>>>    19.5 10.85 \t\t|\t\t 8.65\n",
      "[Feb 25, 09:05:43] 895 0.6539455103709204\n",
      "\t\t\t\t 0.1201176568865776 0.42438358068466187\n",
      "#>>>    19.56 10.32 \t\t|\t\t 9.239999999999998\n",
      "[Feb 25, 09:05:43] 896 0.6538360661055713\n",
      "\t\t\t\t 0.031209222972393036 0.18089424073696136\n",
      "#>>>    20.24 10.76 \t\t|\t\t 9.479999999999999\n",
      "[Feb 25, 09:05:43] 897 0.6533943334957245\n",
      "\t\t\t\t 0.12820376455783844 0.5180956721305847\n",
      "#>>>    19.24 10.55 \t\t|\t\t 8.689999999999998\n",
      "[Feb 25, 09:05:44] 898 0.653387238584016\n",
      "\t\t\t\t 0.07505525648593903 0.22500552237033844\n",
      "#>>>    19.92 10.98 \t\t|\t\t 8.940000000000001\n",
      "[Feb 25, 09:05:44] 899 0.6530339121242883\n",
      "\t\t\t\t 0.019856901839375496 0.16191712021827698\n",
      "#>>>    19.39 10.89 \t\t|\t\t 8.5\n",
      "[Feb 25, 09:05:44] 900 0.6525626522323591\n",
      "\t\t\t\t 0.03234944865107536 0.1830570548772812\n",
      "#>>>    19.65 10.69 \t\t|\t\t 8.959999999999999\n",
      "[Feb 25, 09:05:44] 901 0.6521254960873804\n",
      "\t\t\t\t 0.21941691637039185 0.29548317193984985\n",
      "#>>>    18.72 10.98 \t\t|\t\t 7.739999999999998\n",
      "[Feb 25, 09:05:45] 902 0.6519882706796032\n",
      "\t\t\t\t 0.1254081130027771 0.17414353787899017\n",
      "#>>>    19.34 11.4 \t\t|\t\t 7.9399999999999995\n",
      "[Feb 25, 09:05:45] 903 0.6516358340747065\n",
      "\t\t\t\t 0.011266963556408882 0.17512790858745575\n",
      "#>>>    18.95 10.66 \t\t|\t\t 8.29\n",
      "[Feb 25, 09:05:45] 904 0.6511705931109131\n",
      "\t\t\t\t 0.0439281091094017 0.05788420885801315\n",
      "#>>>    19.94 11.86 \t\t|\t\t 8.080000000000002\n",
      "[Feb 25, 09:05:46] 905 0.6506212348357696\n",
      "\t\t\t\t 0.06572889536619186 0.1933518499135971\n",
      "#>>>    19.62 10.55 \t\t|\t\t 9.07\n",
      "[Feb 25, 09:05:46] 906 0.6502296943387631\n",
      "\t\t\t\t 0.13861772418022156 0.4591468870639801\n",
      "#>>>    18.52 10.6 \t\t|\t\t 7.92\n",
      "[Feb 25, 09:05:46] 907 0.6501772292556685\n",
      "\t\t\t\t 0.10168243199586868 0.33409592509269714\n",
      "#>>>    19.77 11.88 \t\t|\t\t 7.889999999999999\n",
      "[Feb 25, 09:05:47] 908 0.6499628303760508\n",
      "\t\t\t\t 0.027666300535202026 0.26543906331062317\n",
      "#>>>    19.12 9.42 \t\t|\t\t 9.700000000000001\n",
      "[Feb 25, 09:05:47] 909 0.6496059729095206\n",
      "\t\t\t\t 0.09303927421569824 0.5031477212905884\n",
      "#>>>    18.76 11.45 \t\t|\t\t 7.310000000000002\n",
      "[Feb 25, 09:05:47] 910 0.6495525539321174\n",
      "\t\t\t\t 0.07036027312278748 0.7626793384552002\n",
      "#>>>    19.09 11.3 \t\t|\t\t 7.789999999999999\n",
      "[Feb 25, 09:05:47] 911 0.6497360410195656\n",
      "\t\t\t\t 0.08842118829488754 0.3092203438282013\n",
      "#>>>    19.21 11.04 \t\t|\t\t 8.170000000000002\n",
      "[Feb 25, 09:05:48] 912 0.6494839465181197\n",
      "\t\t\t\t 0.06416762620210648 0.2139509618282318\n",
      "#>>>    19.29 11.76 \t\t|\t\t 7.529999999999999\n",
      "[Feb 25, 09:05:48] 913 0.6491125811521814\n",
      "\t\t\t\t 0.1542517989873886 0.5868632197380066\n",
      "#>>>    18.56 11.25 \t\t|\t\t 7.309999999999999\n",
      "[Feb 25, 09:05:48] 914 0.6492045836046557\n",
      "\t\t\t\t 0.05886176601052284 0.22682495415210724\n",
      "#>>>    19.4 11.14 \t\t|\t\t 8.259999999999998\n",
      "[Feb 25, 09:05:49] 915 0.6488410657523896\n",
      "\t\t\t\t 0.08804522454738617 0.21367385983467102\n",
      "#>>>    19.35 10.7 \t\t|\t\t 8.650000000000002\n",
      "[Feb 25, 09:05:49] 916 0.6484939437561181\n",
      "\t\t\t\t 0.03738385811448097 0.17071101069450378\n",
      "#>>>    19.72 11.04 \t\t|\t\t 8.68\n",
      "[Feb 25, 09:05:49] 917 0.6480535446774457\n",
      "\t\t\t\t 0.037604186683893204 0.3171152174472809\n",
      "#>>>    19.24 11.34 \t\t|\t\t 7.899999999999999\n",
      "[Feb 25, 09:05:50] 918 0.6477602105331741\n",
      "\t\t\t\t 0.005397091154009104 0.19693432748317719\n",
      "#>>>    19.33 10.53 \t\t|\t\t 8.799999999999999\n",
      "[Feb 25, 09:05:50] 919 0.6473147817464004\n",
      "\t\t\t\t 0.03798339515924454 0.11559713631868362\n",
      "#>>>    20.03 10.9 \t\t|\t\t 9.13\n",
      "[Feb 25, 09:05:50] 920 0.646821047496132\n",
      "\t\t\t\t 0.07919194549322128 0.22631534934043884\n",
      "#>>>    19.97 10.75 \t\t|\t\t 9.219999999999999\n",
      "[Feb 25, 09:05:51] 921 0.6464797337509202\n",
      "\t\t\t\t 0.036063194274902344 0.22536267340183258\n",
      "#>>>    19.24 10.24 \t\t|\t\t 8.999999999999998\n",
      "[Feb 25, 09:05:51] 922 0.6460946798699448\n",
      "\t\t\t\t 0.012545575387775898 0.2399481236934662\n",
      "#>>>    19.5 9.77 \t\t|\t\t 9.73\n",
      "[Feb 25, 09:05:51] 923 0.6457010788994006\n",
      "\t\t\t\t 0.05165351554751396 0.2645920515060425\n",
      "#>>>    19.06 9.47 \t\t|\t\t 9.589999999999998\n",
      "[Feb 25, 09:05:51] 924 0.645371623376379\n",
      "\t\t\t\t 0.1583811342716217 0.44863811135292053\n",
      "#>>>    18.17 12.53 \t\t|\t\t 5.640000000000002\n",
      "[Feb 25, 09:05:52] 925 0.6453332709986271\n",
      "\t\t\t\t 0.055300042033195496 0.08456071466207504\n",
      "#>>>    20.06 10.67 \t\t|\t\t 9.389999999999999\n",
      "[Feb 25, 09:05:52] 926 0.6448277984768732\n",
      "\t\t\t\t 0.031202198937535286 0.33341184258461\n",
      "#>>>    18.92 10.84 \t\t|\t\t 8.080000000000002\n",
      "[Feb 25, 09:05:52] 927 0.6445475847180558\n",
      "\t\t\t\t 0.022490244358778 0.19470180571079254\n",
      "#>>>    19.08 9.97 \t\t|\t\t 9.109999999999998\n",
      "[Feb 25, 09:05:53] 928 0.6441202291871326\n",
      "\t\t\t\t 0.051903076469898224 0.3326713442802429\n",
      "#>>>    19.57 10.88 \t\t|\t\t 8.69\n",
      "[Feb 25, 09:05:53] 929 0.643860683371245\n",
      "\t\t\t\t 0.06494910269975662 0.30328646302223206\n",
      "#>>>    19.38 11.26 \t\t|\t\t 8.12\n",
      "[Feb 25, 09:05:53] 930 0.6435850582461451\n",
      "\t\t\t\t 0.1087983176112175 0.20846232771873474\n",
      "#>>>    19.36 10.56 \t\t|\t\t 8.799999999999999\n",
      "[Feb 25, 09:05:54] 931 0.6432587338406794\n",
      "\t\t\t\t 0.032907869666814804 0.15390843152999878\n",
      "#>>>    20.13 10.47 \t\t|\t\t 9.659999999999998\n",
      "[Feb 25, 09:05:54] 932 0.6428022914117609\n",
      "\t\t\t\t 0.014147425070405006 0.17829616367816925\n",
      "#>>>    19.18 9.86 \t\t|\t\t 9.32\n",
      "[Feb 25, 09:05:54] 933 0.6423519327146856\n",
      "\t\t\t\t 0.008950003422796726 0.12812836468219757\n",
      "#>>>    20.32 10.82 \t\t|\t\t 9.5\n",
      "[Feb 25, 09:05:54] 934 0.6418466591565952\n",
      "\t\t\t\t 0.09694802016019821 0.1886560469865799\n",
      "#>>>    20.21 12.1 \t\t|\t\t 8.110000000000001\n",
      "[Feb 25, 09:05:55] 935 0.6414904165571349\n",
      "\t\t\t\t 0.07504530251026154 0.2023119181394577\n",
      "#>>>    19.53 11.13 \t\t|\t\t 8.4\n",
      "[Feb 25, 09:05:55] 936 0.6411262833612275\n",
      "\t\t\t\t 0.009861071594059467 0.11573818325996399\n",
      "#>>>    20.03 10.65 \t\t|\t\t 9.38\n",
      "[Feb 25, 09:05:55] 937 0.6406107563280636\n",
      "\t\t\t\t 0.07712723314762115 0.15788570046424866\n",
      "#>>>    20.54 11.69 \t\t|\t\t 8.85\n",
      "[Feb 25, 09:05:56] 938 0.6402051585053474\n",
      "\t\t\t\t 0.11882788687944412 0.38968968391418457\n",
      "#>>>    18.73 11.09 \t\t|\t\t 7.640000000000001\n",
      "[Feb 25, 09:05:56] 939 0.6400734709101851\n",
      "\t\t\t\t 0.0274921003729105 0.19893093407154083\n",
      "#>>>    20.11 9.9 \t\t|\t\t 10.209999999999999\n",
      "[Feb 25, 09:05:56] 940 0.6396598204793073\n",
      "\t\t\t\t 0.10692859441041946 0.3513031601905823\n",
      "#>>>    18.62 11.54 \t\t|\t\t 7.080000000000002\n",
      "[Feb 25, 09:05:57] 941 0.6394783924059784\n",
      "\t\t\t\t 0.047157324850559235 0.16866619884967804\n",
      "#>>>    19.51 10.87 \t\t|\t\t 8.640000000000002\n",
      "[Feb 25, 09:05:57] 942 0.6390547375447232\n",
      "\t\t\t\t 0.06405340880155563 0.2832176387310028\n",
      "#>>>    19.64 10.7 \t\t|\t\t 8.940000000000001\n",
      "[Feb 25, 09:05:57] 943 0.6387629538621615\n",
      "\t\t\t\t 0.0697827860713005 0.23907147347927094\n",
      "#>>>    20.32 10.81 \t\t|\t\t 9.51\n",
      "[Feb 25, 09:05:58] 944 0.6384330451603994\n",
      "\t\t\t\t 0.03738011047244072 0.49381890892982483\n",
      "#>>>    18.92 11.41 \t\t|\t\t 7.510000000000002\n",
      "[Feb 25, 09:05:58] 945 0.6383258111532677\n",
      "\t\t\t\t 0.03293823450803757 0.2407752275466919\n",
      "#>>>    19.87 10.51 \t\t|\t\t 9.360000000000001\n",
      "[Feb 25, 09:05:58] 946 0.6379611988116197\n",
      "\t\t\t\t 0.05546616017818451 0.27458325028419495\n",
      "#>>>    18.92 10.35 \t\t|\t\t 8.570000000000002\n",
      "[Feb 25, 09:05:58] 947 0.6376532870083693\n",
      "\t\t\t\t 0.1312001645565033 0.11429603397846222\n",
      "#>>>    19.67 10.64 \t\t|\t\t 9.030000000000001\n",
      "[Feb 25, 09:05:59] 948 0.6372611299198959\n",
      "\t\t\t\t 0.09230415523052216 0.16902324557304382\n",
      "#>>>    19.43 11.0 \t\t|\t\t 8.43\n",
      "[Feb 25, 09:05:59] 949 0.6368851961758785\n",
      "\t\t\t\t 0.03179017826914787 0.2748040556907654\n",
      "#>>>    19.56 10.24 \t\t|\t\t 9.319999999999999\n",
      "[Feb 25, 09:05:59] 950 0.6365549052024866\n",
      "\t\t\t\t 0.08810360729694366 0.24529537558555603\n",
      "#>>>    19.78 10.69 \t\t|\t\t 9.090000000000002\n",
      "[Feb 25, 09:06:00] 951 0.6362517492950677\n",
      "\t\t\t\t 0.23899109661579132 0.27025124430656433\n",
      "#>>>    18.96 11.78 \t\t|\t\t 7.1800000000000015\n",
      "[Feb 25, 09:06:00] 952 0.6361247399015961\n",
      "\t\t\t\t 0.04674499109387398 0.33273065090179443\n",
      "#>>>    18.58 10.03 \t\t|\t\t 8.549999999999999\n",
      "[Feb 25, 09:06:00] 953 0.6358680908148661\n",
      "\t\t\t\t 0.02879440225660801 0.23812876641750336\n",
      "#>>>    19.93 11.53 \t\t|\t\t 8.4\n",
      "[Feb 25, 09:06:01] 954 0.635499145883412\n",
      "\t\t\t\t 0.08476537466049194 0.10748983174562454\n",
      "#>>>    19.46 10.22 \t\t|\t\t 9.24\n",
      "[Feb 25, 09:06:01] 955 0.6350559019364841\n",
      "\t\t\t\t 0.04626625403761864 0.2813928425312042\n",
      "#>>>    20.0 11.28 \t\t|\t\t 8.72\n",
      "[Feb 25, 09:06:01] 956 0.6347485051348417\n",
      "\t\t\t\t 0.06815612316131592 0.10728134959936142\n",
      "#>>>    20.28 10.8 \t\t|\t\t 9.48\n",
      "[Feb 25, 09:06:01] 957 0.634289194109918\n",
      "\t\t\t\t 0.05521027371287346 0.46255195140838623\n",
      "#>>>    20.53 10.36 \t\t|\t\t 10.170000000000002\n",
      "[Feb 25, 09:06:02] 958 0.6341726671595558\n",
      "\t\t\t\t 0.023881560191512108 0.3887004554271698\n",
      "#>>>    18.65 9.96 \t\t|\t\t 8.689999999999998\n",
      "[Feb 25, 09:06:02] 959 0.6339510765024269\n",
      "\t\t\t\t 0.0697936862707138 0.24783577024936676\n",
      "#>>>    20.25 11.45 \t\t|\t\t 8.8\n",
      "[Feb 25, 09:06:02] 960 0.6336347548824445\n",
      "\t\t\t\t 0.03683338686823845 0.18488670885562897\n",
      "#>>>    19.85 11.15 \t\t|\t\t 8.700000000000001\n",
      "[Feb 25, 09:06:03] 961 0.6332228402270113\n",
      "\t\t\t\t 0.04805992916226387 0.11570308357477188\n",
      "#>>>    19.55 10.48 \t\t|\t\t 9.07\n",
      "[Feb 25, 09:06:03] 962 0.6327533804032466\n",
      "\t\t\t\t 0.04511081799864769 0.37405702471733093\n",
      "#>>>    19.65 11.21 \t\t|\t\t 8.439999999999998\n",
      "[Feb 25, 09:06:03] 963 0.6325397948692846\n",
      "\t\t\t\t 0.07328366488218307 0.4784243404865265\n",
      "#>>>    19.85 10.68 \t\t|\t\t 9.170000000000002\n",
      "[Feb 25, 09:06:04] 964 0.6324589630574322\n",
      "\t\t\t\t 0.10346853733062744 0.5179024934768677\n",
      "#>>>    19.74 11.61 \t\t|\t\t 8.129999999999999\n",
      "[Feb 25, 09:06:04] 965 0.6324478751251823\n",
      "\t\t\t\t 0.08373896777629852 0.20498040318489075\n",
      "#>>>    19.97 11.74 \t\t|\t\t 8.229999999999999\n",
      "[Feb 25, 09:06:04] 966 0.6321041466061171\n",
      "\t\t\t\t 0.05822845548391342 0.15617778897285461\n",
      "#>>>    20.2 11.69 \t\t|\t\t 8.51\n",
      "[Feb 25, 09:06:05] 967 0.6316864487114184\n",
      "\t\t\t\t 0.20223665237426758 0.4093371629714966\n",
      "#>>>    19.57 11.89 \t\t|\t\t 7.68\n",
      "[Feb 25, 09:06:05] 968 0.6316663360780528\n",
      "\t\t\t\t 0.09310682117938995 0.41191568970680237\n",
      "#>>>    19.41 11.22 \t\t|\t\t 8.19\n",
      "[Feb 25, 09:06:05] 969 0.6315396922677621\n",
      "\t\t\t\t 0.01750774309039116 0.24603307247161865\n",
      "#>>>    18.94 10.52 \t\t|\t\t 8.420000000000002\n",
      "[Feb 25, 09:06:05] 970 0.6311716933798804\n",
      "\t\t\t\t 0.0494314581155777 0.4271564185619354\n",
      "#>>>    19.2 11.47 \t\t|\t\t 7.729999999999999\n",
      "[Feb 25, 09:06:06] 971 0.6310171095780792\n",
      "\t\t\t\t 0.06786425411701202 0.400446355342865\n",
      "#>>>    19.75 11.04 \t\t|\t\t 8.71\n",
      "[Feb 25, 09:06:06] 972 0.63085440306306\n",
      "\t\t\t\t 0.05848432704806328 0.24663913249969482\n",
      "#>>>    19.36 10.32 \t\t|\t\t 9.04\n",
      "[Feb 25, 09:06:06] 973 0.6305286721083687\n",
      "\t\t\t\t 0.07682310789823532 0.4492034316062927\n",
      "#>>>    19.45 11.16 \t\t|\t\t 8.29\n",
      "[Feb 25, 09:06:07] 974 0.6304241699832155\n",
      "\t\t\t\t 0.09109051525592804 0.06959665566682816\n",
      "#>>>    19.48 11.76 \t\t|\t\t 7.720000000000001\n",
      "[Feb 25, 09:06:07] 975 0.6299544329916056\n",
      "\t\t\t\t 0.024120090529322624 0.2594263255596161\n",
      "#>>>    19.54 10.41 \t\t|\t\t 9.129999999999999\n",
      "[Feb 25, 09:06:07] 976 0.6296080249765655\n",
      "\t\t\t\t 0.05542633682489395 0.05070224404335022\n",
      "#>>>    19.78 11.07 \t\t|\t\t 8.71\n",
      "[Feb 25, 09:06:08] 977 0.6290845455324572\n",
      "\t\t\t\t 0.025528226047754288 0.20422102510929108\n",
      "#>>>    19.44 10.16 \t\t|\t\t 9.280000000000001\n",
      "[Feb 25, 09:06:08] 978 0.6286852102343565\n",
      "\t\t\t\t 0.005248457659035921 0.24096913635730743\n",
      "#>>>    19.49 10.65 \t\t|\t\t 8.839999999999998\n",
      "[Feb 25, 09:06:08] 979 0.6283027426176728\n",
      "\t\t\t\t 0.061620958149433136 0.15616875886917114\n",
      "#>>>    19.01 11.43 \t\t|\t\t 7.580000000000002\n",
      "[Feb 25, 09:06:08] 980 0.6278922295846231\n",
      "\t\t\t\t 0.07220730185508728 0.26032552123069763\n",
      "#>>>    19.93 11.75 \t\t|\t\t 8.18\n",
      "[Feb 25, 09:06:09] 981 0.6275968701781243\n",
      "\t\t\t\t 0.0695522129535675 0.401624470949173\n",
      "#>>>    19.19 10.63 \t\t|\t\t 8.56\n",
      "[Feb 25, 09:06:09] 982 0.6274404499918489\n",
      "\t\t\t\t 0.02764882519841194 0.26404380798339844\n",
      "#>>>    19.22 10.01 \t\t|\t\t 9.209999999999999\n",
      "[Feb 25, 09:06:09] 983 0.6271047021862147\n",
      "\t\t\t\t 0.036641061305999756 0.27182459831237793\n",
      "#>>>    19.52 10.02 \t\t|\t\t 9.5\n",
      "[Feb 25, 09:06:10] 984 0.6267860631436469\n",
      "\t\t\t\t 0.03664794936776161 0.32279491424560547\n",
      "#>>>    19.01 11.13 \t\t|\t\t 7.880000000000001\n",
      "[Feb 25, 09:06:10] 985 0.6265187199403913\n",
      "\t\t\t\t 0.034863803535699844 0.3973637521266937\n",
      "#>>>    19.12 10.37 \t\t|\t\t 8.750000000000002\n",
      "[Feb 25, 09:06:10] 986 0.626324428772388\n",
      "\t\t\t\t 0.025159696117043495 0.15642277896404266\n",
      "#>>>    19.69 10.04 \t\t|\t\t 9.650000000000002\n",
      "[Feb 25, 09:06:11] 987 0.6258796868242846\n",
      "\t\t\t\t 0.2787315249443054 0.524753212928772\n",
      "#>>>    18.31 11.7 \t\t|\t\t 6.609999999999999\n",
      "[Feb 25, 09:06:11] 988 0.6260572918753334\n",
      "\t\t\t\t 0.06018596142530441 0.3065507113933563\n",
      "#>>>    19.23 10.08 \t\t|\t\t 9.15\n",
      "[Feb 25, 09:06:11] 989 0.6257979712637273\n",
      "\t\t\t\t 0.026171188801527023 0.3108016550540924\n",
      "#>>>    20.14 10.78 \t\t|\t\t 9.360000000000001\n",
      "[Feb 25, 09:06:12] 990 0.6255091461251433\n",
      "\t\t\t\t 0.0725305825471878 0.4438629150390625\n",
      "#>>>    18.4 10.43 \t\t|\t\t 7.969999999999999\n",
      "[Feb 25, 09:06:12] 991 0.6254000304617032\n",
      "\t\t\t\t 0.00265351845882833 0.16583125293254852\n",
      "#>>>    19.78 10.06 \t\t|\t\t 9.72\n",
      "[Feb 25, 09:06:12] 992 0.6249431152084536\n",
      "\t\t\t\t 0.017049459740519524 0.34679335355758667\n",
      "#>>>    18.43 10.5 \t\t|\t\t 7.93\n",
      "[Feb 25, 09:06:12] 993 0.6246820149084059\n",
      "\t\t\t\t 0.01726195588707924 0.21299932897090912\n",
      "#>>>    19.47 10.68 \t\t|\t\t 8.79\n",
      "[Feb 25, 09:06:13] 994 0.6242875941746302\n",
      "\t\t\t\t 0.2116803377866745 0.1723468154668808\n",
      "#>>>    18.57 10.6 \t\t|\t\t 7.970000000000001\n",
      "[Feb 25, 09:06:13] 995 0.6240473337337091\n",
      "\t\t\t\t 0.010943320579826832 0.1448601484298706\n",
      "#>>>    19.42 10.81 \t\t|\t\t 8.610000000000001\n",
      "[Feb 25, 09:06:13] 996 0.6235790898717791\n",
      "\t\t\t\t 0.07199925929307938 0.4793952703475952\n",
      "#>>>    18.36 10.27 \t\t|\t\t 8.09\n",
      "[Feb 25, 09:06:14] 997 0.6235069053040975\n",
      "\t\t\t\t 0.08656542748212814 0.21276673674583435\n",
      "#>>>    19.74 11.27 \t\t|\t\t 8.469999999999999\n",
      "[Feb 25, 09:06:14] 998 0.6231827305704719\n",
      "\t\t\t\t 0.03782451152801514 0.259054571390152\n",
      "#>>>    18.95 10.17 \t\t|\t\t 8.78\n",
      "[Feb 25, 09:06:14] 999 0.6228564269228196\n",
      "\t\t\t\t 0.0766717940568924 0.39988619089126587\n",
      "#>>>    19.22 11.22 \t\t|\t\t 7.999999999999998\n",
      "[Feb 25, 09:06:15] 1000 0.6227101284659438\n",
      "\t\t\t\t 0.042852502316236496 0.15294289588928223\n",
      "#>>>    19.57 10.19 \t\t|\t\t 9.38\n",
      "[Feb 25, 09:06:15] 1001 0.6222832137394086\n",
      "\t\t\t\t 0.09384354948997498 0.5449342727661133\n",
      "#>>>    17.96 11.11 \t\t|\t\t 6.850000000000001\n",
      "[Feb 25, 09:06:15] 1002 0.6222997083777277\n",
      "\t\t\t\t 0.05677299574017525 0.1873197853565216\n",
      "#>>>    19.09 9.89 \t\t|\t\t 9.2\n",
      "[Feb 25, 09:06:15] 1003 0.6219215014467214\n",
      "\t\t\t\t 0.044536981731653214 0.20882710814476013\n",
      "#>>>    19.78 10.23 \t\t|\t\t 9.55\n",
      "[Feb 25, 09:06:16] 1004 0.6215529440314259\n",
      "\t\t\t\t 0.026518184691667557 0.23301659524440765\n",
      "#>>>    19.54 10.17 \t\t|\t\t 9.37\n",
      "[Feb 25, 09:06:16] 1005 0.6211909258636052\n",
      "\t\t\t\t 0.16900590062141418 0.45162999629974365\n",
      "#>>>    18.79 11.96 \t\t|\t\t 6.829999999999998\n",
      "[Feb 25, 09:06:16] 1006 0.6211903708048604\n",
      "\t\t\t\t 0.09221649169921875 0.3005870580673218\n",
      "#>>>    20.07 11.64 \t\t|\t\t 8.43\n",
      "[Feb 25, 09:06:17] 1007 0.6209619839838221\n",
      "\t\t\t\t 0.029252467676997185 0.3084401786327362\n",
      "#>>>    19.32 9.51 \t\t|\t\t 9.81\n",
      "[Feb 25, 09:06:17] 1008 0.6206787146480106\n",
      "\t\t\t\t 0.04676060378551483 0.39541125297546387\n",
      "#>>>    19.24 10.64 \t\t|\t\t 8.599999999999998\n",
      "[Feb 25, 09:06:17] 1009 0.6205002078050248\n",
      "\t\t\t\t 0.021461904048919678 0.4047950208187103\n",
      "#>>>    19.85 10.3 \t\t|\t\t 9.55\n",
      "[Feb 25, 09:06:18] 1010 0.6203059645220874\n",
      "\t\t\t\t 0.06410304456949234 0.05921719968318939\n",
      "#>>>    19.64 10.0 \t\t|\t\t 9.64\n",
      "[Feb 25, 09:06:18] 1011 0.619808978801818\n",
      "\t\t\t\t 0.04767974093556404 0.442805677652359\n",
      "#>>>    18.74 10.6 \t\t|\t\t 8.139999999999999\n",
      "[Feb 25, 09:06:18] 1012 0.6196796552527799\n",
      "\t\t\t\t 0.042801402509212494 0.0041985465213656425\n",
      "#>>>    19.71 9.44 \t\t|\t\t 10.270000000000001\n",
      "[Feb 25, 09:06:19] 1013 0.6191069755474891\n",
      "\t\t\t\t 0.02272685244679451 0.18009833991527557\n",
      "#>>>    18.88 10.34 \t\t|\t\t 8.54\n",
      "[Feb 25, 09:06:19] 1014 0.6186906937605784\n",
      "\t\t\t\t 0.18926049768924713 0.08550959825515747\n",
      "#>>>    19.42 11.88 \t\t|\t\t 7.540000000000001\n",
      "[Feb 25, 09:06:19] 1015 0.6183467731478611\n",
      "\t\t\t\t 0.11079766601324081 0.24108946323394775\n",
      "#>>>    19.71 11.81 \t\t|\t\t 7.9\n",
      "[Feb 25, 09:06:19] 1016 0.618080313511411\n",
      "\t\t\t\t 0.03586351126432419 0.0664762556552887\n",
      "#>>>    18.75 8.69 \t\t|\t\t 10.06\n",
      "[Feb 25, 09:06:20] 1017 0.6175645729648193\n",
      "\t\t\t\t 0.017339719459414482 0.1196412518620491\n",
      "#>>>    19.4 10.43 \t\t|\t\t 8.969999999999999\n",
      "[Feb 25, 09:06:20] 1018 0.617083989357588\n",
      "\t\t\t\t 0.0781000480055809 0.02333252876996994\n",
      "#>>>    19.34 10.19 \t\t|\t\t 9.15\n",
      "[Feb 25, 09:06:20] 1019 0.616568337945006\n",
      "\t\t\t\t 0.05744030326604843 0.340822696685791\n",
      "#>>>    19.27 11.19 \t\t|\t\t 8.08\n",
      "[Feb 25, 09:06:21] 1020 0.6163500326144634\n",
      "\t\t\t\t 0.03256282955408096 0.12972092628479004\n",
      "#>>>    20.29 10.53 \t\t|\t\t 9.76\n",
      "[Feb 25, 09:06:21] 1021 0.6158959663302372\n",
      "\t\t\t\t 0.005938708316534758 0.49115389585494995\n",
      "#>>>    18.77 9.39 \t\t|\t\t 9.379999999999999\n",
      "[Feb 25, 09:06:21] 1022 0.6157771629685441\n",
      "\t\t\t\t 0.010830281302332878 0.16387920081615448\n",
      "#>>>    19.6 9.82 \t\t|\t\t 9.780000000000001\n",
      "[Feb 25, 09:06:22] 1023 0.6153360952895567\n",
      "\t\t\t\t 0.04631791263818741 0.15024831891059875\n",
      "#>>>    19.52 10.67 \t\t|\t\t 8.85\n",
      "[Feb 25, 09:06:22] 1024 0.6149173254183654\n",
      "\t\t\t\t 0.10001995414495468 0.22023208439350128\n",
      "#>>>    18.41 9.54 \t\t|\t\t 8.870000000000001\n",
      "[Feb 25, 09:06:22] 1025 0.6146226601240349\n",
      "\t\t\t\t 0.009218473918735981 0.21482330560684204\n",
      "#>>>    19.47 9.87 \t\t|\t\t 9.6\n",
      "[Feb 25, 09:06:22] 1026 0.6142320792387799\n",
      "\t\t\t\t 0.048453789204359055 0.24491596221923828\n",
      "#>>>    18.83 10.55 \t\t|\t\t 8.279999999999998\n",
      "[Feb 25, 09:06:23] 1027 0.6139112168997888\n",
      "\t\t\t\t 0.005870765075087547 0.18152841925621033\n",
      "#>>>    19.74 10.06 \t\t|\t\t 9.679999999999998\n",
      "[Feb 25, 09:06:23] 1028 0.6134847048616323\n",
      "\t\t\t\t 0.11456163227558136 0.32236015796661377\n",
      "#>>>    18.51 10.22 \t\t|\t\t 8.290000000000001\n",
      "[Feb 25, 09:06:23] 1029 0.6133081419321117\n",
      "\t\t\t\t 0.10562349855899811 0.19189272820949554\n",
      "#>>>    19.35 11.25 \t\t|\t\t 8.100000000000001\n",
      "[Feb 25, 09:06:24] 1030 0.6129923500169481\n",
      "\t\t\t\t 0.01879018545150757 0.16301476955413818\n",
      "#>>>    19.44 11.04 \t\t|\t\t 8.400000000000002\n",
      "[Feb 25, 09:06:24] 1031 0.6125611626219367\n",
      "\t\t\t\t 0.0496465340256691 0.18663541972637177\n",
      "#>>>    19.76 10.1 \t\t|\t\t 9.660000000000002\n",
      "[Feb 25, 09:06:24] 1032 0.6121848834205174\n",
      "\t\t\t\t 0.05362972244620323 0.25684621930122375\n",
      "#>>>    18.03 10.08 \t\t|\t\t 7.950000000000001\n",
      "[Feb 25, 09:06:25] 1033 0.6118831744825696\n",
      "\t\t\t\t 0.04659513756632805 0.3424084186553955\n",
      "#>>>    18.77 9.77 \t\t|\t\t 9.0\n",
      "[Feb 25, 09:06:25] 1034 0.6116602948531329\n",
      "\t\t\t\t 0.03484990820288658 0.461546927690506\n",
      "#>>>    18.82 11.17 \t\t|\t\t 7.65\n",
      "[Feb 25, 09:06:25] 1035 0.6115450313978984\n",
      "\t\t\t\t 0.016732407733798027 0.18466439843177795\n",
      "#>>>    18.89 9.11 \t\t|\t\t 9.780000000000001\n",
      "[Feb 25, 09:06:26] 1036 0.6111348831745287\n",
      "\t\t\t\t 0.056110844016075134 0.08934490382671356\n",
      "#>>>    19.93 11.0 \t\t|\t\t 8.93\n",
      "[Feb 25, 09:06:26] 1037 0.6106692040391969\n",
      "\t\t\t\t 0.004660967271775007 0.09733200818300247\n",
      "#>>>    19.38 9.65 \t\t|\t\t 9.729999999999999\n",
      "[Feb 25, 09:06:26] 1038 0.6101605278073529\n",
      "\t\t\t\t 0.014567512087523937 0.21647988259792328\n",
      "#>>>    18.84 9.98 \t\t|\t\t 8.86\n",
      "[Feb 25, 09:06:26] 1039 0.609781414671437\n",
      "\t\t\t\t 0.11013549566268921 0.3704178035259247\n",
      "#>>>    18.91 10.09 \t\t|\t\t 8.82\n",
      "[Feb 25, 09:06:27] 1040 0.6096521865559542\n",
      "\t\t\t\t 0.009655817411839962 0.13284695148468018\n",
      "#>>>    20.06 10.24 \t\t|\t\t 9.819999999999999\n",
      "[Feb 25, 09:06:27] 1041 0.6091850371392261\n",
      "\t\t\t\t 0.06122034415602684 0.15282371640205383\n",
      "#>>>    19.76 10.33 \t\t|\t\t 9.430000000000001\n",
      "[Feb 25, 09:06:27] 1042 0.6087898961663702\n",
      "\t\t\t\t 0.07241617888212204 0.20848041772842407\n",
      "#>>>    19.02 10.24 \t\t|\t\t 8.78\n",
      "[Feb 25, 09:06:28] 1043 0.608462002874265\n",
      "\t\t\t\t 0.005632051732391119 0.0827869325876236\n",
      "#>>>    19.27 9.98 \t\t|\t\t 9.29\n",
      "[Feb 25, 09:06:28] 1044 0.6079419598543138\n",
      "\t\t\t\t 0.07449541985988617 0.2035234272480011\n",
      "#>>>    19.02 10.04 \t\t|\t\t 8.98\n",
      "[Feb 25, 09:06:28] 1045 0.6076120367266661\n",
      "\t\t\t\t 0.05427868664264679 0.21652314066886902\n",
      "#>>>    18.83 9.92 \t\t|\t\t 8.909999999999998\n",
      "[Feb 25, 09:06:29] 1046 0.6072752265321522\n",
      "\t\t\t\t 0.011522568762302399 0.12124646455049515\n",
      "#>>>    19.85 9.87 \t\t|\t\t 9.980000000000002\n",
      "[Feb 25, 09:06:29] 1047 0.6068007203389328\n",
      "\t\t\t\t 0.05846351757645607 0.49621260166168213\n",
      "#>>>    18.05 10.14 \t\t|\t\t 7.91\n",
      "[Feb 25, 09:06:29] 1048 0.6067485957341068\n",
      "\t\t\t\t 0.01566845364868641 0.1419060081243515\n",
      "#>>>    19.0 9.82 \t\t|\t\t 9.18\n",
      "[Feb 25, 09:06:29] 1049 0.6062994215982831\n",
      "\t\t\t\t 0.0887894332408905 0.41576024889945984\n",
      "#>>>    19.24 11.02 \t\t|\t\t 8.219999999999999\n",
      "[Feb 25, 09:06:30] 1050 0.6061976718588251\n",
      "\t\t\t\t 0.07750120759010315 0.31909096240997314\n",
      "#>>>    19.01 10.17 \t\t|\t\t 8.840000000000002\n",
      "[Feb 25, 09:06:30] 1051 0.6059880663569663\n",
      "\t\t\t\t 0.2819398045539856 0.33081915974617004\n",
      "#>>>    18.84 10.77 \t\t|\t\t 8.07\n",
      "[Feb 25, 09:06:30] 1052 0.6059948372847119\n",
      "\t\t\t\t 0.01869829371571541 0.03407120704650879\n",
      "#>>>    19.3 9.76 \t\t|\t\t 9.540000000000001\n",
      "[Feb 25, 09:06:31] 1053 0.6054416119481895\n",
      "\t\t\t\t 0.10253901034593582 0.2915806770324707\n",
      "#>>>    19.74 10.79 \t\t|\t\t 8.95\n",
      "[Feb 25, 09:06:31] 1054 0.6052302900161691\n",
      "\t\t\t\t 0.010575762018561363 0.19791650772094727\n",
      "#>>>    18.94 9.56 \t\t|\t\t 9.38\n",
      "[Feb 25, 09:06:31] 1055 0.6048335519903045\n",
      "\t\t\t\t 0.07073502987623215 0.07957501709461212\n",
      "#>>>    19.09 9.42 \t\t|\t\t 9.67\n",
      "[Feb 25, 09:06:32] 1056 0.6043790284778345\n",
      "\t\t\t\t 0.05279405415058136 0.1887495219707489\n",
      "#>>>    19.1 10.25 \t\t|\t\t 8.850000000000001\n",
      "[Feb 25, 09:06:32] 1057 0.604016193025478\n",
      "\t\t\t\t 0.020858021453022957 0.3441902697086334\n",
      "#>>>    18.72 9.92 \t\t|\t\t 8.799999999999999\n",
      "[Feb 25, 09:06:32] 1058 0.6037772251217516\n",
      "\t\t\t\t 0.020488005131483078 0.16590198874473572\n",
      "#>>>    19.87 11.47 \t\t|\t\t 8.4\n",
      "[Feb 25, 09:06:33] 1059 0.6033598378942313\n",
      "\t\t\t\t 0.20068833231925964 0.36070719361305237\n",
      "#>>>    19.42 11.45 \t\t|\t\t 7.970000000000002\n",
      "[Feb 25, 09:06:33] 1060 0.6033178735822693\n",
      "\t\t\t\t 0.11815273761749268 0.13927142322063446\n",
      "#>>>    18.58 10.33 \t\t|\t\t 8.249999999999998\n",
      "[Feb 25, 09:06:33] 1061 0.6029719798844263\n",
      "\t\t\t\t 0.051361311227083206 0.559931755065918\n",
      "#>>>    18.73 10.79 \t\t|\t\t 7.940000000000001\n",
      "[Feb 25, 09:06:33] 1062 0.6029803009820107\n",
      "\t\t\t\t 0.05498865619301796 0.29551637172698975\n",
      "#>>>    18.99 10.62 \t\t|\t\t 8.37\n",
      "[Feb 25, 09:06:34] 1063 0.6027278257052234\n",
      "\t\t\t\t 0.12024331092834473 0.2523757517337799\n",
      "#>>>    19.17 10.44 \t\t|\t\t 8.730000000000002\n",
      "[Feb 25, 09:06:34] 1064 0.6024977169421804\n",
      "\t\t\t\t 0.006085267290472984 0.13635554909706116\n",
      "#>>>    19.66 10.18 \t\t|\t\t 9.48\n",
      "[Feb 25, 09:06:34] 1065 0.6020376600360378\n",
      "\t\t\t\t 0.06780742853879929 0.31816473603248596\n",
      "#>>>    20.22 11.7 \t\t|\t\t 8.52\n",
      "[Feb 25, 09:06:35] 1066 0.6018215945480235\n",
      "\t\t\t\t 0.048686932772397995 0.13056616485118866\n",
      "#>>>    19.47 11.09 \t\t|\t\t 8.379999999999999\n",
      "[Feb 25, 09:06:35] 1067 0.6013990260548244\n",
      "\t\t\t\t 0.11395839601755142 0.3194781243801117\n",
      "#>>>    19.24 11.36 \t\t|\t\t 7.879999999999999\n",
      "[Feb 25, 09:06:35] 1068 0.6012310635417166\n",
      "\t\t\t\t 0.013082935474812984 0.20556123554706573\n",
      "#>>>    19.9 10.64 \t\t|\t\t 9.259999999999998\n",
      "[Feb 25, 09:06:36] 1069 0.6008484766501281\n",
      "\t\t\t\t 0.02527325227856636 0.4367489814758301\n",
      "#>>>    18.84 10.39 \t\t|\t\t 8.45\n",
      "[Feb 25, 09:06:36] 1070 0.6007096504184083\n",
      "\t\t\t\t 0.035908035933971405 0.06541941314935684\n",
      "#>>>    20.02 10.78 \t\t|\t\t 9.24\n",
      "[Feb 25, 09:06:36] 1071 0.6002102682170732\n",
      "\t\t\t\t 0.15205195546150208 0.11031069606542587\n",
      "#>>>    19.76 9.97 \t\t|\t\t 9.790000000000001\n",
      "[Feb 25, 09:06:36] 1072 0.5998724206078336\n",
      "\t\t\t\t 0.06889684498310089 0.2382880300283432\n",
      "#>>>    18.81 10.41 \t\t|\t\t 8.399999999999999\n",
      "[Feb 25, 09:06:37] 1073 0.5995797330622372\n",
      "\t\t\t\t 0.06098191440105438 0.28970757126808167\n",
      "#>>>    19.15 10.1 \t\t|\t\t 9.049999999999999\n",
      "[Feb 25, 09:06:37] 1074 0.599330842799943\n",
      "\t\t\t\t 0.05401485413312912 0.24923701584339142\n",
      "#>>>    19.75 10.28 \t\t|\t\t 9.47\n",
      "[Feb 25, 09:06:37] 1075 0.5990347638196689\n",
      "\t\t\t\t 0.031793516129255295 0.15516918897628784\n",
      "#>>>    19.77 9.64 \t\t|\t\t 10.129999999999999\n",
      "[Feb 25, 09:06:38] 1076 0.5986226917646801\n",
      "\t\t\t\t 0.0629354789853096 0.221668541431427\n",
      "#>>>    18.9 10.62 \t\t|\t\t 8.28\n",
      "[Feb 25, 09:06:38] 1077 0.5983086730858815\n",
      "\t\t\t\t 0.015116333030164242 0.1919434517621994\n",
      "#>>>    19.85 10.63 \t\t|\t\t 9.22\n",
      "[Feb 25, 09:06:38] 1078 0.5979174241985193\n",
      "\t\t\t\t 0.10766386985778809 0.21373221278190613\n",
      "#>>>    19.77 11.61 \t\t|\t\t 8.16\n",
      "[Feb 25, 09:06:39] 1079 0.5976409028569605\n",
      "\t\t\t\t 0.01129610650241375 0.10129174590110779\n",
      "#>>>    19.65 9.9 \t\t|\t\t 9.749999999999998\n",
      "[Feb 25, 09:06:39] 1080 0.5971558498083697\n",
      "\t\t\t\t 0.023234957829117775 0.14852496981620789\n",
      "#>>>    19.26 10.23 \t\t|\t\t 9.030000000000001\n",
      "[Feb 25, 09:06:39] 1081 0.5967304538917946\n",
      "\t\t\t\t 0.019979048520326614 0.1816510260105133\n",
      "#>>>    20.07 10.65 \t\t|\t\t 9.42\n",
      "[Feb 25, 09:06:40] 1082 0.5963353535087084\n",
      "\t\t\t\t 0.022625356912612915 0.09518232196569443\n",
      "#>>>    19.49 9.41 \t\t|\t\t 10.079999999999998\n",
      "[Feb 25, 09:06:40] 1083 0.595856825834078\n",
      "\t\t\t\t 0.0826260969042778 0.27537617087364197\n",
      "#>>>    18.49 9.64 \t\t|\t\t 8.849999999999998\n",
      "[Feb 25, 09:06:40] 1084 0.5956189712834724\n",
      "\t\t\t\t 0.016383390873670578 0.3374146819114685\n",
      "#>>>    19.67 9.92 \t\t|\t\t 9.750000000000002\n",
      "[Feb 25, 09:06:40] 1085 0.5953771503737982\n",
      "\t\t\t\t 0.007248581852763891 0.13219530880451202\n",
      "#>>>    19.58 10.69 \t\t|\t\t 8.889999999999999\n",
      "[Feb 25, 09:06:41] 1086 0.5949212171126846\n",
      "\t\t\t\t 0.0388646274805069 0.28803691267967224\n",
      "#>>>    20.23 11.34 \t\t|\t\t 8.89\n",
      "[Feb 25, 09:06:41] 1087 0.5946531974506333\n",
      "\t\t\t\t 0.061883408576250076 0.30917197465896606\n",
      "#>>>    19.61 10.42 \t\t|\t\t 9.19\n",
      "[Feb 25, 09:06:41] 1088 0.5944295996475937\n",
      "\t\t\t\t 0.04578055068850517 0.21445734798908234\n",
      "#>>>    20.3 10.61 \t\t|\t\t 9.690000000000001\n",
      "[Feb 25, 09:06:42] 1089 0.594095407950349\n",
      "\t\t\t\t 0.11761051416397095 0.24684622883796692\n",
      "#>>>    19.46 12.17 \t\t|\t\t 7.290000000000001\n",
      "[Feb 25, 09:06:42] 1090 0.5938657692854006\n",
      "\t\t\t\t 0.07538800686597824 0.3379315435886383\n",
      "#>>>    19.2 11.06 \t\t|\t\t 8.139999999999999\n",
      "[Feb 25, 09:06:42] 1091 0.5936852230740204\n",
      "\t\t\t\t 0.037956271320581436 0.17086251080036163\n",
      "#>>>    20.29 10.43 \t\t|\t\t 9.86\n",
      "[Feb 25, 09:06:43] 1092 0.593300356629342\n",
      "\t\t\t\t 0.04015832021832466 0.23208361864089966\n",
      "#>>>    19.04 10.41 \t\t|\t\t 8.629999999999999\n",
      "[Feb 25, 09:06:43] 1093 0.5929792982227478\n",
      "\t\t\t\t 0.017934327945113182 0.21067866683006287\n",
      "#>>>    18.89 10.11 \t\t|\t\t 8.780000000000001\n",
      "[Feb 25, 09:06:43] 1094 0.5926149319137122\n",
      "\t\t\t\t 0.04683973640203476 0.15833210945129395\n",
      "#>>>    19.18 10.19 \t\t|\t\t 8.99\n",
      "[Feb 25, 09:06:43] 1095 0.5922274888351023\n",
      "\t\t\t\t 0.056959107518196106 0.583981990814209\n",
      "#>>>    18.68 10.63 \t\t|\t\t 8.049999999999999\n",
      "[Feb 25, 09:06:44] 1096 0.5922762024296985\n",
      "\t\t\t\t 0.03995688259601593 0.1542143076658249\n",
      "#>>>    19.81 10.7 \t\t|\t\t 9.11\n",
      "[Feb 25, 09:06:44] 1097 0.5918780974175306\n",
      "\t\t\t\t 0.05616496130824089 0.12051718682050705\n",
      "#>>>    19.78 9.61 \t\t|\t\t 10.170000000000002\n",
      "[Feb 25, 09:06:44] 1098 0.5914629014645165\n",
      "\t\t\t\t 0.019961975514888763 0.17505468428134918\n",
      "#>>>    19.34 10.38 \t\t|\t\t 8.959999999999999\n",
      "[Feb 25, 09:06:45] 1099 0.5910664552153977\n",
      "\t\t\t\t 0.013242257758975029 0.39189016819000244\n",
      "#>>>    18.74 10.99 \t\t|\t\t 7.749999999999998\n",
      "[Feb 25, 09:06:45] 1100 0.5908805211730928\n",
      "\t\t\t\t 0.05857372656464577 0.3505415916442871\n",
      "#>>>    18.56 11.66 \t\t|\t\t 6.899999999999999\n",
      "[Feb 25, 09:06:45] 1101 0.5906987559664033\n",
      "\t\t\t\t 0.026949850842356682 0.26043394207954407\n",
      "#>>>    19.48 10.68 \t\t|\t\t 8.8\n",
      "[Feb 25, 09:06:46] 1102 0.5903954410052215\n",
      "\t\t\t\t 0.057784345000982285 0.4593355655670166\n",
      "#>>>    19.25 10.82 \t\t|\t\t 8.43\n",
      "[Feb 25, 09:06:46] 1103 0.5903221654487072\n",
      "\t\t\t\t 0.06558091938495636 0.06808651238679886\n",
      "#>>>    19.29 10.49 \t\t|\t\t 8.799999999999999\n",
      "[Feb 25, 09:06:46] 1104 0.5898655107224808\n",
      "\t\t\t\t 0.11002801358699799 0.25233912467956543\n",
      "#>>>    19.44 9.78 \t\t|\t\t 9.660000000000002\n",
      "[Feb 25, 09:06:47] 1105 0.5896380123649261\n",
      "\t\t\t\t 0.021194616332650185 0.0952926054596901\n",
      "#>>>    19.75 9.7 \t\t|\t\t 10.05\n",
      "[Feb 25, 09:06:47] 1106 0.5891648615724909\n",
      "\t\t\t\t 0.02267555333673954 0.32672977447509766\n",
      "#>>>    18.88 10.14 \t\t|\t\t 8.739999999999998\n",
      "[Feb 25, 09:06:47] 1107 0.5889251020294171\n",
      "\t\t\t\t 0.0005072112544439733 0.19591504335403442\n",
      "#>>>    20.58 9.34 \t\t|\t\t 11.239999999999998\n",
      "[Feb 25, 09:06:47] 1108 0.5885325991764664\n",
      "\t\t\t\t 0.02571595273911953 0.331217885017395\n",
      "#>>>    19.18 9.3 \t\t|\t\t 9.879999999999999\n",
      "[Feb 25, 09:06:48] 1109 0.5883010004094585\n",
      "\t\t\t\t 0.06788557767868042 0.22048893570899963\n",
      "#>>>    19.66 10.55 \t\t|\t\t 9.11\n",
      "[Feb 25, 09:06:48] 1110 0.5880010739224368\n",
      "\t\t\t\t 0.12833118438720703 0.5467589497566223\n",
      "#>>>    19.14 10.28 \t\t|\t\t 8.860000000000001\n",
      "[Feb 25, 09:06:48] 1111 0.5880881629826582\n",
      "\t\t\t\t 0.03212668001651764 0.24273137748241425\n",
      "#>>>    19.86 10.15 \t\t|\t\t 9.709999999999999\n",
      "[Feb 25, 09:06:49] 1112 0.5877749328771745\n",
      "\t\t\t\t 0.061110757291316986 0.1796732395887375\n",
      "#>>>    19.39 11.09 \t\t|\t\t 8.3\n",
      "[Feb 25, 09:06:49] 1113 0.5874279419337268\n",
      "\t\t\t\t 0.014573373831808567 0.2000218778848648\n",
      "#>>>    18.91 9.79 \t\t|\t\t 9.120000000000001\n",
      "[Feb 25, 09:06:49] 1114 0.587055109250029\n",
      "\t\t\t\t 0.041677430272102356 0.08297686278820038\n",
      "#>>>    20.46 10.03 \t\t|\t\t 10.430000000000001\n",
      "[Feb 25, 09:06:50] 1115 0.5865927084338393\n",
      "\t\t\t\t 0.010369141586124897 0.2690763473510742\n",
      "#>>>    20.01 11.11 \t\t|\t\t 8.900000000000002\n",
      "[Feb 25, 09:06:50] 1116 0.5862855612245872\n",
      "\t\t\t\t 0.08886884897947311 0.3238565921783447\n",
      "#>>>    19.12 10.66 \t\t|\t\t 8.46\n",
      "[Feb 25, 09:06:50] 1117 0.5861120011119709\n",
      "\t\t\t\t 0.0476195253431797 0.19558562338352203\n",
      "#>>>    19.54 10.51 \t\t|\t\t 9.03\n",
      "[Feb 25, 09:06:50] 1118 0.5857690942558603\n",
      "\t\t\t\t 0.07364795356988907 0.42822960019111633\n",
      "#>>>    18.81 10.19 \t\t|\t\t 8.62\n",
      "[Feb 25, 09:06:51] 1119 0.5856852027079149\n",
      "\t\t\t\t 0.05237561836838722 0.2889607846736908\n",
      "#>>>    19.63 10.66 \t\t|\t\t 8.969999999999999\n",
      "[Feb 25, 09:06:51] 1120 0.5854408539045237\n",
      "\t\t\t\t 0.07533235847949982 0.2184675633907318\n",
      "#>>>    19.23 11.31 \t\t|\t\t 7.92\n",
      "[Feb 25, 09:06:51] 1121 0.5851492129873905\n",
      "\t\t\t\t 0.028095945715904236 0.10883329808712006\n",
      "#>>>    19.38 10.92 \t\t|\t\t 8.459999999999999\n",
      "[Feb 25, 09:06:52] 1122 0.5847009930182062\n",
      "\t\t\t\t 0.02810933068394661 0.21836376190185547\n",
      "#>>>    19.32 11.78 \t\t|\t\t 7.540000000000001\n",
      "[Feb 25, 09:06:52] 1123 0.5843627651140485\n",
      "\t\t\t\t 0.09247744083404541 0.11809565126895905\n",
      "#>>>    19.75 10.83 \t\t|\t\t 8.92\n",
      "[Feb 25, 09:06:52] 1124 0.5839889754410375\n",
      "\t\t\t\t 0.018702231347560883 0.24647840857505798\n",
      "#>>>    19.41 10.69 \t\t|\t\t 8.72\n",
      "[Feb 25, 09:06:53] 1125 0.5836701671129697\n",
      "\t\t\t\t 0.021502265706658363 0.13309364020824432\n",
      "#>>>    19.89 9.37 \t\t|\t\t 10.520000000000001\n",
      "[Feb 25, 09:06:53] 1126 0.5832410928573596\n",
      "\t\t\t\t 0.04498257488012314 0.20142193138599396\n",
      "#>>>    19.95 11.07 \t\t|\t\t 8.879999999999999\n",
      "[Feb 25, 09:06:53] 1127 0.5829042562633178\n",
      "\t\t\t\t 0.06680027395486832 0.250828355550766\n",
      "#>>>    18.7 10.33 \t\t|\t\t 8.37\n",
      "[Feb 25, 09:06:54] 1128 0.5826389806291096\n",
      "\t\t\t\t 0.024836482480168343 0.11872280389070511\n",
      "#>>>    19.22 10.86 \t\t|\t\t 8.36\n",
      "[Feb 25, 09:06:54] 1129 0.5821999009404393\n",
      "\t\t\t\t 0.08748086541891098 0.08772598952054977\n",
      "#>>>    20.21 10.98 \t\t|\t\t 9.23\n",
      "[Feb 25, 09:06:54] 1130 0.5817929078944383\n",
      "\t\t\t\t 0.006398912984877825 0.14193391799926758\n",
      "#>>>    19.16 9.27 \t\t|\t\t 9.89\n",
      "[Feb 25, 09:06:54] 1131 0.5813594478207876\n",
      "\t\t\t\t 0.16549399495124817 0.21347253024578094\n",
      "#>>>    19.61 11.36 \t\t|\t\t 8.25\n",
      "[Feb 25, 09:06:55] 1132 0.5811570548832627\n",
      "\t\t\t\t 0.10093004256486893 0.15800751745700836\n",
      "#>>>    20.21 11.98 \t\t|\t\t 8.23\n",
      "[Feb 25, 09:06:55] 1133 0.5808348353958519\n",
      "\t\t\t\t 0.008550500497221947 0.13950231671333313\n",
      "#>>>    19.8 9.02 \t\t|\t\t 10.780000000000001\n",
      "[Feb 25, 09:06:55] 1134 0.5804020533720787\n",
      "\t\t\t\t 0.008567171171307564 0.17067916691303253\n",
      "#>>>    19.08 9.79 \t\t|\t\t 9.29\n",
      "[Feb 25, 09:06:56] 1135 0.5800008976549283\n",
      "\t\t\t\t 0.0768624022603035 0.28758132457733154\n",
      "#>>>    18.62 10.58 \t\t|\t\t 8.040000000000001\n",
      "[Feb 25, 09:06:56] 1136 0.5797853404766604\n",
      "\t\t\t\t 0.013032235205173492 0.34706419706344604\n",
      "#>>>    18.74 9.82 \t\t|\t\t 8.919999999999998\n",
      "[Feb 25, 09:06:56] 1137 0.5795656515610018\n",
      "\t\t\t\t 0.04697822034358978 0.20453283190727234\n",
      "#>>>    18.88 10.27 \t\t|\t\t 8.61\n",
      "[Feb 25, 09:06:57] 1138 0.5792375969467904\n",
      "\t\t\t\t 0.03584275767207146 0.17531874775886536\n",
      "#>>>    19.58 11.07 \t\t|\t\t 8.509999999999998\n",
      "[Feb 25, 09:06:57] 1139 0.5788695208589999\n",
      "\t\t\t\t 0.09686639904975891 0.3198167681694031\n",
      "#>>>    18.52 10.98 \t\t|\t\t 7.539999999999999\n",
      "[Feb 25, 09:06:57] 1140 0.5787073345053602\n",
      "\t\t\t\t 0.03509799391031265 0.3537037968635559\n",
      "#>>>    19.19 9.68 \t\t|\t\t 9.510000000000002\n",
      "[Feb 25, 09:06:57] 1141 0.578517428954178\n",
      "\t\t\t\t 0.006906944792717695 0.3115636706352234\n",
      "#>>>    19.53 11.19 \t\t|\t\t 8.340000000000002\n",
      "[Feb 25, 09:06:58] 1142 0.5782573821522933\n",
      "\t\t\t\t 0.011325149796903133 0.2103015035390854\n",
      "#>>>    19.7 10.62 \t\t|\t\t 9.08\n",
      "[Feb 25, 09:06:58] 1143 0.5779007514244082\n",
      "\t\t\t\t 0.0438714362680912 0.269380122423172\n",
      "#>>>    18.93 10.0 \t\t|\t\t 8.93\n",
      "[Feb 25, 09:06:58] 1144 0.5776361022279498\n",
      "\t\t\t\t 0.004998746793717146 0.14226892590522766\n",
      "#>>>    18.86 9.25 \t\t|\t\t 9.61\n",
      "[Feb 25, 09:06:59] 1145 0.5772057337951612\n",
      "\t\t\t\t 0.19042280316352844 0.16996359825134277\n",
      "#>>>    19.45 10.51 \t\t|\t\t 8.94\n",
      "[Feb 25, 09:06:59] 1146 0.5769889144627809\n",
      "\t\t\t\t 0.07513737678527832 0.1966184824705124\n",
      "#>>>    19.31 10.31 \t\t|\t\t 8.999999999999998\n",
      "[Feb 25, 09:06:59] 1147 0.5766836814224751\n",
      "\t\t\t\t 0.01410789042711258 0.15527145564556122\n",
      "#>>>    19.27 9.45 \t\t|\t\t 9.82\n",
      "[Feb 25, 09:07:00] 1148 0.5762763770945758\n",
      "\t\t\t\t 0.006690055597573519 0.12483590841293335\n",
      "#>>>    18.85 10.1 \t\t|\t\t 8.750000000000002\n",
      "[Feb 25, 09:07:00] 1149 0.575831626681026\n",
      "\t\t\t\t 0.029327210038900375 0.1917974352836609\n",
      "#>>>    18.49 9.88 \t\t|\t\t 8.609999999999998\n",
      "[Feb 25, 09:07:00] 1150 0.5754769197033929\n",
      "\t\t\t\t 0.019327837973833084 0.04546681046485901\n",
      "#>>>    20.22 9.97 \t\t|\t\t 10.249999999999998\n",
      "[Feb 25, 09:07:01] 1151 0.5749662374284029\n",
      "\t\t\t\t 0.048069193959236145 0.11368501931428909\n",
      "#>>>    19.39 9.76 \t\t|\t\t 9.63\n",
      "[Feb 25, 09:07:01] 1152 0.5745530254116986\n",
      "\t\t\t\t 0.02448798343539238 0.23817667365074158\n",
      "#>>>    19.31 9.27 \t\t|\t\t 10.04\n",
      "[Feb 25, 09:07:01] 1153 0.5742411370321973\n",
      "\t\t\t\t 0.03825264424085617 0.21940447390079498\n",
      "#>>>    19.53 10.34 \t\t|\t\t 9.190000000000001\n",
      "[Feb 25, 09:07:01] 1154 0.5739245530058561\n",
      "\t\t\t\t 0.02746293693780899 0.24648374319076538\n",
      "#>>>    18.54 9.35 \t\t|\t\t 9.19\n",
      "[Feb 25, 09:07:02] 1155 0.5736245751255282\n",
      "\t\t\t\t 0.10822854936122894 0.1865045428276062\n",
      "#>>>    18.85 9.83 \t\t|\t\t 9.020000000000001\n",
      "[Feb 25, 09:07:02] 1156 0.5733456836574927\n",
      "\t\t\t\t 0.018838446587324142 0.33950144052505493\n",
      "#>>>    19.52 9.88 \t\t|\t\t 9.639999999999999\n",
      "[Feb 25, 09:07:02] 1157 0.5731306778497717\n",
      "\t\t\t\t 0.06759042292833328 0.09581610560417175\n",
      "#>>>    19.55 11.28 \t\t|\t\t 8.270000000000001\n",
      "[Feb 25, 09:07:03] 1158 0.5727209536930038\n",
      "\t\t\t\t 0.0361819751560688 0.315690815448761\n",
      "#>>>    18.97 9.66 \t\t|\t\t 9.309999999999999\n",
      "[Feb 25, 09:07:03] 1159 0.5725001055410915\n",
      "\t\t\t\t 0.03455023095011711 0.11569663882255554\n",
      "#>>>    19.94 10.41 \t\t|\t\t 9.530000000000001\n",
      "[Feb 25, 09:07:03] 1160 0.5720778523090484\n",
      "\t\t\t\t 0.032822493463754654 0.37752828001976013\n",
      "#>>>    18.71 10.43 \t\t|\t\t 8.280000000000001\n",
      "[Feb 25, 09:07:04] 1161 0.5719161252264976\n",
      "\t\t\t\t 0.0318741574883461 0.24559541046619415\n",
      "#>>>    19.96 10.28 \t\t|\t\t 9.680000000000001\n",
      "[Feb 25, 09:07:04] 1162 0.5716216786766762\n",
      "\t\t\t\t 0.03458293527364731 0.16237998008728027\n",
      "#>>>    19.83 11.48 \t\t|\t\t 8.349999999999998\n",
      "[Feb 25, 09:07:04] 1163 0.5712470199208111\n",
      "\t\t\t\t 0.060707468539476395 0.3116205334663391\n",
      "#>>>    19.58 11.16 \t\t|\t\t 8.419999999999998\n",
      "[Feb 25, 09:07:04] 1164 0.5710481009140719\n",
      "\t\t\t\t 0.013406217098236084 0.1933939903974533\n",
      "#>>>    19.31 10.02 \t\t|\t\t 9.29\n",
      "[Feb 25, 09:07:05] 1165 0.5706838530206535\n",
      "\t\t\t\t 0.10220568627119064 0.36981335282325745\n",
      "#>>>    18.77 11.33 \t\t|\t\t 7.4399999999999995\n",
      "[Feb 25, 09:07:05] 1166 0.5705851882141779\n",
      "\t\t\t\t 0.019485866650938988 0.015085047110915184\n",
      "#>>>    20.33 9.42 \t\t|\t\t 10.909999999999998\n",
      "[Feb 25, 09:07:05] 1167 0.5700491739397255\n",
      "\t\t\t\t 0.015502825379371643 0.282272070646286\n",
      "#>>>    19.34 10.02 \t\t|\t\t 9.32\n",
      "[Feb 25, 09:07:06] 1168 0.5697768996767126\n",
      "\t\t\t\t 0.02139187604188919 0.22742290794849396\n",
      "#>>>    19.24 9.5 \t\t|\t\t 9.739999999999998\n",
      "[Feb 25, 09:07:06] 1169 0.5694559375684769\n",
      "\t\t\t\t 0.027814287692308426 0.09608093649148941\n",
      "#>>>    19.15 10.4 \t\t|\t\t 8.749999999999998\n",
      "[Feb 25, 09:07:06] 1170 0.5690103768588175\n",
      "\t\t\t\t 0.013724264688789845 0.0445258803665638\n",
      "#>>>    19.66 9.9 \t\t|\t\t 9.76\n",
      "[Feb 25, 09:07:07] 1171 0.5684996166260827\n",
      "\t\t\t\t 0.062071286141872406 0.14497777819633484\n",
      "#>>>    18.87 8.48 \t\t|\t\t 10.39\n",
      "[Feb 25, 09:07:07] 1172 0.5681381660812455\n",
      "\t\t\t\t 0.08223876357078552 0.1460178643465042\n",
      "#>>>    19.66 10.69 \t\t|\t\t 8.97\n",
      "[Feb 25, 09:07:07] 1173 0.5677982845430815\n",
      "\t\t\t\t 0.09932653605937958 0.1468929946422577\n",
      "#>>>    19.23 9.77 \t\t|\t\t 9.46\n",
      "[Feb 25, 09:07:08] 1174 0.5674767057892399\n",
      "\t\t\t\t 0.005866128485649824 0.030070330947637558\n",
      "#>>>    19.93 9.61 \t\t|\t\t 10.32\n",
      "[Feb 25, 09:07:08] 1175 0.5669451655433496\n",
      "\t\t\t\t 0.0711660161614418 0.12728899717330933\n",
      "#>>>    19.08 11.18 \t\t|\t\t 7.899999999999999\n",
      "[Feb 25, 09:07:08] 1176 0.5665766753836905\n",
      "\t\t\t\t 0.021329043433070183 0.23624436557292938\n",
      "#>>>    19.17 8.94 \t\t|\t\t 10.230000000000002\n",
      "[Feb 25, 09:07:08] 1177 0.5662676721042742\n",
      "\t\t\t\t 0.08818387240171432 0.1271740347146988\n",
      "#>>>    19.24 10.4 \t\t|\t\t 8.839999999999998\n",
      "[Feb 25, 09:07:09] 1178 0.5659167623318357\n",
      "\t\t\t\t 0.15490414202213287 0.11453835666179657\n",
      "#>>>    19.62 10.99 \t\t|\t\t 8.63\n",
      "[Feb 25, 09:07:09] 1179 0.5656202880681878\n",
      "\t\t\t\t 0.03460979461669922 0.3454206585884094\n",
      "#>>>    19.93 10.91 \t\t|\t\t 9.02\n",
      "[Feb 25, 09:07:09] 1180 0.5654346982333247\n",
      "\t\t\t\t 0.17028789222240448 0.2674781382083893\n",
      "#>>>    19.08 12.47 \t\t|\t\t 6.609999999999998\n",
      "[Feb 25, 09:07:10] 1181 0.565307029550621\n",
      "\t\t\t\t 0.03594103083014488 0.11407186090946198\n",
      "#>>>    20.41 10.16 \t\t|\t\t 10.25\n",
      "[Feb 25, 09:07:10] 1182 0.5648917354165353\n",
      "\t\t\t\t 0.03761356323957443 0.18513105809688568\n",
      "#>>>    19.6 10.33 \t\t|\t\t 9.270000000000001\n",
      "[Feb 25, 09:07:10] 1183 0.5645495882950046\n",
      "\t\t\t\t 0.0050670551136136055 0.13494275510311127\n",
      "#>>>    19.65 9.29 \t\t|\t\t 10.36\n",
      "[Feb 25, 09:07:11] 1184 0.5641250485122696\n",
      "\t\t\t\t 0.0811912789940834 0.35951077938079834\n",
      "#>>>    18.33 10.3 \t\t|\t\t 8.029999999999998\n",
      "[Feb 25, 09:07:11] 1185 0.5640016255146817\n",
      "\t\t\t\t 0.04815657436847687 0.2183820903301239\n",
      "#>>>    19.0 11.18 \t\t|\t\t 7.82\n",
      "[Feb 25, 09:07:11] 1186 0.5637041625687668\n",
      "\t\t\t\t 0.011307764798402786 0.11696497350931168\n",
      "#>>>    20.16 9.33 \t\t|\t\t 10.83\n",
      "[Feb 25, 09:07:11] 1187 0.563268731148231\n",
      "\t\t\t\t 0.0452813096344471 0.06310547143220901\n",
      "#>>>    19.69 10.99 \t\t|\t\t 8.700000000000001\n",
      "[Feb 25, 09:07:12] 1188 0.5628138492018747\n",
      "\t\t\t\t 0.02260967716574669 0.22750644385814667\n",
      "#>>>    19.61 10.32 \t\t|\t\t 9.29\n",
      "[Feb 25, 09:07:12] 1189 0.5625011514625209\n",
      "\t\t\t\t 0.007098977454006672 0.18224172294139862\n",
      "#>>>    19.11 10.5 \t\t|\t\t 8.61\n",
      "[Feb 25, 09:07:12] 1190 0.5621279910067972\n",
      "\t\t\t\t 0.04136771708726883 0.09245076030492783\n",
      "#>>>    19.52 10.06 \t\t|\t\t 9.459999999999999\n",
      "[Feb 25, 09:07:13] 1191 0.5616996814931827\n",
      "\t\t\t\t 0.10743191093206406 0.1303669959306717\n",
      "#>>>    19.49 10.88 \t\t|\t\t 8.609999999999998\n",
      "[Feb 25, 09:07:13] 1192 0.5613757807111016\n",
      "\t\t\t\t 0.11107581853866577 0.26581642031669617\n",
      "#>>>    19.25 10.8 \t\t|\t\t 8.45\n",
      "[Feb 25, 09:07:13] 1193 0.5611912971692459\n",
      "\t\t\t\t 0.023096326738595963 0.3226242661476135\n",
      "#>>>    19.47 10.4 \t\t|\t\t 9.069999999999999\n",
      "[Feb 25, 09:07:14] 1194 0.5609758264612376\n",
      "\t\t\t\t 0.07353998720645905 0.19038188457489014\n",
      "#>>>    19.32 10.87 \t\t|\t\t 8.450000000000001\n",
      "[Feb 25, 09:07:14] 1195 0.5606787724916565\n",
      "\t\t\t\t 0.025946127250790596 0.07518076151609421\n",
      "#>>>    19.79 9.86 \t\t|\t\t 9.93\n",
      "[Feb 25, 09:07:14] 1196 0.5602192206060691\n",
      "\t\t\t\t 0.0438622310757637 0.2126394659280777\n",
      "#>>>    19.15 10.76 \t\t|\t\t 8.389999999999999\n",
      "[Feb 25, 09:07:15] 1197 0.5599155030899174\n",
      "\t\t\t\t 0.0032858552876859903 0.23462048172950745\n",
      "#>>>    19.35 9.56 \t\t|\t\t 9.790000000000001\n",
      "[Feb 25, 09:07:15] 1198 0.5595934939236119\n",
      "\t\t\t\t 0.028660889714956284 0.07873348146677017\n",
      "#>>>    19.18 9.35 \t\t|\t\t 9.83\n",
      "[Feb 25, 09:07:15] 1199 0.5591412947971447\n",
      "\t\t\t\t 0.048777706921100616 0.20134422183036804\n",
      "#>>>    18.84 10.2 \t\t|\t\t 8.64\n",
      "[Feb 25, 09:07:15] 1200 0.5588322754236484\n",
      "\t\t\t\t 0.03309168294072151 0.04343517869710922\n",
      "#>>>    19.3 8.95 \t\t|\t\t 10.350000000000001\n",
      "[Feb 25, 09:07:16] 1201 0.5583499700135879\n",
      "\t\t\t\t 0.025678304955363274 0.26361310482025146\n",
      "#>>>    18.46 9.36 \t\t|\t\t 9.100000000000001\n",
      "[Feb 25, 09:07:16] 1202 0.5580809114552125\n",
      "\t\t\t\t 0.05356074497103691 0.31557518243789673\n",
      "#>>>    19.23 9.7 \t\t|\t\t 9.530000000000001\n",
      "[Feb 25, 09:07:16] 1203 0.5578919664599904\n",
      "\t\t\t\t 0.06065881997346878 0.08196420222520828\n",
      "#>>>    20.53 12.15 \t\t|\t\t 8.38\n",
      "[Feb 25, 09:07:17] 1204 0.5574766975157291\n",
      "\t\t\t\t 0.02513522282242775 0.23914283514022827\n",
      "#>>>    19.35 9.86 \t\t|\t\t 9.490000000000002\n",
      "[Feb 25, 09:07:17] 1205 0.5571834988724508\n",
      "\t\t\t\t 0.11763421446084976 0.35835546255111694\n",
      "#>>>    19.3 10.9 \t\t|\t\t 8.4\n",
      "[Feb 25, 09:07:17] 1206 0.5571023050431397\n",
      "\t\t\t\t 0.06426678597927094 0.04673874005675316\n",
      "#>>>    19.84 10.69 \t\t|\t\t 9.15\n",
      "[Feb 25, 09:07:18] 1207 0.5566562082678579\n",
      "\t\t\t\t 0.04147571697831154 0.17875558137893677\n",
      "#>>>    19.78 11.0 \t\t|\t\t 8.780000000000001\n",
      "[Feb 25, 09:07:18] 1208 0.556319783354222\n",
      "\t\t\t\t 0.12118302285671234 0.24122852087020874\n",
      "#>>>    18.89 11.02 \t\t|\t\t 7.870000000000001\n",
      "[Feb 25, 09:07:18] 1209 0.5561258751294959\n",
      "\t\t\t\t 0.10356643050909042 0.3621189594268799\n",
      "#>>>    18.73 9.93 \t\t|\t\t 8.8\n",
      "[Feb 25, 09:07:18] 1210 0.5560354346517529\n",
      "\t\t\t\t 0.01290417741984129 0.25039416551589966\n",
      "#>>>    19.93 8.71 \t\t|\t\t 11.219999999999999\n",
      "[Feb 25, 09:07:19] 1211 0.5557426975497923\n",
      "\t\t\t\t 0.017024284228682518 0.04012174531817436\n",
      "#>>>    19.91 11.38 \t\t|\t\t 8.53\n",
      "[Feb 25, 09:07:19] 1212 0.5552441008799268\n",
      "\t\t\t\t 0.029102886095643044 0.25472739338874817\n",
      "#>>>    17.78 10.16 \t\t|\t\t 7.620000000000001\n",
      "[Feb 25, 09:07:19] 1213 0.5549726870641192\n",
      "\t\t\t\t 0.07957935333251953 0.35862353444099426\n",
      "#>>>    19.6 11.39 \t\t|\t\t 8.21\n",
      "[Feb 25, 09:07:20] 1214 0.5548559172648285\n",
      "\t\t\t\t 0.2714093327522278 0.287541002035141\n",
      "#>>>    18.44 10.46 \t\t|\t\t 7.98\n",
      "[Feb 25, 09:07:20] 1215 0.5548600116525488\n",
      "\t\t\t\t 0.06242235377430916 0.4048621952533722\n",
      "#>>>    18.27 10.22 \t\t|\t\t 8.049999999999999\n",
      "[Feb 25, 09:07:20] 1216 0.5547724362010998\n",
      "\t\t\t\t 0.09725790470838547 0.1850298047065735\n",
      "#>>>    18.97 10.74 \t\t|\t\t 8.229999999999999\n",
      "[Feb 25, 09:07:21] 1217 0.5544999514817643\n",
      "\t\t\t\t 0.02220943383872509 0.11865633726119995\n",
      "#>>>    19.18 10.64 \t\t|\t\t 8.54\n",
      "[Feb 25, 09:07:21] 1218 0.5540863173032452\n",
      "\t\t\t\t 0.020117955282330513 0.10523168742656708\n",
      "#>>>    19.43 9.95 \t\t|\t\t 9.48\n",
      "[Feb 25, 09:07:21] 1219 0.5536575806267882\n",
      "\t\t\t\t 0.0058405399322509766 0.1283973902463913\n",
      "#>>>    19.24 9.45 \t\t|\t\t 9.79\n",
      "[Feb 25, 09:07:22] 1220 0.55323816097634\n",
      "\t\t\t\t 0.04560188576579094 0.3688754439353943\n",
      "#>>>    18.75 11.53 \t\t|\t\t 7.220000000000001\n",
      "[Feb 25, 09:07:22] 1221 0.553099400133889\n",
      "\t\t\t\t 0.03515298292040825 0.3472994863986969\n",
      "#>>>    19.32 9.23 \t\t|\t\t 10.09\n",
      "[Feb 25, 09:07:22] 1222 0.5529287531918984\n",
      "\t\t\t\t 0.0027446928434073925 0.2956758439540863\n",
      "#>>>    19.68 9.91 \t\t|\t\t 9.77\n",
      "[Feb 25, 09:07:22] 1223 0.5526742449871455\n",
      "\t\t\t\t 0.03801925852894783 0.08154847472906113\n",
      "#>>>    19.72 8.2 \t\t|\t\t 11.52\n",
      "[Feb 25, 09:07:23] 1224 0.5522411384791416\n",
      "\t\t\t\t 0.0908762663602829 0.17815065383911133\n",
      "#>>>    18.97 10.73 \t\t|\t\t 8.239999999999998\n",
      "[Feb 25, 09:07:23] 1225 0.551957924275763\n",
      "\t\t\t\t 0.0291183702647686 0.22803938388824463\n",
      "#>>>    18.78 10.37 \t\t|\t\t 8.410000000000002\n",
      "[Feb 25, 09:07:23] 1226 0.5516631240944644\n",
      "\t\t\t\t 0.06987082213163376 0.2605898976325989\n",
      "#>>>    18.49 9.96 \t\t|\t\t 8.529999999999998\n",
      "[Feb 25, 09:07:24] 1227 0.5514419216975847\n",
      "\t\t\t\t 0.009097515605390072 0.07560548186302185\n",
      "#>>>    19.1 9.43 \t\t|\t\t 9.670000000000002\n",
      "[Feb 25, 09:07:24] 1228 0.5509751827742868\n",
      "[Feb 25, 09:07:24] #> Done with all triples!\n",
      "#> Saving a checkpoint to .ragatouille/colbert/none/2024-02/25/08.57.38/checkpoints/colbert ..\n",
      "#> Joined...\n"
     ]
    }
   ],
   "source": [
    "from ragatouille import RAGTrainer\n",
    "from ragatouille.utils import get_wikipedia_page\n",
    "\n",
    "\n",
    "trainer = RAGTrainer(model_name = \"fintunedonautomate\",\n",
    "        pretrained_model_name = \"colbert-ir/colbertv2.0\", n_usable_gpus=1) # In this example, we run fine-tuning\n",
    "\n",
    "# This step handles all the data processing, check the examples for more details!\n",
    "trainer.prepare_training_data(raw_data=pairs,\n",
    "                                data_out_path=\"/home/scratch/yifuc/data/colbert\",\n",
    "                                all_documents=documents)\n",
    "\n",
    "trainer.train(batch_size=32) # Train with the default hyperparams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "________________________________________________________________________________\n",
      "WARNING! You have a GPU available, but only `faiss-cpu` is currently installed.\n",
      " This means that indexing will be slow. To make use of your GPU.\n",
      "Please install `faiss-gpu` by running:\n",
      "pip uninstall --y faiss-cpu & pip install faiss-gpu\n",
      " ________________________________________________________________________________\n",
      "Will continue with CPU indexing in 5 seconds...\n",
      "\n",
      "\n",
      "[Feb 25, 09:10:56] #> Creating directory /home/scratch/yifuc/checkpoints/colbert_ft2 \n",
      "\n",
      "\n",
      "[Feb 25, 09:10:56] [0] \t\t #> Encoding 20912 passages..\n",
      "[Feb 25, 09:11:10] [0] \t\t avg_doclen_est = 56.90211486816406 \t len(local_sample) = 20,912\n",
      "[Feb 25, 09:11:10] [0] \t\t Creating 16,384 partitions.\n",
      "[Feb 25, 09:11:10] [0] \t\t *Estimated* 1,189,937 embeddings.\n",
      "[Feb 25, 09:11:10] [0] \t\t #> Saving the indexing plan to /home/scratch/yifuc/checkpoints/colbert_ft2/plan.json ..\n",
      "Clustering 1139937 points in 128D to 16384 clusters, redo 1 times, 20 iterations\n",
      "  Preprocessing in 0.08 s\n",
      "  Iteration 19 (2383.50 s, search 2382.78 s): objective=167882 imbalance=1.387 nsplit=0       \n",
      "[Feb 25, 09:50:54] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Feb 25, 09:50:55] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[0.024, 0.025, 0.028, 0.024, 0.025, 0.029, 0.025, 0.023, 0.023, 0.024, 0.025, 0.024, 0.025, 0.027, 0.025, 0.027, 0.024, 0.024, 0.024, 0.025, 0.024, 0.025, 0.025, 0.027, 0.025, 0.024, 0.026, 0.025, 0.027, 0.026, 0.026, 0.026, 0.025, 0.024, 0.024, 0.024, 0.024, 0.026, 0.025, 0.027, 0.025, 0.026, 0.027, 0.025, 0.026, 0.026, 0.024, 0.026, 0.026, 0.025, 0.024, 0.025, 0.026, 0.025, 0.025, 0.027, 0.027, 0.024, 0.027, 0.025, 0.024, 0.025, 0.027, 0.026, 0.025, 0.027, 0.025, 0.026, 0.025, 0.025, 0.026, 0.024, 0.025, 0.025, 0.024, 0.024, 0.027, 0.025, 0.026, 0.026, 0.027, 0.025, 0.028, 0.025, 0.023, 0.024, 0.027, 0.024, 0.024, 0.026, 0.026, 0.026, 0.024, 0.026, 0.025, 0.026, 0.027, 0.023, 0.026, 0.026, 0.026, 0.025, 0.023, 0.026, 0.027, 0.024, 0.026, 0.026, 0.025, 0.023, 0.026, 0.025, 0.027, 0.024, 0.026, 0.023, 0.026, 0.025, 0.025, 0.026, 0.025, 0.025, 0.025, 0.026, 0.024, 0.026, 0.024, 0.024]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 25, 09:50:55] [0] \t\t #> Encoding 20912 passages..\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:13, 13.76s/it]\n",
      "100%|██████████| 1/1 [00:00<00:00, 670.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 25, 09:51:09] #> Optimizing IVF to store map from centroids to list of pids..\n",
      "[Feb 25, 09:51:09] #> Building the emb2pid mapping..\n",
      "[Feb 25, 09:51:09] len(emb2pid) = 1189937\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 16384/16384 [00:00<00:00, 104372.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 25, 09:51:09] #> Saved optimized IVF to /home/scratch/yifuc/checkpoints/colbert_ft2/ivf.pid.pt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done indexing!\n"
     ]
    }
   ],
   "source": [
    "#evaluate zero shot retrieval \n",
    "from ragatouille import RAGPretrainedModel\n",
    "from ragatouille.utils import get_wikipedia_page\n",
    "\n",
    "RAG = RAGPretrainedModel.from_pretrained(\"/zfsauton2/home/yifuc/11711-RAG/experiment/.ragatouille/colbert/none/2024-02/25/08.57.38/checkpoints/colbert/\", n_gpu=1)\n",
    "index_path = RAG.index(index_name=\"/home/scratch/yifuc/checkpoints/colbert_ft2\", collection=documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading searcher for index /home/scratch/yifuc/checkpoints/colbert_ft2 for the first time... This may take a few seconds\n",
      "[Feb 25, 10:04:32] #> Loading codec...\n",
      "[Feb 25, 10:04:32] #> Loading IVF...\n",
      "[Feb 25, 10:04:32] #> Loading doclens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 1993.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Feb 25, 10:04:32] #> Loading codes and residuals...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 1/1 [00:00<00:00, 95.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searcher loaded!\n",
      "\n",
      "#> QueryTokenizer.tensorize(batch_text[0], batch_background[0], bsize) ==\n",
      "#> Input: . Where can a mural celebrating Buggy be found within CMU, and what does it depict?, \t\t True, \t\t None\n",
      "#> Output IDs: torch.Size([32]), tensor([  101,     1,  2073,  2064,  1037, 15533, 12964, 11829,  6292,  2022,\n",
      "         2179,  2306,  4642,  2226,  1010,  1998,  2054,  2515,  2009, 17120,\n",
      "         1029,   102,   103,   103,   103,   103,   103,   103,   103,   103,\n",
      "          103,   103], device='cuda:0')\n",
      "#> Output Mask: torch.Size([32]), tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "manual accu: 0.3333333333333333\n",
      "automate accu: 0.6081081081081081\n"
     ]
    }
   ],
   "source": [
    "#evaluate zero shot retrieval \n",
    "from ragatouille import RAGPretrainedModel\n",
    "\n",
    "RAG = RAGPretrainedModel.from_index(\"/home/scratch/yifuc/checkpoints/colbert_ft2\")\n",
    "manual_data = jload('/zfsauton2/home/yifuc/11711-RAG/data/questions.json')\n",
    "automate_data = jload('/zfsauton2/home/yifuc/11711-RAG/data/automated_questions.json')\n",
    "\n",
    "manual_correct, manual_total = 0, 0\n",
    "for data in manual_data:\n",
    "    manual_total += 1\n",
    "    question = data['question']\n",
    "    results = RAG.search(question)\n",
    "    for result in results:\n",
    "        if data['answer'] in result['content']:\n",
    "            manual_correct += 1\n",
    "            break\n",
    "print(f'manual accu: {manual_correct/manual_total}')\n",
    "\n",
    "automate_correct, automate_total = 0, 0\n",
    "for data in automate_data:\n",
    "    automate_total += 1\n",
    "    question = data['question']\n",
    "    results = RAG.search(question)\n",
    "    for result in results:\n",
    "        if data['answer'] in result['content']:\n",
    "            automate_correct += 1\n",
    "            break\n",
    "print(f'automate accu: {automate_correct/automate_total}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# reranker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reranker(reranker_model, tokenizer, question, documents):\n",
    "    '''\n",
    "    Reranks the documents based on the question\n",
    "    '''\n",
    "    pairs = [[question, doc] for doc in documents]\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(pairs, padding=True, truncation=True, return_tensors='pt', max_length=512).to(reranker_model.device)\n",
    "        scores = reranker_model(**inputs, return_dict=True).logits.view(-1, ).float()\n",
    "        return scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/zfsauton2/home/yifuc/.conda/envs/llama_hw/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:04<00:00,  1.41s/it]\n",
      "Some weights of MistralForCausalLM were not initialized from the model checkpoint at Salesforce/SFR-Embedding-Mistral and are newly initialized: ['lm_head.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    HfArgumentParser,\n",
    "    GenerationConfig,\n",
    "    AutoModelForSequenceClassification\n",
    ")\n",
    "reranking_prompt = \"<s>[INST]Please rank the following documents based on their relevance to the question. Output rank in a python list format. Below is the question and the documents.\\n Question: {question}\\nDocuments: {context}\\n Output: [/INST]\"\n",
    "reranker_tokenizer = AutoTokenizer.from_pretrained('Salesforce/SFR-Embedding-Mistral', padding_side='left', trust_remote_code=True)\n",
    "reranker_model = AutoModelForSequenceClassification.from_pretrained('Salesforce/SFR-Embedding-Mistral', \n",
    "                                                                    trust_remote_code=True).to('cuda:7')\n",
    "reranker_model.eval()\n",
    "\n",
    "passage = ['apple is bad', 'apple is good', 'banana is good', 'cat loves apple', 'banana is good for health', 'apple is bad for health']\n",
    "question = 'is apple good for health?'\n",
    "\n",
    "documents = [f'Document {i+1}. {p}' for i, p in enumerate(passage)]\n",
    "documents = ' '.join(documents)\n",
    "\n",
    "p = reranking_prompt.format(question=question, context=documents)\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "    max_length=4096, temperature=0.01, top_p=0.95, repetition_penalty=1.1,\n",
    "    do_sample=True, use_cache=True,\n",
    "    eos_token_id=reranker_tokenizer.eos_token_id, pad_token_id=reranker_tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "inputs_with_doc = reranker_tokenizer(p, return_tensors=\"pt\", return_attention_mask=False).to(reranker_model.device)\n",
    "answers = reranker_model.generate(**inputs_with_doc, pad_token_id=reranker_tokenizer.eos_token_id, generation_config=generation_config)\n",
    "answers = reranker_tokenizer.batch_decode(answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/zfsauton2/home/yifuc/.conda/envs/llama_hw/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:05<00:00,  1.67s/it]\n",
      "Some weights of MistralForSequenceClassification were not initialized from the model checkpoint at Salesforce/SFR-Embedding-Mistral and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 10.0138,  -4.5976,   8.0047,  -4.0530,   6.5506,  -1.3838,   8.7122,\n",
       "         -3.0544,  -4.3724, -11.6452,   9.6951,  -5.4365], device='cuda:7')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    HfArgumentParser,\n",
    "    GenerationConfig,\n",
    "    AutoModelForSequenceClassification\n",
    ")\n",
    "import torch\n",
    "reranking_prompt = \"<s>[INST]Please rank the following documents based on their relevance to the question. Output rank in a python list format. Below is the question and the documents.\\n Question: {question}\\nDocuments: {context}\\n Output: [/INST]\"\n",
    "reranker_tokenizer = AutoTokenizer.from_pretrained('Salesforce/SFR-Embedding-Mistral', padding_side='left', trust_remote_code=True)\n",
    "reranker_model = AutoModelForSequenceClassification.from_pretrained('Salesforce/SFR-Embedding-Mistral', \n",
    "                                                                    trust_remote_code=True).to('cuda:7')\n",
    "reranker_model.eval()\n",
    "\n",
    "passage = ['apple is bad', 'apple is good', 'banana is good', 'cat loves apple', 'banana is good for health', 'apple is bad for health']\n",
    "question = 'is apple good for health?'\n",
    "\n",
    "reranker(reranker_model, reranker_tokenizer, question, passage)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_hw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
