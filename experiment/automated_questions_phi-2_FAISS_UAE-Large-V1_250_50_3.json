[
    {
        "question": "Is Jessica Maguire ssica Maguire Assistant to the Institute Director of LTI?, Please answer yes or no: ",
        "context": "Jessica Maguire Assistant to the Institute Director Email: jlm4@andrew.cmu.edu Office: 5721 Gates & Hillman Centers Phone: 412-268-5480",
        "retrieved_doc": "Jessica Maguire Assistant to the Institute Director Email: jlm4@andrew.cmu.edu Office: 5721 Gates & Hillman Centers Phone: 412-268-5480|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|LTI Ph.D. Graduate Student Handbook Page 10 This handbook applies to the LTI Ph.D. The Ph.D. in Language and Information Technologies (LTI Ph.D. ) is focused on understanding and extending the state of the art in computational linguistics, natural language processing, dialogue systems, information retrieval, machine translation, speech processing, video understanding, multimodal systems, automated reasoning, and other topics related to analysis and understanding of unstructured information (e.g., machine learning, and software engineering of intelligent systems). 1.2 Department Personnel The people responsible for administering the LTI Ph.D. degree are: Jamie Callan Ph.D. Program Director Professor GHC 5419 callan@cs.cmu.edu 412-268-4525 Stacey Young Ph.D. Academic Program Manager LTI Graduate Program Manager GHC 6415 staceyy@cs.cmu.edu 412-268-2623 Mona Diab LTI Director Professor GHC 5723 mdiab@andrew.cmu.edu 412-268-3669 Joan Axelson Office Manager GHC 5405 jaxelson@andrew.cmu.edu 412-268-7517 Julie Nys Employment Processes Manger GHC 5405 jnys@andrew.cmu.edu 412-268-3515 1.3 University Policies and Expectations It is the responsibility of each member of the Carnegie Mellon community to be familiar with university policies and guidelines. In addition to this departmental graduate student handbook, the following resources are available to assist you in understanding community expectations. \u2022 The Word/Student Handbook: https://www.cmu.edu/student-a ffairs/theword//index.html \u2022 Academic Integrity Policy: https:// www.cmu.edu/policies/student-and-student-life/academic-integrity.html",
        "retrieved_relevant_part": true,
        "model_answer": "Yes",
        "actual_answer": "Yes",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "How can I contact LTI staff Vicente Malave?",
        "context": "Vicente Malave Data Science Course Developer Email: vlm@andrew.cmu.edu",
        "retrieved_doc": "Vicente Malave Data Science Course Developer Email: vlm@andrew.cmu.edu|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": "You can contact LTI staff Vicente Malave via email at vlm@andrew.cmu.edu or by phone at 412-268-3669.<|endoftext|>",
        "actual_answer": "Email: vlm@andrew.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0.11,
            0.11,
            0.11
        ],
        "recall_max_min_avg": [
            0.5,
            0.5,
            0.5
        ]
    },
    {
        "question": "Please provide the contact information of LTI staff Susan E.",
        "context": "Susan E. Holm Mitamura Lab/Sr. Knowledge Engineer Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591",
        "retrieved_doc": "Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788|Susan E. Holm Mitamura Lab/Sr. Knowledge Engineer Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591",
        "retrieved_relevant_part": true,
        "model_answer": "Susan E. can be contacted via email at sh4s@andrew.cmu.edu or by phone at 412-268-6591.<|endoftext|>",
        "actual_answer": "Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591",
        "exact_match": false,
        "f1_max_min_avg": [
            0.09,
            0.09,
            0.09
        ],
        "recall_max_min_avg": [
            0.12,
            0.12,
            0.12
        ]
    },
    {
        "question": "How can I contact LTI staff Daniel Vosler?",
        "context": "Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu",
        "retrieved_doc": "Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": "You can contact LTI staff Daniel Vosler via email at dvosler@andrew.cmu.edu or by phone at 412-268-3669",
        "actual_answer": "Email: dvosler@andrew.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0.11,
            0.11,
            0.11
        ],
        "recall_max_min_avg": [
            0.5,
            0.5,
            0.5
        ]
    },
    {
        "question": "Please tell me the Email of LTI staff Daniel Vosler based on the background information: ",
        "context": "Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu",
        "retrieved_doc": "Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": "dvosler@andrew.cmu.edu",
        "actual_answer": "Daniel Vosler's Email is dvosler@andrew.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0.33,
            0.33,
            0.33
        ],
        "recall_max_min_avg": [
            0.2,
            0.2,
            0.2
        ]
    },
    {
        "question": "What is the Email of LTI staff Casey Walker?",
        "context": "Casey Walker Academic Program Coordinator Email: clwalker@andrew.cmu.edu Phone: +1 412 268 9315",
        "retrieved_doc": "Casey Walker Academic Program Coordinator Email: clwalker@andrew.cmu.edu Phone: +1 412 268 9315|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": "clwalker@andrew.cmu.edu",
        "actual_answer": "Casey Walker's Email is clwalker@andrew.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0.33,
            0.33,
            0.33
        ],
        "recall_max_min_avg": [
            0.2,
            0.2,
            0.2
        ]
    },
    {
        "question": "What is the Phone of LTI staff Emma Thomas?",
        "context": "Emma Thomas Sponsored Research Administrator Email: emmat@andrew.cmu.edu Office: 5715 Gates & Hillman Centers Phone: 412-268-7812",
        "retrieved_doc": "Emma Thomas Sponsored Research Administrator Email: emmat@andrew.cmu.edu Office: 5715 Gates & Hillman Centers Phone: 412-268-7812|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": "The Phone of LTI staff Emma Thomas is 412-268-7812.<|endoftext|>",
        "actual_answer": "412-268-7812",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Is Kate Schaich kari Yamakawa Mitamura Lab/Associate Data Analyst of LTI?, Please answer yes or no: ",
        "context": "Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_doc": "Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788|Yukari Yamakawa Mitamura Lab/Associate Data Analyst Email: yukariy@andrew.cmu.edu Office: 6717 Gates & Hillman Centers Phone: 412-268-6596|Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu",
        "retrieved_relevant_part": true,
        "model_answer": "Yes, Kate Schaich is an Associate Data Analyst of LTI",
        "actual_answer": "No",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Please tell me the Office of LTI staff Joan Axelson based on the background information: ",
        "context": "Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517",
        "retrieved_doc": "Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|LTI Ph.D. Graduate Student Handbook Page 18 study projects. This personal student-advisor relationship ensures that every student receives the necessary faculty mentoring. The advisor also provides the student with career advice. How a nd when the advisor is selected (The Matching Process) : Advisors are selected during the first month of the program. Students are expected to attend the LTI's Orientation lectures, (Section 3.3) in which faculty describe their interests and areas of expertise, and to be proactive about meeting individually with potential advisors. After meeting with several potential advisors, each student will be asked to indicate a first, second and third choice for advisor. Meanwhile, the potential advisors determine how many new students they can take and indicate their preferences. Finally, the entire LTI faculty meets as a \" Matching Committee\" to determine the best set of matches. Until a student finds a specific advisor, the Progr am Director of the LTI graduate programs serves as the advisor. How to change advisors: The initial advisor selection should be made carefully. However, it is possible to change advisors. To do so, the student should find another faculty member willing to serve as an advisor, and then make a request to the Program Director to switch advisors. The LTI follows the long-standing SCS policy that both the new and old advisors need to agree to the change; typically, this is not a problem (assuming the new advisor has agreed in advance, as described here). It is to the student's advantage to avoid switching advisors, especially late in their graduate studies, because forging a strong student-advisor relationship takes time. 3.3 LTI Orientation At the beginning of each Fall semester, the LTI provides 2-3 weeks of lectures and talks to help students learn about the work done by CMU faculty and to provide an opportunity for advisors to recruit new students. Students are expected to attend them and treat them as seriously a s a standard course, because they provide a good introduction to the broad range of research done at the LTI. Students do not register for the LTI Orientation, nor do they receive a grade; however, the department is serious about its expectation that new students will attend these talks. For many years, these talks were called \"the Immigration Course (IC).\" The intuition behind this name is that the talks help orient newly-arrived students \u2013 immigrants into the CMU environment. As a result, some students u nderstood the name to indicate that the talks would be about US immigration and visa requirements. The older name has been retired; however, people who have been at CMU a long time may occasionally use the older name . 3.4 End of Semester Evaluation Near the e nd of each semester, the student must prepare a statement that describes his or her achievements in the current semester and plans for the next semester. At the end of the semester, the faculty evaluates each student's academic progress. The student's advisor serves as the student's advocate in this process. The result of the evaluation is a letter from",
        "retrieved_relevant_part": true,
        "model_answer": "Joan Axelson is the Office Manager of LTI",
        "actual_answer": "Joan Axelson's Office is Manager",
        "exact_match": false,
        "f1_max_min_avg": [
            0.62,
            0.62,
            0.62
        ],
        "recall_max_min_avg": [
            0.8,
            0.8,
            0.8
        ]
    },
    {
        "question": "How can I contact LTI staff Yukari Yamakawa?",
        "context": "Yukari Yamakawa Mitamura Lab/Associate Data Analyst Email: yukariy@andrew.cmu.edu Office: 6717 Gates & Hillman Centers Phone: 412-268-6596",
        "retrieved_doc": "Yukari Yamakawa Mitamura Lab/Associate Data Analyst Email: yukariy@andrew.cmu.edu Office: 6717 Gates & Hillman Centers Phone: 412-268-6596|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": "You can contact Yukari Yamakawa at yukariy@andrew.cmu.edu or by phone at 412-268-6596",
        "actual_answer": "Email: yukariy@andrew.cmu.edu Office: 6717 Gates & Hillman Centers Phone: 412-268-6596",
        "exact_match": false,
        "f1_max_min_avg": [
            0.18,
            0.18,
            0.18
        ],
        "recall_max_min_avg": [
            0.2,
            0.2,
            0.2
        ]
    },
    {
        "question": "Is Nicole Perrotta cole Perrotta Administrative Coordinator of LTI?, Please answer yes or no: ",
        "context": "Nicole Perrotta Administrative Coordinator Email: nperrott@andrew.cmu.edu",
        "retrieved_doc": "Nicole Perrotta Administrative Coordinator Email: nperrott@andrew.cmu.edu|MIIS Graduate Student Handbook Page 19 Each student has a faculty advisor charged with guiding the education and monitoring the progress of the student through the program. The advisor also supervises the student's directed study projects. This personal student-advisor relationship ensures that every student receives the necessary faculty mentoring. The advisor also provides the student with career advice. How and when the advisor is selected: Advisors are selected during the first month of the program. Students are expected to attend the LTI Orientation Faculty Research Talks in which faculty describe their interests and areas of expertise, and to be proactive about meeting individually with potential advisors. Typically, if a student and faculty member agree to enter an advising relationship, it is approved by the Program Director. If a student has not found an advisor after one month in the program, the Pr ogram Director will assign an advisor to the student. How to change advisors: The initial advisor selection should be made carefully. However, it is possible to change advisors. To do so, the student should find another faculty member willing to serve as an advisor, and then make a request to the Program Director to switch advisors . The LTI follows the long-standing SCS policy that both the new and old advisors need to agree to the change; typically, this is not a problem, assuming the new advisor has agreed in advance, as described here. It is to the student' s advantage to avoid switching advisors, especially late in their graduate studies, because forging a strong student-advisor relationship takes time. 4.11 LTI Orientation Prior to the beginning of each Fall semester the LTI provides lectures and faculty research talks to help students learn about the work done by CMU faculty and to provide an opportunity for advisors to recruit new students. Students are expected to attend them and to treat them as seriously as a course, because they provide a good introduction to the broad range of research done at the LTI. Students do not have to register for the LTI Orientation Faculty Research Talks, nor do they receive a grade, however the department is serious about its expectation that new students will attend these talks. The LTI Orientation Canvas course is a mandatory course designed to increase incoming LTI graduate students' awareness and understanding of CMU's community standards and the resources that CMU offers the students. The course includes several administrative tasks, training, and educational sessions. The students will be enrolled into the course by the program administrator in early August. All MIIS students are expected to complete all assignments by the end of their first semester in the program. Each assignment has its own due date. 4.12 Summary of Graduate Student Appeal and Grievance Procedures Graduate students will find the Summary of Graduate Student Appeal and Grievance Procedures on the Graduate Education Resource webpage. This document summarizes|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669",
        "retrieved_relevant_part": true,
        "model_answer": "Yes",
        "actual_answer": "Yes",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "How can I contact LTI staff Emma Thomas?",
        "context": "Emma Thomas Sponsored Research Administrator Email: emmat@andrew.cmu.edu Office: 5715 Gates & Hillman Centers Phone: 412-268-7812",
        "retrieved_doc": "Emma Thomas Sponsored Research Administrator Email: emmat@andrew.cmu.edu Office: 5715 Gates & Hillman Centers Phone: 412-268-7812|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": "You can contact Emma Thomas via email at emmat@andrew.cmu.edu or by phone at 412-268-7812",
        "actual_answer": "Email: emmat@andrew.cmu.edu Office: 5715 Gates & Hillman Centers Phone: 412-268-7812",
        "exact_match": false,
        "f1_max_min_avg": [
            0.17,
            0.17,
            0.17
        ],
        "recall_max_min_avg": [
            0.2,
            0.2,
            0.2
        ]
    },
    {
        "question": "Please tell me the Email of LTI staff Annette Han based on the background information: ",
        "context": "Annette Han Rose Lab/Data Science Course Developer Email: ahan2@andrew.cmu.edu",
        "retrieved_doc": "Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Annette Han Rose Lab/Data Science Course Developer Email: ahan2@andrew.cmu.edu|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": "ahan2@andrew.cmu.edu",
        "actual_answer": "Annette Han's Email is ahan2@andrew.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0.33,
            0.33,
            0.33
        ],
        "recall_max_min_avg": [
            0.2,
            0.2,
            0.2
        ]
    },
    {
        "question": "What is the Email of LTI staff Nicole Perrotta?",
        "context": "Nicole Perrotta Administrative Coordinator Email: nperrott@andrew.cmu.edu",
        "retrieved_doc": "Nicole Perrotta Administrative Coordinator Email: nperrott@andrew.cmu.edu|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": "nperrott@andrew.cmu.edu",
        "actual_answer": "nperrott@andrew.cmu.edu",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Please tell me the Email of LTI staff Daniel Vosler based on the background information: ",
        "context": "Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu",
        "retrieved_doc": "Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": "dvosler@andrew.cmu.edu",
        "actual_answer": "dvosler@andrew.cmu.edu",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "How can I contact LTI staff Allison Blair?",
        "context": "Allison Blair Associate Business Manager Email: ablair2@cs.cmu.edu Office: 5715 TCS Hall Phone: 412-268-5489",
        "retrieved_doc": "Allison Blair Associate Business Manager Email: ablair2@cs.cmu.edu Office: 5715 TCS Hall Phone: 412-268-5489|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": "You can contact Allison Blair at ablair2@cs.cmu.edu or by phone at 412-268-5489",
        "actual_answer": "Email: ablair2@cs.cmu.edu Office: 5715 TCS Hall Phone: 412-268-5489",
        "exact_match": false,
        "f1_max_min_avg": [
            0.2,
            0.2,
            0.2
        ],
        "recall_max_min_avg": [
            0.25,
            0.25,
            0.25
        ]
    },
    {
        "question": "Is Caitlin Korpus itlin Korpus Senior Academic Program Coordinator of LTI?, Please answer yes or no: ",
        "context": "Caitlin Korpus Senior Academic Program Coordinator Email: ckorpus@andrew.cmu.edu Office: 6719 Gates & Hillman Centers Phone: +1 412 268 7096",
        "retrieved_doc": "Caitlin Korpus Senior Academic Program Coordinator Email: ckorpus@andrew.cmu.edu Office: 6719 Gates & Hillman Centers Phone: +1 412 268 7096|LTI Ph.D. Graduate Student Handbook Page 10 This handbook applies to the LTI Ph.D. The Ph.D. in Language and Information Technologies (LTI Ph.D. ) is focused on understanding and extending the state of the art in computational linguistics, natural language processing, dialogue systems, information retrieval, machine translation, speech processing, video understanding, multimodal systems, automated reasoning, and other topics related to analysis and understanding of unstructured information (e.g., machine learning, and software engineering of intelligent systems). 1.2 Department Personnel The people responsible for administering the LTI Ph.D. degree are: Jamie Callan Ph.D. Program Director Professor GHC 5419 callan@cs.cmu.edu 412-268-4525 Stacey Young Ph.D. Academic Program Manager LTI Graduate Program Manager GHC 6415 staceyy@cs.cmu.edu 412-268-2623 Mona Diab LTI Director Professor GHC 5723 mdiab@andrew.cmu.edu 412-268-3669 Joan Axelson Office Manager GHC 5405 jaxelson@andrew.cmu.edu 412-268-7517 Julie Nys Employment Processes Manger GHC 5405 jnys@andrew.cmu.edu 412-268-3515 1.3 University Policies and Expectations It is the responsibility of each member of the Carnegie Mellon community to be familiar with university policies and guidelines. In addition to this departmental graduate student handbook, the following resources are available to assist you in understanding community expectations. \u2022 The Word/Student Handbook: https://www.cmu.edu/student-a ffairs/theword//index.html \u2022 Academic Integrity Policy: https:// www.cmu.edu/policies/student-and-student-life/academic-integrity.html|LTI Ph.D. Graduate Student Handbook Page 17 projected timeline for completion. Before presenting the proposal, the student will also secure approval for a dissertation committee, which consists of the student\u2019s advisor, at least two other SCS faculty members working in language technologies, and at least one external member. University rules require that the time and place of the proposal presentation be publicly announced at least one week before the presentation. The student should coordinate this with the Ph.D. Program Coordinator. The public thesis proposal protects the student by guaranteeing that the proposed research is interesting to the larger scientific community and demonstrating that the student will finish the program if the work is completed as outlined. \u2022 Thesis Defense: The dissertation document itself, normally expected at the end of the fifth year, will include a detailed description of all the work done, including a clear evaluation and a discussion of its scientific contributions. There are no fixed style or document length guidelines or requirements; the only format requirement is that the title page should be suitable to be issued as an LTI Technical Report. The dissertation defense is a public presentation and defense of the dissertation results. Note: University rules again require that the time and place of the dissertation defense be publicly an nounced at least one week prior to the defense. This should also be coordinated with Ph.D. Program Coordinator. Note: At the time of the public announcement, a draft of the thesis document must be made available online. 3.1.5 Research Speaking Requirement To emphasize student research, improve public speaking skills and increase internal awareness of LTI work, all LTI Ph.D. students must complete an oral presentation at the LTI each year (by the end of May). The presentation should consist of a 20-minute ta lk plus time for discussion. It must be advertised to the LTI mailing lists at least one week before your presentation, and the public will be invited. (The thesis proposal and defense each count towards this requirement.) At least two LTI faculty members will attend the presentation, including the research advisor. After the presentation, these faculty members will convene and provide written feedback to both the student and the Program Director of graduate programs (the form for this is available on the LTI website). This feedback will help the student refine speaking skills with respect to talk structure, content, and delivery. 3.2 Advising Each student has a faculty advisor charged with guiding the education and monitoring the progress of the student through the program. The advisor also supervises the student's directed",
        "retrieved_relevant_part": true,
        "model_answer": "Yes",
        "actual_answer": "Yes",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Please provide the contact information of LTI staff Susan E.",
        "context": "Susan E. Holm Mitamura Lab/Sr. Knowledge Engineer Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591",
        "retrieved_doc": "Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788|Susan E. Holm Mitamura Lab/Sr. Knowledge Engineer Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591",
        "retrieved_relevant_part": true,
        "model_answer": "Susan E. can be contacted via email at sh4s@andrew.cmu.edu or by phone at 412-268-6591.<|endoftext|>",
        "actual_answer": "Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591",
        "exact_match": false,
        "f1_max_min_avg": [
            0.09,
            0.09,
            0.09
        ],
        "recall_max_min_avg": [
            0.12,
            0.12,
            0.12
        ]
    },
    {
        "question": "How can I contact LTI staff Daniel Vosler?",
        "context": "Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu",
        "retrieved_doc": "Daniel Vosler LTI High Performance Cluster Administrator Email: dvosler@andrew.cmu.edu|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": "You can contact LTI staff Daniel Vosler via email at dvosler@andrew.cmu.edu or by phone at 412-268-3669",
        "actual_answer": "Email: dvosler@andrew.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0.11,
            0.11,
            0.11
        ],
        "recall_max_min_avg": [
            0.5,
            0.5,
            0.5
        ]
    },
    {
        "question": "What is the Phone of LTI staff Brianna Eriksen?",
        "context": "Brianna Eriksen Academic Program Manager Email: bfreema2@andrew.cmu.edu Phone: 412-268-4277",
        "retrieved_doc": "Brianna Eriksen Academic Program Manager Email: bfreema2@andrew.cmu.edu Phone: 412-268-4277|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": "The Phone of LTI staff Brianna Eriksen is 412-268-4277",
        "actual_answer": "Brianna Eriksen's Phone is 412-268-4277",
        "exact_match": false,
        "f1_max_min_avg": [
            0.57,
            0.57,
            0.57
        ],
        "recall_max_min_avg": [
            0.8,
            0.8,
            0.8
        ]
    },
    {
        "question": "Please tell me the Phone of LTI staff John Friday based on the background information: ",
        "context": "John Friday Sr. Administrative Coordinator Email: jfriday@andrew.cmu.edu Office: 5715 Gates & Hillman Centers Phone: 412-268-1810",
        "retrieved_doc": "John Friday Sr. Administrative Coordinator Email: jfriday@andrew.cmu.edu Office: 5715 Gates & Hillman Centers Phone: 412-268-1810|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": "The phone number for LTI staff John Friday is 412-268-1810.<|endoftext|>",
        "actual_answer": "412-268-1810",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "What is the Phone of LTI staff Susan E.?",
        "context": "Susan E. Holm Mitamura Lab/Sr. Knowledge Engineer Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591",
        "retrieved_doc": "Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788|Susan E. Holm Mitamura Lab/Sr. Knowledge Engineer Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591",
        "retrieved_relevant_part": true,
        "model_answer": "The Phone of LTI staff Susan E. is 412-268-6591.<|endoftext|>",
        "actual_answer": "412-268-6591",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Is Stacey Young umi Maiti Watanabe Lab/Postdoctoral Research Associate of LTI?, Please answer yes or no: ",
        "context": "Stacey Young Academic Program Manager \u2014 Ph.D. Email: staceyy@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-2623",
        "retrieved_doc": "Soumi Maiti Watanabe Lab/Postdoctoral Research Associate Email: smaiti@andrew.cmu.edu|Susan E. Holm Mitamura Lab/Sr. Knowledge Engineer Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669",
        "retrieved_relevant_part": false,
        "model_answer": "Yes, Stacey Young is an employee of Carnegie Mellon University and holds a postdoctoral research position in the LTI department.<|endoftext|>",
        "actual_answer": "No",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Please provide the contact information of LTI staff Amber Vivis",
        "context": "Amber Vivis Sr. Academic Program Manager - MSAII Email: avivis@cs.cmu.edu Office: 357 TCS Hall Phone: 412-268-9998",
        "retrieved_doc": "Amber Vivis Sr. Academic Program Manager - MSAII Email: avivis@cs.cmu.edu Office: 357 TCS Hall Phone: 412-268-9998|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": "Amber Vivis' email address is avivis@cs.cmu.edu, her office location is in TCS Hall, and her phone number is 412-268-9998.<|endoftext|>",
        "actual_answer": "Email: avivis@cs.cmu.edu Office: 357 TCS Hall Phone: 412-268-9998",
        "exact_match": false,
        "f1_max_min_avg": [
            0.07,
            0.07,
            0.07
        ],
        "recall_max_min_avg": [
            0.12,
            0.12,
            0.12
        ]
    },
    {
        "question": "Please provide the contact information of LTI staff Joan Axelson",
        "context": "Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517",
        "retrieved_doc": "Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|MLT Graduate Student Handbook Page 7 1.3 MLT Contact Information The people responsible for administering the MLT degree are: Kate Schaich Robert Frederking Program Manager, MLT Program Director, MLT Graduate Program Manager, LTI Principal Systems Scientist GHC 6415 GHC 6515 412-268-4788 412-268-6656 kschaich@cs.cmu.edu ref@cs.cmu.edu Robert Frederking Mona Diab Cha ir of Graduate Programs, LTI Director, LTI Principal Systems Scientist GHC 5415 GHC 6515 412-268-6656 mdiab@andrew.cmu.edu In addition, students may confer with the Graduate Education Office (graded@andrew.cmu.edu ) regarding issues of process or other concerns as they navigate conflicts. The Language Technologies Institute is located primarily on the 5 th and 6th floors of the Gates Hillman Complex (GHC) on Carnegie Mellon\u2019s Pittsburgh campus. The address and contact information of the LTI are: Language Technologies Institute Carnegie Mellon University 5000 Forbes Avenue Gates Hillman Complex 5402, LTI Pittsburgh, PA 152 41-3891 412-268-6591 (phone) 412-268-6298 (fax) http://www.lti.cs.cmu.edu/",
        "retrieved_relevant_part": true,
        "model_answer": "Joan Axelson's email address is jaxelson@andrew.cmu.edu and her phone number is 412-268-7517",
        "actual_answer": "Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517",
        "exact_match": false,
        "f1_max_min_avg": [
            0.25,
            0.25,
            0.25
        ],
        "recall_max_min_avg": [
            0.5,
            0.5,
            0.5
        ]
    },
    {
        "question": "Is Wenhe Liu hn Friday Sr. Administrative Coordinator of LTI?, Please answer yes or no: ",
        "context": "Wenhe Liu Research Scholar Email: wenhel@andrew.cmu.edu Office: 5403 Gates & Hillman Centers",
        "retrieved_doc": "Wenhe Liu Research Scholar Email: wenhel@andrew.cmu.edu Office: 5403 Gates & Hillman Centers|LTI Ph.D. Graduate Student Handbook Page 8 A.3.3 Campus Wellness ................................................................................................................... 48 A.3.4 Religious and Spiritual Life Initiatives (RSLI) ................................................................ 48 A.3.5 University Police ..................................................................................................................... 48 A.3.6 Shuttle and Escort Services .................................................................................................. 49 A.4 The WORD ....................................................................................................................................... 49|Fall offering: Course: 11700 Title: LTI Colloquium Units: 6.0 Lec/Sec: Section PP Days: To be announced Begin: 12:30PM End: 01:50PM Bldg/Room: TBA Location: Lisbon, Portugal Instructor(s): Diaz, Bisk ",
        "retrieved_relevant_part": true,
        "model_answer": "Yes",
        "actual_answer": "No",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "How can I contact LTI staff Brianna Eriksen?",
        "context": "Brianna Eriksen Academic Program Manager Email: bfreema2@andrew.cmu.edu Phone: 412-268-4277",
        "retrieved_doc": "Brianna Eriksen Academic Program Manager Email: bfreema2@andrew.cmu.edu Phone: 412-268-4277|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": "You can contact Brianna Eriksen via email at bfreema2@andrew.cmu.edu or by phone at 412-268-4277",
        "actual_answer": "Email: bfreema2@andrew.cmu.edu Phone: 412-268-4277",
        "exact_match": false,
        "f1_max_min_avg": [
            0.22,
            0.22,
            0.22
        ],
        "recall_max_min_avg": [
            0.5,
            0.5,
            0.5
        ]
    },
    {
        "question": "What is the Phone of LTI staff Kate Schaich?",
        "context": "Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_doc": "Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|MLT Graduate Student Handbook Page 7 1.3 MLT Contact Information The people responsible for administering the MLT degree are: Kate Schaich Robert Frederking Program Manager, MLT Program Director, MLT Graduate Program Manager, LTI Principal Systems Scientist GHC 6415 GHC 6515 412-268-4788 412-268-6656 kschaich@cs.cmu.edu ref@cs.cmu.edu Robert Frederking Mona Diab Cha ir of Graduate Programs, LTI Director, LTI Principal Systems Scientist GHC 5415 GHC 6515 412-268-6656 mdiab@andrew.cmu.edu In addition, students may confer with the Graduate Education Office (graded@andrew.cmu.edu ) regarding issues of process or other concerns as they navigate conflicts. The Language Technologies Institute is located primarily on the 5 th and 6th floors of the Gates Hillman Complex (GHC) on Carnegie Mellon\u2019s Pittsburgh campus. The address and contact information of the LTI are: Language Technologies Institute Carnegie Mellon University 5000 Forbes Avenue Gates Hillman Complex 5402, LTI Pittsburgh, PA 152 41-3891 412-268-6591 (phone) 412-268-6298 (fax) http://www.lti.cs.cmu.edu/",
        "retrieved_relevant_part": true,
        "model_answer": "412-268-4788",
        "actual_answer": "Kate Schaich's Phone is 412-268-4788",
        "exact_match": false,
        "f1_max_min_avg": [
            0.33,
            0.33,
            0.33
        ],
        "recall_max_min_avg": [
            0.2,
            0.2,
            0.2
        ]
    },
    {
        "question": "Is Zaid Sheikh i-Qi Cheng Hauptmann Lab/Postdoctoral Research Associate of LTI?, Please answer yes or no: ",
        "context": "Zaid Sheikh Neubig Lab/Senior Systems/Software Engineer Email: zsheikh@andrew.cmu.edu Office: 6416 Gates & Hillman Centers Phone:",
        "retrieved_doc": "Zhi-Qi Cheng Hauptmann Lab/Postdoctoral Research Associate Email: zhiqic@andrew.cmu.edu|Zaid Sheikh Neubig Lab/Senior Systems/Software Engineer Email: zsheikh@andrew.cmu.edu Office: 6416 Gates & Hillman Centers Phone:|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669",
        "retrieved_relevant_part": true,
        "model_answer": "Yes, Zaid Sheikh is an employee of Carnegie Mellon University and holds a postdoctoral research associate position at the LTI",
        "actual_answer": "No",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "What is the Email of LTI staff Joan Axelson?",
        "context": "Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517",
        "retrieved_doc": "Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": "jaxelson@andrew.cmu.edu",
        "actual_answer": "jaxelson@andrew.cmu.edu",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Please provide the contact information of LTI staff Annette Han",
        "context": "Annette Han Rose Lab/Data Science Course Developer Email: ahan2@andrew.cmu.edu",
        "retrieved_doc": "Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788|Annette Han Rose Lab/Data Science Course Developer Email: ahan2@andrew.cmu.edu",
        "retrieved_relevant_part": true,
        "model_answer": "Annette Han's email address is ahan2@andrew.cmu.edu, her phone number is 412-268-4788, and her office location is 6415 Gates & Hillman Centers.<|endoftext|>",
        "actual_answer": "Email: ahan2@andrew.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Please provide the contact information of LTI staff Kira Sullivan",
        "context": "Kira Sullivan Administrative Coordinator Email: kiras@andrew.cmu.edu Phone: 412-268-8737",
        "retrieved_doc": "Kira Sullivan Administrative Coordinator Email: kiras@andrew.cmu.edu Phone: 412-268-8737|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": "Kira Sullivan's email address is kiras@andrew.cmu.edu, her phone number is 412-268-8737, and her office location is 6415 Gates & Hillman Centers.<|endoftext|>",
        "actual_answer": "Email: kiras@andrew.cmu.edu Phone: 412-268-8737",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Please tell me the Phone of LTI staff Yukari Yamakawa based on the background information: ",
        "context": "Yukari Yamakawa Mitamura Lab/Associate Data Analyst Email: yukariy@andrew.cmu.edu Office: 6717 Gates & Hillman Centers Phone: 412-268-6596",
        "retrieved_doc": "Yukari Yamakawa Mitamura Lab/Associate Data Analyst Email: yukariy@andrew.cmu.edu Office: 6717 Gates & Hillman Centers Phone: 412-268-6596|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": "412-268-6596",
        "actual_answer": "Yukari Yamakawa's Phone is 412-268-6596",
        "exact_match": false,
        "f1_max_min_avg": [
            0.33,
            0.33,
            0.33
        ],
        "recall_max_min_avg": [
            0.2,
            0.2,
            0.2
        ]
    },
    {
        "question": "Please tell me the Email of LTI staff Robin Hammer based on the background information: ",
        "context": "Robin Hammer Levin Lab/Project Coordinator Email: robinham@andrew.cmu.edu",
        "retrieved_doc": "Robin Hammer Levin Lab/Project Coordinator Email: robinham@andrew.cmu.edu|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": "robinham@andrew.cmu.edu",
        "actual_answer": "robinham@andrew.cmu.edu",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "How can I contact LTI staff Susan E.?",
        "context": "Susan E. Holm Mitamura Lab/Sr. Knowledge Engineer Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591",
        "retrieved_doc": "Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788|Susan E. Holm Mitamura Lab/Sr. Knowledge Engineer Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591",
        "retrieved_relevant_part": true,
        "model_answer": "You can contact Susan E. at sh4s@andrew.cmu.edu or by phone at 412-268-6591.<|endoftext|>",
        "actual_answer": "Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591",
        "exact_match": false,
        "f1_max_min_avg": [
            0.1,
            0.1,
            0.1
        ],
        "recall_max_min_avg": [
            0.12,
            0.12,
            0.12
        ]
    },
    {
        "question": "What is the Email of LTI staff Zhi-Qi Cheng?",
        "context": "Zhi-Qi Cheng Hauptmann Lab/Postdoctoral Research Associate Email: zhiqic@andrew.cmu.edu",
        "retrieved_doc": "Zhi-Qi Cheng Hauptmann Lab/Postdoctoral Research Associate Email: zhiqic@andrew.cmu.edu|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": "zhiqic@andrew.cmu.edu",
        "actual_answer": "Zhi-Qi Cheng's Email is zhiqic@andrew.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0.33,
            0.33,
            0.33
        ],
        "recall_max_min_avg": [
            0.2,
            0.2,
            0.2
        ]
    },
    {
        "question": "Is Jessica Maguire san E. Holm Mitamura Lab/Sr. Knowledge Engineer of LTI?, Please answer yes or no: ",
        "context": "Jessica Maguire Assistant to the Institute Director Email: jlm4@andrew.cmu.edu Office: 5721 Gates & Hillman Centers Phone: 412-268-5480",
        "retrieved_doc": "Susan E. Holm Mitamura Lab/Sr. Knowledge Engineer Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591|LTI Ph.D. Graduate Student Handbook Page 10 This handbook applies to the LTI Ph.D. The Ph.D. in Language and Information Technologies (LTI Ph.D. ) is focused on understanding and extending the state of the art in computational linguistics, natural language processing, dialogue systems, information retrieval, machine translation, speech processing, video understanding, multimodal systems, automated reasoning, and other topics related to analysis and understanding of unstructured information (e.g., machine learning, and software engineering of intelligent systems). 1.2 Department Personnel The people responsible for administering the LTI Ph.D. degree are: Jamie Callan Ph.D. Program Director Professor GHC 5419 callan@cs.cmu.edu 412-268-4525 Stacey Young Ph.D. Academic Program Manager LTI Graduate Program Manager GHC 6415 staceyy@cs.cmu.edu 412-268-2623 Mona Diab LTI Director Professor GHC 5723 mdiab@andrew.cmu.edu 412-268-3669 Joan Axelson Office Manager GHC 5405 jaxelson@andrew.cmu.edu 412-268-7517 Julie Nys Employment Processes Manger GHC 5405 jnys@andrew.cmu.edu 412-268-3515 1.3 University Policies and Expectations It is the responsibility of each member of the Carnegie Mellon community to be familiar with university policies and guidelines. In addition to this departmental graduate student handbook, the following resources are available to assist you in understanding community expectations. \u2022 The Word/Student Handbook: https://www.cmu.edu/student-a ffairs/theword//index.html \u2022 Academic Integrity Policy: https:// www.cmu.edu/policies/student-and-student-life/academic-integrity.html|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669",
        "retrieved_relevant_part": false,
        "model_answer": "Yes",
        "actual_answer": "No",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Is Susan E. san E. Holm Mitamura Lab/Sr. Knowledge Engineer of LTI?, Please answer yes or no: ",
        "context": "Susan E. Holm Mitamura Lab/Sr. Knowledge Engineer Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591",
        "retrieved_doc": "Susan E. Holm Mitamura Lab/Sr. Knowledge Engineer Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591|Teruko Mitamura Research Professor Email: teruko@cs.cmu.edu Office: 6711 Gates & Hillman Centers Phone: 412-268-6596 Research Areas: Information Extraction, Summarization and Question Answering, Information Retrieval, Text Mining and Analytics, Language Technologies for Education, Natural Language Processing and Computational Linguistics|Undergraduate LT Concentration: Human language technologies have become an increasingly central component of computer science. Information retrieval, machine translation and speech technology are used daily by the general public, while text mining, natural language processing and language-based tutoring are common within more specialized professional or educational environments. The LTI prepares students for this world by offering a minor that gives you the opportunity to not only learn about language technologies, but to also apply that knowledge through a directed project.",
        "retrieved_relevant_part": true,
        "model_answer": "Yes",
        "actual_answer": "Yes",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Is Vicente Malave cente Malave Data Science Course Developer of LTI?, Please answer yes or no: ",
        "context": "Vicente Malave Data Science Course Developer Email: vlm@andrew.cmu.edu",
        "retrieved_doc": "Vicente Malave Data Science Course Developer Email: vlm@andrew.cmu.edu|Spring offering: Course: 18668 Title: Data Science for Software Engineering Units: 12.0 Lec/Sec: Section SV Days: Friday Begin: 12:30PM End: 01:50PM Bldg/Room: B23 109 Location: San Jose, California Instructor(s): da Silva Sousa |Spring offering: Course: 18668 Title: Data Science for Software Engineering Units: 12.0 Lec/Sec: Section SV Days: Monday, Wednesday Begin: 03:00PM End: 04:50PM Bldg/Room: B23 109 Location: San Jose, California Instructor(s): da Silva Sousa ",
        "retrieved_relevant_part": true,
        "model_answer": "Yes",
        "actual_answer": "Yes",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "What is the Email of LTI staff Julie Nys?",
        "context": "Julie Nys Employment Processes Manager Email: jnys@andrew.cmu.edu Office: 5405 TCS Hall Phone: 412-268-3515",
        "retrieved_doc": "Julie Nys Employment Processes Manager Email: jnys@andrew.cmu.edu Office: 5405 TCS Hall Phone: 412-268-3515|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": "jnys@andrew.cmu.edu",
        "actual_answer": "Julie Nys's Email is jnys@andrew.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0.33,
            0.33,
            0.33
        ],
        "recall_max_min_avg": [
            0.2,
            0.2,
            0.2
        ]
    },
    {
        "question": "Is Casey Walker nnifer Clark Administrative Coordinator of LTI?, Please answer yes or no: ",
        "context": "Casey Walker Academic Program Coordinator Email: clwalker@andrew.cmu.edu Phone: +1 412 268 9315",
        "retrieved_doc": "Casey Walker Academic Program Coordinator Email: clwalker@andrew.cmu.edu Phone: +1 412 268 9315|Jennifer Clark Administrative Coordinator Email: jclark2@andrew.cmu.edu|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669",
        "retrieved_relevant_part": true,
        "model_answer": "Yes, Jennifer Clark is the Administrative Coordinator of LTI.<|endoftext|>",
        "actual_answer": "No",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "How can I contact LTI staff Daniel Gongaware?",
        "context": "Daniel Gongaware Sponsored Research Administrator Email: dgongawa@cs.cmu.edu Office: 355 TCS Hall Phone: 412-268-6195",
        "retrieved_doc": "Daniel Gongaware Sponsored Research Administrator Email: dgongawa@cs.cmu.edu Office: 355 TCS Hall Phone: 412-268-6195|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": "You can contact LTI staff Daniel Gongaware via email at dgongawa@cs.cmu.edu or by phone at 412-268-6195",
        "actual_answer": "Email: dgongawa@cs.cmu.edu Office: 355 TCS Hall Phone: 412-268-6195",
        "exact_match": false,
        "f1_max_min_avg": [
            0.17,
            0.17,
            0.17
        ],
        "recall_max_min_avg": [
            0.25,
            0.25,
            0.25
        ]
    },
    {
        "question": "Please tell me the Office of LTI staff Bryan Burtner based on the background information: ",
        "context": "Bryan Burtner Associate Business Manager Email: bburtner@cs.cmu.edu Office: 6413 TCS Hall Phone: 412-268-2805",
        "retrieved_doc": "Bryan Burtner Associate Business Manager Email: bburtner@cs.cmu.edu Office: 6413 TCS Hall Phone: 412-268-2805|LTI Ph.D. Graduate Student Handbook Page 8 A.3.3 Campus Wellness ................................................................................................................... 48 A.3.4 Religious and Spiritual Life Initiatives (RSLI) ................................................................ 48 A.3.5 University Police ..................................................................................................................... 48 A.3.6 Shuttle and Escort Services .................................................................................................. 49 A.4 The WORD ....................................................................................................................................... 49|LTI Ph.D. Graduate Student Handbook Page 13 412-268-6298 (fax) http://www.lti.cs.cmu.edu/ Office locations for faculty, staff, and Ph.D. students can be found on the LTI website. 2.2 Office Space Most Ph.D. students in residence on the Pittsburgh campus are given an office in which to study and do research. Typically, offices are shared with other Ph.D. students, but they may also be shared with staff, visitors, or other members of the LTI. Offices are assigned by the LTI\u2019s Office Manager (see Section 1.2 , Department Personnel, for contact information). 2.3 Mailboxes and Office Supplies Mailboxes and office supplies are in GHC 5404. 2.4 Photocopies and Printers Printers and photocopies are available to LTI students. The use of a photocopier or printer requires you to log in with your CMU ID card. LTI students may use printers/photocopiers scattered throughout the School of Computer Science buildings , but the machines in GHC 5404 and GHC 6604 are the most convenient . The SCS Computing Facilities publishes a list of printers online at https://computing.cs.cmu.edu/desktop/printer-list. 2.5 Computers for LTI Ph.D. Students Ph.D. students are responsible for having their own laptop computers to support their education and research. Students are free to choose their own operating system (e.g., Linux, MacOs, Windows). Many Ph.D. advisors also provide access to computer clusters, cloud computing, or other resources to support computationally-intense research. Ph.D. students are given access to the LTI\u2019s computer cluster on an as-needed basis, to be used for course assignments, directed study projects, and/or capstone projects. The LTI cluster provides storage and computation for projects involving large datasets and/or lengthy computation. Ph.D. students receive two types of user ids: An Andrew id and a CS id. All CMU students have an Andrew id. Computer Science students also have a CS id that provides access to SCS-specific resources (e.g., computer clusters). CS ids are being phased out very slowly, so it is likely that you will need both types of user id",
        "retrieved_relevant_part": true,
        "model_answer": "Bryan Burtner is an Associate Business Manager at LTI",
        "actual_answer": "Bryan Burtner's Office is 6413 TCS Hall",
        "exact_match": false,
        "f1_max_min_avg": [
            0.25,
            0.25,
            0.25
        ],
        "recall_max_min_avg": [
            0.29,
            0.29,
            0.29
        ]
    },
    {
        "question": "How can I contact LTI staff Kira Sullivan?",
        "context": "Kira Sullivan Administrative Coordinator Email: kiras@andrew.cmu.edu Phone: 412-268-8737",
        "retrieved_doc": "Kira Sullivan Administrative Coordinator Email: kiras@andrew.cmu.edu Phone: 412-268-8737|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": "You can contact Kira Sullivan via email at kiras@andrew.cmu.edu or by phone at 412-268-8737",
        "actual_answer": "Email: kiras@andrew.cmu.edu Phone: 412-268-8737",
        "exact_match": false,
        "f1_max_min_avg": [
            0.22,
            0.22,
            0.22
        ],
        "recall_max_min_avg": [
            0.5,
            0.5,
            0.5
        ]
    },
    {
        "question": "What is the Email of LTI staff Zhong-Qiu Wang?",
        "context": "Zhong-Qiu Wang Wantanabe Lab/Postdoctoral Research Associate Email: zhongqiu@andrew.cmu.edu Office: 6414 Gates & Hillman Centers Phone: 415-200-8662",
        "retrieved_doc": "Zhong-Qiu Wang Wantanabe Lab/Postdoctoral Research Associate Email: zhongqiu@andrew.cmu.edu Office: 6414 Gates & Hillman Centers Phone: 415-200-8662|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": "zhongqiu@andrew.cmu.edu",
        "actual_answer": "zhongqiu@andrew.cmu.edu",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "What is the Email of LTI staff Wenhe Liu?",
        "context": "Wenhe Liu Research Scholar Email: wenhel@andrew.cmu.edu Office: 5403 Gates & Hillman Centers",
        "retrieved_doc": "Wenhe Liu Research Scholar Email: wenhel@andrew.cmu.edu Office: 5403 Gates & Hillman Centers|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669",
        "retrieved_relevant_part": true,
        "model_answer": "Wenhe Liu's email address is wenhel@andrew.cmu.edu",
        "actual_answer": "Wenhe Liu's Email is wenhel@andrew.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0.73,
            0.73,
            0.73
        ],
        "recall_max_min_avg": [
            0.8,
            0.8,
            0.8
        ]
    },
    {
        "question": "What is the Phone of LTI staff Stacey Young?",
        "context": "Stacey Young Academic Program Manager \u2014 Ph.D. Email: staceyy@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-2623",
        "retrieved_doc": "Stacey Young Academic Program Manager \u2014 Ph.D. Email: staceyy@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-2623|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": "Stacey Young's phone number is 412-268-2623.<|endoftext|>",
        "actual_answer": "Stacey Young's Phone is 412-268-2623",
        "exact_match": false,
        "f1_max_min_avg": [
            0.55,
            0.55,
            0.55
        ],
        "recall_max_min_avg": [
            0.6,
            0.6,
            0.6
        ]
    },
    {
        "question": "Please tell me the Email of LTI staff Joan Axelson based on the background information: ",
        "context": "Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517",
        "retrieved_doc": "Joan Axelson Office Manager Email: jaxelson@andrew.cmu.edu Phone: 412-268-7517|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": "jaxelson@andrew.cmu.edu",
        "actual_answer": "jaxelson@andrew.cmu.edu",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Please provide the contact information of LTI staff Jennifer Lucas",
        "context": "Jennifer Lucas Academic Program Manager \u2014 MCDS Email: jmlucas@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-9870",
        "retrieved_doc": "Jennifer Lucas Academic Program Manager \u2014 MCDS Email: jmlucas@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-9870|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": "Jennifer Lucas' email address is jmlucas@cs.cmu.edu, her office location is 6415 Gates & Hillman Centers, and her phone number is 412-268-9870",
        "actual_answer": "Email: jmlucas@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-9870",
        "exact_match": false,
        "f1_max_min_avg": [
            0.32,
            0.32,
            0.32
        ],
        "recall_max_min_avg": [
            0.5,
            0.5,
            0.5
        ]
    },
    {
        "question": "What is the Phone of LTI staff Allison Blair?",
        "context": "Allison Blair Associate Business Manager Email: ablair2@cs.cmu.edu Office: 5715 TCS Hall Phone: 412-268-5489",
        "retrieved_doc": "Allison Blair Associate Business Manager Email: ablair2@cs.cmu.edu Office: 5715 TCS Hall Phone: 412-268-5489|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Summer 1/Summer All offering: Course: 11935 Title: LTI Practicum Units: 1-36 Lec/Sec: Section P Days: To be announced Begin: 08:00AM End: 09:20AM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Callan ",
        "retrieved_relevant_part": true,
        "model_answer": "The Phone of LTI staff Allison Blair is 412-268-5489.<|endoftext|>",
        "actual_answer": "412-268-5489",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Please provide the contact information of LTI faculty Kemal Oflazer",
        "context": "Kemal Oflazer Teaching Professor of Computer Science Email: ko@qatar.cmu.edu Office: 1009 Carnegie Mellon - Qatar Campus Phone:",
        "retrieved_doc": "Kemal Oflazer Teaching Professor of Computer Science Email: ko@qatar.cmu.edu Office: 1009 Carnegie Mellon - Qatar Campus Phone:|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|MLT Graduate Student Handbook Page 7 1.3 MLT Contact Information The people responsible for administering the MLT degree are: Kate Schaich Robert Frederking Program Manager, MLT Program Director, MLT Graduate Program Manager, LTI Principal Systems Scientist GHC 6415 GHC 6515 412-268-4788 412-268-6656 kschaich@cs.cmu.edu ref@cs.cmu.edu Robert Frederking Mona Diab Cha ir of Graduate Programs, LTI Director, LTI Principal Systems Scientist GHC 5415 GHC 6515 412-268-6656 mdiab@andrew.cmu.edu In addition, students may confer with the Graduate Education Office (graded@andrew.cmu.edu ) regarding issues of process or other concerns as they navigate conflicts. The Language Technologies Institute is located primarily on the 5 th and 6th floors of the Gates Hillman Complex (GHC) on Carnegie Mellon\u2019s Pittsburgh campus. The address and contact information of the LTI are: Language Technologies Institute Carnegie Mellon University 5000 Forbes Avenue Gates Hillman Complex 5402, LTI Pittsburgh, PA 152 41-3891 412-268-6591 (phone) 412-268-6298 (fax) http://www.lti.cs.cmu.edu/",
        "retrieved_relevant_part": true,
        "model_answer": "Kemal Oflazer's email address is ko@qatar.cmu.edu and his phone number is 412-268-3669.<|endoftext|>",
        "actual_answer": "Email: ko@qatar.cmu.edu Office: 1009 Carnegie Mellon - Qatar Campus Phone:",
        "exact_match": false,
        "f1_max_min_avg": [
            0.09,
            0.09,
            0.09
        ],
        "recall_max_min_avg": [
            0.1,
            0.1,
            0.1
        ]
    },
    {
        "question": "Is Teruko Mitamura ruko Mitamura Research Professor of LTI?, Please answer yes or no: ",
        "context": "Teruko Mitamura Research Professor Email: teruko@cs.cmu.edu Office: 6711 Gates & Hillman Centers Phone: 412-268-6596 Research Areas: Information Extraction, Summarization and Question Answering, Information Retrieval, Text Mining and Analytics, Language Technologies for Education, Natural Language Processing and Computational Linguistics",
        "retrieved_doc": "Teruko Mitamura Research Professor Email: teruko@cs.cmu.edu Office: 6711 Gates & Hillman Centers Phone: 412-268-6596 Research Areas: Information Extraction, Summarization and Question Answering, Information Retrieval, Text Mining and Analytics, Language Technologies for Education, Natural Language Processing and Computational Linguistics|Susan E. Holm Mitamura Lab/Sr. Knowledge Engineer Email: sh4s@andrew.cmu.edu Office: 3128 Wean Hall Phone: 412-268-6591|LTI Ph.D. Graduate Student Handbook Page 2",
        "retrieved_relevant_part": true,
        "model_answer": "Yes",
        "actual_answer": "Yes",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Is Chenyan Xiong i Li Assistant Professor of LTI?, Please answer yes or no: ",
        "context": "Chenyan Xiong Associate Professor Email: cx@andrew.cmu.edu Phone: 412-268-7641",
        "retrieved_doc": "Chenyan Xiong Associate Professor Email: cx@andrew.cmu.edu Phone: 412-268-7641|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Ph.D. in Language and Information Technology: The Ph.D. in LTI focuses on developing the next generation of scientific and entrepreneurial leaders. The first two years of the Ph.D. program are similar to our MLT program. After the second year, you will spend most of your time working closely with your faculty advisor on research that advances the state-of-the-art in computer science. Ph.D. students are expected to publish papers about original research in the most competitive scientific journals and international conference proceedings, and to present their research at conferences and workshops. Most of our Ph.D. graduates become professors and research scientists, while a few have started their own companies.",
        "retrieved_relevant_part": true,
        "model_answer": "Yes",
        "actual_answer": "No",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "How can I contact LTI faculty Eric Nyberg?",
        "context": "Eric Nyberg Professor Email: ehn@cs.cmu.edu Office: 6715 Gates & Hillman Centers Phone: 412-268-7281 Research Areas: Information Extraction, Summarization and Question Answering, Information Retrieval, Text Mining and Analytics, Language Technologies for Education",
        "retrieved_doc": "Eric Nyberg Professor Email: ehn@cs.cmu.edu Office: 6715 Gates & Hillman Centers Phone: 412-268-7281 Research Areas: Information Extraction, Summarization and Question Answering, Information Retrieval, Text Mining and Analytics, Language Technologies for Education|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": "You can contact LTI faculty Eric Nyberg via email at ehn@cs.cmu.edu or by phone at 412-268-7281",
        "actual_answer": "Email: ehn@cs.cmu.edu Office: 6715 Gates & Hillman Centers Phone: 412-268-7281",
        "exact_match": false,
        "f1_max_min_avg": [
            0.15,
            0.15,
            0.15
        ],
        "recall_max_min_avg": [
            0.2,
            0.2,
            0.2
        ]
    },
    {
        "question": "Please provide the contact information of LTI faculty Bhiksha Ramakrishnan",
        "context": "Bhiksha Ramakrishnan Professor Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826 Research Areas: Machine Learning, Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing, Privacy",
        "retrieved_doc": "Bhiksha Ramakrishnan Professor Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826 Research Areas: Machine Learning, Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing, Privacy|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": "Bhiksha Ramakrishnan's email address is bhiksha@cs.cmu.edu, office location is at 6705 Gates & Hillman Centers, and phone number is 412-268-9826",
        "actual_answer": "Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826",
        "exact_match": false,
        "f1_max_min_avg": [
            0.33,
            0.33,
            0.33
        ],
        "recall_max_min_avg": [
            0.5,
            0.5,
            0.5
        ]
    },
    {
        "question": "Is Bhiksha Ramakrishnan iksha Ramakrishnan Professor of LTI?, Please answer yes or no: ",
        "context": "Bhiksha Ramakrishnan Professor Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826 Research Areas: Machine Learning, Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing, Privacy",
        "retrieved_doc": "Bhiksha Ramakrishnan Professor Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826 Research Areas: Machine Learning, Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing, Privacy|Ph.D. in Language and Information Technology: The Ph.D. in LTI focuses on developing the next generation of scientific and entrepreneurial leaders. The first two years of the Ph.D. program are similar to our MLT program. After the second year, you will spend most of your time working closely with your faculty advisor on research that advances the state-of-the-art in computer science. Ph.D. students are expected to publish papers about original research in the most competitive scientific journals and international conference proceedings, and to present their research at conferences and workshops. Most of our Ph.D. graduates become professors and research scientists, while a few have started their own companies.|Spring offering: Course: 11700 Title: LTI Colloquium Units: 6.0 Lec/Sec: Section A Days: Friday Begin: 02:00PM End: 03:20PM Bldg/Room: SH 105 Location: Pittsburgh, Pennsylvania Instructor(s): Bisk, Diaz ",
        "retrieved_relevant_part": true,
        "model_answer": "Yes",
        "actual_answer": "Yes",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "What is the Office of LTI faculty Lori Levin?",
        "context": "Lori Levin Research Professor Email: lsl@cs.cmu.edu Office: 5717 Gates & Hillman Centers Phone: 412-268-6193 Research Areas: Machine Translation, Natural Language Processing and Computational Linguistics, Corpus Annotation and Resources",
        "retrieved_doc": "Lori Levin Research Professor Email: lsl@cs.cmu.edu Office: 5717 Gates & Hillman Centers Phone: 412-268-6193 Research Areas: Machine Translation, Natural Language Processing and Computational Linguistics, Corpus Annotation and Resources|LTI Ph.D. Graduate Student Handbook Page 2|LTI Ph.D. Graduate Student Handbook Page 10 This handbook applies to the LTI Ph.D. The Ph.D. in Language and Information Technologies (LTI Ph.D. ) is focused on understanding and extending the state of the art in computational linguistics, natural language processing, dialogue systems, information retrieval, machine translation, speech processing, video understanding, multimodal systems, automated reasoning, and other topics related to analysis and understanding of unstructured information (e.g., machine learning, and software engineering of intelligent systems). 1.2 Department Personnel The people responsible for administering the LTI Ph.D. degree are: Jamie Callan Ph.D. Program Director Professor GHC 5419 callan@cs.cmu.edu 412-268-4525 Stacey Young Ph.D. Academic Program Manager LTI Graduate Program Manager GHC 6415 staceyy@cs.cmu.edu 412-268-2623 Mona Diab LTI Director Professor GHC 5723 mdiab@andrew.cmu.edu 412-268-3669 Joan Axelson Office Manager GHC 5405 jaxelson@andrew.cmu.edu 412-268-7517 Julie Nys Employment Processes Manger GHC 5405 jnys@andrew.cmu.edu 412-268-3515 1.3 University Policies and Expectations It is the responsibility of each member of the Carnegie Mellon community to be familiar with university policies and guidelines. In addition to this departmental graduate student handbook, the following resources are available to assist you in understanding community expectations. \u2022 The Word/Student Handbook: https://www.cmu.edu/student-a ffairs/theword//index.html \u2022 Academic Integrity Policy: https:// www.cmu.edu/policies/student-and-student-life/academic-integrity.html",
        "retrieved_relevant_part": true,
        "model_answer": "The Office of LTI faculty Lori Levin is located at 5717 Gates & Hillman Centers and can be contacted via phone at 412-268-6193.<|endoftext|>",
        "actual_answer": "Lori Levin's Office is 5717 Gates & Hillman Centers",
        "exact_match": false,
        "f1_max_min_avg": [
            0.5,
            0.5,
            0.5
        ],
        "recall_max_min_avg": [
            0.89,
            0.89,
            0.89
        ]
    },
    {
        "question": "How can I contact LTI faculty Emma Strubell?",
        "context": "Emma Strubell Assistant Professor Email: estrubel@andrew.cmu.edu Office: Gates & Hillman Centers",
        "retrieved_doc": "Emma Strubell Assistant Professor Email: estrubel@andrew.cmu.edu Office: Gates & Hillman Centers|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669",
        "retrieved_relevant_part": true,
        "model_answer": "You can contact Emma Strubell via email at estrubel@andrew.cmu.edu or by phone at 412-268-4788",
        "actual_answer": "Email: estrubel@andrew.cmu.edu Office: Gates & Hillman Centers",
        "exact_match": false,
        "f1_max_min_avg": [
            0.1,
            0.1,
            0.1
        ],
        "recall_max_min_avg": [
            0.14,
            0.14,
            0.14
        ]
    },
    {
        "question": "What is the Research Areas of LTI faculty Daniel Fried?",
        "context": "Daniel Fried Assistant Professor Email: dfried@andrew.cmu.edu Research Areas: Natural Language Processing: Language and Code, Conversational AI, Intelligent Agents, and Dialogue, Discourse and Pragmatics, Multimodal AI",
        "retrieved_doc": "Daniel Fried Assistant Professor Email: dfried@andrew.cmu.edu Research Areas: Natural Language Processing: Language and Code, Conversational AI, Intelligent Agents, and Dialogue, Discourse and Pragmatics, Multimodal AI|Ph.D. in Language and Information Technology: The Ph.D. in LTI focuses on developing the next generation of scientific and entrepreneurial leaders. The first two years of the Ph.D. program are similar to our MLT program. After the second year, you will spend most of your time working closely with your faculty advisor on research that advances the state-of-the-art in computer science. Ph.D. students are expected to publish papers about original research in the most competitive scientific journals and international conference proceedings, and to present their research at conferences and workshops. Most of our Ph.D. graduates become professors and research scientists, while a few have started their own companies.|Author: Daniel Fried Title: AutoReply: Detecting Nonsense in Dialogue with Discriminative Replies Publication year: 2023 Coauthors: Weiyan Shi, Emily Dinan, Adi Renduchintala, Daniel Fried, Athul Paul Jacob, Zhou Yu, Mike Lewis ",
        "retrieved_relevant_part": true,
        "model_answer": "Daniel Fried's research areas include Natural Language Processing, Conversational AI, Intelligent Agents and Dialogue, Discourse and Pragmatics, and Multimodal AIPh.D. in Language and Information Technology",
        "actual_answer": "Daniel Fried's Research Areas is Natural Language Processing: Language and Code, Conversational AI, Intelligent Agents, and Dialogue, Discourse and Pragmatics, Multimodal AI",
        "exact_match": false,
        "f1_max_min_avg": [
            0.51,
            0.51,
            0.51
        ],
        "recall_max_min_avg": [
            0.55,
            0.55,
            0.55
        ]
    },
    {
        "question": "How can I contact LTI faculty Robert Frederking?",
        "context": "Robert Frederking Principal Systems Scientist/Associate Dean of Doctoral Programs/MLT Program Director Email: ref@cs.cmu.edu Office: 6515 Gates & Hillman Centers Phone: 412-268-6656",
        "retrieved_doc": "Robert Frederking Principal Systems Scientist/Associate Dean of Doctoral Programs/MLT Program Director Email: ref@cs.cmu.edu Office: 6515 Gates & Hillman Centers Phone: 412-268-6656|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|MLT Graduate Student Handbook Page 7 1.3 MLT Contact Information The people responsible for administering the MLT degree are: Kate Schaich Robert Frederking Program Manager, MLT Program Director, MLT Graduate Program Manager, LTI Principal Systems Scientist GHC 6415 GHC 6515 412-268-4788 412-268-6656 kschaich@cs.cmu.edu ref@cs.cmu.edu Robert Frederking Mona Diab Cha ir of Graduate Programs, LTI Director, LTI Principal Systems Scientist GHC 5415 GHC 6515 412-268-6656 mdiab@andrew.cmu.edu In addition, students may confer with the Graduate Education Office (graded@andrew.cmu.edu ) regarding issues of process or other concerns as they navigate conflicts. The Language Technologies Institute is located primarily on the 5 th and 6th floors of the Gates Hillman Complex (GHC) on Carnegie Mellon\u2019s Pittsburgh campus. The address and contact information of the LTI are: Language Technologies Institute Carnegie Mellon University 5000 Forbes Avenue Gates Hillman Complex 5402, LTI Pittsburgh, PA 152 41-3891 412-268-6591 (phone) 412-268-6298 (fax) http://www.lti.cs.cmu.edu/",
        "retrieved_relevant_part": true,
        "model_answer": "You can contact LTI faculty Robert Frederking at ref@cs.cmu.edu or by phone at 412-268-6656",
        "actual_answer": "Email: ref@cs.cmu.edu Office: 6515 Gates & Hillman Centers Phone: 412-268-6656",
        "exact_match": false,
        "f1_max_min_avg": [
            0.17,
            0.17,
            0.17
        ],
        "recall_max_min_avg": [
            0.2,
            0.2,
            0.2
        ]
    },
    {
        "question": "Please tell me the Phone of LTI faculty Justine Cassell based on the background information: ",
        "context": "Justine Cassell Professor (On Leave) Email: jcassell@andrew.cmu.edu Office: 5107 Gates & Hillman Centers Phone: 412-204-6268",
        "retrieved_doc": "Justine Cassell Professor (On Leave) Email: jcassell@andrew.cmu.edu Office: 5107 Gates & Hillman Centers Phone: 412-204-6268|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": "412-204-6268",
        "actual_answer": "Justine Cassell's Phone is 412-204-6268",
        "exact_match": false,
        "f1_max_min_avg": [
            0.33,
            0.33,
            0.33
        ],
        "recall_max_min_avg": [
            0.2,
            0.2,
            0.2
        ]
    },
    {
        "question": "What is the Research Areas of LTI faculty Shinji Watanabe?",
        "context": "Shinji Watanabe Associate Professor Email: swatanab@andrew.cmu.edu Phone: 412-268-3687 Research Areas: Natural Language Processing: Conversational AI, Intelligent Agents, and Dialogue, Speech Processing (ASR, Speech Synthesis): Speech Recognition, Speech Synthesis, Multilingual/Low-Resource Speech Processing, Speech-to-Speech Translation, Speech Enhancement / Robust Speech Processing",
        "retrieved_doc": "Shinji Watanabe Associate Professor Email: swatanab@andrew.cmu.edu Phone: 412-268-3687 Research Areas: Natural Language Processing: Conversational AI, Intelligent Agents, and Dialogue, Speech Processing (ASR, Speech Synthesis): Speech Recognition, Speech Synthesis, Multilingual/Low-Resource Speech Processing, Speech-to-Speech Translation, Speech Enhancement / Robust Speech Processing|Author: Shinji Watanabe Title: Unsupervised Data Selection for TTS: Using Arabic Broadcast News as a Case Study Publication year: 2023 Coauthors: Massa Baali, Tomoki Hayashi, Hamdy Mubarak, Soumi Maiti, Shinji Watanabe, W. El-Hajj, Ahmed Ali Abstract: .|Author: Shinji Watanabe Title: Multilingual TTS Accent Impressions for Accented ASR Publication year: 2023 Coauthors: G. Karakasidis, Nathaniel R. Robinson, Yaroslav Getman, Atieno Ogayo, Ragheb Al-Ghezi, Ananya Ayasi, Shinji Watanabe, David R. Mortensen, M. Kurimo ",
        "retrieved_relevant_part": true,
        "model_answer": "The Research Areas of LTI faculty Shinji Watanabe include Natural Language Processing, Conversational AI, Intelligent Agents and Dialogue, Speech Processing (ASR, Speech Synthesis), Speech Recognition, Speech Synthesis, Multilingual/Low-Resource Speech Processing, Speech-to-Speech Translation, Speech Enhancement / Robust Speech Processing",
        "actual_answer": "Shinji Watanabe's Research Areas is Natural Language Processing: Conversational AI, Intelligent Agents, and Dialogue, Speech Processing (ASR, Speech Synthesis): Speech Recognition, Speech Synthesis, Multilingual/Low-Resource Speech Processing, Speech-to-Speech Translation, Speech Enhancement / Robust Speech Processing",
        "exact_match": false,
        "f1_max_min_avg": [
            0.61,
            0.61,
            0.61
        ],
        "recall_max_min_avg": [
            0.65,
            0.65,
            0.65
        ]
    },
    {
        "question": "Please tell me the Email of LTI faculty Alexander Hauptmann based on the background information: ",
        "context": "Alexander Hauptmann Research Professor Email: alex@cs.cmu.edu Office: 5519 Gates & Hillman Centers Phone: 412-268-1448 Research Areas: Information Extraction, Summarization and Question Answering, Information Retrieval, Text Mining and Analytics, Machine Learning, Multimodal Computing and Interaction",
        "retrieved_doc": "Alexander Hauptmann Research Professor Email: alex@cs.cmu.edu Office: 5519 Gates & Hillman Centers Phone: 412-268-1448 Research Areas: Information Extraction, Summarization and Question Answering, Information Retrieval, Text Mining and Analytics, Machine Learning, Multimodal Computing and Interaction|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": "alex@cs.cmu.edu",
        "actual_answer": "alex@cs.cmu.edu",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Please provide the contact information of LTI faculty Eric Nyberg",
        "context": "Eric Nyberg Professor Email: ehn@cs.cmu.edu Office: 6715 Gates & Hillman Centers Phone: 412-268-7281 Research Areas: Information Extraction, Summarization and Question Answering, Information Retrieval, Text Mining and Analytics, Language Technologies for Education",
        "retrieved_doc": "Eric Nyberg Professor Email: ehn@cs.cmu.edu Office: 6715 Gates & Hillman Centers Phone: 412-268-7281 Research Areas: Information Extraction, Summarization and Question Answering, Information Retrieval, Text Mining and Analytics, Language Technologies for Education|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": "Eric Nyberg's email address is ehn@cs.cmu.edu, his office location is 6715 Gates & Hillman Centers, and his phone number is 412-268-7281",
        "actual_answer": "Email: ehn@cs.cmu.edu Office: 6715 Gates & Hillman Centers Phone: 412-268-7281",
        "exact_match": false,
        "f1_max_min_avg": [
            0.32,
            0.32,
            0.32
        ],
        "recall_max_min_avg": [
            0.5,
            0.5,
            0.5
        ]
    },
    {
        "question": "Please tell me the Phone of LTI faculty Robert Frederking based on the background information: ",
        "context": "Robert Frederking Principal Systems Scientist/Associate Dean of Doctoral Programs/MLT Program Director Email: ref@cs.cmu.edu Office: 6515 Gates & Hillman Centers Phone: 412-268-6656",
        "retrieved_doc": "Robert Frederking Principal Systems Scientist/Associate Dean of Doctoral Programs/MLT Program Director Email: ref@cs.cmu.edu Office: 6515 Gates & Hillman Centers Phone: 412-268-6656|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": "412-268-6656",
        "actual_answer": "412-268-6656",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Please provide the contact information of LTI faculty Rita Singh",
        "context": "Rita Singh Associate Research Professor Email: rsingh@cs.cmu.edu Office: 6703 Gates & Hillman Centers Phone: 412-268-9859",
        "retrieved_doc": "Rita Singh Associate Research Professor Email: rsingh@cs.cmu.edu Office: 6703 Gates & Hillman Centers Phone: 412-268-9859|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": "Rita Singh's email address is rsingh@cs.cmu.edu, her office location is at 6703 Gates & Hillman Centers, and her phone number is 412-268-9859",
        "actual_answer": "Email: rsingh@cs.cmu.edu Office: 6703 Gates & Hillman Centers Phone: 412-268-9859",
        "exact_match": false,
        "f1_max_min_avg": [
            0.31,
            0.31,
            0.31
        ],
        "recall_max_min_avg": [
            0.5,
            0.5,
            0.5
        ]
    },
    {
        "question": "Please tell me the Phone of LTI faculty Robert Frederking based on the background information: ",
        "context": "Robert Frederking Principal Systems Scientist/Associate Dean of Doctoral Programs/MLT Program Director Email: ref@cs.cmu.edu Office: 6515 Gates & Hillman Centers Phone: 412-268-6656",
        "retrieved_doc": "Robert Frederking Principal Systems Scientist/Associate Dean of Doctoral Programs/MLT Program Director Email: ref@cs.cmu.edu Office: 6515 Gates & Hillman Centers Phone: 412-268-6656|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": "The phone number for Robert Frederking is 412-268-6656.<|endoftext|>",
        "actual_answer": "Robert Frederking's Phone is 412-268-6656",
        "exact_match": false,
        "f1_max_min_avg": [
            0.31,
            0.31,
            0.31
        ],
        "recall_max_min_avg": [
            0.4,
            0.4,
            0.4
        ]
    },
    {
        "question": "How can I contact LTI faculty Eric P.?",
        "context": "Eric P. Xing Professor (On Leave) Email: epxing@andrew.cmu.edu Office: 8101 Gates & Hillman Centers Phone: 412-268-2559",
        "retrieved_doc": "Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Eric P. Xing Professor (On Leave) Email: epxing@andrew.cmu.edu Office: 8101 Gates & Hillman Centers Phone: 412-268-2559|Brianna Eriksen Academic Program Manager Email: bfreema2@andrew.cmu.edu Phone: 412-268-4277",
        "retrieved_relevant_part": true,
        "model_answer": "You can contact LTI faculty Eric P. by emailing him at epxing@andrew.cmu.edu or by phoneing him at 412-268-2559.<|endoftext|>",
        "actual_answer": "Email: epxing@andrew.cmu.edu Office: 8101 Gates & Hillman Centers Phone: 412-268-2559",
        "exact_match": false,
        "f1_max_min_avg": [
            0.07,
            0.07,
            0.07
        ],
        "recall_max_min_avg": [
            0.1,
            0.1,
            0.1
        ]
    },
    {
        "question": "What is the Email of LTI faculty Eric Nyberg?",
        "context": "Eric Nyberg Professor Email: ehn@cs.cmu.edu Office: 6715 Gates & Hillman Centers Phone: 412-268-7281 Research Areas: Information Extraction, Summarization and Question Answering, Information Retrieval, Text Mining and Analytics, Language Technologies for Education",
        "retrieved_doc": "Eric Nyberg Professor Email: ehn@cs.cmu.edu Office: 6715 Gates & Hillman Centers Phone: 412-268-7281 Research Areas: Information Extraction, Summarization and Question Answering, Information Retrieval, Text Mining and Analytics, Language Technologies for Education|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": "ehn@cs.cmu.edu",
        "actual_answer": "Eric Nyberg's Email is ehn@cs.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0.33,
            0.33,
            0.33
        ],
        "recall_max_min_avg": [
            0.2,
            0.2,
            0.2
        ]
    },
    {
        "question": "Is Anatole Gershman natan Bisk Assistant Professor of LTI?, Please answer yes or no: ",
        "context": "Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering",
        "retrieved_doc": "Yonatan Bisk Assistant Professor Email: ybisk@cs.cmu.edu Office: Gates & Hillman Centers Research Areas: Grounding, RoboNLP, Vision and Language, Embodiment, Unsupervised Learning|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering",
        "retrieved_relevant_part": true,
        "model_answer": "No",
        "actual_answer": "No",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "How can I contact LTI faculty Rita Singh?",
        "context": "Rita Singh Associate Research Professor Email: rsingh@cs.cmu.edu Office: 6703 Gates & Hillman Centers Phone: 412-268-9859",
        "retrieved_doc": "Rita Singh Associate Research Professor Email: rsingh@cs.cmu.edu Office: 6703 Gates & Hillman Centers Phone: 412-268-9859|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": "You can contact Rita Singh at rsingh@cs.cmu.edu or by phone at 412-268-9859",
        "actual_answer": "Email: rsingh@cs.cmu.edu Office: 6703 Gates & Hillman Centers Phone: 412-268-9859",
        "exact_match": false,
        "f1_max_min_avg": [
            0.18,
            0.18,
            0.18
        ],
        "recall_max_min_avg": [
            0.2,
            0.2,
            0.2
        ]
    },
    {
        "question": "Please tell me the Research Areas of LTI faculty Maarten Sap based on the background information: ",
        "context": "Maarten Sap Assistant Professor Email: msap2@andrew.cmu.edu Research Areas: Fairness and Ethics in Language Technology, Computational Social Science, Discourse and Pragmatics, Conversational AI, Intelligent Agents, and Dialogue",
        "retrieved_doc": "Maarten Sap Assistant Professor Email: msap2@andrew.cmu.edu Research Areas: Fairness and Ethics in Language Technology, Computational Social Science, Discourse and Pragmatics, Conversational AI, Intelligent Agents, and Dialogue|Ph.D. in Language and Information Technology: The Ph.D. in LTI focuses on developing the next generation of scientific and entrepreneurial leaders. The first two years of the Ph.D. program are similar to our MLT program. After the second year, you will spend most of your time working closely with your faculty advisor on research that advances the state-of-the-art in computer science. Ph.D. students are expected to publish papers about original research in the most competitive scientific journals and international conference proceedings, and to present their research at conferences and workshops. Most of our Ph.D. graduates become professors and research scientists, while a few have started their own companies.|Dual-Degree Ph.D. in Language and Information Technologies (Portugal Partnership): The LTI offers a dual-degree Ph.D. in Language and Information Technologies in cooperation with: Universidade de Aveiro (Ph.D. in Computer Engineering), Universidade do Minho (Ph.D. in Informatics) and the Universidade do Porto (FCUP, Ph.D. in Computer Science and FEUP, Ph.D. in Computer Science) as part of MAPi; Universidade de Lisboa, Faculdade de Ci\u00eancias \u2013 FCUL (Ph.D. in Informatics) Universidade de Lisboa, Instituto Superior T\u00e9cnico \u2013 IST (Ph.D. in Computer Science and Engineering, Ph.D. in Electrical and Computer Engineering, Ph.D. in Information Security) Universidade Nova de Lisboa, Faculdade de Ci\u00eancias e Tecnologia \u2013 FCTUNL (Ph.D. in Computer Science) Universidade de Coimbra, Faculdade de Ci\u00eancias e Tecnologia \u2013 FCTUC (Ph.D. in Information Science and Technology) Students jointly enrolled in the LTI Ph.D program spend a year in Portugal, then two years at Carnegie Mellon taking classes in linguistics, computer science, statistical learning and task orientation. After completing the majority of their academic requirements, students return to Portugal for the next two years to conduct extensive research, ultimately leading to a dissertation topic that will be publicly defended. One adviser from each institution co-supervises their student\u2019s progress and helps to define their final thesis topic.",
        "retrieved_relevant_part": true,
        "model_answer": "Fairness and Ethics in Language Technology, Computational Social Science, Discourse and Pragmatics, Conversational AI, Intelligent Agents, and DialoguePh.D. in Language and Information Technologies",
        "actual_answer": "Fairness and Ethics in Language Technology, Computational Social Science, Discourse and Pragmatics, Conversational AI, Intelligent Agents, and Dialogue",
        "exact_match": false,
        "f1_max_min_avg": [
            0.73,
            0.73,
            0.73
        ],
        "recall_max_min_avg": [
            0.83,
            0.83,
            0.83
        ]
    },
    {
        "question": "What is the Phone of LTI faculty Anatole Gershman?",
        "context": "Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering",
        "retrieved_doc": "Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": "412-268-8259",
        "actual_answer": "Anatole Gershman's Phone is 412-268-8259",
        "exact_match": false,
        "f1_max_min_avg": [
            0.33,
            0.33,
            0.33
        ],
        "recall_max_min_avg": [
            0.2,
            0.2,
            0.2
        ]
    },
    {
        "question": "How can I contact LTI faculty Yonatan Bisk?",
        "context": "Yonatan Bisk Assistant Professor Email: ybisk@cs.cmu.edu Office: Gates & Hillman Centers Research Areas: Grounding, RoboNLP, Vision and Language, Embodiment, Unsupervised Learning",
        "retrieved_doc": "Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Yonatan Bisk Assistant Professor Email: ybisk@cs.cmu.edu Office: Gates & Hillman Centers Research Areas: Grounding, RoboNLP, Vision and Language, Embodiment, Unsupervised Learning|MLT Graduate Student Handbook Page 7 1.3 MLT Contact Information The people responsible for administering the MLT degree are: Kate Schaich Robert Frederking Program Manager, MLT Program Director, MLT Graduate Program Manager, LTI Principal Systems Scientist GHC 6415 GHC 6515 412-268-4788 412-268-6656 kschaich@cs.cmu.edu ref@cs.cmu.edu Robert Frederking Mona Diab Cha ir of Graduate Programs, LTI Director, LTI Principal Systems Scientist GHC 5415 GHC 6515 412-268-6656 mdiab@andrew.cmu.edu In addition, students may confer with the Graduate Education Office (graded@andrew.cmu.edu ) regarding issues of process or other concerns as they navigate conflicts. The Language Technologies Institute is located primarily on the 5 th and 6th floors of the Gates Hillman Complex (GHC) on Carnegie Mellon\u2019s Pittsburgh campus. The address and contact information of the LTI are: Language Technologies Institute Carnegie Mellon University 5000 Forbes Avenue Gates Hillman Complex 5402, LTI Pittsburgh, PA 152 41-3891 412-268-6591 (phone) 412-268-6298 (fax) http://www.lti.cs.cmu.edu/",
        "retrieved_relevant_part": true,
        "model_answer": "Yonatan Bisk's email address is ybisk@cs.cmu.edu and his phone number is 412-268-3669.<|endoftext|>",
        "actual_answer": "Email: ybisk@cs.cmu.edu Office: Gates & Hillman Centers",
        "exact_match": false,
        "f1_max_min_avg": [
            0.11,
            0.11,
            0.11
        ],
        "recall_max_min_avg": [
            0.14,
            0.14,
            0.14
        ]
    },
    {
        "question": "What is the Phone of LTI faculty Shinji Watanabe?",
        "context": "Shinji Watanabe Associate Professor Email: swatanab@andrew.cmu.edu Phone: 412-268-3687 Research Areas: Natural Language Processing: Conversational AI, Intelligent Agents, and Dialogue, Speech Processing (ASR, Speech Synthesis): Speech Recognition, Speech Synthesis, Multilingual/Low-Resource Speech Processing, Speech-to-Speech Translation, Speech Enhancement / Robust Speech Processing",
        "retrieved_doc": "Shinji Watanabe Associate Professor Email: swatanab@andrew.cmu.edu Phone: 412-268-3687 Research Areas: Natural Language Processing: Conversational AI, Intelligent Agents, and Dialogue, Speech Processing (ASR, Speech Synthesis): Speech Recognition, Speech Synthesis, Multilingual/Low-Resource Speech Processing, Speech-to-Speech Translation, Speech Enhancement / Robust Speech Processing|Author: Shinji Watanabe Title: Multilingual TTS Accent Impressions for Accented ASR Publication year: 2023 Coauthors: G. Karakasidis, Nathaniel R. Robinson, Yaroslav Getman, Atieno Ogayo, Ragheb Al-Ghezi, Ananya Ayasi, Shinji Watanabe, David R. Mortensen, M. Kurimo |Author: Shinji Watanabe Title: Software Design and User Interface of ESPnet-SE++: Speech Enhancement for Robust Speech Processing Publication year: 2023 Coauthors: Yen-Ju Lu, Xuankai Chang, Chenda Li, Wangyou Zhang, Samuele Cornell, Zhaoheng Ni, Yoshiki Masuyama, Brian Yan, Robin Scheibler, Zhong-Qiu Wang, Yu Tsao, Yanmin Qian, Shinji Watanabe ",
        "retrieved_relevant_part": true,
        "model_answer": "The Phone of LTI faculty Shinji Watanabe is 412-268-3687",
        "actual_answer": "Shinji Watanabe's Phone is 412-268-3687",
        "exact_match": false,
        "f1_max_min_avg": [
            0.57,
            0.57,
            0.57
        ],
        "recall_max_min_avg": [
            0.8,
            0.8,
            0.8
        ]
    },
    {
        "question": "How can I contact LTI faculty Louis-Philippe Morency?",
        "context": "Louis-Philippe Morency Leonardo Associate Professor of Computer Science Email: morency@cs.cmu.edu Office: 5411 Gates & Hillman Centers Phone: 412-268-5508 Research Areas: Machine Learning, Multimodal Computing and Interaction, Spoken Interfaces and Dialogue Processing",
        "retrieved_doc": "Louis-Philippe Morency Leonardo Associate Professor of Computer Science Email: morency@cs.cmu.edu Office: 5411 Gates & Hillman Centers Phone: 412-268-5508 Research Areas: Machine Learning, Multimodal Computing and Interaction, Spoken Interfaces and Dialogue Processing|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|MLT Graduate Student Handbook Page 7 1.3 MLT Contact Information The people responsible for administering the MLT degree are: Kate Schaich Robert Frederking Program Manager, MLT Program Director, MLT Graduate Program Manager, LTI Principal Systems Scientist GHC 6415 GHC 6515 412-268-4788 412-268-6656 kschaich@cs.cmu.edu ref@cs.cmu.edu Robert Frederking Mona Diab Cha ir of Graduate Programs, LTI Director, LTI Principal Systems Scientist GHC 5415 GHC 6515 412-268-6656 mdiab@andrew.cmu.edu In addition, students may confer with the Graduate Education Office (graded@andrew.cmu.edu ) regarding issues of process or other concerns as they navigate conflicts. The Language Technologies Institute is located primarily on the 5 th and 6th floors of the Gates Hillman Complex (GHC) on Carnegie Mellon\u2019s Pittsburgh campus. The address and contact information of the LTI are: Language Technologies Institute Carnegie Mellon University 5000 Forbes Avenue Gates Hillman Complex 5402, LTI Pittsburgh, PA 152 41-3891 412-268-6591 (phone) 412-268-6298 (fax) http://www.lti.cs.cmu.edu/",
        "retrieved_relevant_part": true,
        "model_answer": "You can contact Louis-Philippe Morency via email at morency@cs.cmu.edu or by phone at 412-268-5508",
        "actual_answer": "Email: morency@cs.cmu.edu Office: 5411 Gates & Hillman Centers Phone: 412-268-5508",
        "exact_match": false,
        "f1_max_min_avg": [
            0.17,
            0.17,
            0.17
        ],
        "recall_max_min_avg": [
            0.2,
            0.2,
            0.2
        ]
    },
    {
        "question": "Please tell me the Phone of LTI faculty Robert Frederking based on the background information: ",
        "context": "Robert Frederking Principal Systems Scientist/Associate Dean of Doctoral Programs/MLT Program Director Email: ref@cs.cmu.edu Office: 6515 Gates & Hillman Centers Phone: 412-268-6656",
        "retrieved_doc": "Robert Frederking Principal Systems Scientist/Associate Dean of Doctoral Programs/MLT Program Director Email: ref@cs.cmu.edu Office: 6515 Gates & Hillman Centers Phone: 412-268-6656|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": "The phone number for Robert Frederking is 412-268-6656.<|endoftext|>",
        "actual_answer": "412-268-6656",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "How can I contact LTI faculty Rita Singh?",
        "context": "Rita Singh Associate Research Professor Email: rsingh@cs.cmu.edu Office: 6703 Gates & Hillman Centers Phone: 412-268-9859",
        "retrieved_doc": "Rita Singh Associate Research Professor Email: rsingh@cs.cmu.edu Office: 6703 Gates & Hillman Centers Phone: 412-268-9859|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": "You can contact Rita Singh at rsingh@cs.cmu.edu or by phone at 412-268-9859",
        "actual_answer": "Email: rsingh@cs.cmu.edu Office: 6703 Gates & Hillman Centers Phone: 412-268-9859",
        "exact_match": false,
        "f1_max_min_avg": [
            0.18,
            0.18,
            0.18
        ],
        "recall_max_min_avg": [
            0.2,
            0.2,
            0.2
        ]
    },
    {
        "question": "How can I contact LTI faculty Maarten Sap?",
        "context": "Maarten Sap Assistant Professor Email: msap2@andrew.cmu.edu Research Areas: Fairness and Ethics in Language Technology, Computational Social Science, Discourse and Pragmatics, Conversational AI, Intelligent Agents, and Dialogue",
        "retrieved_doc": "Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Maarten Sap Assistant Professor Email: msap2@andrew.cmu.edu Research Areas: Fairness and Ethics in Language Technology, Computational Social Science, Discourse and Pragmatics, Conversational AI, Intelligent Agents, and Dialogue|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": "Maarten Sap's email address is msap2@andrew.cmu.edu and his phone number is 412-268-3669.<|endoftext|>",
        "actual_answer": "Email: msap2@andrew.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0.14,
            0.14,
            0.14
        ],
        "recall_max_min_avg": [
            0.5,
            0.5,
            0.5
        ]
    },
    {
        "question": "What is the Research Areas of LTI faculty Shinji Watanabe?",
        "context": "Shinji Watanabe Associate Professor Email: swatanab@andrew.cmu.edu Phone: 412-268-3687 Research Areas: Natural Language Processing: Conversational AI, Intelligent Agents, and Dialogue, Speech Processing (ASR, Speech Synthesis): Speech Recognition, Speech Synthesis, Multilingual/Low-Resource Speech Processing, Speech-to-Speech Translation, Speech Enhancement / Robust Speech Processing",
        "retrieved_doc": "Shinji Watanabe Associate Professor Email: swatanab@andrew.cmu.edu Phone: 412-268-3687 Research Areas: Natural Language Processing: Conversational AI, Intelligent Agents, and Dialogue, Speech Processing (ASR, Speech Synthesis): Speech Recognition, Speech Synthesis, Multilingual/Low-Resource Speech Processing, Speech-to-Speech Translation, Speech Enhancement / Robust Speech Processing|Author: Shinji Watanabe Title: Unsupervised Data Selection for TTS: Using Arabic Broadcast News as a Case Study Publication year: 2023 Coauthors: Massa Baali, Tomoki Hayashi, Hamdy Mubarak, Soumi Maiti, Shinji Watanabe, W. El-Hajj, Ahmed Ali Abstract: .|Author: Shinji Watanabe Title: Multilingual TTS Accent Impressions for Accented ASR Publication year: 2023 Coauthors: G. Karakasidis, Nathaniel R. Robinson, Yaroslav Getman, Atieno Ogayo, Ragheb Al-Ghezi, Ananya Ayasi, Shinji Watanabe, David R. Mortensen, M. Kurimo ",
        "retrieved_relevant_part": true,
        "model_answer": "The Research Areas of LTI faculty Shinji Watanabe include Natural Language Processing, Conversational AI, Intelligent Agents and Dialogue, Speech Processing (ASR, Speech Synthesis), Speech Recognition, Speech Synthesis, Multilingual/Low-Resource Speech Processing, Speech-to-Speech Translation, Speech Enhancement / Robust Speech Processing",
        "actual_answer": "Shinji Watanabe's Research Areas is Natural Language Processing: Conversational AI, Intelligent Agents, and Dialogue, Speech Processing (ASR, Speech Synthesis): Speech Recognition, Speech Synthesis, Multilingual/Low-Resource Speech Processing, Speech-to-Speech Translation, Speech Enhancement / Robust Speech Processing",
        "exact_match": false,
        "f1_max_min_avg": [
            0.61,
            0.61,
            0.61
        ],
        "recall_max_min_avg": [
            0.65,
            0.65,
            0.65
        ]
    },
    {
        "question": "How can I contact LTI faculty Eric P.?",
        "context": "Eric P. Xing Professor (On Leave) Email: epxing@andrew.cmu.edu Office: 8101 Gates & Hillman Centers Phone: 412-268-2559",
        "retrieved_doc": "Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Eric P. Xing Professor (On Leave) Email: epxing@andrew.cmu.edu Office: 8101 Gates & Hillman Centers Phone: 412-268-2559|Brianna Eriksen Academic Program Manager Email: bfreema2@andrew.cmu.edu Phone: 412-268-4277",
        "retrieved_relevant_part": true,
        "model_answer": "You can contact LTI faculty Eric P. by emailing him at epxing@andrew.cmu.edu or by phoneing him at 412-268-2559.<|endoftext|>",
        "actual_answer": "Email: epxing@andrew.cmu.edu Office: 8101 Gates & Hillman Centers Phone: 412-268-2559",
        "exact_match": false,
        "f1_max_min_avg": [
            0.07,
            0.07,
            0.07
        ],
        "recall_max_min_avg": [
            0.1,
            0.1,
            0.1
        ]
    },
    {
        "question": "Please tell me the Research Areas of LTI faculty Shinji Watanabe based on the background information: ",
        "context": "Shinji Watanabe Associate Professor Email: swatanab@andrew.cmu.edu Phone: 412-268-3687 Research Areas: Natural Language Processing: Conversational AI, Intelligent Agents, and Dialogue, Speech Processing (ASR, Speech Synthesis): Speech Recognition, Speech Synthesis, Multilingual/Low-Resource Speech Processing, Speech-to-Speech Translation, Speech Enhancement / Robust Speech Processing",
        "retrieved_doc": "Author: Shinji Watanabe Title: Unsupervised Data Selection for TTS: Using Arabic Broadcast News as a Case Study Publication year: 2023 Coauthors: Massa Baali, Tomoki Hayashi, Hamdy Mubarak, Soumi Maiti, Shinji Watanabe, W. El-Hajj, Ahmed Ali Abstract: .|Shinji Watanabe Associate Professor Email: swatanab@andrew.cmu.edu Phone: 412-268-3687 Research Areas: Natural Language Processing: Conversational AI, Intelligent Agents, and Dialogue, Speech Processing (ASR, Speech Synthesis): Speech Recognition, Speech Synthesis, Multilingual/Low-Resource Speech Processing, Speech-to-Speech Translation, Speech Enhancement / Robust Speech Processing|Author: Shinji Watanabe Title: The Pipeline System of ASR and NLU with MLM-based data Augmentation Toward Stop Low-Resource Challenge Publication year: 2023 Coauthors: Hayato Futami, Jessica Huynh, Siddhant Arora, Shih-Lun Wu, Yosuke Kashiwagi, Yifan Peng, Brian Yan, E. Tsunoo, Shinji Watanabe Abstract: This paper describes our system for the low-resource domain adaptation track (Track 3) in Spoken Language Understanding Grand Challenge, which is a part of ICASSP Signal Processing Grand Challenge 2023. In the track, we adopt a pipeline approach of ASR and NLU. For ASR, we fine-tune Whisper for each domain with upsampling. For NLU, we fine-tune BART on all the Track3 data and then on low-resource domain data. We apply masked LM (MLM) -based data augmentation, where some of input tokens and corresponding target labels are replaced using MLM. We also apply a retrieval-based approach, where model input is augmented with similar training samples. As a result, we achieved exact match (EM) accuracy 63.3/75.0 (average: 69.15) for reminder/weather domain, and won the 1st place at the challenge.",
        "retrieved_relevant_part": true,
        "model_answer": "Natural Language Processing: Conversational AI, Intelligent Agents and Dialogue, Speech Processing (ASR, Speech Synthesis): Speech Recognition, Speech Synthesis, Multilingual/Low-Resource Speech Processing, Speech-to-Speech Translation, Speech Enhancement / Robust Speech Processing",
        "actual_answer": "Natural Language Processing: Conversational AI, Intelligent Agents, and Dialogue, Speech Processing (ASR, Speech Synthesis): Speech Recognition, Speech Synthesis, Multilingual/Low-Resource Speech Processing, Speech-to-Speech Translation, Speech Enhancement / Robust Speech Processing",
        "exact_match": false,
        "f1_max_min_avg": [
            0.72,
            0.72,
            0.72
        ],
        "recall_max_min_avg": [
            0.72,
            0.72,
            0.72
        ]
    },
    {
        "question": "Is Scott Fahlman i Li Assistant Professor of LTI?, Please answer yes or no: ",
        "context": "Scott Fahlman Research Professor Emeritus Email: sef@cs.cmu.edu Office: 6417 Gates & Hillman Centers Phone: 412-268-2575 Research Areas: AI, Knowledge Representation and Reasoning, Natural Language Understanding",
        "retrieved_doc": "Scott Fahlman Research Professor Emeritus Email: sef@cs.cmu.edu Office: 6417 Gates & Hillman Centers Phone: 412-268-2575 Research Areas: AI, Knowledge Representation and Reasoning, Natural Language Understanding|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|LTI Ph.D. Graduate Student Handbook Page 18 study projects. This personal student-advisor relationship ensures that every student receives the necessary faculty mentoring. The advisor also provides the student with career advice. How a nd when the advisor is selected (The Matching Process) : Advisors are selected during the first month of the program. Students are expected to attend the LTI's Orientation lectures, (Section 3.3) in which faculty describe their interests and areas of expertise, and to be proactive about meeting individually with potential advisors. After meeting with several potential advisors, each student will be asked to indicate a first, second and third choice for advisor. Meanwhile, the potential advisors determine how many new students they can take and indicate their preferences. Finally, the entire LTI faculty meets as a \" Matching Committee\" to determine the best set of matches. Until a student finds a specific advisor, the Progr am Director of the LTI graduate programs serves as the advisor. How to change advisors: The initial advisor selection should be made carefully. However, it is possible to change advisors. To do so, the student should find another faculty member willing to serve as an advisor, and then make a request to the Program Director to switch advisors. The LTI follows the long-standing SCS policy that both the new and old advisors need to agree to the change; typically, this is not a problem (assuming the new advisor has agreed in advance, as described here). It is to the student's advantage to avoid switching advisors, especially late in their graduate studies, because forging a strong student-advisor relationship takes time. 3.3 LTI Orientation At the beginning of each Fall semester, the LTI provides 2-3 weeks of lectures and talks to help students learn about the work done by CMU faculty and to provide an opportunity for advisors to recruit new students. Students are expected to attend them and treat them as seriously a s a standard course, because they provide a good introduction to the broad range of research done at the LTI. Students do not register for the LTI Orientation, nor do they receive a grade; however, the department is serious about its expectation that new students will attend these talks. For many years, these talks were called \"the Immigration Course (IC).\" The intuition behind this name is that the talks help orient newly-arrived students \u2013 immigrants into the CMU environment. As a result, some students u nderstood the name to indicate that the talks would be about US immigration and visa requirements. The older name has been retired; however, people who have been at CMU a long time may occasionally use the older name . 3.4 End of Semester Evaluation Near the e nd of each semester, the student must prepare a statement that describes his or her achievements in the current semester and plans for the next semester. At the end of the semester, the faculty evaluates each student's academic progress. The student's advisor serves as the student's advocate in this process. The result of the evaluation is a letter from",
        "retrieved_relevant_part": true,
        "model_answer": "Yes",
        "actual_answer": "No",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "How can I contact LTI faculty Bhiksha Ramakrishnan?",
        "context": "Bhiksha Ramakrishnan Professor Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826 Research Areas: Machine Learning, Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing, Privacy",
        "retrieved_doc": "Bhiksha Ramakrishnan Professor Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826 Research Areas: Machine Learning, Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing, Privacy|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": "You can contact Bhiksha Ramakrishnan via email at bhiksha@cs.cmu.edu or by phone at 412-268-9826",
        "actual_answer": "Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826",
        "exact_match": false,
        "f1_max_min_avg": [
            0.17,
            0.17,
            0.17
        ],
        "recall_max_min_avg": [
            0.2,
            0.2,
            0.2
        ]
    },
    {
        "question": "How can I contact LTI faculty Chenyan Xiong?",
        "context": "Chenyan Xiong Associate Professor Email: cx@andrew.cmu.edu Phone: 412-268-7641",
        "retrieved_doc": "Chenyan Xiong Associate Professor Email: cx@andrew.cmu.edu Phone: 412-268-7641|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Eric P. Xing Professor (On Leave) Email: epxing@andrew.cmu.edu Office: 8101 Gates & Hillman Centers Phone: 412-268-2559",
        "retrieved_relevant_part": true,
        "model_answer": "Chenyan Xiong's email address is cx@andrew.cmu.edu, and her phone number is 412-268-7641",
        "actual_answer": "Email: cx@andrew.cmu.edu Phone: 412-268-7641",
        "exact_match": false,
        "f1_max_min_avg": [
            0.12,
            0.12,
            0.12
        ],
        "recall_max_min_avg": [
            0.25,
            0.25,
            0.25
        ]
    },
    {
        "question": "How can I contact LTI faculty Bhiksha Ramakrishnan?",
        "context": "Bhiksha Ramakrishnan Professor Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826 Research Areas: Machine Learning, Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing, Privacy",
        "retrieved_doc": "Bhiksha Ramakrishnan Professor Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826 Research Areas: Machine Learning, Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing, Privacy|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": "You can contact Bhiksha Ramakrishnan via email at bhiksha@cs.cmu.edu or by phone at 412-268-9826",
        "actual_answer": "Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826",
        "exact_match": false,
        "f1_max_min_avg": [
            0.17,
            0.17,
            0.17
        ],
        "recall_max_min_avg": [
            0.2,
            0.2,
            0.2
        ]
    },
    {
        "question": "How can I contact LTI faculty Maarten Sap?",
        "context": "Maarten Sap Assistant Professor Email: msap2@andrew.cmu.edu Research Areas: Fairness and Ethics in Language Technology, Computational Social Science, Discourse and Pragmatics, Conversational AI, Intelligent Agents, and Dialogue",
        "retrieved_doc": "Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Maarten Sap Assistant Professor Email: msap2@andrew.cmu.edu Research Areas: Fairness and Ethics in Language Technology, Computational Social Science, Discourse and Pragmatics, Conversational AI, Intelligent Agents, and Dialogue|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": "Maarten Sap's email address is msap2@andrew.cmu.edu and his phone number is 412-268-3669.<|endoftext|>",
        "actual_answer": "Email: msap2@andrew.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0.14,
            0.14,
            0.14
        ],
        "recall_max_min_avg": [
            0.5,
            0.5,
            0.5
        ]
    },
    {
        "question": "What is the Office of LTI faculty Justine Cassell?",
        "context": "Justine Cassell Professor (On Leave) Email: jcassell@andrew.cmu.edu Office: 5107 Gates & Hillman Centers Phone: 412-204-6268",
        "retrieved_doc": "Justine Cassell Professor (On Leave) Email: jcassell@andrew.cmu.edu Office: 5107 Gates & Hillman Centers Phone: 412-204-6268|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|LTI Ph.D. Graduate Student Handbook Page 18 study projects. This personal student-advisor relationship ensures that every student receives the necessary faculty mentoring. The advisor also provides the student with career advice. How a nd when the advisor is selected (The Matching Process) : Advisors are selected during the first month of the program. Students are expected to attend the LTI's Orientation lectures, (Section 3.3) in which faculty describe their interests and areas of expertise, and to be proactive about meeting individually with potential advisors. After meeting with several potential advisors, each student will be asked to indicate a first, second and third choice for advisor. Meanwhile, the potential advisors determine how many new students they can take and indicate their preferences. Finally, the entire LTI faculty meets as a \" Matching Committee\" to determine the best set of matches. Until a student finds a specific advisor, the Progr am Director of the LTI graduate programs serves as the advisor. How to change advisors: The initial advisor selection should be made carefully. However, it is possible to change advisors. To do so, the student should find another faculty member willing to serve as an advisor, and then make a request to the Program Director to switch advisors. The LTI follows the long-standing SCS policy that both the new and old advisors need to agree to the change; typically, this is not a problem (assuming the new advisor has agreed in advance, as described here). It is to the student's advantage to avoid switching advisors, especially late in their graduate studies, because forging a strong student-advisor relationship takes time. 3.3 LTI Orientation At the beginning of each Fall semester, the LTI provides 2-3 weeks of lectures and talks to help students learn about the work done by CMU faculty and to provide an opportunity for advisors to recruit new students. Students are expected to attend them and treat them as seriously a s a standard course, because they provide a good introduction to the broad range of research done at the LTI. Students do not register for the LTI Orientation, nor do they receive a grade; however, the department is serious about its expectation that new students will attend these talks. For many years, these talks were called \"the Immigration Course (IC).\" The intuition behind this name is that the talks help orient newly-arrived students \u2013 immigrants into the CMU environment. As a result, some students u nderstood the name to indicate that the talks would be about US immigration and visa requirements. The older name has been retired; however, people who have been at CMU a long time may occasionally use the older name . 3.4 End of Semester Evaluation Near the e nd of each semester, the student must prepare a statement that describes his or her achievements in the current semester and plans for the next semester. At the end of the semester, the faculty evaluates each student's academic progress. The student's advisor serves as the student's advocate in this process. The result of the evaluation is a letter from",
        "retrieved_relevant_part": true,
        "model_answer": "Justine Cassell's office is located in the Gates & Hillman Centers",
        "actual_answer": "Justine Cassell's Office is 5107 Gates & Hillman Centers",
        "exact_match": false,
        "f1_max_min_avg": [
            0.7,
            0.7,
            0.7
        ],
        "recall_max_min_avg": [
            0.78,
            0.78,
            0.78
        ]
    },
    {
        "question": "What is the Research Areas of LTI faculty Louis-Philippe Morency?",
        "context": "Louis-Philippe Morency Leonardo Associate Professor of Computer Science Email: morency@cs.cmu.edu Office: 5411 Gates & Hillman Centers Phone: 412-268-5508 Research Areas: Machine Learning, Multimodal Computing and Interaction, Spoken Interfaces and Dialogue Processing",
        "retrieved_doc": "Louis-Philippe Morency Leonardo Associate Professor of Computer Science Email: morency@cs.cmu.edu Office: 5411 Gates & Hillman Centers Phone: 412-268-5508 Research Areas: Machine Learning, Multimodal Computing and Interaction, Spoken Interfaces and Dialogue Processing|Author: Louis-Philippe Morency Title: MultiZoo and MultiBench: A Standardized Toolkit for Multimodal Deep Learning Publication year: 2023 Coauthors: P. Liang, Yiwei Lyu, Xiang Fan, Arav Agarwal, Yun Cheng, Louis-Philippe Morency, R. Salakhutdinov |Author: Louis-Philippe Morency Title: Multimodal Learning Without Labeled Multimodal Data: Guarantees and Applications Publication year: 2023 Coauthors: P. Liang, Chun Kai Ling, Yun Cheng, A. Obolenskiy, Yudong Liu, Rohan Pandey, Alex Wilf, Louis-Philippe Morency, R. Salakhutdinov Abstract: In many machine learning systems that jointly learn from multiple modalities, a core research question is to understand the nature of multimodal interactions: the emergence of new task-relevant information during learning from both modalities that was not present in either alone. We study this challenge of interaction quantification in a semi-supervised setting with only labeled unimodal data and naturally co-occurring multimodal data (e.g., unlabeled images and captions, video and corresponding audio) but when labeling them is time-consuming. Using a precise information-theoretic definition of interactions, our key contributions are the derivations of lower and upper bounds to quantify the amount of multimodal interactions in this semi-supervised setting. We propose two lower bounds based on the amount of shared information between modalities and the disagreement between separately trained unimodal classifiers, and derive an upper bound through connections to approximate algorithms for min-entropy couplings. We validate these estimated bounds and show how they accurately track true interactions. Finally, two semi-supervised multimodal applications are explored based on these theoretical results: (1) analyzing the relationship between multimodal performance and estimated interactions, and (2) self-supervised learning that embraces disagreement between modalities beyond agreement as is typically done.",
        "retrieved_relevant_part": true,
        "model_answer": "Machine Learning, Multimodal Computing and Interaction, Spoken Interfaces and Dialogue Processing",
        "actual_answer": "Louis-Philippe Morency's Research Areas is Machine Learning, Multimodal Computing and Interaction, Spoken Interfaces and Dialogue Processing",
        "exact_match": false,
        "f1_max_min_avg": [
            0.74,
            0.74,
            0.74
        ],
        "recall_max_min_avg": [
            0.62,
            0.62,
            0.62
        ]
    },
    {
        "question": "What is the Office of LTI faculty Ralf Brown?",
        "context": "Ralf Brown Principal Systems Scientist Email: ralf@andrew.cmu.edu Office: 5711 Gates & Hillman Centers Phone: 412-268-8298 Research Areas: Information Extraction, Summarization and Question Answering, Information Retrieval, Text Mining and Analytics, Machine Translation, Natural Language Processing and Computational Linguistics",
        "retrieved_doc": "Ralf Brown Principal Systems Scientist Email: ralf@andrew.cmu.edu Office: 5711 Gates & Hillman Centers Phone: 412-268-8298 Research Areas: Information Extraction, Summarization and Question Answering, Information Retrieval, Text Mining and Analytics, Machine Translation, Natural Language Processing and Computational Linguistics|LTI Ph.D. Graduate Student Handbook Page 13 412-268-6298 (fax) http://www.lti.cs.cmu.edu/ Office locations for faculty, staff, and Ph.D. students can be found on the LTI website. 2.2 Office Space Most Ph.D. students in residence on the Pittsburgh campus are given an office in which to study and do research. Typically, offices are shared with other Ph.D. students, but they may also be shared with staff, visitors, or other members of the LTI. Offices are assigned by the LTI\u2019s Office Manager (see Section 1.2 , Department Personnel, for contact information). 2.3 Mailboxes and Office Supplies Mailboxes and office supplies are in GHC 5404. 2.4 Photocopies and Printers Printers and photocopies are available to LTI students. The use of a photocopier or printer requires you to log in with your CMU ID card. LTI students may use printers/photocopiers scattered throughout the School of Computer Science buildings , but the machines in GHC 5404 and GHC 6604 are the most convenient . The SCS Computing Facilities publishes a list of printers online at https://computing.cs.cmu.edu/desktop/printer-list. 2.5 Computers for LTI Ph.D. Students Ph.D. students are responsible for having their own laptop computers to support their education and research. Students are free to choose their own operating system (e.g., Linux, MacOs, Windows). Many Ph.D. advisors also provide access to computer clusters, cloud computing, or other resources to support computationally-intense research. Ph.D. students are given access to the LTI\u2019s computer cluster on an as-needed basis, to be used for course assignments, directed study projects, and/or capstone projects. The LTI cluster provides storage and computation for projects involving large datasets and/or lengthy computation. Ph.D. students receive two types of user ids: An Andrew id and a CS id. All CMU students have an Andrew id. Computer Science students also have a CS id that provides access to SCS-specific resources (e.g., computer clusters). CS ids are being phased out very slowly, so it is likely that you will need both types of user id|Spring offering: Course: 11935 Title: LTI Practicum Units: 1-36 Lec/Sec: Section A Days: To be announced Begin: 03:30PM End: 04:50PM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Frederking ",
        "retrieved_relevant_part": true,
        "model_answer": "The Office of LTI faculty Ralf Brown is located in the Gates & Hillman Centers",
        "actual_answer": "5711 Gates & Hillman Centers",
        "exact_match": false,
        "f1_max_min_avg": [
            0.4,
            0.4,
            0.4
        ],
        "recall_max_min_avg": [
            0.8,
            0.8,
            0.8
        ]
    },
    {
        "question": "Please tell me the Email of LTI faculty Robert Frederking based on the background information: ",
        "context": "Robert Frederking Principal Systems Scientist/Associate Dean of Doctoral Programs/MLT Program Director Email: ref@cs.cmu.edu Office: 6515 Gates & Hillman Centers Phone: 412-268-6656",
        "retrieved_doc": "Robert Frederking Principal Systems Scientist/Associate Dean of Doctoral Programs/MLT Program Director Email: ref@cs.cmu.edu Office: 6515 Gates & Hillman Centers Phone: 412-268-6656|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": "ref@cs.cmu.edu",
        "actual_answer": "Robert Frederking's Email is ref@cs.cmu.edu",
        "exact_match": false,
        "f1_max_min_avg": [
            0.33,
            0.33,
            0.33
        ],
        "recall_max_min_avg": [
            0.2,
            0.2,
            0.2
        ]
    },
    {
        "question": "Please provide the contact information of LTI faculty Jamie Callan",
        "context": "Jamie Callan Professor and PhD Program Director Email: callan@cs.cmu.edu Office: 5419 Gates & Hillman Centers Phone: 412-268-4525 Research Areas: Information Retrieval, Text Mining and Analytics",
        "retrieved_doc": "Jamie Callan Professor and PhD Program Director Email: callan@cs.cmu.edu Office: 5419 Gates & Hillman Centers Phone: 412-268-4525 Research Areas: Information Retrieval, Text Mining and Analytics|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Fall offering: Course: 11935 Title: LTI Practicum Units: 1-36 Lec/Sec: Section B Days: To be announced Begin: 11:00AM End: 11:50AM Bldg/Room: TBA Location: Pittsburgh, Pennsylvania Instructor(s): Callan ",
        "retrieved_relevant_part": true,
        "model_answer": "Jamie Callan's email address is callan@cs.cmu.edu, office location is in the Gates & Hillman Centers, and phone number is 412-268-4525.<|endoftext|>",
        "actual_answer": "Email: callan@cs.cmu.edu Office: 5419 Gates & Hillman Centers Phone: 412-268-4525",
        "exact_match": false,
        "f1_max_min_avg": [
            0.2,
            0.2,
            0.2
        ],
        "recall_max_min_avg": [
            0.3,
            0.3,
            0.3
        ]
    },
    {
        "question": "What is the Email of LTI faculty Bhiksha Ramakrishnan?",
        "context": "Bhiksha Ramakrishnan Professor Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826 Research Areas: Machine Learning, Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing, Privacy",
        "retrieved_doc": "Bhiksha Ramakrishnan Professor Email: bhiksha@cs.cmu.edu Office: 6705 Gates & Hillman Centers Phone: 412-268-9826 Research Areas: Machine Learning, Multimodal Computing and Interaction, Speech Processing, Spoken Interfaces and Dialogue Processing, Privacy|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": "bhiksha@cs.cmu.edu",
        "actual_answer": "bhiksha@cs.cmu.edu",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "What is the Phone of LTI faculty Justine Cassell?",
        "context": "Justine Cassell Professor (On Leave) Email: jcassell@andrew.cmu.edu Office: 5107 Gates & Hillman Centers Phone: 412-204-6268",
        "retrieved_doc": "Justine Cassell Professor (On Leave) Email: jcassell@andrew.cmu.edu Office: 5107 Gates & Hillman Centers Phone: 412-204-6268|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": "412-204-6268",
        "actual_answer": "412-204-6268",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Is Rita Singh lf Brown Principal Systems Scientist of LTI?, Please answer yes or no: ",
        "context": "Rita Singh Associate Research Professor Email: rsingh@cs.cmu.edu Office: 6703 Gates & Hillman Centers Phone: 412-268-9859",
        "retrieved_doc": "Ralf Brown Principal Systems Scientist Email: ralf@andrew.cmu.edu Office: 5711 Gates & Hillman Centers Phone: 412-268-8298 Research Areas: Information Extraction, Summarization and Question Answering, Information Retrieval, Text Mining and Analytics, Machine Translation, Natural Language Processing and Computational Linguistics|Rita Singh Associate Research Professor Email: rsingh@cs.cmu.edu Office: 6703 Gates & Hillman Centers Phone: 412-268-9859|Chas Murray Rose Lab/Systems/Software Engineer Email: rcmurray@andrew.cmu.edu",
        "retrieved_relevant_part": true,
        "model_answer": "No",
        "actual_answer": "No",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Is Maarten Sap iksha Ramakrishnan Professor of LTI?, Please answer yes or no: ",
        "context": "Maarten Sap Assistant Professor Email: msap2@andrew.cmu.edu Research Areas: Fairness and Ethics in Language Technology, Computational Social Science, Discourse and Pragmatics, Conversational AI, Intelligent Agents, and Dialogue",
        "retrieved_doc": "Maarten Sap Assistant Professor Email: msap2@andrew.cmu.edu Research Areas: Fairness and Ethics in Language Technology, Computational Social Science, Discourse and Pragmatics, Conversational AI, Intelligent Agents, and Dialogue|Author: Maarten Sap Title: Don't Take This Out of Context!: On the Need for Contextual Models and Evaluations for Stylistic Rewriting Publication year: 2023 Coauthors: Akhila Yerukola, Xuhui Zhou, Elizabeth Clark, Maarten Sap |Ph.D. in Language and Information Technology: The Ph.D. in LTI focuses on developing the next generation of scientific and entrepreneurial leaders. The first two years of the Ph.D. program are similar to our MLT program. After the second year, you will spend most of your time working closely with your faculty advisor on research that advances the state-of-the-art in computer science. Ph.D. students are expected to publish papers about original research in the most competitive scientific journals and international conference proceedings, and to present their research at conferences and workshops. Most of our Ph.D. graduates become professors and research scientists, while a few have started their own companies.",
        "retrieved_relevant_part": true,
        "model_answer": "No",
        "actual_answer": "No",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Please tell me the Email of LTI faculty Sean Welleck based on the background information: ",
        "context": "Sean Welleck Assistant Professor (Starting January 2024) Email: swelleck@andrew.cmu.edu",
        "retrieved_doc": "Sean Welleck Assistant Professor (Starting January 2024) Email: swelleck@andrew.cmu.edu|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|Kate Schaich LTI Graduate Program Manager Email: kschaich@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-4788",
        "retrieved_relevant_part": true,
        "model_answer": "swelleck@andrew.cmu.edu",
        "actual_answer": "swelleck@andrew.cmu.edu",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Is Louis-Philippe Morency niel Fried Assistant Professor of LTI?, Please answer yes or no: ",
        "context": "Louis-Philippe Morency Leonardo Associate Professor of Computer Science Email: morency@cs.cmu.edu Office: 5411 Gates & Hillman Centers Phone: 412-268-5508 Research Areas: Machine Learning, Multimodal Computing and Interaction, Spoken Interfaces and Dialogue Processing",
        "retrieved_doc": "Louis-Philippe Morency Leonardo Associate Professor of Computer Science Email: morency@cs.cmu.edu Office: 5411 Gates & Hillman Centers Phone: 412-268-5508 Research Areas: Machine Learning, Multimodal Computing and Interaction, Spoken Interfaces and Dialogue Processing|Author: Louis-Philippe Morency Title: MultiZoo and MultiBench: A Standardized Toolkit for Multimodal Deep Learning Publication year: 2023 Coauthors: P. Liang, Yiwei Lyu, Xiang Fan, Arav Agarwal, Yun Cheng, Louis-Philippe Morency, R. Salakhutdinov |Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669",
        "retrieved_relevant_part": true,
        "model_answer": "No",
        "actual_answer": "No",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "What is the Office of LTI faculty Anatole Gershman?",
        "context": "Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering",
        "retrieved_doc": "Anatole Gershman Distinguished Service Professor Email: anatole.gershman@cs.cmu.edu Office: 6415 Gates & Hillman Centers Phone: 412-268-8259 Research Areas: Information Extraction, Summarization and Question Answering|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|LTI Ph.D. Graduate Student Handbook Page 13 412-268-6298 (fax) http://www.lti.cs.cmu.edu/ Office locations for faculty, staff, and Ph.D. students can be found on the LTI website. 2.2 Office Space Most Ph.D. students in residence on the Pittsburgh campus are given an office in which to study and do research. Typically, offices are shared with other Ph.D. students, but they may also be shared with staff, visitors, or other members of the LTI. Offices are assigned by the LTI\u2019s Office Manager (see Section 1.2 , Department Personnel, for contact information). 2.3 Mailboxes and Office Supplies Mailboxes and office supplies are in GHC 5404. 2.4 Photocopies and Printers Printers and photocopies are available to LTI students. The use of a photocopier or printer requires you to log in with your CMU ID card. LTI students may use printers/photocopiers scattered throughout the School of Computer Science buildings , but the machines in GHC 5404 and GHC 6604 are the most convenient . The SCS Computing Facilities publishes a list of printers online at https://computing.cs.cmu.edu/desktop/printer-list. 2.5 Computers for LTI Ph.D. Students Ph.D. students are responsible for having their own laptop computers to support their education and research. Students are free to choose their own operating system (e.g., Linux, MacOs, Windows). Many Ph.D. advisors also provide access to computer clusters, cloud computing, or other resources to support computationally-intense research. Ph.D. students are given access to the LTI\u2019s computer cluster on an as-needed basis, to be used for course assignments, directed study projects, and/or capstone projects. The LTI cluster provides storage and computation for projects involving large datasets and/or lengthy computation. Ph.D. students receive two types of user ids: An Andrew id and a CS id. All CMU students have an Andrew id. Computer Science students also have a CS id that provides access to SCS-specific resources (e.g., computer clusters). CS ids are being phased out very slowly, so it is likely that you will need both types of user id",
        "retrieved_relevant_part": true,
        "model_answer": "The Office of LTI faculty Anatole Gershman is located in the Gates & Hillman Centers and is responsible for assigning offices to Ph.D. students",
        "actual_answer": "Anatole Gershman's Office is 6415 Gates & Hillman Centers",
        "exact_match": false,
        "f1_max_min_avg": [
            0.42,
            0.42,
            0.42
        ],
        "recall_max_min_avg": [
            0.78,
            0.78,
            0.78
        ]
    },
    {
        "question": "How can I contact LTI faculty Robert Frederking?",
        "context": "Robert Frederking Principal Systems Scientist/Associate Dean of Doctoral Programs/MLT Program Director Email: ref@cs.cmu.edu Office: 6515 Gates & Hillman Centers Phone: 412-268-6656",
        "retrieved_doc": "Robert Frederking Principal Systems Scientist/Associate Dean of Doctoral Programs/MLT Program Director Email: ref@cs.cmu.edu Office: 6515 Gates & Hillman Centers Phone: 412-268-6656|Mona Diab LTI Director and Tenured Professor Email: mdiab@andrew.cmu.edu Phone: 412-268-3669|MLT Graduate Student Handbook Page 7 1.3 MLT Contact Information The people responsible for administering the MLT degree are: Kate Schaich Robert Frederking Program Manager, MLT Program Director, MLT Graduate Program Manager, LTI Principal Systems Scientist GHC 6415 GHC 6515 412-268-4788 412-268-6656 kschaich@cs.cmu.edu ref@cs.cmu.edu Robert Frederking Mona Diab Cha ir of Graduate Programs, LTI Director, LTI Principal Systems Scientist GHC 5415 GHC 6515 412-268-6656 mdiab@andrew.cmu.edu In addition, students may confer with the Graduate Education Office (graded@andrew.cmu.edu ) regarding issues of process or other concerns as they navigate conflicts. The Language Technologies Institute is located primarily on the 5 th and 6th floors of the Gates Hillman Complex (GHC) on Carnegie Mellon\u2019s Pittsburgh campus. The address and contact information of the LTI are: Language Technologies Institute Carnegie Mellon University 5000 Forbes Avenue Gates Hillman Complex 5402, LTI Pittsburgh, PA 152 41-3891 412-268-6591 (phone) 412-268-6298 (fax) http://www.lti.cs.cmu.edu/",
        "retrieved_relevant_part": true,
        "model_answer": "You can contact LTI faculty Robert Frederking at ref@cs.cmu.edu or by phone at 412-268-6656.<|endoftext|>",
        "actual_answer": "Email: ref@cs.cmu.edu Office: 6515 Gates & Hillman Centers Phone: 412-268-6656",
        "exact_match": false,
        "f1_max_min_avg": [
            0.08,
            0.08,
            0.08
        ],
        "recall_max_min_avg": [
            0.1,
            0.1,
            0.1
        ]
    },
    {
        "question": "According to background information, What happens in 2024-06-21 on university calendar?",
        "context": "Summer One_All 2024: Date: 2024-06-21 Day: F Event: Mini-5 Final Exams",
        "retrieved_doc": "Spring 2024: Date: 2024-05-06 Day: M Event: Semester & Mini-4 Faculty Course Evaluations close |Spring 2024: Date: 2024-04-01 Day: M Event: Semester pass/no pass & withdrawal deadline (3)|Spring 2024: Date: 2024-04-26 Day: F Event: Semester & Mini-4 voucher deadline (4)",
        "retrieved_relevant_part": false,
        "model_answer": "The semester & mini-4 faculty course evaluations close",
        "actual_answer": "Mini-5 Final Exams",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Based on background information, What is the specific event listed for 2024-05-17, in Summer One_All 2024",
        "context": "Summer One_All 2024: Date: 2024-05-17 Day: F Event: Mini-5 add, audit, & tuition adjustment drop deadline (1)",
        "retrieved_doc": "Summer One_All 2024: Date: 2024-05-17 Day: F Event: Mini-5 add, audit, & tuition adjustment drop deadline (1)|Summer One_All 2024: Date: 2024-05-13 Day: M Event: Semester & Mini-5 Classes Begin|Summer One_All 2024: Date: 2024-06-25 Day: T Event: Mini-5 Final Grades Due by 4 pm",
        "retrieved_relevant_part": true,
        "model_answer": "The mini-5 add, audit, and tuition adjustment drop deadline",
        "actual_answer": "Mini-5 add, audit, & tuition adjustment drop deadline (1)",
        "exact_match": false,
        "f1_max_min_avg": [
            0.67,
            0.67,
            0.67
        ],
        "recall_max_min_avg": [
            0.67,
            0.67,
            0.67
        ]
    },
    {
        "question": "According to background information, When is the [Mini-5 Last Day of Classes] in Summer One_All 2024?",
        "context": "Summer One_All 2024: Date: 2024-06-20 Day: Th Event: Mini-5 Last Day of Classes",
        "retrieved_doc": "Summer One_All 2024: Date: 2024-06-20 Day: Th Event: Mini-5 Last Day of Classes|Summer One_All 2024: Date: 2024-08-01 Day: Th Event: Semester & Mini-6 Last Day of Classes|Summer One_All 2024: Date: 2024-05-13 Day: M Event: Semester & Mini-5 Classes Begin",
        "retrieved_relevant_part": true,
        "model_answer": "The [Mini-5 Last Day of Classes] in Summer One_All 2024 is on May 13",
        "actual_answer": "The event Mini-5 Last Day of Classes happens on 2024-06-20",
        "exact_match": false,
        "f1_max_min_avg": [
            0.42,
            0.42,
            0.42
        ],
        "recall_max_min_avg": [
            0.5,
            0.5,
            0.5
        ]
    },
    {
        "question": "Based on background information, What is the specific event listed for 2023-11-27, in Fall 2023",
        "context": "Fall 2023: Date: 2023-11-27 Day: M Event: Mini-2 pass/no pass & withdrawal deadline (2)",
        "retrieved_doc": "Fall 2023: Date: 2023-11-11 Day: Sa Event: Homecoming|Fall 2023: Date: 2023-10-27 Day: F Event: Mini-2 add, audit & tuition adjustment drop deadline (1)|Fall 2023: Date: 2023-08-28 Day: M Event: Semester & Mini-1 Classes Begin",
        "retrieved_relevant_part": false,
        "model_answer": "The event for 2023-11-27, in Fall 2023, is \"Homecoming.\"",
        "actual_answer": "The event happening on 2023-11-27 is Mini-2 pass/no pass & withdrawal deadline (2)",
        "exact_match": false,
        "f1_max_min_avg": [
            0.27,
            0.27,
            0.27
        ],
        "recall_max_min_avg": [
            0.23,
            0.23,
            0.23
        ]
    },
    {
        "question": "Based on background information, What is the specific event listed for 2023-10-23, in Fall 2023",
        "context": "Fall 2023: Date: 2023-10-23 Day: M Event: Mini-2 Classes Begin ",
        "retrieved_doc": "Fall 2023: Date: 2023-10-23 Day: M Event: Mini-2 Classes Begin |Fall 2023: Date: 2023-01-08 Day: M Event: Fall Deans' Lists Posted|Fall 2023: Date: 2023-12-08 Day: F Event: Semester & Mini-2 Last Day of Classes ",
        "retrieved_relevant_part": true,
        "model_answer": "The mini-2 classes begin fall 2023",
        "actual_answer": "The event happening on 2023-10-23 is Mini-2 Classes Begin",
        "exact_match": false,
        "f1_max_min_avg": [
            0.13,
            0.13,
            0.13
        ],
        "recall_max_min_avg": [
            0.11,
            0.11,
            0.11
        ]
    },
    {
        "question": "Based on background information, What is the specific event listed for 2024-01-15, in Spring 2024",
        "context": "Spring 2024: Date: 2024-01-15 Day: M Event: Martin Luther King Day; No Classes",
        "retrieved_doc": "Spring 2024: Date: 2024-03-15 Day: F Event: Mini-4 add, audit & tuition adjustment drop deadline (1)|April 11, 2024: Event Name:|Spring 2024: Date: 2024-05-01 Day: W Event: Reading Day",
        "retrieved_relevant_part": false,
        "model_answer": "The mini-4 add, audit & tuition adjustment drop deadline",
        "actual_answer": "Martin Luther King Day",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "According to background information, When is the [Make-Up Final Examinations] in Spring 2024?",
        "context": "Spring 2024: Date: 2024-05-07 Day: Tu Event: Make-Up Final Examinations",
        "retrieved_doc": "Spring 2024: Date: 2024-05-07 Day: Tu Event: Make-Up Final Examinations|Fall 2023: Date: 2023-12-18 Day: M Event: Make-Up Final Exams|Spring 2024: Date: 2024-05-02-2024-05-03 Day: Th-F Event: Final Examinations ",
        "retrieved_relevant_part": true,
        "model_answer": "The [Make-Up Final Examinations] in Spring 2024 is on May 2 and May 3",
        "actual_answer": "The event Make-Up Final Examinations happens on 2024-05-07",
        "exact_match": false,
        "f1_max_min_avg": [
            0.27,
            0.27,
            0.27
        ],
        "recall_max_min_avg": [
            0.38,
            0.38,
            0.38
        ]
    },
    {
        "question": "According to background information, What happens in 2023-10-13 on university calendar?",
        "context": "Fall 2023: Date: 2023-10-13 Day: F Event: Mini-1 Last Day of Classes",
        "retrieved_doc": "Fall 2023: Date: 2023-08-28 Day: M Event: Semester & Mini-1 Classes Begin|Fall 2023: Date: 2023-12-08 Day: F Event: Semester & Mini-2 Last Day of Classes |Fall 2023: Date: 2023-10-13 Day: F Event: Mini-1 Last Day of Classes",
        "retrieved_relevant_part": true,
        "model_answer": "The last day of classes for the semester and mini-1 is on October 13.<|endoftext|>",
        "actual_answer": "The event happening on 2023-10-13 is Mini-1 Last Day of Classes",
        "exact_match": false,
        "f1_max_min_avg": [
            0.32,
            0.32,
            0.32
        ],
        "recall_max_min_avg": [
            0.36,
            0.36,
            0.36
        ]
    },
    {
        "question": "Based on background information, What is the specific event listed for 2024-07-08, in Summer One_All 2024",
        "context": "Summer One_All 2024: Date: 2024-07-08 Day: M Event: Mini-6 drop deadline; withdrawal grade assigned after this date (2)",
        "retrieved_doc": "Summer One_All 2024: Date: 2024-08-01 Day: Th Event: Semester & Mini-6 Last Day of Classes|Summer One_All 2024: Date: 2024-07-08 Day: M Event: Mini-6 drop deadline; withdrawal grade assigned after this date (2)|Summer One_All 2024: Date: 2024-05-13 Day: M Event: Semester & Mini-5 Classes Begin",
        "retrieved_relevant_part": true,
        "model_answer": "The mini-6 drop deadline.<|endoftext|>",
        "actual_answer": "The event happening on 2024-07-08 is Mini-6 drop deadline",
        "exact_match": false,
        "f1_max_min_avg": [
            0.31,
            0,
            0.155
        ],
        "recall_max_min_avg": [
            0.22,
            0,
            0.11
        ]
    },
    {
        "question": "According to background information, When is the [Reading Day] in Spring 2024?",
        "context": "Spring 2024: Date: 2024-05-01 Day: W Event: Reading Day",
        "retrieved_doc": "Spring 2024: Date: 2024-05-01 Day: W Event: Reading Day|Spring 2024: Date: 2024-05-04-2024-05-05 Day: Sa-Su Event: Reading Days|Fall 2023: Date: 2023-12-13 Day: W Event: Reading Day ",
        "retrieved_relevant_part": true,
        "model_answer": "The [Reading Day] in Spring 2024 is on May 1 and May 4",
        "actual_answer": "2024-05-01",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Based on the given information, Does Spring 2024 Mini-3 Faculty Course Evaluations open happen at 2024-01-15",
        "context": "Spring 2024: Date: 2024-02-19 Day: M Event: Mini-3 Faculty Course Evaluations open",
        "retrieved_doc": "Spring 2024: Date: 2024-02-19 Day: M Event: Mini-3 Faculty Course Evaluations open|Spring 2024: Date: 2024-04-15 Day: M Event: Semester & Mini-4 Faculty Course Evaluations open |Spring 2024: Date: 2024-03-02 Day: Sa Event: Mini-3 Faculty Course Evaluations close",
        "retrieved_relevant_part": true,
        "model_answer": "No",
        "actual_answer": "No",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "According to background information, When is the [Mini-1 Exams] in Fall 2023?",
        "context": "Fall 2023: Date: 2023-10-14 Day: Sa Event: Mini-1 Exams",
        "retrieved_doc": "Fall 2023: Date: 2023-10-14 Day: Sa Event: Mini-1 Exams|Fall 2023: Date: 2023-08-28 Day: M Event: Semester & Mini-1 Classes Begin|Fall 2023: Date: 2023-10-23 Day: M Event: Mini-2 Classes Begin ",
        "retrieved_relevant_part": true,
        "model_answer": "The mini-1 exams are on October 14",
        "actual_answer": "2023-10-14",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Based on background information, What is the specific event listed for 2024-07-29, in Summer One_All 2024",
        "context": "Summer One_All 2024: Date: 2024-07-29 Day: M Event: Semester & Mini-6 Faculty Course Evalutations open ",
        "retrieved_doc": "Summer One_All 2024: Date: 2024-06-24 Day: M Event: Mini-6 First Day of Classes|Summer One_All 2024: Date: 2024-06-25 Day: T Event: Mini-5 Final Grades Due by 4 pm|Summer One_All 2024: Date: 2024-05-13 Day: M Event: Semester & Mini-5 Classes Begin",
        "retrieved_relevant_part": false,
        "model_answer": "The mini-6 first day of classes",
        "actual_answer": "Semester & Mini-6 Faculty Course Evalutations open",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "According to background information, When is the [Mini-1 add, audit & tuition adjustment drop deadline  (1)] in Fall 2023?",
        "context": "Fall 2023: Date: 2023-09-01 Day: F Event: Mini-1 add, audit & tuition adjustment drop deadline  (1)",
        "retrieved_doc": "Fall 2023: Date: 2023-10-27 Day: F Event: Mini-2 add, audit & tuition adjustment drop deadline (1)|Fall 2023: Date: 2023-09-01 Day: F Event: Mini-1 add, audit & tuition adjustment drop deadline  (1)|Fall 2023: Date: 2023-09-11 Day: M Event: Semester add, audit & tuition adjustment drop deadline (1)",
        "retrieved_relevant_part": true,
        "model_answer": "The mini-1 add, audit & tuition adjustment drop deadline for Fall 2023 is on October 27",
        "actual_answer": "2023-09-01",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Based on the given information, Does Summer One_All 2024 Independence Day; University Closed & No Classes happen at 2024-05-06",
        "context": "Summer One_All 2024: Date: 2024-07-04 Day: Th Event: Independence Day; University Closed & No Classes",
        "retrieved_doc": "Summer One_All 2024: Date: 2024-07-04 Day: Th Event: Independence Day; University Closed & No Classes|Summer One_All 2024: Date: 2024-05-27 Day: M Event: Memorial Day; University Closed & No Classes|Summer Two 2024: Date: 2024-07-04 Day: Th Event: Independence Day; University Closed & No Classes",
        "retrieved_relevant_part": true,
        "model_answer": "False",
        "actual_answer": "No",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Based on background information, What is the specific event listed for 2023-09-01, in Fall 2023",
        "context": "Fall 2023: Date: 2023-09-01 Day: F Event: Mini-1 add, audit & tuition adjustment drop deadline  (1)",
        "retrieved_doc": "Fall 2023: Date: 2023-08-28 Day: M Event: Semester & Mini-1 Classes Begin|Fall 2023: Date: 2023-01-08 Day: M Event: Fall Deans' Lists Posted|Fall 2023: Date: 2023-09-01 Day: F Event: Mini-1 add, audit & tuition adjustment drop deadline  (1)",
        "retrieved_relevant_part": true,
        "model_answer": "The mini-1 add, audit & tuition adjustment drop deadline.<|endoftext|>",
        "actual_answer": "Mini-1 add, audit & tuition adjustment drop deadline  (1)",
        "exact_match": false,
        "f1_max_min_avg": [
            0.67,
            0.67,
            0.67
        ],
        "recall_max_min_avg": [
            0.67,
            0.67,
            0.67
        ]
    },
    {
        "question": "Based on background information, What is the specific event listed for 2024-05-27, in Summer One_All 2024",
        "context": "Summer One_All 2024: Date: 2024-05-27 Day: M Event: Memorial Day; University Closed & No Classes",
        "retrieved_doc": "Summer One_All 2024: Date: 2024-06-25 Day: T Event: Mini-5 Final Grades Due by 4 pm|Summer One_All 2024: Date: 2024-05-13 Day: M Event: Semester & Mini-5 Classes Begin|Summer One_All 2024: Date: 2024-06-20 Day: Th Event: Mini-5 Last Day of Classes",
        "retrieved_relevant_part": false,
        "model_answer": "The mini-5 final grades are due by 4 pm",
        "actual_answer": "Memorial Day",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Based on the given information, Does Fall 2023 Mini-2 drop deadline; withdrawal grade assigned after this date (2) happen at 2023-11-15",
        "context": "Fall 2023: Date: 2023-11-15 Day: W Event: Mini-2 drop deadline; withdrawal grade assigned after this date (2)",
        "retrieved_doc": "Fall 2023: Date: 2023-11-15 Day: W Event: Mini-2 drop deadline; withdrawal grade assigned after this date (2)|Fall 2023: Date: 2023-09-20 Day: W Event: Mini-1 drop deadline; withdrawal grade assigned after this date (2)|Fall 2023: Date: 2023-10-09 Day: M Event: Semester drop deadline; withdrawal grade assigned after this date",
        "retrieved_relevant_part": true,
        "model_answer": "Yes",
        "actual_answer": "Yes",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Based on the given information, Does Summer One_All 2024 Mini-5 Faculty Course Evaluations open happen at 2024-06-14",
        "context": "Summer One_All 2024: Date: 2024-06-14 Day: F Event: Mini-5 Faculty Course Evaluations open",
        "retrieved_doc": "Summer One_All 2024: Date: 2024-06-14 Day: F Event: Mini-5 Faculty Course Evaluations open|Summer One_All 2024: Date: 2024-06-21 Day: F Event: Mini-5 Faculty Course Evaluations close |Summer One_All 2024: Date: 2024-07-29 Day: M Event: Semester & Mini-6 Faculty Course Evalutations open ",
        "retrieved_relevant_part": true,
        "model_answer": "Yes",
        "actual_answer": "Yes",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Based on background information, What is the specific event listed for 2024-05-08, in Spring 2024",
        "context": "Spring 2024: Date: 2024-05-08 Day: W Event: Graduating Final Grades Due by 4 pm",
        "retrieved_doc": "Spring 2024: Date: 2024-05-08 Day: W Event: Graduating Final Grades Due by 4 pm|Spring 2024: Date: 2024-03-01 Day: F Event: Mini-3 Last Day of Classes |April 11, 2024: Event Name:",
        "retrieved_relevant_part": true,
        "model_answer": "W",
        "actual_answer": "Graduating Final Grades Due by 4 pm",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Based on the given information, Does Summer One_All 2024 Mini-6 First Day of Classes happen at 2024-06-24",
        "context": "Summer One_All 2024: Date: 2024-06-24 Day: M Event: Mini-6 First Day of Classes",
        "retrieved_doc": "Summer One_All 2024: Date: 2024-06-24 Day: M Event: Mini-6 First Day of Classes|Summer One_All 2024: Date: 2024-05-13 Day: M Event: Semester & Mini-5 Classes Begin|Summer One_All 2024: Date: 2024-08-01 Day: Th Event: Semester & Mini-6 Last Day of Classes",
        "retrieved_relevant_part": true,
        "model_answer": "Yes",
        "actual_answer": "Yes",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "According to background information, What happens in 2024-07-29 on university calendar?",
        "context": "Summer One_All 2024: Date: 2024-07-29 Day: M Event: Semester & Mini-6 Faculty Course Evalutations open ",
        "retrieved_doc": "Spring 2024: Date: 2024-01-29 Day: M Event: Semester add, audit & tuition adjustment drop deadline (1)|Summer Two 2024: Date: 2024-07-04 Day: Th Event: Independence Day; University Closed & No Classes|Spring 2024: Date: 2024-05-12 Day: Su Event: Commencement",
        "retrieved_relevant_part": false,
        "model_answer": "The semester add, audit & tuition adjustment drop deadline is on the university calendar",
        "actual_answer": "The event happening on 2024-07-29 is Semester & Mini-6 Faculty Course Evalutations open",
        "exact_match": false,
        "f1_max_min_avg": [
            0.3,
            0.3,
            0.3
        ],
        "recall_max_min_avg": [
            0.31,
            0.31,
            0.31
        ]
    },
    {
        "question": "Based on the given information, Does Fall 2023 Mini-1 voucher election deadline (4) happen at 2023-10-13",
        "context": "Fall 2023: Date: 2023-10-13 Day: F Event: Mini-1 voucher election deadline (4)",
        "retrieved_doc": "Fall 2023: Date: 2023-10-13 Day: F Event: Mini-1 voucher election deadline (4)|Spring 2024: Date: 2024-03-01 Day: F Event: Mini-3 voucher deadline (4)|Fall 2023: Date: 2023-12-08 Day: F Event: Semester & Mini-2 voucher deadline (4)",
        "retrieved_relevant_part": true,
        "model_answer": "Yes",
        "actual_answer": "Yes",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "According to background information, When is the [Final Examinations ] in Spring 2024?",
        "context": "Spring 2024: Date: 2024-04-29-2024-04-30 Day: M-Tu Event: Final Examinations ",
        "retrieved_doc": "Spring 2024: Date: 2024-05-06 Day: M Event: Final Examinations|Spring 2024: Date: 2024-05-02-2024-05-03 Day: Th-F Event: Final Examinations |Spring 2024: Date: 2024-04-29-2024-04-30 Day: M-Tu Event: Final Examinations ",
        "retrieved_relevant_part": true,
        "model_answer": "The [Final Examinations] in Spring 2024 is on May 6, 2024",
        "actual_answer": "2024-04-29-2024-04-30",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Based on the given information, Does Summer Two 2024 Summer Semester  Two drop deadline; withdrawal grade assigned after this date (2) happen at 2024-07-29",
        "context": "Summer Two 2024: Date: 2024-07-08 Day: M Event: Summer Semester  Two drop deadline; withdrawal grade assigned after this date (2)",
        "retrieved_doc": "Summer Two 2024: Date: 2024-07-08 Day: M Event: Summer Semester  Two drop deadline; withdrawal grade assigned after this date (2)|Summer One_All 2024: Date: 2024-06-18 Day: T Event: Semester course drop deadline; withdrawal grade assigned after this date (2)|Summer One_All 2024: Date: 2024-07-08 Day: M Event: Mini-6 drop deadline; withdrawal grade assigned after this date (2)",
        "retrieved_relevant_part": true,
        "model_answer": "No",
        "actual_answer": "No",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "According to background information, When is the [Semester pass/no pass & withdrawal deadline (3)] in Summer One_All 2024?",
        "context": "Summer One_All 2024: Date: 2024-07-05 Day: F Event: Semester pass/no pass & withdrawal deadline (3)",
        "retrieved_doc": "Summer One_All 2024: Date: 2024-07-05 Day: F Event: Semester pass/no pass & withdrawal deadline (3)|Summer Two 2024: Date: 2024-07-22 Day: F Event: Summer Semester  Two pass/no pass & withdrawal deadline (3)|Spring 2024: Date: 2024-04-01 Day: M Event: Semester pass/no pass & withdrawal deadline (3)",
        "retrieved_relevant_part": true,
        "model_answer": "The semester pass/no pass & withdrawal deadline (3) in Summer One_All 2024 is on July 5",
        "actual_answer": "The event Semester pass/no pass & withdrawal deadline (3) happens on 2024-07-05",
        "exact_match": false,
        "f1_max_min_avg": [
            0.57,
            0.57,
            0.57
        ],
        "recall_max_min_avg": [
            0.67,
            0.67,
            0.67
        ]
    },
    {
        "question": "According to background information, What happens in 2024-05-06 on university calendar?",
        "context": "Spring 2024: Date: 2024-05-06 Day: M Event: Final Examinations",
        "retrieved_doc": "Spring 2024: Date: 2024-05-06 Day: M Event: Semester & Mini-4 Faculty Course Evaluations close |Spring 2024: Date: 2024-05-06 Day: M Event: Final Examinations|Summer Two 2024: Date: 2024-06-24 Day: M Event: Summer Semester Two Classes Begin",
        "retrieved_relevant_part": true,
        "model_answer": "The semester and mini-4 courses begin",
        "actual_answer": "The event happening on 2024-05-06 is Final Examinations",
        "exact_match": false,
        "f1_max_min_avg": [
            0.14,
            0.14,
            0.14
        ],
        "recall_max_min_avg": [
            0.12,
            0.12,
            0.12
        ]
    },
    {
        "question": "Based on the given information, Does Summer One_All 2024 Mini-5 pass/no pass & withdrawal deadline (3) happen at 2024-06-24",
        "context": "Summer One_All 2024: Date: 2024-06-07 Day: F Event: Mini-5 pass/no pass & withdrawal deadline (3)",
        "retrieved_doc": "Summer One_All 2024: Date: 2024-06-07 Day: F Event: Mini-5 pass/no pass & withdrawal deadline (3)|Summer One_All 2024: Date: 2024-07-22 Day: F Event: Mini-6 pass/no pass & withdrawal deadline (3)|Summer One_All 2024: Date: 2024-07-05 Day: F Event: Semester pass/no pass & withdrawal deadline (3)",
        "retrieved_relevant_part": true,
        "model_answer": "No",
        "actual_answer": "No",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "According to background information, When is the [Summer Semester  Two pass/no pass & withdrawal deadline (3)] in Summer Two 2024?",
        "context": "Summer Two 2024: Date: 2024-07-22 Day: F Event: Summer Semester  Two pass/no pass & withdrawal deadline (3)",
        "retrieved_doc": "Summer Two 2024: Date: 2024-07-22 Day: F Event: Summer Semester  Two pass/no pass & withdrawal deadline (3)|Summer One_All 2024: Date: 2024-07-05 Day: F Event: Semester pass/no pass & withdrawal deadline (3)|Spring 2024: Date: 2024-04-01 Day: M Event: Semester pass/no pass & withdrawal deadline (3)",
        "retrieved_relevant_part": true,
        "model_answer": "The [Summer Semester  Two pass/no pass & withdrawal deadline (3)] in Summer Two 2024 is on July 22",
        "actual_answer": "The event Summer Semester  Two pass/no pass & withdrawal deadline (3) happens on 2024-07-22",
        "exact_match": false,
        "f1_max_min_avg": [
            0.63,
            0.63,
            0.63
        ],
        "recall_max_min_avg": [
            0.71,
            0.71,
            0.71
        ]
    },
    {
        "question": "Based on background information, What is the specific event listed for 2024-02-19, in Spring 2024",
        "context": "Spring 2024: Date: 2024-02-19 Day: M Event: Mini-3 Faculty Course Evaluations open",
        "retrieved_doc": "Spring 2024: Date: 2024-03-12 Day: T Event: Summer 2024 Registration Opens |Spring 2024: Date: 2024-05-06 Day: M Event: Final Examinations|April 11, 2024: Event Name:",
        "retrieved_relevant_part": false,
        "model_answer": "The specific event for 2024-02-19, in Spring 2024, is not mentioned in the context.<|endoftext|>",
        "actual_answer": "The event happening on 2024-02-19 is Mini-3 Faculty Course Evaluations open",
        "exact_match": false,
        "f1_max_min_avg": [
            0.24,
            0.24,
            0.24
        ],
        "recall_max_min_avg": [
            0.27,
            0.27,
            0.27
        ]
    },
    {
        "question": "Based on the given information, Does Spring 2024 Mini-3 voucher deadline (4) happen at 2023-10-13",
        "context": "Spring 2024: Date: 2024-03-01 Day: F Event: Mini-3 voucher deadline (4)",
        "retrieved_doc": "Spring 2024: Date: 2024-03-01 Day: F Event: Mini-3 voucher deadline (4)|Spring 2024: Date: 2024-04-26 Day: F Event: Semester & Mini-4 voucher deadline (4)|Fall 2023: Date: 2023-10-13 Day: F Event: Mini-1 voucher election deadline (4)",
        "retrieved_relevant_part": true,
        "model_answer": "No",
        "actual_answer": "No",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Based on background information, What is the specific event listed for 2024-04-01, in Spring 2024",
        "context": "Spring 2024: Date: 2024-04-01 Day: M Event: Semester pass/no pass & withdrawal deadline (3)",
        "retrieved_doc": "Spring 2024: Date: 2024-05-01 Day: W Event: Reading Day|April 11, 2024: Event Name:|Spring 2024: Date: 2024-03-11 Day: M Event: First day of Mini-4 Classes",
        "retrieved_relevant_part": false,
        "model_answer": "W",
        "actual_answer": "The event happening on 2024-04-01 is Semester pass/no pass & withdrawal deadline (3)",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "According to background information, When is the [Mini-5 pass/no pass & withdrawal deadline (3)] in Summer One_All 2024?",
        "context": "Summer One_All 2024: Date: 2024-06-07 Day: F Event: Mini-5 pass/no pass & withdrawal deadline (3)",
        "retrieved_doc": "Summer One_All 2024: Date: 2024-06-07 Day: F Event: Mini-5 pass/no pass & withdrawal deadline (3)|Summer One_All 2024: Date: 2024-07-22 Day: F Event: Mini-6 pass/no pass & withdrawal deadline (3)|Summer One_All 2024: Date: 2024-07-05 Day: F Event: Semester pass/no pass & withdrawal deadline (3)",
        "retrieved_relevant_part": true,
        "model_answer": "The [Mini-5 pass/no pass & withdrawal deadline (3)] in Summer One_All 2024 is on June 7, 2024",
        "actual_answer": "The event Mini-5 pass/no pass & withdrawal deadline (3) happens on 2024-06-07",
        "exact_match": false,
        "f1_max_min_avg": [
            0.48,
            0.48,
            0.48
        ],
        "recall_max_min_avg": [
            0.58,
            0.58,
            0.58
        ]
    },
    {
        "question": "Based on the given information, Does Summer One_All 2024 Mini-5 Faculty Course Evaluations open happen at 2024-06-14",
        "context": "Summer One_All 2024: Date: 2024-06-14 Day: F Event: Mini-5 Faculty Course Evaluations open",
        "retrieved_doc": "Summer One_All 2024: Date: 2024-06-14 Day: F Event: Mini-5 Faculty Course Evaluations open|Summer One_All 2024: Date: 2024-06-21 Day: F Event: Mini-5 Faculty Course Evaluations close |Summer One_All 2024: Date: 2024-07-29 Day: M Event: Semester & Mini-6 Faculty Course Evalutations open ",
        "retrieved_relevant_part": true,
        "model_answer": "Yes",
        "actual_answer": "Yes",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Based on the given information, Does Spring 2024 Final Grades Due by 4 pm happen at 2023-12-17",
        "context": "Spring 2024: Date: 2024-05-14 Day: Tu Event: Final Grades Due by 4 pm",
        "retrieved_doc": "Spring 2024: Date: 2024-05-08 Day: W Event: Graduating Final Grades Due by 4 pm|Spring 2024: Date: 2024-05-14 Day: Tu Event: Final Grades Due by 4 pm|Spring 2024: Date: 2024-03-11 Day: M Event: Mid-Semester & Mini-3 grades due by 4 pm",
        "retrieved_relevant_part": true,
        "model_answer": "No",
        "actual_answer": "No",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Based on background information, What is the specific event listed for 2024-06-21, in Summer One_All 2024",
        "context": "Summer One_All 2024: Date: 2024-06-21 Day: F Event: Mini-5 Final Exams",
        "retrieved_doc": "Summer One_All 2024: Date: 2024-06-24 Day: M Event: Mini-6 First Day of Classes|Summer One_All 2024: Date: 2024-06-21 Day: F Event: Mini-5 Final Exams|Summer One_All 2024: Date: 2024-06-25 Day: T Event: Mini-5 Final Grades Due by 4 pm",
        "retrieved_relevant_part": true,
        "model_answer": "The mini-5 final exams",
        "actual_answer": "Mini-5 Final Exams",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "According to background information, What happens in 2023-12-18 on university calendar?",
        "context": "Fall 2023: Date: 2023-12-18 Day: M Event: Semester & Mini-2 Faculty Course Evaluations close",
        "retrieved_doc": "Fall 2023: Date: 2023-12-08 Day: F Event: Semester & Mini-2 Last Day of Classes |Fall 2023: Date: 2023-08-28 Day: M Event: Semester & Mini-1 Classes Begin|Fall 2023: Date: 2023-12-08 Day: F Event: Semester & Mini-2 voucher deadline (4)",
        "retrieved_relevant_part": false,
        "model_answer": "The semester & mini-2 classes begin",
        "actual_answer": "The event happening on 2023-12-18 is Semester & Mini-2 Faculty Course Evaluations close",
        "exact_match": false,
        "f1_max_min_avg": [
            0.21,
            0.21,
            0.21
        ],
        "recall_max_min_avg": [
            0.15,
            0.15,
            0.15
        ]
    },
    {
        "question": "Based on the given information, Does Summer Two 2024 Summer Semester  Two Last Day of Classes happen at 2024-08-01",
        "context": "Summer Two 2024: Date: 2024-08-01 Day: Th Event: Summer Semester  Two Last Day of Classes",
        "retrieved_doc": "Summer Two 2024: Date: 2024-08-01 Day: Th Event: Summer Semester  Two Last Day of Classes|Summer Two 2024: Date: 2024-06-24 Day: M Event: Summer Semester Two Classes Begin|Summer One_All 2024: Date: 2024-08-01 Day: Th Event: Semester & Mini-6 Last Day of Classes",
        "retrieved_relevant_part": true,
        "model_answer": "Yes",
        "actual_answer": "Yes",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Based on background information, What is the specific event listed for 2024-05-14, in Spring 2024",
        "context": "Spring 2024: Date: 2024-05-14 Day: Tu Event: Final Grades Due by 4 pm",
        "retrieved_doc": "Spring 2024: Date: 2024-05-01 Day: W Event: Reading Day|Spring 2024: Date: 2024-03-12 Day: T Event: Summer 2024 Registration Opens |April 11, 2024: Event Name:",
        "retrieved_relevant_part": false,
        "model_answer": "W",
        "actual_answer": "The event happening on 2024-05-14 is Final Grades Due by 4 pm",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "According to background information, When is the [Final Exams ] in Fall 2023?",
        "context": "Fall 2023: Date: 2023-12-11-2023-12-12 Day: M-Tu Event: Final Exams ",
        "retrieved_doc": "Fall 2023: Date: 2023-12-14-2023-12-15 Day: Th-F Event: Final Exams |Fall 2023: Date: 2023-12-11-2023-12-12 Day: M-Tu Event: Final Exams |Fall 2023: Date: 2023-12-17 Day: Su Event: Final Exams ",
        "retrieved_relevant_part": true,
        "model_answer": "The [Final Exams] in Fall 2023 is on December 11-12",
        "actual_answer": "The event Final Exams  happens on 2023-12-11-2023-12-12",
        "exact_match": false,
        "f1_max_min_avg": [
            0.24,
            0.24,
            0.24
        ],
        "recall_max_min_avg": [
            0.29,
            0.29,
            0.29
        ]
    },
    {
        "question": "Based on background information, What is the specific event listed for 2024-07-04, in Summer Two 2024",
        "context": "Summer Two 2024: Date: 2024-07-04 Day: Th Event: Independence Day; University Closed & No Classes",
        "retrieved_doc": "Summer Two 2024: Date: 2024-07-22 Day: F Event: Summer Semester  Two pass/no pass & withdrawal deadline (3)|Summer Two 2024: Date: 2024-06-24 Day: M Event: Summer Semester Two Classes Begin|Summer Two 2024: Date: 2024-06-28 Day: F Event: Summer Semester  Two add, audit & tuition adjustment drop deadline (1)",
        "retrieved_relevant_part": false,
        "model_answer": "Summer Semester Two classes begin",
        "actual_answer": "The event happening on 2024-07-04 is Independence Day",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Based on background information, What is the specific event listed for 2024-02-26, in Spring 2024",
        "context": "Spring 2024: Date: 2024-02-26 Day: M Event: Semester course drop deadline; withdrawal grade assigned after this date (2)",
        "retrieved_doc": "Spring 2024: Date: 2024-04-26 Day: F Event: Last Day of Classes|Spring 2024: Date: 2024-04-26 Day: F Event: Semester & Mini-4 voucher deadline (4)|Spring 2024: Date: 2024-03-12 Day: T Event: Summer 2024 Registration Opens ",
        "retrieved_relevant_part": false,
        "model_answer": "The last day of classes.<|endoftext|>",
        "actual_answer": "The event happening on 2024-02-26 is Semester course drop deadline",
        "exact_match": false,
        "f1_max_min_avg": [
            0.13,
            0,
            0.065
        ],
        "recall_max_min_avg": [
            0.1,
            0,
            0.05
        ]
    },
    {
        "question": "According to background information, When is the [Reading Day] in Spring 2024?",
        "context": "Spring 2024: Date: 2024-05-01 Day: W Event: Reading Day",
        "retrieved_doc": "Spring 2024: Date: 2024-05-01 Day: W Event: Reading Day|Spring 2024: Date: 2024-05-04-2024-05-05 Day: Sa-Su Event: Reading Days|Fall 2023: Date: 2023-12-13 Day: W Event: Reading Day ",
        "retrieved_relevant_part": true,
        "model_answer": "The [Reading Day] in Spring 2024 is on May 1 and May 4",
        "actual_answer": "2024-05-01",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "According to background information, When is the [Spring Deans' Lists Posted] in Spring 2024?",
        "context": "Spring 2024: Date: 2023-05-21 Day: Tu Event: Spring Deans' Lists Posted",
        "retrieved_doc": "Spring 2024: Date: 2023-05-21 Day: Tu Event: Spring Deans' Lists Posted|Fall 2023: Date: 2023-01-08 Day: M Event: Fall Deans' Lists Posted|Spring 2024: Date: 2024-03-11 Day: M Event: Mid-Semester & Mini-3 grades due by 4 pm",
        "retrieved_relevant_part": true,
        "model_answer": "The [Spring Deans' Lists Posted] in Spring 2024 is on May 21",
        "actual_answer": "The event Spring Deans' Lists Posted happens on 2023-05-21",
        "exact_match": false,
        "f1_max_min_avg": [
            0.48,
            0.48,
            0.48
        ],
        "recall_max_min_avg": [
            0.56,
            0.56,
            0.56
        ]
    },
    {
        "question": "Based on the given information, Does Fall 2023 Spring 2024 Registration Week happen at 2023-11-13-2023-11-17",
        "context": "Fall 2023: Date: 2023-11-13-2023-11-17 Day: M-F Event: Spring 2024 Registration Week",
        "retrieved_doc": "Fall 2023: Date: 2023-11-13-2023-11-17 Day: M-F Event: Spring 2024 Registration Week|Spring 2024: Date: 2024-04-15-2024-04-19 Day: M-F Event: Fall 2024 Registration Week|Spring 2024: Date: 2024-03-12 Day: T Event: Summer 2024 Registration Opens ",
        "retrieved_relevant_part": true,
        "model_answer": "Yes",
        "actual_answer": "Yes",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Based on the given information, Does Summer One_All 2024 Semester & Mini-6 Faculty Course Evaluations close happen at 2024-08-02",
        "context": "Summer One_All 2024: Date: 2024-08-02 Day: F Event: Semester & Mini-6 Faculty Course Evaluations close",
        "retrieved_doc": "Summer One_All 2024: Date: 2024-08-02 Day: F Event: Semester & Mini-6 Faculty Course Evaluations close|Summer One_All 2024: Date: 2024-06-21 Day: F Event: Mini-5 Faculty Course Evaluations close |Summer One_All 2024: Date: 2024-07-29 Day: M Event: Semester & Mini-6 Faculty Course Evalutations open ",
        "retrieved_relevant_part": true,
        "model_answer": "No",
        "actual_answer": "Yes",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Based on the given information, Does Spring 2024 Mid-Semester & Mini-3 grades due by 4 pm happen at 2024-04-11-2024-04-13",
        "context": "Spring 2024: Date: 2024-03-11 Day: M Event: Mid-Semester & Mini-3 grades due by 4 pm",
        "retrieved_doc": "Spring 2024: Date: 2024-03-11 Day: M Event: Mid-Semester & Mini-3 grades due by 4 pm|Fall 2023: Date: 2023-10-23 Day: M Event: Mid-Semester & Mini-1 grades due by 4 pm|Spring 2024: Date: 2024-05-14 Day: Tu Event: Final Grades Due by 4 pm",
        "retrieved_relevant_part": true,
        "model_answer": "No",
        "actual_answer": "No",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "According to background information, What happens in 2024-05-13 on university calendar?",
        "context": "Summer One_All 2024: Date: 2024-05-13 Day: M Event: Semester & Mini-5 Classes Begin",
        "retrieved_doc": "Summer One_All 2024: Date: 2024-05-13 Day: M Event: Semester & Mini-5 Classes Begin|Summer Two 2024: Date: 2024-07-04 Day: Th Event: Independence Day; University Closed & No Classes|Spring 2024: Date: 2024-04-01 Day: M Event: Semester pass/no pass & withdrawal deadline (3)",
        "retrieved_relevant_part": true,
        "model_answer": "The university is closed and there are no classes",
        "actual_answer": "Semester & Mini-5 Classes Begin",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Based on background information, What is the specific event listed for 2024-06-07, in Summer One_All 2024",
        "context": "Summer One_All 2024: Date: 2024-06-07 Day: F Event: Mini-5 pass/no pass & withdrawal deadline (3)",
        "retrieved_doc": "Summer One_All 2024: Date: 2024-06-24 Day: M Event: Mini-6 First Day of Classes|Summer One_All 2024: Date: 2024-06-07 Day: F Event: Mini-5 pass/no pass & withdrawal deadline (3)|Summer One_All 2024: Date: 2024-06-25 Day: T Event: Mini-5 Final Grades Due by 4 pm",
        "retrieved_relevant_part": true,
        "model_answer": "The mini-5 pass/no pass & withdrawal deadline.<|endoftext|>",
        "actual_answer": "The event happening on 2024-06-07 is Mini-5 pass/no pass & withdrawal deadline (3)",
        "exact_match": false,
        "f1_max_min_avg": [
            0.5,
            0.5,
            0.5
        ],
        "recall_max_min_avg": [
            0.38,
            0.38,
            0.38
        ]
    },
    {
        "question": "Based on the given information, Does Fall 2023 Final Exams  happen at 2023-12-11-2023-12-12",
        "context": "Fall 2023: Date: 2023-12-11-2023-12-12 Day: M-Tu Event: Final Exams ",
        "retrieved_doc": "Fall 2023: Date: 2023-12-14-2023-12-15 Day: Th-F Event: Final Exams |Fall 2023: Date: 2023-12-11-2023-12-12 Day: M-Tu Event: Final Exams |Fall 2023: Date: 2023-12-17 Day: Su Event: Final Exams ",
        "retrieved_relevant_part": true,
        "model_answer": "Yes",
        "actual_answer": "Yes",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "When does the class International Finance Recitation Section A4  offered in Sp end?",
        "context": "Spring offering: Course: 45825 Title: International Finance Units: 6.0 Lec/Sec: Section A4 Days: Tuesday, Thursday Begin: 10:00AM End: 11:45AM Bldg/Room: TEP 2118 Location: Pittsburgh, Pennsylvania Instructor(s): Khokher ",
        "retrieved_doc": "Fall offering: Course: 70398 Title: International Finance Units: 9.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 12:30PM End: 01:50PM Bldg/Room: TEP 2701 Location: Pittsburgh, Pennsylvania Instructor(s): Khokher |Spring offering: Course: 45825 Title: International Finance Units: 6.0 Lec/Sec: Section A4 Days: Tuesday, Thursday Begin: 10:00AM End: 11:45AM Bldg/Room: TEP 2118 Location: Pittsburgh, Pennsylvania Instructor(s): Khokher |Fall offering: Course: 70505 Title: Independent Study in Finance Units: 3-18 Lec/Sec: Section A Days: To be announced Begin: 02:00PM End: 03:20PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Hollifield ",
        "retrieved_relevant_part": true,
        "model_answer": "The course International Finance Recitation Section A4 is offered in the fall semester",
        "actual_answer": "11:45AM",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "At what time does the class Financial Statements and Analysis of Companies Recitation Section C1  offered in Fa begin?",
        "context": "Fall offering: Course: 90723 Title: Financial Statements and Analysis of Companies Units: 6.0 Lec/Sec: Section C1 Days: Monday, Wednesday Begin: 11:00AM End: 12:20PM Bldg/Room: HBH 2008 Location: Pittsburgh, Pennsylvania Instructor(s): Gogolak ",
        "retrieved_doc": "Spring offering: Course: 90723 Title: Financial Statements and Analysis of Companies Units: 6.0 Lec/Sec: Section C4 Days: Monday, Wednesday Begin: 02:00PM End: 03:20PM Bldg/Room: HBH 1002 Location: Pittsburgh, Pennsylvania Instructor(s): Stephenson |Spring offering: Course: 90723 Title: Financial Statements and Analysis of Companies Units: 6.0 Lec/Sec: Section C4 Days: Friday Begin: 02:00PM End: 03:20PM Bldg/Room: HBH 1002 Location: Pittsburgh, Pennsylvania Instructor(s): Stephenson |Spring offering: Course: 90723 Title: Financial Statements and Analysis of Companies Units: 6.0 Lec/Sec: Section A3 Days: Monday, Wednesday Begin: 02:00PM End: 03:20PM Bldg/Room: HBH 1202 Location: Pittsburgh, Pennsylvania Instructor(s): Stephenson ",
        "retrieved_relevant_part": false,
        "model_answer": "The class Financial Statements and Analysis of Companies Recitation Section C1 begins at 2:00PM",
        "actual_answer": "11:00AM",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "What is the units of course Principles of Imperative Computation offered in Fa?",
        "context": "Fall offering: Course: 15122 Title: Principles of Imperative Computation Units: 12.0 Lec/Sec: Section M Days: Monday Begin: 12:00PM End: 12:50PM Bldg/Room: GHC CLSTR Location: Pittsburgh, Pennsylvania Instructor(s): Cervesato, Kaynar ",
        "retrieved_doc": "Spring offering: Course: 15122 Title: Principles of Imperative Computation Units: 12.0 Lec/Sec: Section F Days: Monday Begin: 02:00PM End: 02:50PM Bldg/Room: GHC CLSTR Location: Pittsburgh, Pennsylvania Instructor(s): Cervesato, Kohlbrenner |Summer 2 offering: Course: 15122 Title: Principles of Imperative Computation Units: 12.0 Lec/Sec: Section V Days: Monday, Tuesday, Wednesday, Thursday, Friday Begin: 09:30AM End: 10:50AM Bldg/Room: TEP 1403 Location: Pittsburgh, Pennsylvania Instructor(s): Kohlbrenner |Spring offering: Course: 15122 Title: Principles of Imperative Computation Units: 12.0 Lec/Sec: Section F Days: Friday Begin: 02:00PM End: 02:50PM Bldg/Room: WEH 5302 Location: Pittsburgh, Pennsylvania Instructor(s): Cervesato, Kohlbrenner ",
        "retrieved_relevant_part": false,
        "model_answer": "12.0 units",
        "actual_answer": "12.0",
        "exact_match": false,
        "f1_max_min_avg": [
            0.67,
            0.67,
            0.67
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "On which days does the class Big Data Science Recitation Section A4  offered in Sp meet?",
        "context": "Spring offering: Course: 18788 Title: Big Data Science Units: 6.0 Lec/Sec: Section A4 Days: Tuesday, Thursday Begin: 08:00AM End: 09:20AM Bldg/Room: HH 1107 Location: Pittsburgh, Pennsylvania Instructor(s): McSharry ",
        "retrieved_doc": "Spring offering: Course: 67364 Title: Practical Data Science Units: 9.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 02:00PM End: 03:20PM Bldg/Room: HBH 1206 Location: Pittsburgh, Pennsylvania Instructor(s): Sooriamurthi |Spring offering: Course: 18788 Title: Big Data Science Units: 6.0 Lec/Sec: Section A4 Days: Tuesday, Thursday Begin: 08:00AM End: 09:20AM Bldg/Room: HH 1107 Location: Pittsburgh, Pennsylvania Instructor(s): McSharry |Fall offering: Course: 67364 Title: Practical Data Science Units: 9.0 Lec/Sec: Section W Days: Monday, Wednesday Begin: 08:30AM End: 09:45AM Bldg/Room: CMB 3046 Location: Doha, Qatar Instructor(s): Bouamor ",
        "retrieved_relevant_part": true,
        "model_answer": "The class Big Data Science Recitation Section A4 meets on Tuesdays and Thursdays.<|endoftext|>",
        "actual_answer": "Tuesday, Thursday",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Based on the background information, What is the units of course Statistics for IT Managers offered in Fa?",
        "context": "Fall offering: Course: 95796 Title: Statistics for IT Managers Units: 6.0 Lec/Sec: Section D1 Days: Tuesday, Thursday Begin: 03:30PM End: 04:50PM Bldg/Room: HBH 1206 Location: Pittsburgh, Pennsylvania Instructor(s): Garin ",
        "retrieved_doc": "Fall offering: Course: 95796 Title: Statistics for IT Managers Units: 6.0 Lec/Sec: Section D1 Days: Tuesday, Thursday Begin: 03:30PM End: 04:50PM Bldg/Room: HBH 1206 Location: Pittsburgh, Pennsylvania Instructor(s): Garin |Fall offering: Course: 95796 Title: Statistics for IT Managers Units: 6.0 Lec/Sec: Section A1 Days: Monday, Wednesday Begin: 11:00AM End: 12:20PM Bldg/Room: HBH 1206 Location: Pittsburgh, Pennsylvania Instructor(s): Nagin |Fall offering: Course: 95796 Title: Statistics for IT Managers Units: 6.0 Lec/Sec: Section D1 Days: Friday Begin: 03:30PM End: 04:50PM Bldg/Room: HBH A301 Location: Pittsburgh, Pennsylvania Instructor(s): Garin ",
        "retrieved_relevant_part": true,
        "model_answer": "6.0 units",
        "actual_answer": "The units of course Statistics for IT Managers is 6.0",
        "exact_match": false,
        "f1_max_min_avg": [
            0.33,
            0.33,
            0.33
        ],
        "recall_max_min_avg": [
            0.2,
            0.2,
            0.2
        ]
    },
    {
        "question": "When does the class Tap II - Rhythmic Technique/Foundational to Complex Recitation Section B  offered in Fa end?",
        "context": "Fall offering: Course: 54223 Title: Tap II - Rhythmic Technique/Foundational to Complex Units: 2.0 Lec/Sec: Section B Days: Friday Begin: 12:30PM End: 01:20PM Bldg/Room: PCA 306 Location: Pittsburgh, Pennsylvania Instructor(s): Conte ",
        "retrieved_doc": "Fall offering: Course: 54223 Title: Tap II - Rhythmic Technique/Foundational to Complex Units: 2.0 Lec/Sec: Section A Days: Tuesday Begin: 12:30PM End: 01:20PM Bldg/Room: PCA 306 Location: Pittsburgh, Pennsylvania Instructor(s): Conte |Fall offering: Course: 54223 Title: Tap II - Rhythmic Technique/Foundational to Complex Units: 2.0 Lec/Sec: Section B Days: Friday Begin: 12:30PM End: 01:20PM Bldg/Room: PCA 306 Location: Pittsburgh, Pennsylvania Instructor(s): Conte |Spring offering: Course: 54224 Title: Tap II: Rhythmic Technique/Foundational to Complex Units: 2.0 Lec/Sec: Section B Days: Friday Begin: 12:30PM End: 01:20PM Bldg/Room: PCA 306 Location: Pittsburgh, Pennsylvania Instructor(s): Conte ",
        "retrieved_relevant_part": true,
        "model_answer": "The class Tap II - Rhythmic Technique/Foundational to Complex Recitation Section B is offered in the fall semester",
        "actual_answer": "01:20PM",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "On which days does the class Orchestration I Recitation Section A  offered in Fa meet?",
        "context": "Fall offering: Course: 57257 Title: Orchestration I Units: 6.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 09:00AM End: 09:50AM Bldg/Room: CFA M160 Location: Pittsburgh, Pennsylvania Instructor(s): Marthaler ",
        "retrieved_doc": "Fall offering: Course: 57257 Title: Orchestration I Units: 6.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 09:00AM End: 09:50AM Bldg/Room: CFA M160 Location: Pittsburgh, Pennsylvania Instructor(s): Marthaler |Fall offering: Course: 57257 Title: Orchestration I Units: 6.0 Lec/Sec: Section B Days: Tuesday, Thursday Begin: 10:00AM End: 10:50AM Bldg/Room: CFA M160 Location: Pittsburgh, Pennsylvania Instructor(s): Marthaler |Fall offering: Course: 57185 Title: Advanced Solfege I Units: 3.0 Lec/Sec: Section A Days: Monday, Wednesday, Friday Begin: 08:00AM End: 08:50AM Bldg/Room: CFA M160 Location: Pittsburgh, Pennsylvania Instructor(s): Moskal ",
        "retrieved_relevant_part": true,
        "model_answer": "The class Orchestration I Recitation Section A meets on Tuesdays and Thursdays",
        "actual_answer": "Tuesday, Thursday",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "On which days does the class Weight Training: Recitation Section B3  offered in Sp meet?",
        "context": "Spring offering: Course: 69102 Title: Weight Training: Units: 3.0 Lec/Sec: Section B3 Days: Tuesday, Thursday Begin: 12:30PM End: 01:50PM Bldg/Room: CUC COURTS Location: Pittsburgh, Pennsylvania Instructor(s): Connelly ",
        "retrieved_doc": "Spring offering: Course: 69102 Title: Weight Training Units: 3.0 Lec/Sec: Section B3 Days: Monday, Wednesday, Friday Begin: 11:00AM End: 11:50AM Bldg/Room: CUC FITNES Location: Pittsburgh, Pennsylvania Instructor(s): Gauntner, Larsen |Fall offering: Course: 69102 Title: Weight Training Units: 3.0 Lec/Sec: Section B1 Days: Monday, Wednesday, Friday Begin: 09:00AM End: 09:50AM Bldg/Room: CUC FITNES Location: Pittsburgh, Pennsylvania Instructor(s): Goldcamp |Fall offering: Course: 69102 Title: Weight Training: Units: 3.0 Lec/Sec: Section B2 Days: Monday, Wednesday, Friday Begin: 11:00AM End: 11:50AM Bldg/Room: CUC COURTS Location: Pittsburgh, Pennsylvania Instructor(s): Connelly ",
        "retrieved_relevant_part": false,
        "model_answer": "The class Weight Training: Recitation Section B3 meets on Monday, Wednesday, and Friday",
        "actual_answer": "Tuesday, Thursday",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Which campus is class Data Analytics for Organizational Impacts Recitation Section A2  offered in Pittsburgh in Fa?",
        "context": "Fall offering: Course: 94865 Title: Data Analytics for Organizational Impacts Units: 6.0 Lec/Sec: Section A2 Days: Tuesday, Thursday Begin: 03:30PM End: 04:50PM Bldg/Room: HBH 1004 Location: Pittsburgh, Pennsylvania Instructor(s): Escallon Barrios ",
        "retrieved_doc": "Fall offering: Course: 94865 Title: Data Analytics for Organizational Impacts Units: 6.0 Lec/Sec: Section A2 Days: Tuesday, Thursday Begin: 03:30PM End: 04:50PM Bldg/Room: HBH 1004 Location: Pittsburgh, Pennsylvania Instructor(s): Escallon Barrios |Fall offering: Course: 94465 Title: Data Analytics for Organizational Impact Units: 6.0 Lec/Sec: Section A2 Days: Tuesday, Thursday Begin: 03:30PM End: 04:50PM Bldg/Room: HBH 1004 Location: Pittsburgh, Pennsylvania Instructor(s): Escallon Barrios |Fall offering: Course: 95865 Title: Unstructured Data Analytics Units: 6.0 Lec/Sec: Section A2 Days: Tuesday, Thursday Begin: 05:00PM End: 06:20PM Bldg/Room: HBH 1002 Location: Pittsburgh, Pennsylvania Instructor(s): Chen ",
        "retrieved_relevant_part": true,
        "model_answer": "HBH 1004",
        "actual_answer": "Pittsburgh, Pennsylvania",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Where does the class Accounting and Finance Analytics Recitation Section E4  offered in Sp meet?",
        "context": "Spring offering: Course: 95719 Title: Accounting and Finance Analytics Units: 6.0 Lec/Sec: Section E4 Days: Tuesday, Thursday Begin: 05:00PM End: 06:20PM Bldg/Room: HBH 1206 Location: Pittsburgh, Pennsylvania Instructor(s): Gogolak ",
        "retrieved_doc": "Spring offering: Course: 95719 Title: Accounting and Finance Analytics Units: 6.0 Lec/Sec: Section D4 Days: Monday, Wednesday Begin: 09:30AM End: 10:50AM Bldg/Room: HBH 1204 Location: Pittsburgh, Pennsylvania Instructor(s): Stephenson |Fall offering: Course: 95719 Title: Accounting and Finance Analytics Units: 6.0 Lec/Sec: Section D2 Days: Monday, Wednesday Begin: 02:00PM End: 03:20PM Bldg/Room: HBH 1202 Location: Pittsburgh, Pennsylvania Instructor(s): Stephenson |Fall offering: Course: 95719 Title: Accounting and Finance Analytics Units: 6.0 Lec/Sec: Section A2 Days: Monday, Wednesday Begin: 11:00AM End: 12:20PM Bldg/Room: HBH 1006 Location: Pittsburgh, Pennsylvania Instructor(s): Stephenson ",
        "retrieved_relevant_part": false,
        "model_answer": "The class Accounting and Finance Analytics Recitation Section E4 is offered in Building HBH 1204, located in Pittsburgh, Pennsylvania.<|endoftext|>",
        "actual_answer": "HBH 1206",
        "exact_match": false,
        "f1_max_min_avg": [
            0.1,
            0.1,
            0.1
        ],
        "recall_max_min_avg": [
            0.5,
            0.5,
            0.5
        ]
    },
    {
        "question": "On which days does the class IDeATe: Soft Fabrication Skills Recitation Section A1  offered in Fa meet?",
        "context": "Fall offering: Course: 99352 Title: IDeATe: Soft Fabrication Skills Units: 1.0 Lec/Sec: Section A1 Days: Saturday Begin: 10:00AM End: 03:00PM Bldg/Room: HL 106B Location: Pittsburgh, Pennsylvania Instructor(s): Pinchuk ",
        "retrieved_doc": "Fall offering: Course: 99352 Title: IDeATe: Soft Fabrication Skills Units: 1.0 Lec/Sec: Section A1 Days: Saturday Begin: 10:00AM End: 03:00PM Bldg/Room: HL 106B Location: Pittsburgh, Pennsylvania Instructor(s): Pinchuk |Spring offering: Course: 99352 Title: IDeATe: Soft Fabrication Skills Units: 1.0 Lec/Sec: Section A3 Days: Saturday Begin: 10:30AM End: 03:20PM Bldg/Room: HL 106B Location: Pittsburgh, Pennsylvania Instructor(s): Pinchuk |Fall offering: Course: 48771 Title: Fabricating Customization: Prototype Units: 9.0 Lec/Sec: Section A Days: Monday, Friday Begin: 09:30AM End: 10:50AM Bldg/Room: MM C4 Location: Pittsburgh, Pennsylvania Instructor(s): Ficca ",
        "retrieved_relevant_part": true,
        "model_answer": "Saturdays",
        "actual_answer": "Saturday",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Which campus is class Principles of Macroeconomics Recitation Section A  offered in Pittsburgh in Sp?",
        "context": "Spring offering: Course: 73103 Title: Principles of Macroeconomics Units: 9.0 Lec/Sec: Section A Days: Friday Begin: 10:00AM End: 10:50AM Bldg/Room: TEP 3801 Location: Pittsburgh, Pennsylvania Instructor(s): Zetlin-Jones ",
        "retrieved_doc": "Fall offering: Course: 73103 Title: Principles of Macroeconomics Units: 9.0 Lec/Sec: Lecture Days: Monday, Wednesday Begin: 01:00PM End: 01:50PM Bldg/Room: TEP 3801 Location: Pittsburgh, Pennsylvania Instructor(s): Pretnar |Fall offering: Course: 73103 Title: Principles of Macroeconomics Units: 9.0 Lec/Sec: Section A Days: Friday Begin: 01:00PM End: 01:50PM Bldg/Room: TEP 3801 Location: Pittsburgh, Pennsylvania Instructor(s): Pretnar |Fall offering: Course: 73103 Title: Principles of Macroeconomics Units: 9.0 Lec/Sec: Section B Days: Friday Begin: 01:00PM End: 01:50PM Bldg/Room: TEP 2701 Location: Pittsburgh, Pennsylvania Instructor(s): Pretnar ",
        "retrieved_relevant_part": false,
        "model_answer": "TEP 3801",
        "actual_answer": "Pittsburgh, Pennsylvania",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "When does the class Funding Early Stage Ventures: Recitation Section A4  offered in Sp end?",
        "context": "Spring offering: Course: 45905 Title: Funding Early Stage Ventures: Units: 6.0 Lec/Sec: Section A4 Days: Tuesday, Thursday Begin: 08:00AM End: 09:45AM Bldg/Room: TEP 2111 Location: Pittsburgh, Pennsylvania Instructor(s): Risch ",
        "retrieved_doc": "Spring offering: Course: 45905 Title: Funding Early Stage Ventures: Units: 6.0 Lec/Sec: Section A4 Days: Tuesday, Thursday Begin: 08:00AM End: 09:45AM Bldg/Room: TEP 2111 Location: Pittsburgh, Pennsylvania Instructor(s): Risch |Spring offering: Course: 45905 Title: Funding Early Stage Ventures Units: 6.0 Lec/Sec: Section A4 Days: Thursday Begin: 06:30PM End: 10:00PM Bldg/Room: TEP 2111 Location: Pittsburgh, Pennsylvania Instructor(s): Demmler |Fall offering: Course: 45905 Title: Funding Early Stage Ventures Units: 6.0 Lec/Sec: Section E2 Days: Wednesday Begin: 06:30PM End: 09:50PM Bldg/Room: TEP 2112 Location: Pittsburgh, Pennsylvania Instructor(s): Demmler ",
        "retrieved_relevant_part": true,
        "model_answer": "The course is offered in the spring semester",
        "actual_answer": "09:45AM",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "When does the class Design Center: Design for Social Innovation Recitation Section A  offered in Sp end?",
        "context": "Spring offering: Course: 51782 Title: Design Center: Design for Social Innovation Units: 12.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 12:00PM End: 01:20PM Bldg/Room: MM 121 Location: Pittsburgh, Pennsylvania Instructor(s): Krishnaswami ",
        "retrieved_doc": "Spring offering: Course: 51382 Title: Design Center: Design for Social Innovation Units: 9.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 12:00PM End: 01:20PM Bldg/Room: MM 121 Location: Pittsburgh, Pennsylvania Instructor(s): Krishnaswami |Spring offering: Course: 51782 Title: Design Center: Design for Social Innovation Units: 12.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 12:00PM End: 01:20PM Bldg/Room: MM 121 Location: Pittsburgh, Pennsylvania Instructor(s): Krishnaswami |Spring offering: Course: 51782 Title: Design Center: Design for Social Innovation Units: 12.0 Lec/Sec: Section B Days: Monday, Wednesday Begin: 10:00AM End: 11:50AM Bldg/Room: MM 121 Location: Pittsburgh, Pennsylvania Instructor(s): Krishnaswami ",
        "retrieved_relevant_part": true,
        "model_answer": "The course Design Center: Design for Social Innovation Recitation Section A is offered in the spring semester",
        "actual_answer": "01:20PM",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "On which days does the class Fairness, Accountability, Transparency, and Ethics in Sociotechnical Recitation Section A  offered in Fa meet?",
        "context": "Fall offering: Course: 05499 Title: Fairness, Accountability, Transparency, and Ethics in Sociotechnical Units: 12.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 11:00AM End: 12:20PM Bldg/Room: HH B103 Location: Pittsburgh, Pennsylvania Instructor(s): Shen ",
        "retrieved_doc": "Fall offering: Course: 05499 Title: Fairness, Accountability, Transparency, and Ethics in Sociotechnical Units: 12.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 11:00AM End: 12:20PM Bldg/Room: HH B103 Location: Pittsburgh, Pennsylvania Instructor(s): Shen |Fall offering: Course: 05899 Title: Fairness, Accountability, Transparency, and Ethics in Sociotechnical Units: 12.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 11:00AM End: 12:20PM Bldg/Room: HH B103 Location: Pittsburgh, Pennsylvania Instructor(s): Shen |Fall offering: Course: 11831 Title: Self-Paced Computational Ethics Lab Units: 6.0 Lec/Sec: Section A Days: To be announced Begin: 02:35PM End: 03:55PM Bldg/Room: DNM DNM Location: Pittsburgh, Pennsylvania Instructor(s): Strubell ",
        "retrieved_relevant_part": true,
        "model_answer": "The class meets on Mondays and Wednesdays.<|endoftext|>",
        "actual_answer": "Monday, Wednesday",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "When does the class Organizational Behavior Recitation Section W  offered in Fa end?",
        "context": "Fall offering: Course: 70311 Title: Organizational Behavior Units: 9.0 Lec/Sec: Section W Days: Monday, Wednesday Begin: 11:30AM End: 12:45PM Bldg/Room: CMB 1190 Location: Doha, Qatar Instructor(s): Haan ",
        "retrieved_doc": "Fall offering: Course: 70311 Title: Organizational Behavior Units: 9.0 Lec/Sec: Section W Days: Monday, Wednesday Begin: 11:30AM End: 12:45PM Bldg/Room: CMB 1190 Location: Doha, Qatar Instructor(s): Haan |Spring offering: Course: 70311 Title: Organizational Behavior Units: 9.0 Lec/Sec: Section W Days: Monday, Wednesday Begin: 02:30PM End: 03:45PM Bldg/Room: CMB 1190 Location: Doha, Qatar Instructor(s): Haan |Fall offering: Course: 70311 Title: Organizational Behavior Units: 9.0 Lec/Sec: Section B Days: Monday, Wednesday Begin: 03:30PM End: 04:50PM Bldg/Room: TEP 2700 Location: Pittsburgh, Pennsylvania Instructor(s): Hahl ",
        "retrieved_relevant_part": true,
        "model_answer": "The course Organizational Behavior Recitation Section W is offered in the fall semester",
        "actual_answer": "12:45PM",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Who is the instructor(s) of course Software Engineering Management: Recitation Section A2  offered in Fa?",
        "context": "Fall offering: Course: 49772 Title: Software Engineering Management: Units: 6.0 Lec/Sec: Section A2 Days: Monday, Wednesday Begin: 11:00AM End: 12:20PM Bldg/Room: B23 227 Location: San Jose, California Instructor(s): Fang ",
        "retrieved_doc": "Fall offering: Course: 49772 Title: Software Engineering Management: Units: 6.0 Lec/Sec: Section R2 Days: Thursday Begin: 05:30PM End: 07:20PM Bldg/Room: TBA Location: San Jose, California Instructor(s): Taborga |Fall offering: Course: 49772 Title: Software Engineering Management: Units: 6.0 Lec/Sec: Section A2 Days: Monday, Wednesday Begin: 11:00AM End: 12:20PM Bldg/Room: B23 227 Location: San Jose, California Instructor(s): Fang |Fall offering: Course: 49772 Title: Software Engineering Management Units: 6.0 Lec/Sec: Section A2 Days: Monday, Wednesday Begin: 11:00AM End: 12:20PM Bldg/Room: B23 227 Location: San Jose, California Instructor(s): Fang ",
        "retrieved_relevant_part": true,
        "model_answer": "Fang",
        "actual_answer": "Fang",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "On which days does the class Business Leadership Endeavor II Recitation Section C1  offered in Fa meet?",
        "context": "Fall offering: Course: 70204 Title: Business Leadership Endeavor II Units: 3.0 Lec/Sec: Section C1 Days: Tuesday Begin: 02:00PM End: 03:20PM Bldg/Room: TEP 3808 Location: Pittsburgh, Pennsylvania Instructor(s): Jafry O'Connor ",
        "retrieved_doc": "Fall offering: Course: 70204 Title: Business Leadership Endeavor II: Units: 3.0 Lec/Sec: Section A1 Days: Tuesday Begin: 08:00AM End: 09:20AM Bldg/Room: TEP 3808 Location: Pittsburgh, Pennsylvania Instructor(s): Jafry O'Connor |Fall offering: Course: 70204 Title: Business Leadership Endeavor II Units: 3.0 Lec/Sec: Section C1 Days: Tuesday Begin: 02:00PM End: 03:20PM Bldg/Room: TEP 3808 Location: Pittsburgh, Pennsylvania Instructor(s): Jafry O'Connor |Fall offering: Course: 70204 Title: Business Leadership Endeavor II Units: 3.0 Lec/Sec: Section A1 Days: Tuesday Begin: 08:00AM End: 09:20AM Bldg/Room: TEP 3808 Location: Pittsburgh, Pennsylvania Instructor(s): Jafry O'Connor ",
        "retrieved_relevant_part": true,
        "model_answer": "Tuesdays",
        "actual_answer": "Tuesday",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "On which days does the class Acting for Leadership and Communication Recitation Section B4  offered in Sp meet?",
        "context": "Spring offering: Course: 94801 Title: Acting for Leadership and Communication Units: 6.0 Lec/Sec: Section B4 Days: Monday, Wednesday Begin: 11:00AM End: 12:20PM Bldg/Room: HBH 2009 Location: Pittsburgh, Pennsylvania Instructor(s): Murphy ",
        "retrieved_doc": "Spring offering: Course: 94401 Title: Acting for Leadership and Communication Units: 6.0 Lec/Sec: Section B4 Days: Monday, Wednesday Begin: 11:00AM End: 12:20PM Bldg/Room: HBH 2009 Location: Pittsburgh, Pennsylvania Instructor(s): Murphy |Spring offering: Course: 94801 Title: Acting for Leadership and Communication Units: 6.0 Lec/Sec: Section B4 Days: Monday, Wednesday Begin: 11:00AM End: 12:20PM Bldg/Room: HBH 2009 Location: Pittsburgh, Pennsylvania Instructor(s): Murphy |Spring offering: Course: 94401 Title: Acting for Leadership and Communication Units: 6.0 Lec/Sec: Section C4 Days: Monday, Wednesday Begin: 02:00PM End: 03:20PM Bldg/Room: HBH 2008 Location: Pittsburgh, Pennsylvania Instructor(s): Murphy ",
        "retrieved_relevant_part": true,
        "model_answer": "The class meets on Mondays and Wednesdays",
        "actual_answer": "Monday, Wednesday",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "On which days does the class Mathematical Models for Consulting Recitation Section W  offered in Fa meet?",
        "context": "Fall offering: Course: 70460 Title: Mathematical Models for Consulting Units: 9.0 Lec/Sec: Section W Days: Sunday, Tuesday Begin: 04:00PM End: 05:15PM Bldg/Room: CMB 1031 Location: Doha, Qatar Instructor(s): Safak ",
        "retrieved_doc": "Fall offering: Course: 70460 Title: Mathematical Models for Consulting Units: 9.0 Lec/Sec: Section W Days: Sunday, Tuesday Begin: 04:00PM End: 05:15PM Bldg/Room: CMB 1031 Location: Doha, Qatar Instructor(s): Safak |Fall offering: Course: 70460 Title: Mathematical Models for Consulting Units: 9.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 09:30AM End: 10:50AM Bldg/Room: TEP 2701 Location: Pittsburgh, Pennsylvania Instructor(s): De Roux Uribe |Spring offering: Course: 21380 Title: Introduction to Mathematical Modeling Units: 9.0 Lec/Sec: Section A Days: Monday, Wednesday, Friday Begin: 11:00AM End: 11:50AM Bldg/Room: WEH 8220 Location: Pittsburgh, Pennsylvania Instructor(s): Altschuler ",
        "retrieved_relevant_part": true,
        "model_answer": "The class meets on Sundays and Tuesdays",
        "actual_answer": "Sunday, Tuesday",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Based on the background information, What is the units of course Special Topics in ICT: offered in Fa?",
        "context": "Fall offering: Course: 04800 Title: Special Topics in ICT: Units: 12.0 Lec/Sec: Section G Days: Tuesday, Thursday Begin: 10:00AM End: 11:50AM Bldg/Room: CMR F309 Location: Kigali, Rwanda Instructor(s): Tucker ",
        "retrieved_doc": "Spring offering: Course: 04800 Title: Special Topics in ICT: Units: 12.0 Lec/Sec: Section AF Days: Monday, Wednesday Begin: 04:00PM End: 05:50PM Bldg/Room: CMU REMOTE Location: Kigali, Rwanda Instructor(s): Racz, McSharry |Spring offering: Course: 04801 Title: Special Topics in ICT: Units: 6.0 Lec/Sec: Section C Days: Monday, Wednesday Begin: 12:00PM End: 01:50PM Bldg/Room: CMR F305 Location: Kigali, Rwanda Instructor(s): Luhanga |Spring offering: Course: 04800 Title: Special Topics in ICT: Units: 12.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 04:30PM End: 06:20PM Bldg/Room: CMR F305 Location: Kigali, Rwanda Instructor(s): Kairu ",
        "retrieved_relevant_part": false,
        "model_answer": "12.0 units",
        "actual_answer": "The units of course Special Topics in ICT: is 12.0",
        "exact_match": false,
        "f1_max_min_avg": [
            0.33,
            0.33,
            0.33
        ],
        "recall_max_min_avg": [
            0.2,
            0.2,
            0.2
        ]
    },
    {
        "question": "Which campus is class Techniques in Quantitative Analysis Recitation Section B  offered in Pittsburgh in Fa?",
        "context": "Fall offering: Course: 09207 Title: Techniques in Quantitative Analysis Units: 9.0 Lec/Sec: Section B Days: Tuesday Begin: 06:30PM End: 08:50PM Bldg/Room: DH 1302 Location: Pittsburgh, Pennsylvania Instructor(s): Botcha ",
        "retrieved_doc": "Fall offering: Course: 09207 Title: Techniques in Quantitative Analysis Units: 9.0 Lec/Sec: Section B Days: Tuesday Begin: 06:30PM End: 08:50PM Bldg/Room: DH 1302 Location: Pittsburgh, Pennsylvania Instructor(s): Botcha |Fall offering: Course: 09207 Title: Techniques in Quantitative Analysis Units: 9.0 Lec/Sec: Section B Days: Tuesday, Thursday Begin: 06:30PM End: 07:20PM Bldg/Room: DH 2302 Location: Pittsburgh, Pennsylvania Instructor(s): Botcha |Fall offering: Course: 09207 Title: Techniques in Quantitative Analysis Units: 9.0 Lec/Sec: Section B Days: Thursday Begin: 06:30PM End: 08:50PM Bldg/Room: DH 1302 Location: Pittsburgh, Pennsylvania Instructor(s): Botcha ",
        "retrieved_relevant_part": true,
        "model_answer": "The course is offered on the Pittsburgh, Pennsylvania campus.<|endoftext|>",
        "actual_answer": "Pittsburgh, Pennsylvania",
        "exact_match": false,
        "f1_max_min_avg": [
            0.36,
            0.36,
            0.36
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Based on the background information, What is the units of course Singers offered in Fa?",
        "context": "Fall offering: Course: 57822 Title: Singers Units: 3.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 05:30PM End: 06:20PM Bldg/Room: MM 119 Location: Pittsburgh, Pennsylvania Instructor(s): Douglas ",
        "retrieved_doc": "Spring offering: Course: 57822 Title: Singers Units: 3.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 05:30PM End: 06:20PM Bldg/Room: MM 119 Location: Pittsburgh, Pennsylvania Instructor(s): Douglas |Spring offering: Course: 57422 Title: Singers Units: 3.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 05:30PM End: 06:20PM Bldg/Room: MM 119 Location: Pittsburgh, Pennsylvania Instructor(s): Douglas |Spring offering: Course: 57557 Title: Vocal Methods Units: 3.0 Lec/Sec: Section A Days: Tuesday, Thursday Begin: 09:00AM End: 09:50AM Bldg/Room: CFA M160 Location: Pittsburgh, Pennsylvania Instructor(s): Gruber ",
        "retrieved_relevant_part": false,
        "model_answer": "The units of course Singers offered in Fa are 3.0",
        "actual_answer": "The units of course Singers is 3.0",
        "exact_match": false,
        "f1_max_min_avg": [
            0.71,
            0.71,
            0.71
        ],
        "recall_max_min_avg": [
            0.86,
            0.86,
            0.86
        ]
    },
    {
        "question": "What is the abstract of the paper Neutral Face Learning and Progressive Fusion Synthesis Network for NIR-VIS Face Recognition published by LTI faculty Yiming Yang in 2023?",
        "context": "Author: Yiming Yang Title: Neutral Face Learning and Progressive Fusion Synthesis Network for NIR-VIS Face Recognition Publication year: 2023 Coauthors: Yiming Yang, Weipeng Hu, Haifeng Hu Abstract: To meet the strong demand for deploying face recognition systems in low-light scenarios, the Near-InfraRed and VISible (NIR-VIS) face recognition task is receiving increasing attention. However, heterogeneous faces have the characteristics of heterogeneity and non-neutrality. Heterogeneity refers to the fact that the matching images are in different modalities, and non-neutrality means that the matching images are significantly different in pose, expression, lighting, etc. Both situations pose challenges for NIR-VIS face matching. To address this problem, we propose a novel Neutral face Learning and Progressive Fusion synthesis (NLPF) network to disentangle the latent attributes of heterogeneous faces and learn neutral face representations. Our approach naturally integrates Identity-related Neutral face Learning (INL) and Attribute Progressive Fusion (APF) into a joint framework. Firstly, INL eliminates modal variations and residual variations by guiding the network to learn homogeneous neutral face feature representations, which tackles the challenge of heterogeneity and non-neutrality by mapping cross-modal images to a common neutral representation subspace. Besides, APF is presented to perform the disentanglement and reintegration of identity-related features, modality-related features and residual features in a progressive fusion manner, which helps to further purify identity-related features. Comprehensive evaluations are carried out on three mainstream NIR-VIS datasets to verify the robustness and effectiveness of the NLPF model. In particular, NLPF has competitive recognition performance on LAMP-HQ, the most challenging NIR-VIS dataset so far.",
        "retrieved_doc": "Author: Yiming Yang Title: Neutral Face Learning and Progressive Fusion Synthesis Network for NIR-VIS Face Recognition Publication year: 2023 Coauthors: Yiming Yang, Weipeng Hu, Haifeng Hu Abstract: To meet the strong demand for deploying face recognition systems in low-light scenarios, the Near-InfraRed and VISible (NIR-VIS) face recognition task is receiving increasing attention. However, heterogeneous faces have the characteristics of heterogeneity and non-neutrality. Heterogeneity refers to the fact that the matching images are in different modalities, and non-neutrality means that the matching images are significantly different in pose, expression, lighting, etc. Both situations pose challenges for NIR-VIS face matching. To address this problem, we propose a novel Neutral face Learning and Progressive Fusion synthesis (NLPF) network to disentangle the latent attributes of heterogeneous faces and learn neutral face representations. Our approach naturally integrates Identity-related Neutral face Learning (INL) and Attribute Progressive Fusion (APF) into a joint framework. Firstly, INL eliminates modal variations and residual variations by guiding the network to learn homogeneous neutral face feature representations, which tackles the challenge of heterogeneity and non-neutrality by mapping cross-modal images to a common neutral representation subspace. Besides, APF is presented to perform the disentanglement and reintegration of identity-related features, modality-related features and residual features in a progressive fusion manner, which helps to further purify identity-related features. Comprehensive evaluations are carried out on three mainstream NIR-VIS datasets to verify the robustness and effectiveness of the NLPF model. In particular, NLPF has competitive recognition performance on LAMP-HQ, the most challenging NIR-VIS dataset so far.|Author: Yiming Yang Title: Syncretic Space Learning Network for NIR-VIS Face Recognition Publication year: 2023 Coauthors: Yiming Yang, Weipeng Hu, Haifeng Hu Abstract: To overcome the technical bottleneck of face recognition in low-light scenarios, Near-InfraRed and VISible (NIR-VIS) heterogeneous face recognition is proposed for matching well-lit VIS faces with poorly lit NIR faces. Current cross-modal synthesis methods visually convert the NIR modality to the VIS modality and then perform face matching in the VIS modality. However, using a heavyweight GAN network on unpaired NIR-VIS faces may lead to high synthesis difficulty, low inference efficiency, and other problems. To alleviate the above problems, we simultaneously synthesize NIR and VIS images into modality-independent syncretic images and propose a novel syncretic space learning (SSL) model to eliminate the modal gap. First, Syncretic Modality Generator (SMG) synthesizes NIR and VIS images into syncretic images using channel-level convolution with a shallow CNN. In particular, the discriminative structural information is well preserved and the face quality can be further improved with small modal variations in a self-supervised learning manner. Second, Modality-adversarial Syncretic space Learning (MSL) projects NIR and VIS images into the syncretic space by a syncretic-modality adversarial learning strategy with syncretic pattern guided objective, so the modal gap of NIR-VIS faces can be effectively reduced. Finally, the Syncretic Distribution Consistency (SDC) constructed by NIR-syncretic, syncretic-syncretic, and VIS-syncretic consistency can enhance the intra-class compactness and learn discriminative representations. Extensive experiments on three challenging datasets demonstrate the effectiveness of the SSL method.|Author: Yiming Yang Title: Robust Cross-Domain Pseudo-Labeling and Contrastive Learning for Unsupervised Domain Adaptation NIR-VIS Face Recognition Publication year: 2023 Coauthors: Yiming Yang, Weipeng Hu, Haiqi Lin, Haifeng Hu Abstract: Near-infrared and visible face recognition (NIR-VIS) is attracting increasing attention because of the need to achieve face recognition in low-light conditions to enable 24-hour secure retrieval. However, annotating identity labels for a large number of heterogeneous face images is time-consuming and expensive, which limits the application of the NIR-VIS face recognition system to larger scale real-world scenarios. In this paper, we attempt to achieve NIR-VIS face recognition in an unsupervised domain adaptation manner. To get rid of the reliance on manual annotations, we propose a novel Robust cross-domain Pseudo-labeling and Contrastive learning (RPC) network which consists of three key components, i.e., NIR cluster-based Pseudo labels Sharing (NPS), Domain-specific cluster Contrastive Learning (DCL) and Inter-domain cluster Contrastive Learning (ICL). Firstly, NPS is presented to generate pseudo labels by exploring robust NIR clusters and sharing reliable label knowledge with VIS domain. Secondly, DCL is designed to learn intra-domain compact yet discriminative representations. Finally, ICL dynamically combines and refines intrinsic identity relationships to guide the instance-level features to learn robust and domain-independent representations. Extensive experiments are conducted to verify an accuracy of over 99% in pseudo label assignment and the advanced performance of RPC network on four mainstream NIR-VIS datasets.",
        "retrieved_relevant_part": true,
        "model_answer": "The paper proposes a novel Neutral face Learning and Progressive Fusion synthesis (NLPF) network to disentangle the latent attributes of heterogeneous faces and learn neutral face representations",
        "actual_answer": "To meet the strong demand for deploying face recognition systems in low-light scenarios, the Near-InfraRed and VISible (NIR-VIS) face recognition task is receiving increasing attention. However, heterogeneous faces have the characteristics of heterogeneity and non-neutrality. Heterogeneity refers to the fact that the matching images are in different modalities, and non-neutrality means that the matching images are significantly different in pose, expression, lighting, etc. Both situations pose challenges for NIR-VIS face matching. To address this problem, we propose a novel Neutral face Learning and Progressive Fusion synthesis (NLPF) network to disentangle the latent attributes of heterogeneous faces and learn neutral face representations. Our approach naturally integrates Identity-related Neutral face Learning (INL) and Attribute Progressive Fusion (APF) into a joint framework. Firstly, INL eliminates modal variations and residual variations by guiding the network to learn homogeneous neutral face feature representations, which tackles the challenge of heterogeneity and non-neutrality by mapping cross-modal images to a common neutral representation subspace. Besides, APF is presented to perform the disentanglement and reintegration of identity-related features, modality-related features and residual features in a progressive fusion manner, which helps to further purify identity-related features. Comprehensive evaluations are carried out on three mainstream NIR-VIS datasets to verify the robustness and effectiveness of the NLPF model. In particular, NLPF has competitive recognition performance on LAMP-HQ, the most challenging NIR-VIS dataset so far.",
        "exact_match": false,
        "f1_max_min_avg": [
            0.17,
            0.17,
            0.17
        ],
        "recall_max_min_avg": [
            0.09,
            0.09,
            0.09
        ]
    },
    {
        "question": "Who are the coauthors of the paper Policy Representation via Diffusion Probability Model for Reinforcement Learning published by LTI faculty Yiming Yang in 2023?",
        "context": "Author: Yiming Yang Title: Policy Representation via Diffusion Probability Model for Reinforcement Learning Publication year: 2023 Coauthors: Long Yang, Zhixiong Huang, Fenghao Lei, Yucun Zhong, Yiming Yang, Cong Fang, Shiting Wen, Binbin Zhou, Zhouchen Lin Abstract: Popular reinforcement learning (RL) algorithms tend to produce a unimodal policy distribution, which weakens the expressiveness of complicated policy and decays the ability of exploration. The diffusion probability model is powerful to learn complicated multimodal distributions, which has shown promising and potential applications to RL. In this paper, we formally build a theoretical foundation of policy representation via the diffusion probability model and provide practical implementations of diffusion policy for online model-free RL. Concretely, we character diffusion policy as a stochastic process, which is a new approach to representing a policy. Then we present a convergence guarantee for diffusion policy, which provides a theory to understand the multimodality of diffusion policy. Furthermore, we propose the DIPO which is an implementation for model-free online RL with DIffusion POlicy. To the best of our knowledge, DIPO is the first algorithm to solve model-free online RL problems with the diffusion model. Finally, extensive empirical results show the effectiveness and superiority of DIPO on the standard continuous control Mujoco benchmark.",
        "retrieved_doc": "Author: Yiming Yang Title: Policy Representation via Diffusion Probability Model for Reinforcement Learning Publication year: 2023 Coauthors: Long Yang, Zhixiong Huang, Fenghao Lei, Yucun Zhong, Yiming Yang, Cong Fang, Shiting Wen, Binbin Zhou, Zhouchen Lin Abstract: Popular reinforcement learning (RL) algorithms tend to produce a unimodal policy distribution, which weakens the expressiveness of complicated policy and decays the ability of exploration. The diffusion probability model is powerful to learn complicated multimodal distributions, which has shown promising and potential applications to RL. In this paper, we formally build a theoretical foundation of policy representation via the diffusion probability model and provide practical implementations of diffusion policy for online model-free RL. Concretely, we character diffusion policy as a stochastic process, which is a new approach to representing a policy. Then we present a convergence guarantee for diffusion policy, which provides a theory to understand the multimodality of diffusion policy. Furthermore, we propose the DIPO which is an implementation for model-free online RL with DIffusion POlicy. To the best of our knowledge, DIPO is the first algorithm to solve model-free online RL problems with the diffusion model. Finally, extensive empirical results show the effectiveness and superiority of DIPO on the standard continuous control Mujoco benchmark.|Author: Yiming Yang Title: Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs Publication year: 2023 Coauthors: Qingyang Zhang, Yiming Yang, Jingqing Ruan, Xuantang Xiong, Dengpeng Xing, Bo Xu Abstract: Goal-Conditioned Hierarchical Reinforcement Learning (GCHRL) is a promising paradigm to address the exploration-exploitation dilemma in reinforcement learning. It decomposes the source task into sub goal conditional subtasks and conducts exploration and exploitation in the subgoal space. The effectiveness of GCHRL heavily relies on sub goal representation functions and sub goal selection strategy. However, existing works often overlook the temporal coherence in GCHRL when learning latent sub goal representations and lack an efficient sub goal selection strategy that balances exploration and exploitation. This paper proposes HIerarchical reinforcement learning via dynamically building Latent Landmark graphs (HILL) to overcome these limitations. HILL learns latent subgoal representations that satisfy temporal coherence using a contrastive representation learning objective. Based on these representations, HILL dynamically builds latent landmark graphs and employs a novelty measure on nodes and a utility measure on edges. Finally, HILL develops a subgoal selection strategy that balances exploration and exploitation by jointly considering both measures. Experimental results demonstrate that HILL outperforms state-of-the-art baselines on continuous control tasks with sparse rewards in sample efficiency and asymptotic performance. Our code is available at https://github.com/papercode2022/HILL.|Author: Maarten Sap Title: Improving Language Models with Advantage-based Offline Policy Gradients Publication year: 2023 Coauthors: Ashutosh Baheti, Ximing Lu, Faeze Brahman, Ronan Le Bras, Maarten Sap, Mark O. Riedl Abstract: Language Models (LMs) achieve substantial language capabilities when finetuned using Reinforcement Learning with Human Feedback (RLHF). However, RLHF is an unstable and data-hungry process that continually requires new high-quality LM-generated data for finetuning. We introduce Advantage-Leftover Lunch RL (A-LoL), a new class of offline policy gradient algorithms that enable RL training on any pre-existing data. By assuming the entire LM output sequence as a single action, A-LoL allows incorporating sequence-level classifiers or human-designed scoring functions as rewards. Subsequently, by using LM's internal sequence-level value estimate, A-LoL filters negative advantage (low-quality) data points during training, making it resilient to noise. Overall, A-LoL is an easy-to-implement LM training recipe that is sample-efficient and stable. We demonstrate the effectiveness of A-LoL and its variants with a set of four different language generation tasks. We compare against both online RL (PPO) and recent preference-based (DPO, PRO) and reward-based (GOLD) offline RL baselines. On the commonly-used RLHF benchmark, Helpful and Harmless Assistant (HHA), LMs trained with A-LoL methods achieve the highest diversity while also being rated more safe and helpful than baselines according to humans. Additionally, in the remaining three tasks, A-LoL could optimize multiple distinct reward functions even when using noisy or suboptimal training data. We also release our experimental code. https://github.com/abaheti95/LoL-RL",
        "retrieved_relevant_part": true,
        "model_answer": "Long Yang, Zhixiong Huang, Fenghao Lei, Yucun Zhong, Cong Fang, Shiting Wen, Binbin Zhou, Zhouchen Lin",
        "actual_answer": "Long Yang, Zhixiong Huang, Fenghao Lei, Yucun Zhong, Yiming Yang, Cong Fang, Shiting Wen, Binbin Zhou, Zhouchen Lin",
        "exact_match": false,
        "f1_max_min_avg": [
            0.94,
            0.94,
            0.94
        ],
        "recall_max_min_avg": [
            0.89,
            0.89,
            0.89
        ]
    },
    {
        "question": "Who is the author of the LTI paper The Multimodal Information Based Speech Processing (MISP) 2023 Challenge: Audio-Visual Target Speaker Extraction published in 2023?",
        "context": "Author: Shinji Watanabe Title: The Multimodal Information Based Speech Processing (MISP) 2023 Challenge: Audio-Visual Target Speaker Extraction Publication year: 2023 Coauthors: Shilong Wu, Chenxi Wang, Hang Chen, Yusheng Dai, Chenyue Zhang, Ruoyu Wang, Hongbo Lan, Jun Du, Chin-Hui Lee, Jingdong Chen, Shinji Watanabe, Sabato Marco Siniscalchi, O. Scharenborg, Zhong-Qiu Wang, Jia Pan, Jianqing Gao Abstract: Previous Multimodal Information based Speech Processing (MISP) challenges mainly focused on audio-visual speech recognition (AVSR) with commendable success. However, the most advanced back-end recognition systems often hit performance limits due to the complex acoustic environments. This has prompted a shift in focus towards the Audio-Visual Target Speaker Extraction (AVTSE) task for the MISP 2023 challenge in ICASSP 2024 Signal Processing Grand Challenges. Unlike existing audio-visual speech enhance-ment challenges primarily focused on simulation data, the MISP 2023 challenge uniquely explores how front-end speech processing, combined with visual clues, impacts back-end tasks in real-world scenarios. This pioneering effort aims to set the first benchmark for the AVTSE task, offering fresh insights into enhancing the ac-curacy of back-end speech recognition systems through AVTSE in challenging and real acoustic environments. This paper delivers a thorough overview of the task setting, dataset, and baseline system of the MISP 2023 challenge. It also includes an in-depth analysis of the challenges participants may encounter. The experimental results highlight the demanding nature of this task, and we look forward to the innovative solutions participants will bring forward.",
        "retrieved_doc": "Author: Shinji Watanabe Title: The Multimodal Information Based Speech Processing (MISP) 2023 Challenge: Audio-Visual Target Speaker Extraction Publication year: 2023 Coauthors: Shilong Wu, Chenxi Wang, Hang Chen, Yusheng Dai, Chenyue Zhang, Ruoyu Wang, Hongbo Lan, Jun Du, Chin-Hui Lee, Jingdong Chen, Shinji Watanabe, Sabato Marco Siniscalchi, O. Scharenborg, Zhong-Qiu Wang, Jia Pan, Jianqing Gao Abstract: Previous Multimodal Information based Speech Processing (MISP) challenges mainly focused on audio-visual speech recognition (AVSR) with commendable success. However, the most advanced back-end recognition systems often hit performance limits due to the complex acoustic environments. This has prompted a shift in focus towards the Audio-Visual Target Speaker Extraction (AVTSE) task for the MISP 2023 challenge in ICASSP 2024 Signal Processing Grand Challenges. Unlike existing audio-visual speech enhance-ment challenges primarily focused on simulation data, the MISP 2023 challenge uniquely explores how front-end speech processing, combined with visual clues, impacts back-end tasks in real-world scenarios. This pioneering effort aims to set the first benchmark for the AVTSE task, offering fresh insights into enhancing the ac-curacy of back-end speech recognition systems through AVTSE in challenging and real acoustic environments. This paper delivers a thorough overview of the task setting, dataset, and baseline system of the MISP 2023 challenge. It also includes an in-depth analysis of the challenges participants may encounter. The experimental results highlight the demanding nature of this task, and we look forward to the innovative solutions participants will bring forward.|Author: Shinji Watanabe Title: Summary on the Multimodal Information Based Speech Processing (MISP) 2022 Challenge Publication year: 2023 Coauthors: Hang Chen, Shilong Wu, Yusheng Dai, Zhe Wang, Jun Du, Chin-Hui Lee, Jingdong Chen, Shinji Watanabe, Sabato Marco Siniscalchi, O. Scharenborg, Diyuan Liu, Baocai Yin, Jia Pan, Jianqing Gao, Cong Liu Abstract: The Multimodal Information based Speech Processing (MISP) 2022 challenge aimed to enhance speech processing performance in harsh acoustic environments by leveraging additional modalities such as video or text. The challenge included two tracks: audio-visual speaker diarization (AVSD) and audio-visual diarization and recognition (AVDR). The training material was based on previous MISP 2021 recordings, but we have accurately synchronized audio and visual data. Additionally, a new evaluation set was provided. This paper gives an overview of the challenge setup, presents the results, and summarizes the effective techniques employed by the participants. We also analyze the current technical challenges and suggest directions for future research in AVSD and AVDR.|Author: Shinji Watanabe Title: The Multimodal Information Based Speech Processing (Misp) 2022 Challenge: Audio-Visual Diarization And Recognition Publication year: 2023 Coauthors: Zhe Wang, Shilong Wu, Hang Chen, Maokui He, Jun Du, Chin-Hui Lee, Jingdong Chen, Shinji Watanabe, Sabato Marco Siniscalchi, O. Scharenborg, Diyuan Liu, Baocai Yin, Jia Pan, Jianqing Gao, Cong Liu Abstract: The Multi-modal Information based Speech Processing (MISP) challenge aims to extend the application of signal processing technology in specific scenarios by promoting the research into wake-up words, speaker diarization, speech recognition, and other technologies. The MISP2022 challenge has two tracks: 1) audio-visual speaker diarization (AVSD), aiming to solve \"who spoken when\" using both audio and visual data; 2) a novel audio-visual diarization and recognition (AVDR) task that focuses on addressing \"who spoken what when\" with audio-visual speaker diarization results. Both tracks focus on the Chinese language, and use far-field audio and video in real home-tv scenarios: 2-6 people communicating each other with TV noise in the background. This paper introduces the dataset, track settings, and baselines of the MISP2022 challenge. Our analyses of experiments and examples indicate the good performance of AVDR baseline system, and the potential difficulties in this challenge due to, e.g., the far-field video quality, the presence of TV noise in the background, and the indistinguishable speakers.",
        "retrieved_relevant_part": true,
        "model_answer": "Shinji Watanabe",
        "actual_answer": "Shinji Watanabe",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Who is the author of the LTI paper FindAdaptNet: Find and Insert Adapters by Learned Layer Importance published in 2023?",
        "context": "Author: Shinji Watanabe Title: FindAdaptNet: Find and Insert Adapters by Learned Layer Importance Publication year: 2023 Coauthors: Junwei Huang, Karthik Ganesan, Soumi Maiti, Young Min Kim, Xuankai Chang, Paul Liang, Shinji Watanabe Abstract: Adapters are lightweight bottleneck modules introduced to assist pre-trained self-supervised learning (SSL) models to be customized to new tasks. However, searching the appropriate layers to insert adapters on large models has become difficult due to the large number of possible layers and thus a vast search space (2N possibilities for N layers). In this paper, we propose a technique that achieves automatic insertion of adapters for downstream automatic speech recognition (ASR) and spoken language understanding (SLU) tasks. Our approach is based on two-stage training. First, we train our model for a specific downstream task with additional shallow learnable layers and weight parameters to obtain the weighted summation over the output of each layer in SSL. This training method is established by the SUPERB baseline [1]. This first-stage training determines the most important layers given their respective weights. In the second stage, we proceed to insert adapters to the most important layers, retaining both performance and neural architecture search efficiency. On the CommonVoice dataset[2] we obtain 20.6% absolute improvement in Word Error Rate (WER) on the Welsh language against the conventional method, which inserts the adapter modules into the highest layers without search. In the SLURP SLU task, our method yields 4.0% intent accuracy improvement against the same conventional baseline.",
        "retrieved_doc": "Author: Shinji Watanabe Title: FindAdaptNet: Find and Insert Adapters by Learned Layer Importance Publication year: 2023 Coauthors: Junwei Huang, Karthik Ganesan, Soumi Maiti, Young Min Kim, Xuankai Chang, Paul Liang, Shinji Watanabe Abstract: Adapters are lightweight bottleneck modules introduced to assist pre-trained self-supervised learning (SSL) models to be customized to new tasks. However, searching the appropriate layers to insert adapters on large models has become difficult due to the large number of possible layers and thus a vast search space (2N possibilities for N layers). In this paper, we propose a technique that achieves automatic insertion of adapters for downstream automatic speech recognition (ASR) and spoken language understanding (SLU) tasks. Our approach is based on two-stage training. First, we train our model for a specific downstream task with additional shallow learnable layers and weight parameters to obtain the weighted summation over the output of each layer in SSL. This training method is established by the SUPERB baseline [1]. This first-stage training determines the most important layers given their respective weights. In the second stage, we proceed to insert adapters to the most important layers, retaining both performance and neural architecture search efficiency. On the CommonVoice dataset[2] we obtain 20.6% absolute improvement in Word Error Rate (WER) on the Welsh language against the conventional method, which inserts the adapter modules into the highest layers without search. In the SLURP SLU task, our method yields 4.0% intent accuracy improvement against the same conventional baseline.|Author: Yonatan Bisk Title: Open X-Embodiment: Robotic Learning Datasets and RT-X Models Publication year: 2023 Coauthors: A. Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, A. Irpan, Alexander Khazatsky, Anant Rai, Anika Singh, Anthony Brohan, A. Raffin, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Sch\u00f6lkopf, Brian Ichter, Cewu Lu, Charles Xu, Chelsea Finn, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Chuer Pan, Chuyuan Fu, Coline Devin, Danny Driess, Deepak Pathak, Dhruv Shah, Dieter B\u00fcchler, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Federico Ceola, Fei Xia, F. Stulp, Gaoyue Zhou, G. Sukhatme, G. Salhotra, Ge Yan, Giulio Schiavi, G. Kahn, Hao Su, Haoshu Fang, Haochen Shi, H. B. Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jan Peters, Jan Schneider, Jasmine Hsu, J. Bohg, Jeff Bingham, Jiajun Wu, Jialin Wu, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jitendra Malik, Jonathan Tompson, Jonathan Yang, Joseph J. Lim, Jo\u00e3o Silv\u00e9rio, Junhyek Han, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, K. Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Zhang, Krishan Rana, K. Srinivasan, Lawrence Yunliang Chen, Lerrel Pinto, Liam Tan, Lionel Ott, Lisa Lee, Masayoshi Tomizuka, Maximilian Du, Michael Ahn, Mingtong Zhang, Mingyu Ding, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, N. Heess, Nikhil Joshi, Niko Suenderhauf, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, P. Sanketi, Paul Wohlhart, Peng Xu, P. Sermanet, Priya Sundaresan, Q. Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan C. Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham D. Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, S. Adebola, Simon Guist, Soroush Nasiriany, S. Schaal, Stefan Welker, Stephen Tian, Sudeep Dasari, Suneel Belkhale, Takayuki Osa, Tatsuya Harada, T. Matsushima, Ted Xiao, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Zhao, Travis Armstrong, Trevor Darrell, Vidhi Jain, Vincent Vanhoucke, W. Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xuanlin Li, Yao Lu, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yueh-hua Wu, Yujin Tang, Yuke Zhu, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zhuo Xu, Zichen Jeff Cui Abstract: \u2014Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer|Author: Yonatan Bisk Title: Open X-Embodiment: Robotic Learning Datasets and RT-X Models Publication year: 2023 Coauthors: A. Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, A. Irpan, Alexander Khazatsky, Anant Rai, Anika Singh, Anthony Brohan, A. Raffin, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Sch\u00f6lkopf, Brian Ichter, Cewu Lu, Charles Xu, Chelsea Finn, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Chuer Pan, Chuyuan Fu, Coline Devin, Danny Driess, Deepak Pathak, Dhruv Shah, Dieter B\u00fcchler, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Federico Ceola, Fei Xia, F. Stulp, Gaoyue Zhou, G. Sukhatme, G. Salhotra, Ge Yan, Giulio Schiavi, G. Kahn, Hao Su, Haoshu Fang, Haochen Shi, H. B. Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jan Peters, Jan Schneider, Jasmine Hsu, J. Bohg, Jeff Bingham, Jiajun Wu, Jialin Wu, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jitendra Malik, Jonathan Tompson, Jonathan Yang, Joseph J. Lim, Jo\u00e3o Silv\u00e9rio, Junhyek Han, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, K. Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Zhang, Krishan Rana, K. Srinivasan, Lawrence Yunliang Chen, Lerrel Pinto, Liam Tan, Lionel Ott, Lisa Lee, Masayoshi Tomizuka, Maximilian Du, Michael Ahn, Mingtong Zhang, Mingyu Ding, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, N. Heess, Nikhil Joshi, Niko Suenderhauf, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, P. Sanketi, Paul Wohlhart, Peng Xu, P. Sermanet, Priya Sundaresan, Q. Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan C. Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham D. Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, S. Adebola, Simon Guist, Soroush Nasiriany, S. Schaal, Stefan Welker, Stephen Tian, Sudeep Dasari, Suneel Belkhale, Takayuki Osa, Tatsuya Harada, T. Matsushima, Ted Xiao, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Zhao, Travis Armstrong, Trevor Darrell, Vidhi Jain, Vincent Vanhoucke, W. Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xuanlin Li, Yao Lu, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yueh-hua Wu, Yujin Tang, Yuke Zhu, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zhuo Xu, Zichen Jeff Cui Abstract: \u2014Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer",
        "retrieved_relevant_part": true,
        "model_answer": "The author of the LTI paper is named Yonatan Bisk",
        "actual_answer": "Shinji Watanabe",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Who is the author of the LTI paper Understanding the Effect of Model Compression on Social Bias in Large Language Models published in 2023?",
        "context": "Author: Emma Strubell Title: Understanding the Effect of Model Compression on Social Bias in Large Language Models Publication year: 2023 Coauthors: Gustavo Gon\u00e7alves, Emma Strubell Abstract: Large Language Models (LLMs) trained with self-supervision on vast corpora of web text fit to the social biases of that text. Without intervention, these social biases persist in the model's predictions in downstream tasks, leading to representational harm. Many strategies have been proposed to mitigate the effects of inappropriate social biases learned during pretraining. Simultaneously, methods for model compression have become increasingly popular to reduce the computational burden of LLMs. Despite the popularity and need for both approaches, little work has been done to explore the interplay between these two. We perform a carefully controlled study of the impact of model compression via quantization and knowledge distillation on measures of social bias in LLMs. Longer pretraining and larger models led to higher social bias, and quantization showed a regularizer effect with its best trade-off around 20% of the original pretraining time.",
        "retrieved_doc": "Author: Emma Strubell Title: Understanding the Effect of Model Compression on Social Bias in Large Language Models Publication year: 2023 Coauthors: Gustavo Gon\u00e7alves, Emma Strubell Abstract: Large Language Models (LLMs) trained with self-supervision on vast corpora of web text fit to the social biases of that text. Without intervention, these social biases persist in the model's predictions in downstream tasks, leading to representational harm. Many strategies have been proposed to mitigate the effects of inappropriate social biases learned during pretraining. Simultaneously, methods for model compression have become increasingly popular to reduce the computational burden of LLMs. Despite the popularity and need for both approaches, little work has been done to explore the interplay between these two. We perform a carefully controlled study of the impact of model compression via quantization and knowledge distillation on measures of social bias in LLMs. Longer pretraining and larger models led to higher social bias, and quantization showed a regularizer effect with its best trade-off around 20% of the original pretraining time.|Author: Emma Strubell Title: How To Train Your (Compressed) Large Language Model Publication year: 2023 Coauthors: A. Jha, Dirk Groeneveld, Emma Strubell, Iz Beltagy Abstract: With the increase in the size of large language models (LLMs), we need compression methods that can reduce the model size while preserving the generality and zero-shot promptability of the model. This goal is more ambitious than the typical compression setup, which reduces the model's size at the expense of specializing it to a specific end-task. To study this, we develop a task-agnostic compression pipeline with a large-scale evaluation comprising language modeling perplexity and 12 zero-shot end-tasks. Our results show that a simple layer-wise pruning followed by continued language model pretraining matches or outperforms three existing state-of-the-art baselines while being 1.5x more computationally efficient. However, unlike typical task-specialized compression, our best-compressed model significantly underperforms a similar-sized model trained from scratch. We posit the half-sized pretrained model as an upper bound for task-agnostic compression and call for future work to bridge this gap under a reasonable token budget. Our findings highlight the inadequacy of existing compression methods for LLMs and establish a requirement for new methods that preserve a model's generality and zero-shot promptability under compression. We release our code and evaluation setup to facilitate reproducibility and help iterate on method design.|Author: Graham Neubig Title: Do LLMs exhibit human-like response biases? A case study in survey design Publication year: 2023 Coauthors: Lindia Tjuatja, Valerie Chen, Sherry Tongshuang Wu, Ameet Talwalkar, Graham Neubig Abstract: As large language models (LLMs) become more capable, there is growing excitement about the possibility of using LLMs as proxies for humans in real-world tasks where subjective labels are desired, such as in surveys and opinion polling. One widely-cited barrier to the adoption of LLMs as proxies for humans in subjective tasks is their sensitivity to prompt wording - but interestingly, humans also display sensitivities to instruction changes in the form of response biases. We investigate the extent to which LLMs reflect human response biases, if at all. We look to survey design, where human response biases caused by changes in the wordings of\"prompts\"have been extensively explored in social psychology literature. Drawing from these works, we design a dataset and framework to evaluate whether LLMs exhibit human-like response biases in survey questionnaires. Our comprehensive evaluation of nine models shows that popular open and commercial LLMs generally fail to reflect human-like behavior, particularly in models that have undergone RLHF. Furthermore, even if a model shows a significant change in the same direction as humans, we find that they are sensitive to perturbations that do not elicit significant changes in humans. These results highlight the pitfalls of using LLMs as human proxies, and underscore the need for finer-grained characterizations of model behavior. Our code, dataset, and collected samples are available at https://github.com/lindiatjuatja/BiasMonkey",
        "retrieved_relevant_part": true,
        "model_answer": "Emma Strubell",
        "actual_answer": "Emma Strubell",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Who are the coauthors of the paper One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning published by LTI faculty Eric P. Xing in 2023?",
        "context": "Author: Eric P. Xing Title: One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning Publication year: 2023 Coauthors: Arnav Chavan, Zhuang Liu, D. Gupta, Eric P. Xing, Zhiqiang Shen Abstract: We present Generalized LoRA (GLoRA), an advanced approach for universal parameter-efficient fine-tuning tasks. Enhancing Low-Rank Adaptation (LoRA), GLoRA employs a generalized prompt module to optimize pre-trained model weights and adjust intermediate activations, providing more flexibility and capability across diverse tasks and datasets. Moreover, GLoRA facilitates efficient parameter adaptation by employing a scalable, modular, layer-wise structure search that learns individual adapter of each layer. Originating from a unified mathematical formulation, GLoRA exhibits strong transfer learning, few-shot learning and domain generalization abilities, as it adapts to new tasks through not only weights but also additional dimensions like activations. Comprehensive experiments demonstrate that GLoRA outperforms all previous methods in natural, specialized, and structured vision benchmarks, achieving superior accuracy with fewer parameters and computations. The proposed method on LLaMA-1 and LLaMA-2 also show considerable enhancements compared to the original LoRA in the language domain. Furthermore, our structural re-parameterization design ensures that GLoRA incurs no extra inference cost, rendering it a practical solution for resource-limited applications. Code and models are available at: https://github.com/Arnav0400/ViT-Slim/tree/master/GLoRA.",
        "retrieved_doc": "Author: Eric P. Xing Title: One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning Publication year: 2023 Coauthors: Arnav Chavan, Zhuang Liu, D. Gupta, Eric P. Xing, Zhiqiang Shen Abstract: We present Generalized LoRA (GLoRA), an advanced approach for universal parameter-efficient fine-tuning tasks. Enhancing Low-Rank Adaptation (LoRA), GLoRA employs a generalized prompt module to optimize pre-trained model weights and adjust intermediate activations, providing more flexibility and capability across diverse tasks and datasets. Moreover, GLoRA facilitates efficient parameter adaptation by employing a scalable, modular, layer-wise structure search that learns individual adapter of each layer. Originating from a unified mathematical formulation, GLoRA exhibits strong transfer learning, few-shot learning and domain generalization abilities, as it adapts to new tasks through not only weights but also additional dimensions like activations. Comprehensive experiments demonstrate that GLoRA outperforms all previous methods in natural, specialized, and structured vision benchmarks, achieving superior accuracy with fewer parameters and computations. The proposed method on LLaMA-1 and LLaMA-2 also show considerable enhancements compared to the original LoRA in the language domain. Furthermore, our structural re-parameterization design ensures that GLoRA incurs no extra inference cost, rendering it a practical solution for resource-limited applications. Code and models are available at: https://github.com/Arnav0400/ViT-Slim/tree/master/GLoRA.|Author: Eric P. Xing Title: One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning Publication year: 2023 Coauthors: Arnav Chavan, Zhuang Liu, D. Gupta, Eric P. Xing, Zhiqiang Shen Abstract: We present Generalized LoRA (GLoRA), an advanced approach for universal parameter-efficient fine-tuning tasks. Enhancing Low-Rank Adaptation (LoRA), GLoRA employs a generalized prompt module to optimize pre-trained model weights and adjust intermediate activations, providing more flexibility and capability across diverse tasks and datasets. Moreover, GLoRA facilitates efficient parameter adaptation by employing a scalable, modular, layer-wise structure search that learns individual adapter of each layer. Originating from a unified mathematical formulation, GLoRA exhibits strong transfer learning, few-shot learning and domain generalization abilities, as it adapts to new tasks through not only weights but also additional dimensions like activations. Comprehensive experiments demonstrate that GLoRA outperforms all previous methods in natural, specialized, and structured vision benchmarks, achieving superior accuracy with fewer parameters and computations. The proposed method on LLaMA-1 and LLaMA-2 also show considerable enhancements compared to the original LoRA in the language domain. Furthermore, our structural re-parameterization design ensures that GLoRA incurs no extra inference cost, rendering it a practical solution for resource-limited applications. Code and models are available at: https://github.com/Arnav0400/ViT-Slim/tree/master/GLoRA.|Author: Eric P. Xing Title: LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning Publication year: 2023 Coauthors: Han Guo, P. Greengard, Eric P. Xing, Yoon Kim Abstract: We propose a simple approach for memory-efficient adaptation of pretrained language models. Our approach uses an iterative algorithm to decompose each pretrained matrix into a high-precision low-rank component and a memory-efficient quantized component. During finetuning, the quantized component remains fixed and only the low-rank component is updated. We present an integer linear programming formulation of the quantization component which enables dynamic configuration of quantization parameters (e.g., bit-width, block size) for each matrix given an overall target memory budget. We further explore a data-aware version of the algorithm which uses an approximation of the Fisher information matrix to weight the reconstruction objective during matrix decomposition. Experiments on finetuning RoBERTa and LLaMA-2 (7B and 70B) demonstrate that our low-rank plus quantized matrix decomposition approach (LQ-LoRA) outperforms strong QLoRA and GPTQ-LoRA baselines and enables aggressive quantization to sub-3 bits with only minor performance degradations. When finetuned on a language modeling calibration dataset, LQ-LoRA can also be used for model compression; in this setting our 2.75-bit LLaMA-2-70B model (which has 2.85 bits on average when including the low-rank components and requires 27GB of GPU memory) performs respectably compared to the 16-bit baseline.",
        "retrieved_relevant_part": true,
        "model_answer": "The coauthors of the paper are Arnav Chavan, Zhuang Liu, D. Gupta, and Zhiqiang Shen",
        "actual_answer": "Arnav Chavan, Zhuang Liu, D. Gupta, Eric P. Xing, Zhiqiang Shen",
        "exact_match": false,
        "f1_max_min_avg": [
            0.62,
            0.62,
            0.62
        ],
        "recall_max_min_avg": [
            0.73,
            0.73,
            0.73
        ]
    },
    {
        "question": "Which LTI faculty published the paper Structured Pruning of Self-Supervised Pre-Trained Models for Speech Recognition and Understanding in 2023?",
        "context": "Author: Shinji Watanabe Title: Structured Pruning of Self-Supervised Pre-Trained Models for Speech Recognition and Understanding Publication year: 2023 Coauthors: Yifan Peng, Kwangyoun Kim, Felix Wu, Prashant Sridhar, Shinji Watanabe Abstract: Self-supervised speech representation learning (SSL) has shown to be effective in various downstream tasks, but SSL models are usually large and slow. Model compression techniques such as pruning aim to reduce the model size and computation without degradation in accuracy. Prior studies focus on the pruning of Transformers; however, speech models not only utilize a stack of Transformer blocks, but also combine a frontend network based on multiple convolutional layers for low-level feature representation learning. This frontend has a small size but a heavy computational cost. In this work, we propose three task-specific structured pruning methods to deal with such heterogeneous networks. Experiments on LibriSpeech and SLURP show that the proposed method is more accurate than the original wav2vec2-base with 10% to 30% less computation, and is able to reduce the computation by 40% to 50% without any degradation.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Structured Pruning of Self-Supervised Pre-Trained Models for Speech Recognition and Understanding Publication year: 2023 Coauthors: Yifan Peng, Kwangyoun Kim, Felix Wu, Prashant Sridhar, Shinji Watanabe Abstract: Self-supervised speech representation learning (SSL) has shown to be effective in various downstream tasks, but SSL models are usually large and slow. Model compression techniques such as pruning aim to reduce the model size and computation without degradation in accuracy. Prior studies focus on the pruning of Transformers; however, speech models not only utilize a stack of Transformer blocks, but also combine a frontend network based on multiple convolutional layers for low-level feature representation learning. This frontend has a small size but a heavy computational cost. In this work, we propose three task-specific structured pruning methods to deal with such heterogeneous networks. Experiments on LibriSpeech and SLURP show that the proposed method is more accurate than the original wav2vec2-base with 10% to 30% less computation, and is able to reduce the computation by 40% to 50% without any degradation.|Author: Shinji Watanabe Title: Reducing Barriers to Self-Supervised Learning: HuBERT Pre-training with Academic Compute Publication year: 2023 Coauthors: William Chen, Xuankai Chang, Yifan Peng, Zhaoheng Ni, Soumi Maiti, Shinji Watanabe Abstract: Self-supervised learning (SSL) has led to great strides in speech processing. However, the resources needed to train these models has become prohibitively large as they continue to scale. Currently, only a few groups with substantial resources are capable of creating SSL models, which harms reproducibility. In this work, we optimize HuBERT SSL to fit in academic constraints. We reproduce HuBERT independently from the original implementation, with no performance loss. Our code and training optimizations make SSL feasible with only 8 GPUs, instead of the 32 used in the original work. We also explore a semi-supervised route, using an ASR model to skip the first pre-training iteration. Within one iteration of pre-training, our models improve over HuBERT on several tasks. Furthermore, our HuBERT Large variant requires only 8 GPUs, achieving similar performance to the original trained on 128. As our contribution to the community, all models, configurations, and code are made open-source in ESPnet.|Author: Graham Neubig Title: Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes Publication year: 2024 Coauthors: L. Dery, Steven Kolawole, Jean-Francois Kagey, Virginia Smith, Graham Neubig, Ameet Talwalkar Abstract: Given the generational gap in available hardware between lay practitioners and the most endowed institutions, LLMs are becoming increasingly inaccessible as they grow in size. Whilst many approaches have been proposed to compress LLMs to make their resource consumption manageable, these methods themselves tend to be resource intensive, putting them out of the reach of the very user groups they target. In this work, we explore the problem of structured pruning of LLMs using only forward passes. We seek to empower practitioners to prune models so large that their available hardware has just enough memory to run inference. We develop Bonsai, a gradient-free, perturbative pruning method capable of delivering small, fast, and accurate pruned models. We observe that Bonsai outputs pruned models that (i) outperform those generated by more expensive gradient-based structured pruning methods, and (ii) are twice as fast (with comparable accuracy) as those generated by semi-structured pruning methods requiring comparable resources as Bonsai. We also leverage Bonsai to produce a new sub-2B model using a single A6000 that yields state-of-the-art performance on 4/6 tasks on the Huggingface Open LLM leaderboard.",
        "retrieved_relevant_part": true,
        "model_answer": "Shinji Watanabe",
        "actual_answer": "Shinji Watanabe",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Who is the author of the LTI paper The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features published in 2023?",
        "context": "Author: Rita Singh Title: The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features Publication year: 2023 Coauthors: Liao Qu, X. Zou, Xiang Li, Yandong Wen, Rita Singh, B. Raj Abstract: This work unveils the enigmatic link between phonemes and facial features. Traditional studies on voice-face correlations typically involve using a long period of voice input, including generating face images from voices and reconstructing 3D face meshes from voices. However, in situations like voice-based crimes, the available voice evidence may be short and limited. Additionally, from a physiological perspective, each segment of speech -- phoneme -- corresponds to different types of airflow and movements in the face. Therefore, it is advantageous to discover the hidden link between phonemes and face attributes. In this paper, we propose an analysis pipeline to help us explore the voice-face relationship in a fine-grained manner, i.e., phonemes v.s. facial anthropometric measurements (AM). We build an estimator for each phoneme-AM pair and evaluate the correlation through hypothesis testing. Our results indicate that AMs are more predictable from vowels compared to consonants, particularly with plosives. Additionally, we observe that if a specific AM exhibits more movement during phoneme pronunciation, it is more predictable. Our findings support those in physiology regarding correlation and lay the groundwork for future research on speech-face multimodal learning.",
        "retrieved_doc": "Author: Rita Singh Title: The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features Publication year: 2023 Coauthors: Liao Qu, X. Zou, Xiang Li, Yandong Wen, Rita Singh, B. Raj Abstract: This work unveils the enigmatic link between phonemes and facial features. Traditional studies on voice-face correlations typically involve using a long period of voice input, including generating face images from voices and reconstructing 3D face meshes from voices. However, in situations like voice-based crimes, the available voice evidence may be short and limited. Additionally, from a physiological perspective, each segment of speech -- phoneme -- corresponds to different types of airflow and movements in the face. Therefore, it is advantageous to discover the hidden link between phonemes and face attributes. In this paper, we propose an analysis pipeline to help us explore the voice-face relationship in a fine-grained manner, i.e., phonemes v.s. facial anthropometric measurements (AM). We build an estimator for each phoneme-AM pair and evaluate the correlation through hypothesis testing. Our results indicate that AMs are more predictable from vowels compared to consonants, particularly with plosives. Additionally, we observe that if a specific AM exhibits more movement during phoneme pronunciation, it is more predictable. Our findings support those in physiology regarding correlation and lay the groundwork for future research on speech-face multimodal learning.|Author: Louis-Philippe Morency Title: Reconstructing the neurodynamics of face perception during real world vision in humans using intracranial EEG recordings Publication year: 2023 Coauthors: Arish Alreja, Michael J. Ward, J. A. Colan, Qianli Ma, R. M. Richardson, Louis-Philippe Morency, A. Ghuman |Author: A. Waibel Title: Audio-driven Talking Face Generation by Overcoming Unintended Information Flow Publication year: 2023 Coauthors: Dogucan Yaman, Fevziye Irem Eyiokur, Leonard Barmann, H. K. Ekenel, A. Waibel Abstract: Audio-driven talking face generation is the task of creating a lip-synchronized, realistic face video from given audio and reference frames. This involves two major challenges: overall visual quality of generated images on the one hand, and audio-visual synchronization of the mouth part on the other hand. In this paper, we start by identifying several problematic aspects of synchronization methods in recent audio-driven talking face generation approaches. Specifically, this involves unintended flow of lip, pose and other information from the reference to the generated image, as well as instabilities during model training. Subsequently, we propose various techniques for obviating these issues: First, a silent-lip reference image generator prevents leaking of lips from the reference to the generated image. Second, an adaptive triplet loss handles the pose leaking problem. Finally, we propose a stabilized formulation of synchronization loss, circumventing aforementioned training instabilities while additionally further alleviating the lip leaking issue. Combining the individual improvements, we present state-of-the-art visual quality and synchronization performance on LRS2 in five out of seven and LRW in six out of seven metrics, and competitive results on the remaining ones. We further validate our design in various ablation experiments, confirming the individual contributions as well as their complementary effects.",
        "retrieved_relevant_part": true,
        "model_answer": "Rita Singh",
        "actual_answer": "Rita Singh",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "What is the abstract of the paper Core loss analysis of soft magnetic composite under non-sinusoidal excitation based on finite element models published by LTI faculty Yiming Yang in 2023?",
        "context": "Author: Yiming Yang Title: Core loss analysis of soft magnetic composite under non-sinusoidal excitation based on finite element models Publication year: 2023 Coauthors: Lei Zhao, Chengcheng Liu, Youhua H. Wang, Yiming Yang Abstract: Due to the effect of higher harmonics on magnetic properties under actual complex operating conditions, the accurate calculation of core losses of soft magnetic composites (SMC) is complicated. First, this paper improves the existing SMC model by introducing a correction factor to correct the hysteresis loss coefficient so that the model can consider the local variation characteristics of the magnetic density waveform and then calculate the core loss under different harmonic excitation. Then, the influence of skin effect and inhomogeneous flux density within the ring sample model is analyzed. Finally, to validate the improved model, it is compared with other models in the reference based on experimental measurements, respectively. The results show that the core loss calculated by the improved model is closer to the experimental results under different harmonic excitations. In addition, the applicability of the improved SMC model under triangular and square wave excitations is also verified by the derivation of the equations.",
        "retrieved_doc": "Author: Yiming Yang Title: Core loss analysis of soft magnetic composite under non-sinusoidal excitation based on finite element models Publication year: 2023 Coauthors: Lei Zhao, Chengcheng Liu, Youhua H. Wang, Yiming Yang Abstract: Due to the effect of higher harmonics on magnetic properties under actual complex operating conditions, the accurate calculation of core losses of soft magnetic composites (SMC) is complicated. First, this paper improves the existing SMC model by introducing a correction factor to correct the hysteresis loss coefficient so that the model can consider the local variation characteristics of the magnetic density waveform and then calculate the core loss under different harmonic excitation. Then, the influence of skin effect and inhomogeneous flux density within the ring sample model is analyzed. Finally, to validate the improved model, it is compared with other models in the reference based on experimental measurements, respectively. The results show that the core loss calculated by the improved model is closer to the experimental results under different harmonic excitations. In addition, the applicability of the improved SMC model under triangular and square wave excitations is also verified by the derivation of the equations.|Author: Yiming Yang Title: Experimental and numerical research on the static behavior of locally corroded OSBD Publication year: 2023 Coauthors: J. Peng, Yi Liu, Yiming Yang, Yadong Zhou, Longzhen Xie |Author: Yiming Yang Title: An Experimental Study on Secondary Transfer Performances of Prestress after Anchoring Failure of Steel Wire Strands Publication year: 2023 Coauthors: Rihua Yang, Yiming Yang, Xuhui Zhang, Xinzhong Wang Abstract: To understand the secondary transfer performances of residual prestress after the anchoring failure of end-anchored steel wire strands due to corrosion fracture, six steel wire strand components of post-tensioning prestress were designed and fabricated. One-side fast corrosion was applied to the steel wire strand components using the electrochemical method until anchoring failure was reached. The sphere of influence, stress changes, and the retraction and swelling effect of broken beams after failure were investigated. The influences of factors such as concrete strength, stirrup area, and the length of the component on the secondary transfer length of residual prestress were discussed. Based on the deformation relationship between prestressed steel wire strands and concrete in the stress transfer zone, a stress equation was established and solved through a bond constitutive model. A prediction model of the effective stress transfer length of prestressed steel wire strand after failure was proposed. The results demonstrated that residual prestress can have a secondary transfer after the corrosion fracture of end-anchored steel wire strands, but some effective prestress may be lost. Moreover, the loss of prestress is inversely proportional to concrete compressive strength. When the specimens are relatively short, the prestress loss increases significantly. Concrete strength has significant influences on the length of secondary transfer. The proposed simplified calculation method of the secondary transfer length of residual prestress has a relatively high accuracy, with an average error of 2.9% and a maximum error of 5.2%.",
        "retrieved_relevant_part": true,
        "model_answer": "The paper discusses the accurate calculation of core losses of soft magnetic composites under complex operating conditions and compares the results with other models",
        "actual_answer": "Due to the effect of higher harmonics on magnetic properties under actual complex operating conditions, the accurate calculation of core losses of soft magnetic composites (SMC) is complicated. First, this paper improves the existing SMC model by introducing a correction factor to correct the hysteresis loss coefficient so that the model can consider the local variation characteristics of the magnetic density waveform and then calculate the core loss under different harmonic excitation. Then, the influence of skin effect and inhomogeneous flux density within the ring sample model is analyzed. Finally, to validate the improved model, it is compared with other models in the reference based on experimental measurements, respectively. The results show that the core loss calculated by the improved model is closer to the experimental results under different harmonic excitations. In addition, the applicability of the improved SMC model under triangular and square wave excitations is also verified by the derivation of the equations.",
        "exact_match": false,
        "f1_max_min_avg": [
            0.21,
            0.21,
            0.21
        ],
        "recall_max_min_avg": [
            0.12,
            0.12,
            0.12
        ]
    },
    {
        "question": "Who is the author of the LTI paper AutoReply: Detecting Nonsense in Dialogue with Discriminative Replies published in 2023?",
        "context": "Author: Daniel Fried Title: AutoReply: Detecting Nonsense in Dialogue with Discriminative Replies Publication year: 2023 Coauthors: Weiyan Shi, Emily Dinan, Adi Renduchintala, Daniel Fried, Athul Paul Jacob, Zhou Yu, Mike Lewis ",
        "retrieved_doc": "Author: Daniel Fried Title: AutoReply: Detecting Nonsense in Dialogue with Discriminative Replies Publication year: 2023 Coauthors: Weiyan Shi, Emily Dinan, Adi Renduchintala, Daniel Fried, Athul Paul Jacob, Zhou Yu, Mike Lewis |Author: Alexander I. Rudnicky Title: Tartan: an LLM Driven SocialBot Publication year: 2023 Coauthors: Liangze Li, Ziyuan Liu, Li-Wei Chen, Ta-Chung Chi, Alexander I. Rudnicky Abstract: We describe our work to date on using large language models (LLMs) in our Alexa Prize Socialbot. Our work has laid the groundwork for looking more closely at using LLMs in a conversational system. We also analyze common patterns in conversations our bot has had with users|Author: Alexander I. Rudnicky Title: A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech Publication year: 2023 Coauthors: Li-Wei Chen, Shinji Watanabe, Alexander I. Rudnicky Abstract: Recent Text-to-Speech (TTS) systems trained on reading or acted corpora have achieved near human-level naturalness. The diversity of human speech, however, often goes beyond the coverage of these corpora. We believe the ability to handle such diversity is crucial for AI systems to achieve human-level communication. Our work explores the use of more abundant real-world data for building speech synthesizers. We train TTS systems using real-world speech from YouTube and podcasts. We observe the mismatch between training and inference alignments in mel-spectrogram based autoregressive models, leading to unintelligible synthesis, and demonstrate that learned discrete codes within multiple code groups effectively resolves this issue. We introduce our MQTTS system whose architecture is designed for multiple code generation and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality. We conduct ablation analyses to identify the efficacy of our methods. We show that MQTTS outperforms existing TTS systems in several objective and subjective measures.",
        "retrieved_relevant_part": true,
        "model_answer": "Daniel Fried",
        "actual_answer": "Daniel Fried",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Who is the author of the LTI paper A Gold Standard Dataset for the Reviewer Assignment Problem published in 2023?",
        "context": "Author: Graham Neubig Title: A Gold Standard Dataset for the Reviewer Assignment Problem Publication year: 2023 Coauthors: Ivan Stelmakh, J. Wieting, Graham Neubig, Nihar B. Shah Abstract: Many peer-review venues are either using or looking to use algorithms to assign submissions to reviewers. The crux of such automated approaches is the notion of the\"similarity score\"--a numerical estimate of the expertise of a reviewer in reviewing a paper--and many algorithms have been proposed to compute these scores. However, these algorithms have not been subjected to a principled comparison, making it difficult for stakeholders to choose the algorithm in an evidence-based manner. The key challenge in comparing existing algorithms and developing better algorithms is the lack of the publicly available gold-standard data that would be needed to perform reproducible research. We address this challenge by collecting a novel dataset of similarity scores that we release to the research community. Our dataset consists of 477 self-reported expertise scores provided by 58 researchers who evaluated their expertise in reviewing papers they have read previously. We use this data to compare several popular algorithms employed in computer science conferences and come up with recommendations for stakeholders. Our main findings are as follows. First, all algorithms make a non-trivial amount of error. For the task of ordering two papers in terms of their relevance for a reviewer, the error rates range from 12%-30% in easy cases to 36%-43% in hard cases, highlighting the vital need for more research on the similarity-computation problem. Second, most existing algorithms are designed to work with titles and abstracts of papers, and in this regime the Specter+MFR algorithm performs best. Third, to improve performance, it may be important to develop modern deep-learning based algorithms that can make use of the full texts of papers: the classical TD-IDF algorithm enhanced with full texts of papers is on par with the deep-learning based Specter+MFR that cannot make use of this information.",
        "retrieved_doc": "Author: Graham Neubig Title: A Gold Standard Dataset for the Reviewer Assignment Problem Publication year: 2023 Coauthors: Ivan Stelmakh, J. Wieting, Graham Neubig, Nihar B. Shah Abstract: Many peer-review venues are either using or looking to use algorithms to assign submissions to reviewers. The crux of such automated approaches is the notion of the\"similarity score\"--a numerical estimate of the expertise of a reviewer in reviewing a paper--and many algorithms have been proposed to compute these scores. However, these algorithms have not been subjected to a principled comparison, making it difficult for stakeholders to choose the algorithm in an evidence-based manner. The key challenge in comparing existing algorithms and developing better algorithms is the lack of the publicly available gold-standard data that would be needed to perform reproducible research. We address this challenge by collecting a novel dataset of similarity scores that we release to the research community. Our dataset consists of 477 self-reported expertise scores provided by 58 researchers who evaluated their expertise in reviewing papers they have read previously. We use this data to compare several popular algorithms employed in computer science conferences and come up with recommendations for stakeholders. Our main findings are as follows. First, all algorithms make a non-trivial amount of error. For the task of ordering two papers in terms of their relevance for a reviewer, the error rates range from 12%-30% in easy cases to 36%-43% in hard cases, highlighting the vital need for more research on the similarity-computation problem. Second, most existing algorithms are designed to work with titles and abstracts of papers, and in this regime the Specter+MFR algorithm performs best. Third, to improve performance, it may be important to develop modern deep-learning based algorithms that can make use of the full texts of papers: the classical TD-IDF algorithm enhanced with full texts of papers is on par with the deep-learning based Specter+MFR that cannot make use of this information.|Author: Graham Neubig Title: DataFinder: Scientific Dataset Recommendation from Natural Language Descriptions Publication year: 2023 Coauthors: Vijay Viswanathan, Luyu Gao, Tongshuang Sherry Wu, Pengfei Liu, Graham Neubig Abstract: Modern machine learning relies on datasets to develop and validate research ideas. Given the growth of publicly available data, finding the right dataset to use is increasingly difficult. Any research question imposes explicit and implicit constraints on how well a given dataset will enable researchers to answer this question, such as dataset size, modality, and domain. We operationalize the task of recommending datasets given a short natural language description of a research idea, to help people find relevant datasets for their needs. Dataset recommendation poses unique challenges as an information retrieval problem; datasets are hard to directly index for search and there are no corpora readily available for this task. To facilitate this task, we build the DataFinder Dataset which consists of a larger automatically-constructed training set (17.5K queries) and a smaller expert-annotated evaluation set (392 queries). Using this data, we compare various information retrieval algorithms on our test set and present a superior bi-encoder retriever for text-based dataset recommendation. This system, trained on the DataFinder Dataset, finds more relevant search results than existing third-party dataset search engines. To encourage progress on dataset recommendation, we release our dataset and models to the public.|Author: Jamie Callan Title: COILcr: Efficient Semantic Matching in Contextualized Exact Match Retrieval Publication year: 2023 Coauthors: Zhen Fan, Luyu Gao, Rohan Jha, Jamie Callan ",
        "retrieved_relevant_part": true,
        "model_answer": "Graham Neubig",
        "actual_answer": "Graham Neubig",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Who is the author of the LTI paper Evaluating Multilingual Speech Translation under Realistic Conditions with Resegmentation and Terminology published in 2023?",
        "context": "Author: Mona T. Diab Title: Evaluating Multilingual Speech Translation under Realistic Conditions with Resegmentation and Terminology Publication year: 2023 Coauthors: Elizabeth Salesky, Kareem Darwish, Mohamed Al-Badrashiny, Mona T. Diab, J. Niehues Abstract: We present the ACL 60/60 evaluation sets for multilingual translation of ACL 2022 technical presentations into 10 target languages. This dataset enables further research into multilingual speech translation under realistic recording conditions with unsegmented audio and domain-specific terminology, applying NLP tools to text and speech in the technical domain, and evaluating and improving model robustness to diverse speaker demographics.",
        "retrieved_doc": "Author: Mona T. Diab Title: Evaluating Multilingual Speech Translation under Realistic Conditions with Resegmentation and Terminology Publication year: 2023 Coauthors: Elizabeth Salesky, Kareem Darwish, Mohamed Al-Badrashiny, Mona T. Diab, J. Niehues Abstract: We present the ACL 60/60 evaluation sets for multilingual translation of ACL 2022 technical presentations into 10 target languages. This dataset enables further research into multilingual speech translation under realistic recording conditions with unsegmented audio and domain-specific terminology, applying NLP tools to text and speech in the technical domain, and evaluating and improving model robustness to diverse speaker demographics.|Author: A. Waibel Title: KIT\u2019s Multilingual Speech Translation System for IWSLT 2023 Publication year: 2023 Coauthors: Danni Liu, T. Nguyen, Sai Koneru, Enes Yavuz Ugan, Ngoc-Quan Pham, Tuan-Nam Nguyen, Tu Anh Dinh, Carlos Mullov, A. Waibel, J. Niehues Abstract: Many existing speech translation benchmarks focus on native-English speech in high-quality recording conditions, which often do not match the conditions in real-life use-cases. In this paper, we describe our speech translation system for the multilingual track of IWSLT 2023, which focuses on the translation of scientific conference talks. The test condition features accented input speech and terminology-dense contents. The tasks requires translation into 10 languages of varying amounts of resources. In absence of training data from the target domain, we use a retrieval-based approach (kNN-MT) for effective adaptation (+0.8 BLEU for speech translation). We also use adapters to easily integrate incremental training data from data augmentation, and show that it matches the performance of re-training. We observe that cascaded systems are more easily adaptable towards specific target domains, due to their separate modules. Our cascaded speech system outperforms its end-to-end counterpart on scientific talk translation, although their performance remains similar on TED talks.|Author: A. Waibel Title: FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN Publication year: 2023 Coauthors: Sweta Agrawal, Antonios Anastasopoulos, L. Bentivogli, Ondrej Bojar, Claudia Borg, Marine Carpuat, Roldano Cattoni, Mauro Cettolo, Mingda Chen, William Chen, K. Choukri, Alexandra Chronopoulou, Anna Currey, T. Declerck, Qianqian Dong, Kevin Duh, Y. Est\u00e8ve, Marcello Federico, Souhir Gahbiche, B. Haddow, B. Hsu, Phu Mon Htut, H. Inaguma, D\u00e1vid Javorsk\u00fd, J. Judge, Yasumasa Kano, Tom Ko, Rishu Kumar, Peng Li, Xutai Ma, Prashant Mathur, E. Matusov, Paul McNamee, John P. McCrae, Kenton Murray, Maria Nadejde, Satoshi Nakamura, Matteo Negri, H. Nguyen, J. Niehues, Xing Niu, Atul Kr. Ojha, John E. Ortega, Proyag Pal, J. Pino, Lonneke van der Plas, Peter Pol\u00e1k, Elijah Matthew Rippeth, Elizabeth Salesky, Jiatong Shi, Matthias Sperber, Sebastian St\u00fcker, Katsuhito Sudoh, Yun Tang, Brian Thompson, Ke M. Tran, M. Turchi, A. Waibel, Mingxuan Wang, Shinji Watanabe, Rodolfo Zevallos Abstract: This paper reports on the shared tasks organized by the 20th IWSLT Conference. The shared tasks address 9 scientific challenges in spoken language translation: simultaneous and offline translation, automatic subtitling and dubbing, speech-to-speech translation, multilingual, dialect and low-resource speech translation, and formality control. The shared tasks attracted a total of 38 submissions by 31 teams. The growing interest towards spoken language translation is also witnessed by the constantly increasing number of shared task organizers and contributors to the overview paper, almost evenly distributed across industry and academia.",
        "retrieved_relevant_part": true,
        "model_answer": "Mona T. Diab",
        "actual_answer": "Mona T. Diab",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "What is the abstract of the paper I3D: Transformer Architectures with Input-Dependent Dynamic Depth for Speech Recognition published by LTI faculty Shinji Watanabe in 2023?",
        "context": "Author: Shinji Watanabe Title: I3D: Transformer Architectures with Input-Dependent Dynamic Depth for Speech Recognition Publication year: 2023 Coauthors: Yifan Peng, Jaesong Lee, Shinji Watanabe Abstract: Transformer-based end-to-end speech recognition has achieved great success. However, the large footprint and computational overhead make it difficult to deploy these models in some real-world applications. Model compression techniques can reduce the model size and speed up inference, but the compressed model has a fixed architecture which might be suboptimal. We propose a novel Transformer encoder with Input-Dependent Dynamic Depth (I3D) to achieve strong performance-efficiency trade-offs. With a similar number of layers at inference time, I3D-based models outperform the vanilla Transformer and the static pruned model via iterative layer pruning. We also present interesting analysis on the gate probabilities and the input-dependency, which helps us better understand deep encoders.",
        "retrieved_doc": "Author: Shinji Watanabe Title: I3D: Transformer Architectures with Input-Dependent Dynamic Depth for Speech Recognition Publication year: 2023 Coauthors: Yifan Peng, Jaesong Lee, Shinji Watanabe Abstract: Transformer-based end-to-end speech recognition has achieved great success. However, the large footprint and computational overhead make it difficult to deploy these models in some real-world applications. Model compression techniques can reduce the model size and speed up inference, but the compressed model has a fixed architecture which might be suboptimal. We propose a novel Transformer encoder with Input-Dependent Dynamic Depth (I3D) to achieve strong performance-efficiency trade-offs. With a similar number of layers at inference time, I3D-based models outperform the vanilla Transformer and the static pruned model via iterative layer pruning. We also present interesting analysis on the gate probabilities and the input-dependency, which helps us better understand deep encoders.|Author: Shinji Watanabe Title: Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study Publication year: 2023 Coauthors: Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma, Jiatong Shi, Jinchuan Tian, Shinji Watanabe, Yuya Fujita, Takashi Maekaku, Pengcheng Guo, Yao-Fei Cheng, Pavel Denisov, Kohei Saijo, Hsiu-Hsuan Wang Abstract: Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of discrete speech units derived from self-supervised learning representations, which significantly compresses the size of speech data. Applying various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech processing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good results in almost all the settings. We intend to release our configurations and trained models to foster future research efforts.|Author: Shinji Watanabe Title: E-Branchformer-Based E2E SLU Toward Stop on-Device Challenge Publication year: 2023 Coauthors: Yosuke Kashiwagi, Siddhant Arora, Hayato Futami, Jessica Huynh, Shih-Lun Wu, Yifan Peng, Brian Yan, E. Tsunoo, Shinji Watanabe Abstract: In this paper, we report our team\u2019s study on track 2 of the Spoken Language Understanding Grand Challenge, which is a component of the ICASSP Signal Processing Grand Challenge 2023. The task is intended for on-device processing and involves estimating semantic parse labels from speech using a model with 15 million parameters. We use E2E E-Branchformer-based spoken language understanding model, which is more parameter controllable than cascade models, and reduced the parameter size through sequential distillation and tensor decomposition techniques. On the STOP dataset, we achieved an exact match accuracy of 70.9% under the tight constraint of 15 million parameters.",
        "retrieved_relevant_part": true,
        "model_answer": "The paper proposes a novel Transformer encoder with Input-Dependent Dynamic Depth (I3D) to achieve strong performance-efficiency trade-offs in speech recognition",
        "actual_answer": "Transformer-based end-to-end speech recognition has achieved great success. However, the large footprint and computational overhead make it difficult to deploy these models in some real-world applications. Model compression techniques can reduce the model size and speed up inference, but the compressed model has a fixed architecture which might be suboptimal. We propose a novel Transformer encoder with Input-Dependent Dynamic Depth (I3D) to achieve strong performance-efficiency trade-offs. With a similar number of layers at inference time, I3D-based models outperform the vanilla Transformer and the static pruned model via iterative layer pruning. We also present interesting analysis on the gate probabilities and the input-dependency, which helps us better understand deep encoders.",
        "exact_match": false,
        "f1_max_min_avg": [
            0.25,
            0.25,
            0.25
        ],
        "recall_max_min_avg": [
            0.15,
            0.15,
            0.15
        ]
    },
    {
        "question": "What is the abstract of the paper ActiveRAG: Revealing the Treasures of Knowledge via Active Learning published by LTI faculty Chenyan Xiong in 2024?",
        "context": "Author: Chenyan Xiong Title: ActiveRAG: Revealing the Treasures of Knowledge via Active Learning Publication year: 2024 Coauthors: Zhipeng Xu, Zhenghao Liu, Yibin Liu, Chenyan Xiong, Yukun Yan, Shuo Wang, Shi Yu, Zhiyuan Liu, Ge Yu Abstract: Retrieval Augmented Generation (RAG) has introduced a new paradigm for Large Language Models (LLMs), aiding in the resolution of knowledge-intensive tasks. However, current RAG models position LLMs as passive knowledge receptors, thereby restricting their capacity for learning and comprehending external knowledge. In this paper, we present ActiveRAG, an innovative RAG framework that shifts from passive knowledge acquisition to an active learning mechanism. This approach utilizes the Knowledge Construction mechanism to develop a deeper understanding of external knowledge by associating it with previously acquired or memorized knowledge. Subsequently, it designs the Cognitive Nexus mechanism to incorporate the outcomes from both chains of thought and knowledge construction, thereby calibrating the intrinsic cognition of LLMs. Our experimental results demonstrate that ActiveRAG surpasses previous RAG models, achieving a 5% improvement on question-answering datasets. All data and codes are available at https://github.com/OpenMatch/ActiveRAG.",
        "retrieved_doc": "Author: Chenyan Xiong Title: ActiveRAG: Revealing the Treasures of Knowledge via Active Learning Publication year: 2024 Coauthors: Zhipeng Xu, Zhenghao Liu, Yibin Liu, Chenyan Xiong, Yukun Yan, Shuo Wang, Shi Yu, Zhiyuan Liu, Ge Yu Abstract: Retrieval Augmented Generation (RAG) has introduced a new paradigm for Large Language Models (LLMs), aiding in the resolution of knowledge-intensive tasks. However, current RAG models position LLMs as passive knowledge receptors, thereby restricting their capacity for learning and comprehending external knowledge. In this paper, we present ActiveRAG, an innovative RAG framework that shifts from passive knowledge acquisition to an active learning mechanism. This approach utilizes the Knowledge Construction mechanism to develop a deeper understanding of external knowledge by associating it with previously acquired or memorized knowledge. Subsequently, it designs the Cognitive Nexus mechanism to incorporate the outcomes from both chains of thought and knowledge construction, thereby calibrating the intrinsic cognition of LLMs. Our experimental results demonstrate that ActiveRAG surpasses previous RAG models, achieving a 5% improvement on question-answering datasets. All data and codes are available at https://github.com/OpenMatch/ActiveRAG.|Author: Eric P. Xing Title: Redco: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU/TPUs Publication year: 2023 Coauthors: Bowen Tan, Yun Zhu, Lijuan Liu, Hongyi Wang, Yonghao Zhuang, Jindong Chen, Eric P. Xing, Zhiting Hu Abstract: .|Author: Eric P. Xing Title: Redco: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU/TPUs Publication year: 2023 Coauthors: Bowen Tan, Yun Zhu, Lijuan Liu, Hongyi Wang, Yonghao Zhuang, Jindong Chen, Eric P. Xing, Zhiting Hu Abstract: .",
        "retrieved_relevant_part": true,
        "model_answer": "The paper presents an innovative RAG framework called ActiveRAG that uses an active learning mechanism to develop a deeper understanding of external knowledge by associating it with previously acquired or memorized knowledge",
        "actual_answer": "Retrieval Augmented Generation (RAG) has introduced a new paradigm for Large Language Models (LLMs), aiding in the resolution of knowledge-intensive tasks. However, current RAG models position LLMs as passive knowledge receptors, thereby restricting their capacity for learning and comprehending external knowledge. In this paper, we present ActiveRAG, an innovative RAG framework that shifts from passive knowledge acquisition to an active learning mechanism. This approach utilizes the Knowledge Construction mechanism to develop a deeper understanding of external knowledge by associating it with previously acquired or memorized knowledge. Subsequently, it designs the Cognitive Nexus mechanism to incorporate the outcomes from both chains of thought and knowledge construction, thereby calibrating the intrinsic cognition of LLMs. Our experimental results demonstrate that ActiveRAG surpasses previous RAG models, achieving a 5% improvement on question-answering datasets. All data and codes are available at https://github.com/OpenMatch/ActiveRAG.",
        "exact_match": false,
        "f1_max_min_avg": [
            0.3,
            0.3,
            0.3
        ],
        "recall_max_min_avg": [
            0.18,
            0.18,
            0.18
        ]
    },
    {
        "question": "Who is the author of the LTI paper Distributionally Robust Unsupervised Dense Retrieval Training on Web Graphs published in 2023?",
        "context": "Author: Chenyan Xiong Title: Distributionally Robust Unsupervised Dense Retrieval Training on Web Graphs Publication year: 2023 Coauthors: Peixuan Han, Zhenghao Liu, Zhiyuan Liu, Chenyan Xiong Abstract: This paper introduces Web-DRO, an unsupervised dense retrieval model, which clusters documents based on web structures and reweights the groups during contrastive training. Specifically, we first leverage web graph links and contrastively train an embedding model for clustering anchor-document pairs. Then we use Group Distributional Robust Optimization to reweight different clusters of anchor-document pairs, which guides the model to assign more weights to the group with higher contrastive loss and pay more attention to the worst case during training. Our experiments on MS MARCO and BEIR show that our model, Web-DRO, significantly improves the retrieval effectiveness in unsupervised scenarios. A comparison of clustering techniques shows that training on the web graph combining URL information reaches optimal performance on clustering. Further analysis confirms that group weights are stable and valid, indicating consistent model preferences as well as effective up-weighting of valuable groups and down-weighting of uninformative ones. The code of this paper can be obtained from https://github.com/OpenMatch/Web-DRO.",
        "retrieved_doc": "Author: Chenyan Xiong Title: Distributionally Robust Unsupervised Dense Retrieval Training on Web Graphs Publication year: 2023 Coauthors: Peixuan Han, Zhenghao Liu, Zhiyuan Liu, Chenyan Xiong Abstract: This paper introduces Web-DRO, an unsupervised dense retrieval model, which clusters documents based on web structures and reweights the groups during contrastive training. Specifically, we first leverage web graph links and contrastively train an embedding model for clustering anchor-document pairs. Then we use Group Distributional Robust Optimization to reweight different clusters of anchor-document pairs, which guides the model to assign more weights to the group with higher contrastive loss and pay more attention to the worst case during training. Our experiments on MS MARCO and BEIR show that our model, Web-DRO, significantly improves the retrieval effectiveness in unsupervised scenarios. A comparison of clustering techniques shows that training on the web graph combining URL information reaches optimal performance on clustering. Further analysis confirms that group weights are stable and valid, indicating consistent model preferences as well as effective up-weighting of valuable groups and down-weighting of uninformative ones. The code of this paper can be obtained from https://github.com/OpenMatch/Web-DRO.|Author: Chenyan Xiong Title: Unsupervised Dense Retrieval Training with Web Anchors Publication year: 2023 Coauthors: Yiqing Xie, X. Liu, Chenyan Xiong Abstract: In this work, we present an unsupervised retrieval method with contrastive learning on web anchors. The anchor text describes the content that is referenced from the linked page. This shows similarities to search queries that aim to retrieve pertinent information from relevant documents. Based on their commonalities, we train an unsupervised dense retriever, Anchor-DR, with a contrastive learning task that matches the anchor text and the linked document. To filter out uninformative anchors (such as \"homepage\" or other functional anchors), we present a novel filtering technique to only select anchors that contain similar types of information as search queries. Experiments show that Anchor-DR outperforms state-of-the-art methods on unsupervised dense retrieval by a large margin (e.g., by 5.3% NDCG@10 on MSMARCO). The gain of our method is especially significant for search and question answering tasks. Our analysis further reveals that the pattern of anchor-document pairs is similar to that of search query-document pairs. Code available at https://github.com/Veronicium/AnchorDR.|Author: Chenyan Xiong Title: Structure-Aware Language Model Pretraining Improves Dense Retrieval on Structured Data Publication year: 2023 Coauthors: Xinze Li, Zhenghao Liu, Chenyan Xiong, Shi Yu, Yu Gu, Zhiyuan Liu, Ge Yu Abstract: This paper presents Structure Aware Dense Retrieval (SANTA) model, which encodes user queries and structured data in one universal embedding space for retrieving structured data. SANTA proposes two pretraining methods to make language models structure-aware and learn effective representations for structured data: 1) Structured Data Alignment, which utilizes the natural alignment relations between structured data and unstructured data for structure-aware pretraining. It contrastively trains language models to represent multi-modal text data and teaches models to distinguish matched structured data for unstructured texts. 2) Masked Entity Prediction, which designs an entity-oriented mask strategy and asks language models to fill in the masked entities. Our experiments show that SANTA achieves state-of-the-art on code search and product search and conducts convincing results in the zero-shot setting. SANTA learns tailored representations for multi-modal text data by aligning structured and unstructured data pairs and capturing structural semantics by masking and predicting entities in the structured data. All codes are available at https://github.com/OpenMatch/OpenMatch.",
        "retrieved_relevant_part": true,
        "model_answer": "Chenyan Xiong",
        "actual_answer": "Chenyan Xiong",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Which LTI faculty published the paper Are aligned neural networks adversarially aligned? in 2023?",
        "context": "Author: Daphne Ippolito Title: Are aligned neural networks adversarially aligned? Publication year: 2023 Coauthors: Nicholas Carlini, Milad Nasr, Christopher A. Choquette-Choo, Matthew Jagielski, Irena Gao, Anas Awadalla, Pang Wei Koh, Daphne Ippolito, Katherine Lee, Florian Tram\u00e8r, Ludwig Schmidt Abstract: Large language models are now tuned to align with the goals of their creators, namely to be\"helpful and harmless.\"These models should respond helpfully to user questions, but refuse to answer requests that could cause harm. However, adversarial users can construct inputs which circumvent attempts at alignment. In this work, we study to what extent these models remain aligned, even when interacting with an adversarial user who constructs worst-case inputs (adversarial examples). These inputs are designed to cause the model to emit harmful content that would otherwise be prohibited. We show that existing NLP-based optimization attacks are insufficiently powerful to reliably attack aligned text models: even when current NLP-based attacks fail, we can find adversarial inputs with brute force. As a result, the failure of current attacks should not be seen as proof that aligned text models remain aligned under adversarial inputs. However the recent trend in large-scale ML models is multimodal models that allow users to provide images that influence the text that is generated. We show these models can be easily attacked, i.e., induced to perform arbitrary un-aligned behavior through adversarial perturbation of the input image. We conjecture that improved NLP attacks may demonstrate this same level of adversarial control over text-only models.",
        "retrieved_doc": "Author: Daphne Ippolito Title: Are aligned neural networks adversarially aligned? Publication year: 2023 Coauthors: Nicholas Carlini, Milad Nasr, Christopher A. Choquette-Choo, Matthew Jagielski, Irena Gao, Anas Awadalla, Pang Wei Koh, Daphne Ippolito, Katherine Lee, Florian Tram\u00e8r, Ludwig Schmidt Abstract: Large language models are now tuned to align with the goals of their creators, namely to be\"helpful and harmless.\"These models should respond helpfully to user questions, but refuse to answer requests that could cause harm. However, adversarial users can construct inputs which circumvent attempts at alignment. In this work, we study to what extent these models remain aligned, even when interacting with an adversarial user who constructs worst-case inputs (adversarial examples). These inputs are designed to cause the model to emit harmful content that would otherwise be prohibited. We show that existing NLP-based optimization attacks are insufficiently powerful to reliably attack aligned text models: even when current NLP-based attacks fail, we can find adversarial inputs with brute force. As a result, the failure of current attacks should not be seen as proof that aligned text models remain aligned under adversarial inputs. However the recent trend in large-scale ML models is multimodal models that allow users to provide images that influence the text that is generated. We show these models can be easily attacked, i.e., induced to perform arbitrary un-aligned behavior through adversarial perturbation of the input image. We conjecture that improved NLP attacks may demonstrate this same level of adversarial control over text-only models.|Spring offering: Course: 14757 Title: Introduction to Adversarial Machine Learning: Units: 12.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 03:30PM End: 04:50PM Bldg/Room: CIC 1201 Location: Pittsburgh, Pennsylvania Instructor(s): Varodayan |Spring offering: Course: 14757 Title: Introduction to Adversarial Machine Learning Units: 12.0 Lec/Sec: Section A Days: Monday, Wednesday Begin: 03:30PM End: 04:50PM Bldg/Room: CIC 1201 Location: Pittsburgh, Pennsylvania Instructor(s): Varodayan ",
        "retrieved_relevant_part": true,
        "model_answer": "The paper was published by LTI faculty Daphne Ippolito, Nicholas Carlini, Milad Nasr, Christopher A. Choquette-Choo, Matthew Jagielski, Irena Gao, Anas Awadalla, Pang Wei Koh, Katherine Lee, Florian Tram\u00e8r, and Ludwig Schmidt",
        "actual_answer": "Daphne Ippolito",
        "exact_match": false,
        "f1_max_min_avg": [
            0.06,
            0.06,
            0.06
        ],
        "recall_max_min_avg": [
            0.5,
            0.5,
            0.5
        ]
    },
    {
        "question": "Which LTI faculty published the paper Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large Language Models in 2024?",
        "context": "Author: Eric Xing Title: Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large Language Models Publication year: 2024 Coauthors: Loka Li, Guan-Hong Chen, Yusheng Su, Zhenhao Chen, Yixuan Zhang, Eric Xing, Kun Zhang Abstract: The recent success of Large Language Models (LLMs) has catalyzed an increasing interest in their self-correction capabilities. This paper presents a comprehensive investigation into the intrinsic self-correction of LLMs, attempting to address the ongoing debate about its feasibility. Our research has identified an important latent factor - the ``confidence'' of LLMs - during the self-correction process. Overlooking this factor may cause the models to over-criticize themselves, resulting in unreliable conclusions regarding the efficacy of self-correction. We have experimentally observed that LLMs possess the capability to understand the ``confidence'' in their own responses. It motivates us to develop an ``If-or-Else'' (IoE) prompting framework, designed to guide LLMs in assessing their own ``confidence'', facilitating intrinsic self-corrections. We conduct extensive experiments and demonstrate that our IoE-based Prompt can achieve a consistent improvement regarding the accuracy of self-corrected responses over the initial answers. Our study not only sheds light on the underlying factors affecting self-correction in LLMs, but also introduces a practical framework that utilizes the IoE prompting principle to efficiently improve self-correction capabilities with ``confidence''. The code is available at \\url{https://github.com/MBZUAI-CLeaR/IoE-Prompting.git}.",
        "retrieved_doc": "Author: Eric Xing Title: Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large Language Models Publication year: 2024 Coauthors: Loka Li, Guan-Hong Chen, Yusheng Su, Zhenhao Chen, Yixuan Zhang, Eric Xing, Kun Zhang Abstract: The recent success of Large Language Models (LLMs) has catalyzed an increasing interest in their self-correction capabilities. This paper presents a comprehensive investigation into the intrinsic self-correction of LLMs, attempting to address the ongoing debate about its feasibility. Our research has identified an important latent factor - the ``confidence'' of LLMs - during the self-correction process. Overlooking this factor may cause the models to over-criticize themselves, resulting in unreliable conclusions regarding the efficacy of self-correction. We have experimentally observed that LLMs possess the capability to understand the ``confidence'' in their own responses. It motivates us to develop an ``If-or-Else'' (IoE) prompting framework, designed to guide LLMs in assessing their own ``confidence'', facilitating intrinsic self-corrections. We conduct extensive experiments and demonstrate that our IoE-based Prompt can achieve a consistent improvement regarding the accuracy of self-corrected responses over the initial answers. Our study not only sheds light on the underlying factors affecting self-correction in LLMs, but also introduces a practical framework that utilizes the IoE prompting principle to efficiently improve self-correction capabilities with ``confidence''. The code is available at \\url{https://github.com/MBZUAI-CLeaR/IoE-Prompting.git}.|Author: Eric Xing Title: Confidence Matters: Revisiting Intrinsic Self-Correction Capabilities of Large Language Models Publication year: 2024 Coauthors: Loka Li, Guan-Hong Chen, Yusheng Su, Zhenhao Chen, Yixuan Zhang, Eric Xing, Kun Zhang Abstract: The recent success of Large Language Models (LLMs) has catalyzed an increasing interest in their self-correction capabilities. This paper presents a comprehensive investigation into the intrinsic self-correction of LLMs, attempting to address the ongoing debate about its feasibility. Our research has identified an important latent factor - the ``confidence'' of LLMs - during the self-correction process. Overlooking this factor may cause the models to over-criticize themselves, resulting in unreliable conclusions regarding the efficacy of self-correction. We have experimentally observed that LLMs possess the capability to understand the ``confidence'' in their own responses. It motivates us to develop an ``If-or-Else'' (IoE) prompting framework, designed to guide LLMs in assessing their own ``confidence'', facilitating intrinsic self-corrections. We conduct extensive experiments and demonstrate that our IoE-based Prompt can achieve a consistent improvement regarding the accuracy of self-corrected responses over the initial answers. Our study not only sheds light on the underlying factors affecting self-correction in LLMs, but also introduces a practical framework that utilizes the IoE prompting principle to efficiently improve self-correction capabilities with ``confidence''. The code is available at \\url{https://github.com/MBZUAI-CLeaR/IoE-Prompting.git}.|Author: S. Welleck Title: Self-Refine: Iterative Refinement with Self-Feedback Publication year: 2023 Coauthors: Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, S. Welleck, Bodhisattwa Prasad Majumder, Shashank Gupta, A. Yazdanbakhsh, Peter Clark Abstract: Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ~20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach.",
        "retrieved_relevant_part": true,
        "model_answer": "Eric Xing",
        "actual_answer": "Eric Xing",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "What is the abstract of the paper Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers published by LTI faculty Yiming Yang in 2023?",
        "context": "Author: Yiming Yang Title: Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers Publication year: 2023 Coauthors: K. Choromanski, Shanda Li, Valerii Likhosherstov, Kumar Avinava Dubey, Shengjie Luo, Di He, Yiming Yang, Tam\u00e1s Sarl\u00f3s, Thomas Weingarten, Adrian Weller Abstract: We propose a new class of linear Transformers called FourierLearner-Transformers (FLTs), which incorporate a wide range of relative positional encoding mechanisms (RPEs). These include regular RPE techniques applied for nongeometric data, as well as novel RPEs operating on the sequences of tokens embedded in higher-dimensional Euclidean spaces (e.g. point clouds). FLTs construct the optimal RPE mechanism implicitly by learning its spectral representation. As opposed to other architectures combining efficient low-rank linear attention with RPEs, FLTs remain practical in terms of their memory usage and do not require additional assumptions about the structure of the RPE-mask. FLTs allow also for applying certain structural inductive bias techniques to specify masking strategies, e.g. they provide a way to learn the so-called local RPEs introduced in this paper and providing accuracy gains as compared with several other linear Transformers for language modeling. We also thoroughly tested FLTs on other data modalities and tasks, such as: image classification and 3D molecular modeling. For 3D-data FLTs are, to the best of our knowledge, the first Transformers architectures providing RPE-enhanced linear attention.",
        "retrieved_doc": "Author: Yiming Yang Title: Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers Publication year: 2023 Coauthors: K. Choromanski, Shanda Li, Valerii Likhosherstov, Kumar Avinava Dubey, Shengjie Luo, Di He, Yiming Yang, Tam\u00e1s Sarl\u00f3s, Thomas Weingarten, Adrian Weller Abstract: We propose a new class of linear Transformers called FourierLearner-Transformers (FLTs), which incorporate a wide range of relative positional encoding mechanisms (RPEs). These include regular RPE techniques applied for nongeometric data, as well as novel RPEs operating on the sequences of tokens embedded in higher-dimensional Euclidean spaces (e.g. point clouds). FLTs construct the optimal RPE mechanism implicitly by learning its spectral representation. As opposed to other architectures combining efficient low-rank linear attention with RPEs, FLTs remain practical in terms of their memory usage and do not require additional assumptions about the structure of the RPE-mask. FLTs allow also for applying certain structural inductive bias techniques to specify masking strategies, e.g. they provide a way to learn the so-called local RPEs introduced in this paper and providing accuracy gains as compared with several other linear Transformers for language modeling. We also thoroughly tested FLTs on other data modalities and tasks, such as: image classification and 3D molecular modeling. For 3D-data FLTs are, to the best of our knowledge, the first Transformers architectures providing RPE-enhanced linear attention.|Author: Yiming Yang Title: Functional Interpolation for Relative Positions Improves Long Context Transformers Publication year: 2023 Coauthors: Shanda Li, Chong You, Guru Guruganesh, J. Ainslie, Santiago Ontanon, M. Zaheer, Sumit K. Sanghai, Yiming Yang, Sanjiv Kumar, Srinadh Bhojanapalli Abstract: Preventing the performance decay of Transformers on inputs longer than those used for training has been an important challenge in extending the context length of these models. Though the Transformer architecture has fundamentally no limits on the input sequence lengths it can process, the choice of position encoding used during training can limit the performance of these models on longer inputs. We propose a novel functional relative position encoding with progressive interpolation, FIRE, to improve Transformer generalization to longer contexts. We theoretically prove that this can represent some of the popular relative position encodings, such as T5's RPE, Alibi, and Kerple. We next empirically show that FIRE models have better generalization to longer contexts on both zero-shot language modeling and long text benchmarks.|Author: A. Rudnicky Title: Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings Publication year: 2023 Coauthors: Ta-Chung Chi, Ting-Han Fan, Li-Wei Chen, A. Rudnicky, P. Ramadge Abstract: The use of positional embeddings in transformer language models is widely accepted. However, recent research has called into question the necessity of such embeddings. We further extend this inquiry by demonstrating that a randomly initialized and frozen transformer language model, devoid of positional embeddings, inherently encodes strong positional information through the shrinkage of self-attention variance. To quantify this variance, we derive the underlying distribution of each step within a transformer layer. Through empirical validation using a fully pretrained model, we show that the variance shrinkage effect still persists after extensive gradient updates. Our findings serve to justify the decision to discard positional embeddings and thus facilitate more efficient pretraining of transformer language models.",
        "retrieved_relevant_part": true,
        "model_answer": "The paper proposes a new class of linear Transformers called FourierLearner-Transformers (FLTs) that incorporate a wide range of relative positional encoding mechanisms (RPEs)",
        "actual_answer": "We propose a new class of linear Transformers called FourierLearner-Transformers (FLTs), which incorporate a wide range of relative positional encoding mechanisms (RPEs). These include regular RPE techniques applied for nongeometric data, as well as novel RPEs operating on the sequences of tokens embedded in higher-dimensional Euclidean spaces (e.g. point clouds). FLTs construct the optimal RPE mechanism implicitly by learning its spectral representation. As opposed to other architectures combining efficient low-rank linear attention with RPEs, FLTs remain practical in terms of their memory usage and do not require additional assumptions about the structure of the RPE-mask. FLTs allow also for applying certain structural inductive bias techniques to specify masking strategies, e.g. they provide a way to learn the so-called local RPEs introduced in this paper and providing accuracy gains as compared with several other linear Transformers for language modeling. We also thoroughly tested FLTs on other data modalities and tasks, such as: image classification and 3D molecular modeling. For 3D-data FLTs are, to the best of our knowledge, the first Transformers architectures providing RPE-enhanced linear attention.",
        "exact_match": false,
        "f1_max_min_avg": [
            0.16,
            0.16,
            0.16
        ],
        "recall_max_min_avg": [
            0.09,
            0.09,
            0.09
        ]
    },
    {
        "question": "Who is the author of the LTI paper Unsupervised Dense Retrieval Training with Web Anchors published in 2023?",
        "context": "Author: Chenyan Xiong Title: Unsupervised Dense Retrieval Training with Web Anchors Publication year: 2023 Coauthors: Yiqing Xie, X. Liu, Chenyan Xiong Abstract: In this work, we present an unsupervised retrieval method with contrastive learning on web anchors. The anchor text describes the content that is referenced from the linked page. This shows similarities to search queries that aim to retrieve pertinent information from relevant documents. Based on their commonalities, we train an unsupervised dense retriever, Anchor-DR, with a contrastive learning task that matches the anchor text and the linked document. To filter out uninformative anchors (such as \"homepage\" or other functional anchors), we present a novel filtering technique to only select anchors that contain similar types of information as search queries. Experiments show that Anchor-DR outperforms state-of-the-art methods on unsupervised dense retrieval by a large margin (e.g., by 5.3% NDCG@10 on MSMARCO). The gain of our method is especially significant for search and question answering tasks. Our analysis further reveals that the pattern of anchor-document pairs is similar to that of search query-document pairs. Code available at https://github.com/Veronicium/AnchorDR.",
        "retrieved_doc": "Author: Chenyan Xiong Title: Unsupervised Dense Retrieval Training with Web Anchors Publication year: 2023 Coauthors: Yiqing Xie, X. Liu, Chenyan Xiong Abstract: In this work, we present an unsupervised retrieval method with contrastive learning on web anchors. The anchor text describes the content that is referenced from the linked page. This shows similarities to search queries that aim to retrieve pertinent information from relevant documents. Based on their commonalities, we train an unsupervised dense retriever, Anchor-DR, with a contrastive learning task that matches the anchor text and the linked document. To filter out uninformative anchors (such as \"homepage\" or other functional anchors), we present a novel filtering technique to only select anchors that contain similar types of information as search queries. Experiments show that Anchor-DR outperforms state-of-the-art methods on unsupervised dense retrieval by a large margin (e.g., by 5.3% NDCG@10 on MSMARCO). The gain of our method is especially significant for search and question answering tasks. Our analysis further reveals that the pattern of anchor-document pairs is similar to that of search query-document pairs. Code available at https://github.com/Veronicium/AnchorDR.|Author: Chenyan Xiong Title: Distributionally Robust Unsupervised Dense Retrieval Training on Web Graphs Publication year: 2023 Coauthors: Peixuan Han, Zhenghao Liu, Zhiyuan Liu, Chenyan Xiong Abstract: This paper introduces Web-DRO, an unsupervised dense retrieval model, which clusters documents based on web structures and reweights the groups during contrastive training. Specifically, we first leverage web graph links and contrastively train an embedding model for clustering anchor-document pairs. Then we use Group Distributional Robust Optimization to reweight different clusters of anchor-document pairs, which guides the model to assign more weights to the group with higher contrastive loss and pay more attention to the worst case during training. Our experiments on MS MARCO and BEIR show that our model, Web-DRO, significantly improves the retrieval effectiveness in unsupervised scenarios. A comparison of clustering techniques shows that training on the web graph combining URL information reaches optimal performance on clustering. Further analysis confirms that group weights are stable and valid, indicating consistent model preferences as well as effective up-weighting of valuable groups and down-weighting of uninformative ones. The code of this paper can be obtained from https://github.com/OpenMatch/Web-DRO.|Author: Jamie Callan Title: Building Retrieval Systems for the ClueWeb22-B Corpus Publication year: 2024 Coauthors: Harshit Mehrotra, Jamie Callan, Zhen Fan Abstract: The ClueWeb22 dataset containing nearly 10 billion documents was released in 2022 to support academic and industry research. The goal of this project was to build retrieval baselines for the English section of the\"super head\"part (category B) of this dataset. These baselines can then be used by the research community to compare their systems and also to generate data to train/evaluate new retrieval and ranking algorithms. The report covers sparse and dense first stage retrievals as well as neural rerankers that were implemented for this dataset. These systems are available as a service on a Carnegie Mellon University cluster.",
        "retrieved_relevant_part": true,
        "model_answer": "Chenyan Xiong",
        "actual_answer": "Chenyan Xiong",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Who is the author of the LTI paper Faith and Fate: Limits of Transformers on Compositionality published in 2023?",
        "context": "Author: S. Welleck Title: Faith and Fate: Limits of Transformers on Compositionality Publication year: 2023 Coauthors: Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jian, Bill Yuchen Lin, Peter West, Chandra Bhagavatula, Ronan Le Bras, Jena D. Hwang, Soumya Sanyal, S. Welleck, Xiang Ren, Allyson Ettinger, Za\u00efd Harchaoui, Yejin Choi Abstract: Transformer large language models (LLMs) have sparked admiration for their exceptional performance on tasks that demand intricate multi-step reasoning. Yet, these models simultaneously show failures on surprisingly trivial problems. This begs the question: Are these errors incidental, or do they signal more substantial limitations? In an attempt to demystify transformer LLMs, we investigate the limits of these models across three representative compositional tasks -- multi-digit multiplication, logic grid puzzles, and a classic dynamic programming problem. These tasks require breaking problems down into sub-steps and synthesizing these steps into a precise answer. We formulate compositional tasks as computation graphs to systematically quantify the level of complexity, and break down reasoning steps into intermediate sub-procedures. Our empirical findings suggest that transformer LLMs solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, without necessarily developing systematic problem-solving skills. To round off our empirical study, we provide theoretical arguments on abstract multi-step reasoning problems that highlight how autoregressive generations' performance can rapidly decay with\\,increased\\,task\\,complexity.",
        "retrieved_doc": "Author: S. Welleck Title: Faith and Fate: Limits of Transformers on Compositionality Publication year: 2023 Coauthors: Nouha Dziri, Ximing Lu, Melanie Sclar, Xiang Lorraine Li, Liwei Jian, Bill Yuchen Lin, Peter West, Chandra Bhagavatula, Ronan Le Bras, Jena D. Hwang, Soumya Sanyal, S. Welleck, Xiang Ren, Allyson Ettinger, Za\u00efd Harchaoui, Yejin Choi Abstract: Transformer large language models (LLMs) have sparked admiration for their exceptional performance on tasks that demand intricate multi-step reasoning. Yet, these models simultaneously show failures on surprisingly trivial problems. This begs the question: Are these errors incidental, or do they signal more substantial limitations? In an attempt to demystify transformer LLMs, we investigate the limits of these models across three representative compositional tasks -- multi-digit multiplication, logic grid puzzles, and a classic dynamic programming problem. These tasks require breaking problems down into sub-steps and synthesizing these steps into a precise answer. We formulate compositional tasks as computation graphs to systematically quantify the level of complexity, and break down reasoning steps into intermediate sub-procedures. Our empirical findings suggest that transformer LLMs solve compositional tasks by reducing multi-step compositional reasoning into linearized subgraph matching, without necessarily developing systematic problem-solving skills. To round off our empirical study, we provide theoretical arguments on abstract multi-step reasoning problems that highlight how autoregressive generations' performance can rapidly decay with\\,increased\\,task\\,complexity.|Author: Yiming Yang Title: Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers Publication year: 2023 Coauthors: K. Choromanski, Shanda Li, Valerii Likhosherstov, Kumar Avinava Dubey, Shengjie Luo, Di He, Yiming Yang, Tam\u00e1s Sarl\u00f3s, Thomas Weingarten, Adrian Weller Abstract: We propose a new class of linear Transformers called FourierLearner-Transformers (FLTs), which incorporate a wide range of relative positional encoding mechanisms (RPEs). These include regular RPE techniques applied for nongeometric data, as well as novel RPEs operating on the sequences of tokens embedded in higher-dimensional Euclidean spaces (e.g. point clouds). FLTs construct the optimal RPE mechanism implicitly by learning its spectral representation. As opposed to other architectures combining efficient low-rank linear attention with RPEs, FLTs remain practical in terms of their memory usage and do not require additional assumptions about the structure of the RPE-mask. FLTs allow also for applying certain structural inductive bias techniques to specify masking strategies, e.g. they provide a way to learn the so-called local RPEs introduced in this paper and providing accuracy gains as compared with several other linear Transformers for language modeling. We also thoroughly tested FLTs on other data modalities and tasks, such as: image classification and 3D molecular modeling. For 3D-data FLTs are, to the best of our knowledge, the first Transformers architectures providing RPE-enhanced linear attention.|Author: Mona T. Diab Title: The Troubling Emergence of Hallucination in Large Language Models - An Extensive Definition, Quantification, and Prescriptive Remediations Publication year: 2023 Coauthors: Vipula Rawte, Swagata Chakraborty, Agnibh Pathak, Anubhav Sarkar, S.M. Towhidul Islam Tonmoy, Islam Tonmoy, Aman Chadha, Amit P. Sheth, Amitava Das, Paris, A. Sridhar, Erik Visser, Improved, Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, Yunfeng Liu, Roformer, Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori Hashimoto, Stanford, Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtow-icz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, Susan Zhang, Stephen Roller, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher De-wan, Mona T. Diab, Xi Xian Li, Todor Victoria Lin, Myle Ott, Kurt Shuster, Punit Daniel Simig, Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer. 2022, Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom Brown, Alec Radford, Dario Amodei, Paul F. Chris-tiano Abstract: The recent advancements in Large Language Models (LLMs) have garnered widespread acclaim for their remarkable emerging capabilities. However, the issue of hallucination has parallelly emerged as a by-product, posing significant concerns. While some recent endeavors have been made to identify and mitigate different types of hallucination, there has been a limited emphasis on the nuanced categorization of hallucination and associated mitigation methods. To address this gap, we offer a fine-grained discourse on profiling hallucination based on its degree, orientation, and category, along with offering strategies for alleviation. As such, we define two overarching orientations of hallucination: (i) factual mirage (FM) and (ii) silver lining (SL). To provide a more comprehensive understanding, both orientations are further sub-categorized into intrinsic and extrinsic, with three degrees of severity - (i) mild, (ii) moderate, and (iii) alarming. We also meticulously categorize hallucination into six types: (i) acronym ambiguity, (ii) numeric nuisance, (iii) generated golem, (iv) virtual voice, (v) geographic erratum, and (vi) time wrap. Furthermore, we curate HallucInation eLiciTation (HILT), a publicly available dataset comprising of 75,000 samples generated using 15 contemporary LLMs along with human annotations for the aforementioned categories. Finally, to establish a method for quantifying and to offer a comparative spectrum that allows us to evaluate and rank LLMs based on their vulnerability to producing hallucinations, we propose Hallucination Vulnerability Index (HVI). We firmly believe that HVI holds significant value as a tool for the wider NLP community, with the potential to serve as a rubric in AI-related policy-making. In conclusion, we propose two solution strategies for mitigating hallucinations.",
        "retrieved_relevant_part": true,
        "model_answer": "The author of the LTI paper Faith and Fate: Limits of Transformers on Compositionality is Mona T. Diab",
        "actual_answer": "S. Welleck",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "Which LTI faculty published the paper Active Retrieval Augmented Generation in 2023?",
        "context": "Author: Graham Neubig Title: Active Retrieval Augmented Generation Publication year: 2023 Coauthors: Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig Abstract: Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.",
        "retrieved_doc": "Author: Jamie Callan Title: Active Retrieval Augmented Generation Publication year: 2023 Coauthors: Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig Abstract: Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.|Author: Graham Neubig Title: Active Retrieval Augmented Generation Publication year: 2023 Coauthors: Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig Abstract: Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.|Author: Yiming Yang Title: Active Retrieval Augmented Generation Publication year: 2023 Coauthors: Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig Abstract: Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.",
        "retrieved_relevant_part": true,
        "model_answer": "Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, and Graham Neubig",
        "actual_answer": "Graham Neubig",
        "exact_match": false,
        "f1_max_min_avg": [
            0.18,
            0.18,
            0.18
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Who are the coauthors of the paper The DARPA Wikidata Overlay: Wikidata as an ontology for natural language processing published by LTI faculty A. Gershman in 2023?",
        "context": "Author: A. Gershman Title: The DARPA Wikidata Overlay: Wikidata as an ontology for natural language processing Publication year: 2023 Coauthors: Elizabeth Spaulding, Kathryn Conger, A. Gershman, Rosario Uceda-Sosa, S. Brown, James Pustejovsky, Peter Anick, Martha Palmer Abstract: With 102,530,067 items currently in its crowd-sourced knowledge base, Wikidata provides NLP practitioners a unique and powerful resource for inference and reasoning over real-world entities. However, because Wikidata is very entity focused, events and actions are often labeled with eventive nouns (e.g., the process of diagnosing a person\u2019s illness is labeled \u201cdiagnosis\u201d), and the typical participants in an event are not described or linked to that event concept (e.g., the medical professional or patient). Motivated by a need for an adaptable, comprehensive, domain-flexible ontology for information extraction, including identifying the roles entities are playing in an event, we present a curated subset of Wikidata in which events have been enriched with PropBank roles. To enable richer narrative understanding between events from Wikidata concepts, we have also provided a comprehensive mapping from temporal Qnodes and Pnodes to the Allen Interval Temporal Logic relations.",
        "retrieved_doc": "Author: A. Gershman Title: The DARPA Wikidata Overlay: Wikidata as an ontology for natural language processing Publication year: 2023 Coauthors: Elizabeth Spaulding, Kathryn Conger, A. Gershman, Rosario Uceda-Sosa, S. Brown, James Pustejovsky, Peter Anick, Martha Palmer Abstract: With 102,530,067 items currently in its crowd-sourced knowledge base, Wikidata provides NLP practitioners a unique and powerful resource for inference and reasoning over real-world entities. However, because Wikidata is very entity focused, events and actions are often labeled with eventive nouns (e.g., the process of diagnosing a person\u2019s illness is labeled \u201cdiagnosis\u201d), and the typical participants in an event are not described or linked to that event concept (e.g., the medical professional or patient). Motivated by a need for an adaptable, comprehensive, domain-flexible ontology for information extraction, including identifying the roles entities are playing in an event, we present a curated subset of Wikidata in which events have been enriched with PropBank roles. To enable richer narrative understanding between events from Wikidata concepts, we have also provided a comprehensive mapping from temporal Qnodes and Pnodes to the Allen Interval Temporal Logic relations.|LTI Ph.D. Graduate Student Handbook Page 10 This handbook applies to the LTI Ph.D. The Ph.D. in Language and Information Technologies (LTI Ph.D. ) is focused on understanding and extending the state of the art in computational linguistics, natural language processing, dialogue systems, information retrieval, machine translation, speech processing, video understanding, multimodal systems, automated reasoning, and other topics related to analysis and understanding of unstructured information (e.g., machine learning, and software engineering of intelligent systems). 1.2 Department Personnel The people responsible for administering the LTI Ph.D. degree are: Jamie Callan Ph.D. Program Director Professor GHC 5419 callan@cs.cmu.edu 412-268-4525 Stacey Young Ph.D. Academic Program Manager LTI Graduate Program Manager GHC 6415 staceyy@cs.cmu.edu 412-268-2623 Mona Diab LTI Director Professor GHC 5723 mdiab@andrew.cmu.edu 412-268-3669 Joan Axelson Office Manager GHC 5405 jaxelson@andrew.cmu.edu 412-268-7517 Julie Nys Employment Processes Manger GHC 5405 jnys@andrew.cmu.edu 412-268-3515 1.3 University Policies and Expectations It is the responsibility of each member of the Carnegie Mellon community to be familiar with university policies and guidelines. In addition to this departmental graduate student handbook, the following resources are available to assist you in understanding community expectations. \u2022 The Word/Student Handbook: https://www.cmu.edu/student-a ffairs/theword//index.html \u2022 Academic Integrity Policy: https:// www.cmu.edu/policies/student-and-student-life/academic-integrity.html|Author: Alexander I. Rudnicky Title: Tartan: an LLM Driven SocialBot Publication year: 2023 Coauthors: Liangze Li, Ziyuan Liu, Li-Wei Chen, Ta-Chung Chi, Alexander I. Rudnicky Abstract: We describe our work to date on using large language models (LLMs) in our Alexa Prize Socialbot. Our work has laid the groundwork for looking more closely at using LLMs in a conversational system. We also analyze common patterns in conversations our bot has had with users",
        "retrieved_relevant_part": true,
        "model_answer": "Elizabeth Spaulding, Kathryn Conger, Rosario Uceda-Sosa, S. Brown, James Pustejovsky, Peter Anick, Martha Palmer",
        "actual_answer": "Elizabeth Spaulding, Kathryn Conger, A. Gershman, Rosario Uceda-Sosa, S. Brown, James Pustejovsky, Peter Anick, Martha Palmer",
        "exact_match": false,
        "f1_max_min_avg": [
            0.93,
            0.93,
            0.93
        ],
        "recall_max_min_avg": [
            0.88,
            0.88,
            0.88
        ]
    },
    {
        "question": "What is the abstract of the paper Exploration of Efficient End-to-End ASR using Discretized Input from Self-Supervised Learning published by LTI faculty Shinji Watanabe in 2023?",
        "context": "Author: Shinji Watanabe Title: Exploration of Efficient End-to-End ASR using Discretized Input from Self-Supervised Learning Publication year: 2023 Coauthors: Xuankai Chang, Brian Yan, Yuya Fujita, Takashi Maekaku, Shinji Watanabe Abstract: Self-supervised learning (SSL) of speech has shown impressive results in speech-related tasks, particularly in automatic speech recognition (ASR). While most methods employ the output of intermediate layers of the SSL model as real-valued features for downstream tasks, there is potential in exploring alternative approaches that use discretized token sequences. This approach offers benefits such as lower storage requirements and the ability to apply techniques from natural language processing. In this paper, we propose a new protocol that utilizes discretized token sequences in ASR tasks, which includes de-duplication and sub-word modeling to enhance the input sequence. It reduces computational cost by decreasing the length of the sequence. Our experiments on the LibriSpeech dataset demonstrate that our proposed protocol performs competitively with conventional ASR systems using continuous input features, while reducing computational and storage costs.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Exploration of Efficient End-to-End ASR using Discretized Input from Self-Supervised Learning Publication year: 2023 Coauthors: Xuankai Chang, Brian Yan, Yuya Fujita, Takashi Maekaku, Shinji Watanabe Abstract: Self-supervised learning (SSL) of speech has shown impressive results in speech-related tasks, particularly in automatic speech recognition (ASR). While most methods employ the output of intermediate layers of the SSL model as real-valued features for downstream tasks, there is potential in exploring alternative approaches that use discretized token sequences. This approach offers benefits such as lower storage requirements and the ability to apply techniques from natural language processing. In this paper, we propose a new protocol that utilizes discretized token sequences in ASR tasks, which includes de-duplication and sub-word modeling to enhance the input sequence. It reduces computational cost by decreasing the length of the sequence. Our experiments on the LibriSpeech dataset demonstrate that our proposed protocol performs competitively with conventional ASR systems using continuous input features, while reducing computational and storage costs.|Author: Shinji Watanabe Title: Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study Publication year: 2023 Coauthors: Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma, Jiatong Shi, Jinchuan Tian, Shinji Watanabe, Yuya Fujita, Takashi Maekaku, Pengcheng Guo, Yao-Fei Cheng, Pavel Denisov, Kohei Saijo, Hsiu-Hsuan Wang Abstract: Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of discrete speech units derived from self-supervised learning representations, which significantly compresses the size of speech data. Applying various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech processing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good results in almost all the settings. We intend to release our configurations and trained models to foster future research efforts.|Author: Shinji Watanabe Title: Integrating Pretrained ASR and LM to Perform Sequence Generation for Spoken Language Understanding Publication year: 2023 Coauthors: Siddhant Arora, Hayato Futami, Yosuke Kashiwagi, E. Tsunoo, Brian Yan, Shinji Watanabe Abstract: There has been an increased interest in the integration of pretrained speech recognition (ASR) and language models (LM) into the SLU framework. However, prior methods often struggle with a vocabulary mismatch between pretrained models, and LM cannot be directly utilized as they diverge from its NLU formulation. In this study, we propose a three-pass end-to-end (E2E) SLU system that effectively integrates ASR and LM subnetworks into the SLU formulation for sequence generation tasks. In the first pass, our architecture predicts ASR transcripts using the ASR subnetwork. This is followed by the LM subnetwork, which makes an initial SLU prediction. Finally, in the third pass, the deliberation subnetwork conditions on representations from the ASR and LM subnetworks to make the final prediction. Our proposed three-pass SLU system shows improved performance over cascaded and E2E SLU models on two benchmark SLU datasets, SLURP and SLUE, especially on acoustically challenging utterances.",
        "retrieved_relevant_part": true,
        "model_answer": "The paper proposes a new protocol that uses discretized token sequences in ASR tasks, which reduces computational cost by decreasing the length of the sequence",
        "actual_answer": "Self-supervised learning (SSL) of speech has shown impressive results in speech-related tasks, particularly in automatic speech recognition (ASR). While most methods employ the output of intermediate layers of the SSL model as real-valued features for downstream tasks, there is potential in exploring alternative approaches that use discretized token sequences. This approach offers benefits such as lower storage requirements and the ability to apply techniques from natural language processing. In this paper, we propose a new protocol that utilizes discretized token sequences in ASR tasks, which includes de-duplication and sub-word modeling to enhance the input sequence. It reduces computational cost by decreasing the length of the sequence. Our experiments on the LibriSpeech dataset demonstrate that our proposed protocol performs competitively with conventional ASR systems using continuous input features, while reducing computational and storage costs.",
        "exact_match": false,
        "f1_max_min_avg": [
            0.24,
            0.24,
            0.24
        ],
        "recall_max_min_avg": [
            0.14,
            0.14,
            0.14
        ]
    },
    {
        "question": "What is the abstract of the paper Data-efficient Active Learning for Structured Prediction with Partial Annotation and Self-Training published by LTI faculty Emma Strubell in 2023?",
        "context": "Author: Emma Strubell Title: Data-efficient Active Learning for Structured Prediction with Partial Annotation and Self-Training Publication year: 2023 Coauthors: Zhisong Zhang, Emma Strubell, E. Hovy Abstract: In this work we propose a pragmatic method that reduces the annotation cost for structured label spaces using active learning. Our approach leverages partial annotation, which reduces labeling costs for structured outputs by selecting only the most informative sub-structures for annotation. We also utilize self-training to incorporate the current model's automatic predictions as pseudo-labels for un-annotated sub-structures. A key challenge in effectively combining partial annotation with self-training to reduce annotation cost is determining which sub-structures to select to label. To address this challenge, we adopt an error estimator to adaptively decide the partial selection ratio according to the current model's capability. In evaluations spanning four structured prediction tasks, we show that our combination of partial annotation and self-training using an adaptive selection ratio reduces annotation cost over strong full annotation baselines under a fair comparison scheme that takes reading time into consideration.",
        "retrieved_doc": "Author: Emma Strubell Title: Data-efficient Active Learning for Structured Prediction with Partial Annotation and Self-Training Publication year: 2023 Coauthors: Zhisong Zhang, Emma Strubell, E. Hovy Abstract: In this work we propose a pragmatic method that reduces the annotation cost for structured label spaces using active learning. Our approach leverages partial annotation, which reduces labeling costs for structured outputs by selecting only the most informative sub-structures for annotation. We also utilize self-training to incorporate the current model's automatic predictions as pseudo-labels for un-annotated sub-structures. A key challenge in effectively combining partial annotation with self-training to reduce annotation cost is determining which sub-structures to select to label. To address this challenge, we adopt an error estimator to adaptively decide the partial selection ratio according to the current model's capability. In evaluations spanning four structured prediction tasks, we show that our combination of partial annotation and self-training using an adaptive selection ratio reduces annotation cost over strong full annotation baselines under a fair comparison scheme that takes reading time into consideration.|Author: Emma Strubell Title: On the Interactions of Structural Constraints and Data Resources for Structured Prediction Publication year: 2023 Coauthors: Zhisong Zhang, Emma Strubell, E. Hovy Abstract: In this work, we provide an analysis on the interactions of the effectiveness of decoding with structural constraints and the amount of available training data for structured prediction tasks in NLP. Our exploration adopts a simple protocol that enforces constraints upon constraint-agnostic local models at testing time. With evaluations on three typical structured prediction tasks (named entity recognition, dependency parsing, and event argument extraction), we find that models trained with less data predict outputs with more structural violations in greedy decoding mode. Incorporating constraints provides consistent performance improvements and such benefits are larger in lower resource scenarios. Moreover, there are similar patterns with regard to the model sizes and more efficient models tend to enjoy more benefits. Finally, we also investigate settings with genre transfer and discover patterns that are related to domain discrepancies.|Author: Emma Strubell Title: Making Scalable Meta Learning Practical Publication year: 2023 Coauthors: Sang Keun Choe, Sanket Vaibhav Mehta, Hwijeen Ahn, W. Neiswanger, Pengtao Xie, Emma Strubell, Eric P. Xing Abstract: Despite its flexibility to learn diverse inductive biases in machine learning programs, meta learning (i.e., learning to learn) has long been recognized to suffer from poor scalability due to its tremendous compute/memory costs, training instability, and a lack of efficient distributed training support. In this work, we focus on making scalable meta learning practical by introducing SAMA, which combines advances in both implicit differentiation algorithms and systems. Specifically, SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients. Evaluated on multiple large-scale meta learning benchmarks, SAMA showcases up to 1.7/4.8x increase in throughput and 2.0/3.8x decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms. Furthermore, we show that SAMA-based data optimization leads to consistent improvements in text classification accuracy with BERT and RoBERTa large language models, and achieves state-of-the-art results in both small- and large-scale data pruning on image classification tasks, demonstrating the practical applicability of scalable meta learning across language and vision domains.",
        "retrieved_relevant_part": true,
        "model_answer": "The paper proposes a method for reducing annotation cost for structured label spaces using active learning and partial annotation",
        "actual_answer": "In this work we propose a pragmatic method that reduces the annotation cost for structured label spaces using active learning. Our approach leverages partial annotation, which reduces labeling costs for structured outputs by selecting only the most informative sub-structures for annotation. We also utilize self-training to incorporate the current model's automatic predictions as pseudo-labels for un-annotated sub-structures. A key challenge in effectively combining partial annotation with self-training to reduce annotation cost is determining which sub-structures to select to label. To address this challenge, we adopt an error estimator to adaptively decide the partial selection ratio according to the current model's capability. In evaluations spanning four structured prediction tasks, we show that our combination of partial annotation and self-training using an adaptive selection ratio reduces annotation cost over strong full annotation baselines under a fair comparison scheme that takes reading time into consideration.",
        "exact_match": false,
        "f1_max_min_avg": [
            0.15,
            0.15,
            0.15
        ],
        "recall_max_min_avg": [
            0.08,
            0.08,
            0.08
        ]
    },
    {
        "question": "Which LTI faculty published the paper Improving Massively Multilingual ASR with Auxiliary CTC Objectives in 2023?",
        "context": "Author: Shinji Watanabe Title: Improving Massively Multilingual ASR with Auxiliary CTC Objectives Publication year: 2023 Coauthors: William Chen, Brian Yan, Jiatong Shi, Yifan Peng, Soumi Maiti, Shinji Watanabe Abstract: Multilingual Automatic Speech Recognition (ASR) models have extended the usability of speech technologies to a wide variety of languages. With how many languages these models have to handle, however, a key to understanding their imbalanced performance across different languages is to examine if the model actually knows which language it should transcribe. In this paper, we introduce our work on improving performance on FLEURS, a 102-language open ASR benchmark, by conditioning the entire model on language identity (LID). We investigate techniques inspired from recent Connectionist Temporal Classification (CTC) studies to help the model handle the large number of languages, conditioning on the LID predictions of auxiliary tasks. Our experimental results demonstrate the effectiveness of our technique over standard CTC/Attention-based hybrid models. Furthermore, our state-of-the-art systems using self-supervised models with the Conformer architecture improve over the results of prior work on FLEURS by a relative 28.4% CER. Trained models are reproducible recipes are available at https://github.com/espnet/espnet/tree/master/egs2/fleurs/asr1.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Improving Massively Multilingual ASR with Auxiliary CTC Objectives Publication year: 2023 Coauthors: William Chen, Brian Yan, Jiatong Shi, Yifan Peng, Soumi Maiti, Shinji Watanabe Abstract: Multilingual Automatic Speech Recognition (ASR) models have extended the usability of speech technologies to a wide variety of languages. With how many languages these models have to handle, however, a key to understanding their imbalanced performance across different languages is to examine if the model actually knows which language it should transcribe. In this paper, we introduce our work on improving performance on FLEURS, a 102-language open ASR benchmark, by conditioning the entire model on language identity (LID). We investigate techniques inspired from recent Connectionist Temporal Classification (CTC) studies to help the model handle the large number of languages, conditioning on the LID predictions of auxiliary tasks. Our experimental results demonstrate the effectiveness of our technique over standard CTC/Attention-based hybrid models. Furthermore, our state-of-the-art systems using self-supervised models with the Conformer architecture improve over the results of prior work on FLEURS by a relative 28.4% CER. Trained models are reproducible recipes are available at https://github.com/espnet/espnet/tree/master/egs2/fleurs/asr1.|Author: Shinji Watanabe Title: Integrating Pretrained ASR and LM to Perform Sequence Generation for Spoken Language Understanding Publication year: 2023 Coauthors: Siddhant Arora, Hayato Futami, Yosuke Kashiwagi, E. Tsunoo, Brian Yan, Shinji Watanabe Abstract: There has been an increased interest in the integration of pretrained speech recognition (ASR) and language models (LM) into the SLU framework. However, prior methods often struggle with a vocabulary mismatch between pretrained models, and LM cannot be directly utilized as they diverge from its NLU formulation. In this study, we propose a three-pass end-to-end (E2E) SLU system that effectively integrates ASR and LM subnetworks into the SLU formulation for sequence generation tasks. In the first pass, our architecture predicts ASR transcripts using the ASR subnetwork. This is followed by the LM subnetwork, which makes an initial SLU prediction. Finally, in the third pass, the deliberation subnetwork conditions on representations from the ASR and LM subnetworks to make the final prediction. Our proposed three-pass SLU system shows improved performance over cascaded and E2E SLU models on two benchmark SLU datasets, SLURP and SLUE, especially on acoustically challenging utterances.|Author: Shinji Watanabe Title: Decoder-only Architecture for Speech Recognition with CTC Prompts and Text Data Augmentation Publication year: 2023 Coauthors: E. Tsunoo, Hayato Futami, Yosuke Kashiwagi, Siddhant Arora, Shinji Watanabe Abstract: Collecting audio-text pairs is expensive; however, it is much easier to access text-only data. Unless using shallow fusion, end-to-end automatic speech recognition (ASR) models require architecture modifications or additional training schemes to use text-only data. Inspired by recent advances in decoder-only language models (LMs), such as GPT-3 and PaLM adopted for speech-processing tasks, we propose using a decoder-only architecture for ASR with simple text augmentation. To provide audio information, encoder features compressed by CTC prediction are used as prompts for the decoder, which can be regarded as refining CTC prediction using the decoder-only model. Because the decoder architecture is the same as an autoregressive LM, it is simple to enhance the model by leveraging external text data with LM training. An experimental comparison using LibriSpeech and Switchboard shows that our proposed models with text augmentation training reduced word error rates from ordinary CTC by 0.3% and 1.4% on LibriSpeech test-clean and testother set, respectively, and 2.9% and 5.0% on Switchboard and CallHome. The proposed model had advantage on computational efficiency compared with conventional encoder-decoder ASR models with a similar parameter setup, and outperformed them on the LibriSpeech 100h and Switchboard training scenarios.",
        "retrieved_relevant_part": true,
        "model_answer": "Shinji Watanabe",
        "actual_answer": "Shinji Watanabe",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Who are the coauthors of the paper LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning published by LTI faculty Eric P. Xing in 2023?",
        "context": "Author: Eric P. Xing Title: LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning Publication year: 2023 Coauthors: Han Guo, P. Greengard, Eric P. Xing, Yoon Kim Abstract: We propose a simple approach for memory-efficient adaptation of pretrained language models. Our approach uses an iterative algorithm to decompose each pretrained matrix into a high-precision low-rank component and a memory-efficient quantized component. During finetuning, the quantized component remains fixed and only the low-rank component is updated. We present an integer linear programming formulation of the quantization component which enables dynamic configuration of quantization parameters (e.g., bit-width, block size) for each matrix given an overall target memory budget. We further explore a data-aware version of the algorithm which uses an approximation of the Fisher information matrix to weight the reconstruction objective during matrix decomposition. Experiments on finetuning RoBERTa and LLaMA-2 (7B and 70B) demonstrate that our low-rank plus quantized matrix decomposition approach (LQ-LoRA) outperforms strong QLoRA and GPTQ-LoRA baselines and enables aggressive quantization to sub-3 bits with only minor performance degradations. When finetuned on a language modeling calibration dataset, LQ-LoRA can also be used for model compression; in this setting our 2.75-bit LLaMA-2-70B model (which has 2.85 bits on average when including the low-rank components and requires 27GB of GPU memory) performs respectably compared to the 16-bit baseline.",
        "retrieved_doc": "Author: Eric P. Xing Title: LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning Publication year: 2023 Coauthors: Han Guo, P. Greengard, Eric P. Xing, Yoon Kim Abstract: We propose a simple approach for memory-efficient adaptation of pretrained language models. Our approach uses an iterative algorithm to decompose each pretrained matrix into a high-precision low-rank component and a memory-efficient quantized component. During finetuning, the quantized component remains fixed and only the low-rank component is updated. We present an integer linear programming formulation of the quantization component which enables dynamic configuration of quantization parameters (e.g., bit-width, block size) for each matrix given an overall target memory budget. We further explore a data-aware version of the algorithm which uses an approximation of the Fisher information matrix to weight the reconstruction objective during matrix decomposition. Experiments on finetuning RoBERTa and LLaMA-2 (7B and 70B) demonstrate that our low-rank plus quantized matrix decomposition approach (LQ-LoRA) outperforms strong QLoRA and GPTQ-LoRA baselines and enables aggressive quantization to sub-3 bits with only minor performance degradations. When finetuned on a language modeling calibration dataset, LQ-LoRA can also be used for model compression; in this setting our 2.75-bit LLaMA-2-70B model (which has 2.85 bits on average when including the low-rank components and requires 27GB of GPU memory) performs respectably compared to the 16-bit baseline.|Author: Eric P. Xing Title: LQ-LoRA: Low-rank Plus Quantized Matrix Decomposition for Efficient Language Model Finetuning Publication year: 2023 Coauthors: Han Guo, P. Greengard, Eric P. Xing, Yoon Kim Abstract: We propose a simple approach for memory-efficient adaptation of pretrained language models. Our approach uses an iterative algorithm to decompose each pretrained matrix into a high-precision low-rank component and a memory-efficient quantized component. During finetuning, the quantized component remains fixed and only the low-rank component is updated. We present an integer linear programming formulation of the quantization component which enables dynamic configuration of quantization parameters (e.g., bit-width, block size) for each matrix given an overall target memory budget. We further explore a data-aware version of the algorithm which uses an approximation of the Fisher information matrix to weight the reconstruction objective during matrix decomposition. Experiments on finetuning RoBERTa and LLaMA-2 (7B and 70B) demonstrate that our low-rank plus quantized matrix decomposition approach (LQ-LoRA) outperforms strong QLoRA and GPTQ-LoRA baselines and enables aggressive quantization to sub-3 bits with only minor performance degradations. When finetuned on a language modeling calibration dataset, LQ-LoRA can also be used for model compression; in this setting our 2.75-bit LLaMA-2-70B model (which has 2.85 bits on average when including the low-rank components and requires 27GB of GPU memory) performs respectably compared to the 16-bit baseline.|Author: Eric P. Xing Title: LLM360: Towards Fully Transparent Open-Source LLMs Publication year: 2023 Coauthors: Zhengzhong Liu, Aurick Qiao, W. Neiswanger, Hongyi Wang, Bowen Tan, Tianhua Tao, Junbo Li, Yuqi Wang, Suqi Sun, Omkar Pangarkar, Richard Fan, Yi Gu, Victor Miller, Yonghao Zhuang, Guowei He, Haonan Li, Fajri Koto, Liping Tang, Nikhil Ranjan, Zhiqiang Shen, Xuguang Ren, Roberto Iriondo, Cun Mu, Zhiting Hu, Mark Schulze, Preslav Nakov, Timothy Baldwin, Eric P. Xing Abstract: The recent surge in open-source Large Language Models (LLMs), such as LLaMA, Falcon, and Mistral, provides diverse options for AI practitioners and researchers. However, most LLMs have only released partial artifacts, such as the final model weights or inference code, and technical reports increasingly limit their scope to high-level design choices and surface statistics. These choices hinder progress in the field by degrading transparency into the training of LLMs and forcing teams to rediscover many details in the training process. We present LLM360, an initiative to fully open-source LLMs, which advocates for all training code and data, model checkpoints, and intermediate results to be made available to the community. The goal of LLM360 is to support open and collaborative AI research by making the end-to-end LLM training process transparent and reproducible by everyone. As a first step of LLM360, we release two 7B parameter LLMs pre-trained from scratch, Amber and CrystalCoder, including their training code, data, intermediate checkpoints, and analyses (at https://www.llm360.ai). We are committed to continually pushing the boundaries of LLMs through this open-source effort. More large-scale and stronger models are underway and will be released in the future.",
        "retrieved_relevant_part": true,
        "model_answer": "Han Guo, P. Greengard, Yoon Kim, Han Guo, P. Greengard, and Yoon Kim",
        "actual_answer": "Han Guo, P. Greengard, Eric P. Xing, Yoon Kim",
        "exact_match": false,
        "f1_max_min_avg": [
            0.55,
            0.55,
            0.55
        ],
        "recall_max_min_avg": [
            0.67,
            0.67,
            0.67
        ]
    },
    {
        "question": "Who are the coauthors of the paper A Self-enhancement Approach for Domain-specific Chatbot Training via Knowledge Mining and Digest published by LTI faculty Yiming Yang in 2023?",
        "context": "Author: Yiming Yang Title: A Self-enhancement Approach for Domain-specific Chatbot Training via Knowledge Mining and Digest Publication year: 2023 Coauthors: Ruohong Zhang, Luyu Gao, Chen Zheng, Zhen Fan, Guokun Lai, Zheng Zhang, Fangzhou Ai, Yiming Yang, Hongxia Yang Abstract: Large Language Models (LLMs), despite their great power in language generation, often encounter challenges when dealing with intricate and knowledge-demanding queries in specific domains. This paper introduces a novel approach to enhance LLMs by effectively extracting the relevant knowledge from domain-specific textual sources, and the adaptive training of a chatbot with domain-specific inquiries. Our two-step approach starts from training a knowledge miner, namely LLMiner, which autonomously extracts Question-Answer pairs from relevant documents through a chain-of-thought reasoning process. Subsequently, we blend the mined QA pairs with a conversational dataset to fine-tune the LLM as a chatbot, thereby enriching its domain-specific expertise and conversational capabilities. We also developed a new evaluation benchmark which comprises four domain-specific text corpora and associated human-crafted QA pairs for testing. Our model shows remarkable performance improvement over generally aligned LLM and surpasses domain-adapted models directly fine-tuned on domain corpus. In particular, LLMiner achieves this with minimal human intervention, requiring only 600 seed instances, thereby providing a pathway towards self-improvement of LLMs through model-synthesized training data.",
        "retrieved_doc": "Author: Yiming Yang Title: A Self-enhancement Approach for Domain-specific Chatbot Training via Knowledge Mining and Digest Publication year: 2023 Coauthors: Ruohong Zhang, Luyu Gao, Chen Zheng, Zhen Fan, Guokun Lai, Zheng Zhang, Fangzhou Ai, Yiming Yang, Hongxia Yang Abstract: Large Language Models (LLMs), despite their great power in language generation, often encounter challenges when dealing with intricate and knowledge-demanding queries in specific domains. This paper introduces a novel approach to enhance LLMs by effectively extracting the relevant knowledge from domain-specific textual sources, and the adaptive training of a chatbot with domain-specific inquiries. Our two-step approach starts from training a knowledge miner, namely LLMiner, which autonomously extracts Question-Answer pairs from relevant documents through a chain-of-thought reasoning process. Subsequently, we blend the mined QA pairs with a conversational dataset to fine-tune the LLM as a chatbot, thereby enriching its domain-specific expertise and conversational capabilities. We also developed a new evaluation benchmark which comprises four domain-specific text corpora and associated human-crafted QA pairs for testing. Our model shows remarkable performance improvement over generally aligned LLM and surpasses domain-adapted models directly fine-tuned on domain corpus. In particular, LLMiner achieves this with minimal human intervention, requiring only 600 seed instances, thereby providing a pathway towards self-improvement of LLMs through model-synthesized training data.|Author: Yiming Yang Title: Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision Publication year: 2023 Coauthors: Zhiqing Sun, Yikang Shen, Qinhong Zhou, Hongxin Zhang, Zhenfang Chen, David D. Cox, Yiming Yang, Chuang Gan Abstract: Recent AI-assistant agents, such as ChatGPT, predominantly rely on supervised fine-tuning (SFT) with human annotations and reinforcement learning from human feedback (RLHF) to align the output of large language models (LLMs) with human intentions, ensuring they are helpful, ethical, and reliable. However, this dependence can significantly constrain the true potential of AI-assistant agents due to the high cost of obtaining human supervision and the related issues on quality, reliability, diversity, self-consistency, and undesirable biases. To address these challenges, we propose a novel approach called SELF-ALIGN, which combines principle-driven reasoning and the generative power of LLMs for the self-alignment of AI agents with minimal human supervision. Our approach encompasses four stages: first, we use an LLM to generate synthetic prompts, and a topic-guided method to augment the prompt diversity; second, we use a small set of human-written principles for AI models to follow, and guide the LLM through in-context learning from demonstrations (of principles application) to produce helpful, ethical, and reliable responses to user's queries; third, we fine-tune the original LLM with the high-quality self-aligned responses so that the resulting model can generate desirable responses for each query directly without the principle set and the demonstrations anymore; and finally, we offer a refinement step to address the issues of overly-brief or indirect responses. Applying SELF-ALIGN to the LLaMA-65b base language model, we develop an AI assistant named Dromedary. With fewer than 300 lines of human annotations (including<200 seed prompts, 16 generic principles, and 5 exemplars for in-context learning). Dromedary significantly surpasses the performance of several state-of-the-art AI systems, including Text-Davinci-003 and Alpaca, on benchmark datasets with various settings.|Author: Alexander I. Rudnicky Title: Tartan: an LLM Driven SocialBot Publication year: 2023 Coauthors: Liangze Li, Ziyuan Liu, Li-Wei Chen, Ta-Chung Chi, Alexander I. Rudnicky Abstract: We describe our work to date on using large language models (LLMs) in our Alexa Prize Socialbot. Our work has laid the groundwork for looking more closely at using LLMs in a conversational system. We also analyze common patterns in conversations our bot has had with users",
        "retrieved_relevant_part": true,
        "model_answer": "The coauthors of the paper are Ruohong Zhang, Luyu Gao, Chen Zheng, Zhen Fan, Guokun Lai, Zheng Zhang, Fangzhou Ai, and Hongxia Yang",
        "actual_answer": "Ruohong Zhang, Luyu Gao, Chen Zheng, Zhen Fan, Guokun Lai, Zheng Zhang, Fangzhou Ai, Yiming Yang, Hongxia Yang",
        "exact_match": false,
        "f1_max_min_avg": [
            0.73,
            0.73,
            0.73
        ],
        "recall_max_min_avg": [
            0.83,
            0.83,
            0.83
        ]
    },
    {
        "question": "Which LTI faculty published the paper Distributionally Robust Unsupervised Dense Retrieval Training on Web Graphs in 2023?",
        "context": "Author: Chenyan Xiong Title: Distributionally Robust Unsupervised Dense Retrieval Training on Web Graphs Publication year: 2023 Coauthors: Peixuan Han, Zhenghao Liu, Zhiyuan Liu, Chenyan Xiong Abstract: This paper introduces Web-DRO, an unsupervised dense retrieval model, which clusters documents based on web structures and reweights the groups during contrastive training. Specifically, we first leverage web graph links and contrastively train an embedding model for clustering anchor-document pairs. Then we use Group Distributional Robust Optimization to reweight different clusters of anchor-document pairs, which guides the model to assign more weights to the group with higher contrastive loss and pay more attention to the worst case during training. Our experiments on MS MARCO and BEIR show that our model, Web-DRO, significantly improves the retrieval effectiveness in unsupervised scenarios. A comparison of clustering techniques shows that training on the web graph combining URL information reaches optimal performance on clustering. Further analysis confirms that group weights are stable and valid, indicating consistent model preferences as well as effective up-weighting of valuable groups and down-weighting of uninformative ones. The code of this paper can be obtained from https://github.com/OpenMatch/Web-DRO.",
        "retrieved_doc": "Author: Chenyan Xiong Title: Distributionally Robust Unsupervised Dense Retrieval Training on Web Graphs Publication year: 2023 Coauthors: Peixuan Han, Zhenghao Liu, Zhiyuan Liu, Chenyan Xiong Abstract: This paper introduces Web-DRO, an unsupervised dense retrieval model, which clusters documents based on web structures and reweights the groups during contrastive training. Specifically, we first leverage web graph links and contrastively train an embedding model for clustering anchor-document pairs. Then we use Group Distributional Robust Optimization to reweight different clusters of anchor-document pairs, which guides the model to assign more weights to the group with higher contrastive loss and pay more attention to the worst case during training. Our experiments on MS MARCO and BEIR show that our model, Web-DRO, significantly improves the retrieval effectiveness in unsupervised scenarios. A comparison of clustering techniques shows that training on the web graph combining URL information reaches optimal performance on clustering. Further analysis confirms that group weights are stable and valid, indicating consistent model preferences as well as effective up-weighting of valuable groups and down-weighting of uninformative ones. The code of this paper can be obtained from https://github.com/OpenMatch/Web-DRO.|Author: Chenyan Xiong Title: Unsupervised Dense Retrieval Training with Web Anchors Publication year: 2023 Coauthors: Yiqing Xie, X. Liu, Chenyan Xiong Abstract: In this work, we present an unsupervised retrieval method with contrastive learning on web anchors. The anchor text describes the content that is referenced from the linked page. This shows similarities to search queries that aim to retrieve pertinent information from relevant documents. Based on their commonalities, we train an unsupervised dense retriever, Anchor-DR, with a contrastive learning task that matches the anchor text and the linked document. To filter out uninformative anchors (such as \"homepage\" or other functional anchors), we present a novel filtering technique to only select anchors that contain similar types of information as search queries. Experiments show that Anchor-DR outperforms state-of-the-art methods on unsupervised dense retrieval by a large margin (e.g., by 5.3% NDCG@10 on MSMARCO). The gain of our method is especially significant for search and question answering tasks. Our analysis further reveals that the pattern of anchor-document pairs is similar to that of search query-document pairs. Code available at https://github.com/Veronicium/AnchorDR.|Author: Chenyan Xiong Title: Structure-Aware Language Model Pretraining Improves Dense Retrieval on Structured Data Publication year: 2023 Coauthors: Xinze Li, Zhenghao Liu, Chenyan Xiong, Shi Yu, Yu Gu, Zhiyuan Liu, Ge Yu Abstract: This paper presents Structure Aware Dense Retrieval (SANTA) model, which encodes user queries and structured data in one universal embedding space for retrieving structured data. SANTA proposes two pretraining methods to make language models structure-aware and learn effective representations for structured data: 1) Structured Data Alignment, which utilizes the natural alignment relations between structured data and unstructured data for structure-aware pretraining. It contrastively trains language models to represent multi-modal text data and teaches models to distinguish matched structured data for unstructured texts. 2) Masked Entity Prediction, which designs an entity-oriented mask strategy and asks language models to fill in the masked entities. Our experiments show that SANTA achieves state-of-the-art on code search and product search and conducts convincing results in the zero-shot setting. SANTA learns tailored representations for multi-modal text data by aligning structured and unstructured data pairs and capturing structural semantics by masking and predicting entities in the structured data. All codes are available at https://github.com/OpenMatch/OpenMatch.",
        "retrieved_relevant_part": true,
        "model_answer": "Chenyan Xiong",
        "actual_answer": "Chenyan Xiong",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Who is the author of the LTI paper Multilingual TTS Accent Impressions for Accented ASR published in 2023?",
        "context": "Author: David R. Mortensen Title: Multilingual TTS Accent Impressions for Accented ASR Publication year: 2023 Coauthors: G. Karakasidis, Nathaniel R. Robinson, Yaroslav Getman, Atieno Ogayo, Ragheb Al-Ghezi, Ananya Ayasi, Shinji Watanabe, David R. Mortensen, M. Kurimo ",
        "retrieved_doc": "Author: Shinji Watanabe Title: Multilingual TTS Accent Impressions for Accented ASR Publication year: 2023 Coauthors: G. Karakasidis, Nathaniel R. Robinson, Yaroslav Getman, Atieno Ogayo, Ragheb Al-Ghezi, Ananya Ayasi, Shinji Watanabe, David R. Mortensen, M. Kurimo |Author: David R. Mortensen Title: Multilingual TTS Accent Impressions for Accented ASR Publication year: 2023 Coauthors: G. Karakasidis, Nathaniel R. Robinson, Yaroslav Getman, Atieno Ogayo, Ragheb Al-Ghezi, Ananya Ayasi, Shinji Watanabe, David R. Mortensen, M. Kurimo |Author: A. Waibel Title: SYNTACC : Synthesizing Multi-Accent Speech By Weight Factorization Publication year: 2023 Coauthors: Tuan-Nam Nguyen, Ngoc-Quan Pham, A. Waibel Abstract: Conventional multi-speaker text-to-speech synthesis (TTS) is known to be capable of synthesizing speech for multiple voices, yet it cannot generate speech in different accents. This limitation has motivated us to develop SYNTACC (Synthesizing speech with accents) which adapts conventional multi-speaker TTS to produce multi-accent speech. Our method uses the YourTTS model and involves a novel multi-accent training mechanism. The method works by decomposing each weight matrix into a shared component and an accent-dependent component, with the former being initialized by the pretrained multi-speaker TTS model and the latter being factorized into vectors using rank-1 matrices to reduce the number of training parameters per accent. This weight factorization method proves to be effective in fine-tuning the SYNTACC on multi-accent data sets in a low-resource condition. Our SYNTACC model eventually allows speech synthesis in not only different voices but also in different accents.",
        "retrieved_relevant_part": true,
        "model_answer": "Shinji Watanabe",
        "actual_answer": "David R. Mortensen",
        "exact_match": false,
        "f1_max_min_avg": [
            0,
            0,
            0.0
        ],
        "recall_max_min_avg": [
            0,
            0,
            0.0
        ]
    },
    {
        "question": "What is the abstract of the paper Transformed Protoform Reconstruction published by LTI faculty David R. Mortensen in 2023?",
        "context": "Author: David R. Mortensen Title: Transformed Protoform Reconstruction Publication year: 2023 Coauthors: Young Min Kim, Kalvin Chang, Chenxuan Cui, David R. Mortensen Abstract: Protoform reconstruction is the task of inferring what morphemes or words appeared like in the ancestral languages of a set of daughter languages. Meloni et al (2021) achieved the state-of-the-art on Latin protoform reconstruction with an RNN-based encoder-decoder with attention model. We update their model with the state-of-the-art seq2seq model: the Transformer. Our model outperforms their model on a suite of different metrics on two different datasets: their Romance data of 8,000 cognates spanning 5 languages and a Chinese dataset (Hou 2004) of 800+ cognates spanning 39 varieties. We also probe our model for potential phylogenetic signal contained in the model. Our code is publicly available at https://github.com/cmu-llab/acl-2023.",
        "retrieved_doc": "Author: David R. Mortensen Title: Transformed Protoform Reconstruction Publication year: 2023 Coauthors: Young Min Kim, Kalvin Chang, Chenxuan Cui, David R. Mortensen Abstract: Protoform reconstruction is the task of inferring what morphemes or words appeared like in the ancestral languages of a set of daughter languages. Meloni et al (2021) achieved the state-of-the-art on Latin protoform reconstruction with an RNN-based encoder-decoder with attention model. We update their model with the state-of-the-art seq2seq model: the Transformer. Our model outperforms their model on a suite of different metrics on two different datasets: their Romance data of 8,000 cognates spanning 5 languages and a Chinese dataset (Hou 2004) of 800+ cognates spanning 39 varieties. We also probe our model for potential phylogenetic signal contained in the model. Our code is publicly available at https://github.com/cmu-llab/acl-2023.|Author: Yonatan Bisk Title: Open X-Embodiment: Robotic Learning Datasets and RT-X Models Publication year: 2023 Coauthors: A. Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, A. Irpan, Alexander Khazatsky, Anant Rai, Anika Singh, Anthony Brohan, A. Raffin, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Sch\u00f6lkopf, Brian Ichter, Cewu Lu, Charles Xu, Chelsea Finn, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Chuer Pan, Chuyuan Fu, Coline Devin, Danny Driess, Deepak Pathak, Dhruv Shah, Dieter B\u00fcchler, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Federico Ceola, Fei Xia, F. Stulp, Gaoyue Zhou, G. Sukhatme, G. Salhotra, Ge Yan, Giulio Schiavi, G. Kahn, Hao Su, Haoshu Fang, Haochen Shi, H. B. Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jan Peters, Jan Schneider, Jasmine Hsu, J. Bohg, Jeff Bingham, Jiajun Wu, Jialin Wu, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jitendra Malik, Jonathan Tompson, Jonathan Yang, Joseph J. Lim, Jo\u00e3o Silv\u00e9rio, Junhyek Han, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, K. Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Zhang, Krishan Rana, K. Srinivasan, Lawrence Yunliang Chen, Lerrel Pinto, Liam Tan, Lionel Ott, Lisa Lee, Masayoshi Tomizuka, Maximilian Du, Michael Ahn, Mingtong Zhang, Mingyu Ding, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, N. Heess, Nikhil Joshi, Niko Suenderhauf, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, P. Sanketi, Paul Wohlhart, Peng Xu, P. Sermanet, Priya Sundaresan, Q. Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan C. Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham D. Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, S. Adebola, Simon Guist, Soroush Nasiriany, S. Schaal, Stefan Welker, Stephen Tian, Sudeep Dasari, Suneel Belkhale, Takayuki Osa, Tatsuya Harada, T. Matsushima, Ted Xiao, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Zhao, Travis Armstrong, Trevor Darrell, Vidhi Jain, Vincent Vanhoucke, W. Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xuanlin Li, Yao Lu, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yueh-hua Wu, Yujin Tang, Yuke Zhu, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zhuo Xu, Zichen Jeff Cui Abstract: \u2014Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer|Author: Yonatan Bisk Title: Open X-Embodiment: Robotic Learning Datasets and RT-X Models Publication year: 2023 Coauthors: A. Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, A. Irpan, Alexander Khazatsky, Anant Rai, Anika Singh, Anthony Brohan, A. Raffin, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Sch\u00f6lkopf, Brian Ichter, Cewu Lu, Charles Xu, Chelsea Finn, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Chuer Pan, Chuyuan Fu, Coline Devin, Danny Driess, Deepak Pathak, Dhruv Shah, Dieter B\u00fcchler, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Federico Ceola, Fei Xia, F. Stulp, Gaoyue Zhou, G. Sukhatme, G. Salhotra, Ge Yan, Giulio Schiavi, G. Kahn, Hao Su, Haoshu Fang, Haochen Shi, H. B. Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jan Peters, Jan Schneider, Jasmine Hsu, J. Bohg, Jeff Bingham, Jiajun Wu, Jialin Wu, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jitendra Malik, Jonathan Tompson, Jonathan Yang, Joseph J. Lim, Jo\u00e3o Silv\u00e9rio, Junhyek Han, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, K. Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Zhang, Krishan Rana, K. Srinivasan, Lawrence Yunliang Chen, Lerrel Pinto, Liam Tan, Lionel Ott, Lisa Lee, Masayoshi Tomizuka, Maximilian Du, Michael Ahn, Mingtong Zhang, Mingyu Ding, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, N. Heess, Nikhil Joshi, Niko Suenderhauf, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, P. Sanketi, Paul Wohlhart, Peng Xu, P. Sermanet, Priya Sundaresan, Q. Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan C. Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham D. Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, S. Adebola, Simon Guist, Soroush Nasiriany, S. Schaal, Stefan Welker, Stephen Tian, Sudeep Dasari, Suneel Belkhale, Takayuki Osa, Tatsuya Harada, T. Matsushima, Ted Xiao, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Zhao, Travis Armstrong, Trevor Darrell, Vidhi Jain, Vincent Vanhoucke, W. Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xuanlin Li, Yao Lu, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yueh-hua Wu, Yujin Tang, Yuke Zhu, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zhuo Xu, Zichen Jeff Cui Abstract: \u2014Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer",
        "retrieved_relevant_part": true,
        "model_answer": "The abstract of the paper Transformed Protoform Reconstruction published by LTI faculty David R. Mortensen in 2023 states that large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications",
        "actual_answer": "Protoform reconstruction is the task of inferring what morphemes or words appeared like in the ancestral languages of a set of daughter languages. Meloni et al (2021) achieved the state-of-the-art on Latin protoform reconstruction with an RNN-based encoder-decoder with attention model. We update their model with the state-of-the-art seq2seq model: the Transformer. Our model outperforms their model on a suite of different metrics on two different datasets: their Romance data of 8,000 cognates spanning 5 languages and a Chinese dataset (Hou 2004) of 800+ cognates spanning 39 varieties. We also probe our model for potential phylogenetic signal contained in the model. Our code is publicly available at https://github.com/cmu-llab/acl-2023.",
        "exact_match": false,
        "f1_max_min_avg": [
            0.07,
            0.07,
            0.07
        ],
        "recall_max_min_avg": [
            0.05,
            0.05,
            0.05
        ]
    },
    {
        "question": "Who is the author of the LTI paper Learning to Ask Questions for Zero-shot Dialogue State Tracking published in 2023?",
        "context": "Author: Alexander I. Rudnicky Title: Learning to Ask Questions for Zero-shot Dialogue State Tracking Publication year: 2023 Coauthors: Diogo Tavares, David Semedo, Alexander I. Rudnicky, Jo\u00e3o Magalh\u00e3es Abstract: We present a method for performing zero-shot Dialogue State Tracking (DST) by casting the task as a learning-to-ask-questions framework. The framework learns to pair the best question generation (QG) strategy with in-domain question answering (QA) methods to extract slot values from a dialogue without any human intervention. A novel self-supervised QA pretraining step using in-domain data is essential to learn the structure without requiring any slot-filling annotations. Moreover, we show that QG methods need to be aligned with the same grammatical person used in the dialogue. Empirical evaluation on the MultiWOZ 2.1 dataset demonstrates that our approach, when used alongside robust QA models, outperforms existing zero-shot methods in the challenging task of zero-shot cross domain adaptation-given a comparable amount of domain knowledge during data creation. Finally, we analyze the impact of the types of questions used, and demonstrate that the algorithmic approach outperforms template-based question generation.",
        "retrieved_doc": "Author: Alexander I. Rudnicky Title: Learning to Ask Questions for Zero-shot Dialogue State Tracking Publication year: 2023 Coauthors: Diogo Tavares, David Semedo, Alexander I. Rudnicky, Jo\u00e3o Magalh\u00e3es Abstract: We present a method for performing zero-shot Dialogue State Tracking (DST) by casting the task as a learning-to-ask-questions framework. The framework learns to pair the best question generation (QG) strategy with in-domain question answering (QA) methods to extract slot values from a dialogue without any human intervention. A novel self-supervised QA pretraining step using in-domain data is essential to learn the structure without requiring any slot-filling annotations. Moreover, we show that QG methods need to be aligned with the same grammatical person used in the dialogue. Empirical evaluation on the MultiWOZ 2.1 dataset demonstrates that our approach, when used alongside robust QA models, outperforms existing zero-shot methods in the challenging task of zero-shot cross domain adaptation-given a comparable amount of domain knowledge during data creation. Finally, we analyze the impact of the types of questions used, and demonstrate that the algorithmic approach outperforms template-based question generation.|Author: Yonatan Bisk Title: Plan, Eliminate, and Track - Language Models are Good Teachers for Embodied Agents Publication year: 2023 Coauthors: Yue Wu, So Yeon Min, Yonatan Bisk, R. Salakhutdinov, A. Azaria, Yuan-Fang Li, Tom M. Mitchell, Shrimai Prabhumoye Abstract: Pre-trained large language models (LLMs) capture procedural knowledge about the world. Recent work has leveraged LLM's ability to generate abstract plans to simplify challenging control tasks, either by action scoring, or action modeling (fine-tuning). However, the transformer architecture inherits several constraints that make it difficult for the LLM to directly serve as the agent: e.g. limited input lengths, fine-tuning inefficiency, bias from pre-training, and incompatibility with non-text environments. To maintain compatibility with a low-level trainable actor, we propose to instead use the knowledge in LLMs to simplify the control problem, rather than solving it. We propose the Plan, Eliminate, and Track (PET) framework. The Plan module translates a task description into a list of high-level sub-tasks. The Eliminate module masks out irrelevant objects and receptacles from the observation for the current sub-task. Finally, the Track module determines whether the agent has accomplished each sub-task. On the AlfWorld instruction following benchmark, the PET framework leads to a significant 15% improvement over SOTA for generalization to human goal specifications.|Author: Alexander H. Waibel Title: Incremental Learning of Humanoid Robot Behavior from Natural Interaction and Large Language Models Publication year: 2023 Coauthors: Leonard B\u00e4rmann, Rainer Kartmann, Fabian Peller-Konrad, Alexander H. Waibel, T. Asfour Abstract: Natural-language dialog is key for intuitive human-robot interaction. It can be used not only to express humans' intents, but also to communicate instructions for improvement if a robot does not understand a command correctly. Of great importance is to endow robots with the ability to learn from such interaction experience in an incremental way to allow them to improve their behaviors or avoid mistakes in the future. In this paper, we propose a system to achieve incremental learning of complex behavior from natural interaction, and demonstrate its implementation on a humanoid robot. Building on recent advances, we present a system that deploys Large Language Models (LLMs) for high-level orchestration of the robot's behavior, based on the idea of enabling the LLM to generate Python statements in an interactive console to invoke both robot perception and action. The interaction loop is closed by feeding back human instructions, environment observations, and execution results to the LLM, thus informing the generation of the next statement. Specifically, we introduce incremental prompt learning, which enables the system to interactively learn from its mistakes. For that purpose, the LLM can call another LLM responsible for code-level improvements of the current interaction based on human feedback. The improved interaction is then saved in the robot's memory, and thus retrieved on similar requests. We integrate the system in the robot cognitive architecture of the humanoid robot ARMAR-6 and evaluate our methods both quantitatively (in simulation) and qualitatively (in simulation and real-world) by demonstrating generalized incrementally-learned knowledge.",
        "retrieved_relevant_part": true,
        "model_answer": "Alexander I. Rudnicky",
        "actual_answer": "Alexander I. Rudnicky",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Who are the coauthors of the paper The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation published by LTI faculty Graham Neubig in 2023?",
        "context": "Author: Graham Neubig Title: The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation Publication year: 2023 Coauthors: Patrick Fernandes, Daniel Deutsch, M. Finkelstein, Parker Riley, Andr\u00e9 F. T. Martins, Graham Neubig, Ankush Garg, J. Clark, Markus Freitag, Orhan Firat Abstract: Automatic evaluation of machine translation (MT) is a critical tool driving the rapid iterative development of MT systems. While considerable progress has been made on estimating a single scalar quality score, current metrics lack the informativeness of more detailed schemes that annotate individual errors, such as Multidimensional Quality Metrics (MQM). In this paper, we help fill this gap by proposing AutoMQM, a prompting technique which leverages the reasoning and in-context learning capabilities of large language models (LLMs) and asks them to identify and categorize errors in translations. We start by evaluating recent LLMs, such as PaLM and PaLM-2, through simple score prediction prompting, and we study the impact of labeled data through in-context learning and finetuning. We then evaluate AutoMQM with PaLM-2 models, and we find that it improves performance compared to just prompting for scores (with particularly large gains for larger models) while providing interpretability through error spans that align with human annotations.",
        "retrieved_doc": "Author: Graham Neubig Title: The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation Publication year: 2023 Coauthors: Patrick Fernandes, Daniel Deutsch, M. Finkelstein, Parker Riley, Andr\u00e9 F. T. Martins, Graham Neubig, Ankush Garg, J. Clark, Markus Freitag, Orhan Firat Abstract: Automatic evaluation of machine translation (MT) is a critical tool driving the rapid iterative development of MT systems. While considerable progress has been made on estimating a single scalar quality score, current metrics lack the informativeness of more detailed schemes that annotate individual errors, such as Multidimensional Quality Metrics (MQM). In this paper, we help fill this gap by proposing AutoMQM, a prompting technique which leverages the reasoning and in-context learning capabilities of large language models (LLMs) and asks them to identify and categorize errors in translations. We start by evaluating recent LLMs, such as PaLM and PaLM-2, through simple score prediction prompting, and we study the impact of labeled data through in-context learning and finetuning. We then evaluate AutoMQM with PaLM-2 models, and we find that it improves performance compared to just prompting for scores (with particularly large gains for larger models) while providing interpretability through error spans that align with human annotations.|Author: Graham Neubig Title: Neural Machine Translation for the Indigenous Languages of the Americas: An Introduction Publication year: 2023 Coauthors: Manuel Mager, R. Bhatnagar, Graham Neubig, Ngoc Thang Vu, Katharina Kann Abstract: Neural models have drastically advanced state of the art for machine translation (MT) between high-resource languages. Traditionally, these models rely on large amounts of training data, but many language pairs lack these resources. However, an important part of the languages in the world do not have this amount of data. Most languages from the Americas are among them, having a limited amount of parallel and monolingual data, if any. Here, we present an introduction to the interested reader to the basic challenges, concepts, and techniques that involve the creation of MT systems for these languages. Finally, we discuss the recent advances and findings and open questions, product of an increased interest of the NLP community in these languages.|Author: Graham Neubig Title: ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages Publication year: 2023 Coauthors: Nathaniel R. Robinson, Perez Ogayo, David R. Mortensen, Graham Neubig Abstract: Large language models (LLMs) implicitly learn to perform a range of language tasks, including machine translation (MT). Previous studies explore aspects of LLMs\u2019 MT capabilities. However, there exist a wide variety of languages for which recent LLM MT performance has never before been evaluated. Without published experimental evidence on the matter, it is difficult for speakers of the world\u2019s diverse languages to know how and whether they can use LLMs for their languages. We present the first experimental evidence for an expansive set of 204 languages, along with MT cost analysis, using the FLORES-200 benchmark. Trends reveal that GPT models approach or exceed traditional MT model performance for some high-resource languages (HRLs) but consistently lag for low-resource languages (LRLs), under-performing traditional MT for 84.1% of languages we covered. Our analysis reveals that a language\u2019s resource level is the most important feature in determining ChatGPT\u2019s relative ability to translate it, and suggests that ChatGPT is especially disadvantaged for LRLs and African languages.",
        "retrieved_relevant_part": true,
        "model_answer": "Patrick Fernandes, Daniel Deutsch, M. Finkelstein, Parker Riley, Andr\u00e9 F. T. Martins, Ankush Garg, J. Clark, Markus Freitag, and Orhan Firat",
        "actual_answer": "Patrick Fernandes, Daniel Deutsch, M. Finkelstein, Parker Riley, Andr\u00e9 F. T. Martins, Graham Neubig, Ankush Garg, J. Clark, Markus Freitag, Orhan Firat",
        "exact_match": false,
        "f1_max_min_avg": [
            0.93,
            0.93,
            0.93
        ],
        "recall_max_min_avg": [
            0.91,
            0.91,
            0.91
        ]
    },
    {
        "question": "What is the abstract of the paper Multi-Channel Speaker Extraction with Adversarial Training: The Wavlab Submission to The Clarity ICASSP 2023 Grand Challenge published by LTI faculty Shinji Watanabe in 2023?",
        "context": "Author: Shinji Watanabe Title: Multi-Channel Speaker Extraction with Adversarial Training: The Wavlab Submission to The Clarity ICASSP 2023 Grand Challenge Publication year: 2023 Coauthors: Samuele Cornell, Zhongqiu Wang, Yoshiki Masuyama, Shinji Watanabe, Manuel Pariente, Nobutaka Ono, S. Squartini Abstract: In this work we detail our submission to the Clarity ICASSP 2023 grand challenge, in which participants have to develop a strong target speech enhancement system for hearing-aid (HA) devices in noisy-reverberant environments. Our system builds on our previous submission at the Second Clarity Enhancement Challenge (CEC2): iNeuBe-X, which consists in an iterative neural/conventional beamforming enhancement pipeline, guided by an enrollment utterance from the target speaker. This model, which won by a large margin the CEC2, is an extension of the state-of-the-art TF-GridNet model for multi-channel, streamable target-speaker speech enhancement. Here, this approach is extended and further improved by leveraging generative adversarial training, which we show proves especially useful when the training data is limited. Using only the official 6k training scenes data, our best model achieves 0.80 hearing-aid speech perception index (HASPI) and 0.41 hearing-aid speech quality index (HASQI) scores on the synthetic evaluation set. However, our model generalized poorly on the semi-real evaluation set. This highlights the fact that our community should focus more on real-world evaluation and less on fully synthetic datasets.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Multi-Channel Speaker Extraction with Adversarial Training: The Wavlab Submission to The Clarity ICASSP 2023 Grand Challenge Publication year: 2023 Coauthors: Samuele Cornell, Zhongqiu Wang, Yoshiki Masuyama, Shinji Watanabe, Manuel Pariente, Nobutaka Ono, S. Squartini Abstract: In this work we detail our submission to the Clarity ICASSP 2023 grand challenge, in which participants have to develop a strong target speech enhancement system for hearing-aid (HA) devices in noisy-reverberant environments. Our system builds on our previous submission at the Second Clarity Enhancement Challenge (CEC2): iNeuBe-X, which consists in an iterative neural/conventional beamforming enhancement pipeline, guided by an enrollment utterance from the target speaker. This model, which won by a large margin the CEC2, is an extension of the state-of-the-art TF-GridNet model for multi-channel, streamable target-speaker speech enhancement. Here, this approach is extended and further improved by leveraging generative adversarial training, which we show proves especially useful when the training data is limited. Using only the official 6k training scenes data, our best model achieves 0.80 hearing-aid speech perception index (HASPI) and 0.41 hearing-aid speech quality index (HASQI) scores on the synthetic evaluation set. However, our model generalized poorly on the semi-real evaluation set. This highlights the fact that our community should focus more on real-world evaluation and less on fully synthetic datasets.|Author: Shinji Watanabe Title: Multi-Channel Target Speaker Extraction with Refinement: The WavLab Submission to the Second Clarity Enhancement Challenge Publication year: 2023 Coauthors: Samuele Cornell, Zhongqiu Wang, Yoshiki Masuyama, Shinji Watanabe, Manuel Pariente, Nobutaka Ono Abstract: This paper describes our submission to the Second Clarity Enhancement Challenge (CEC2), which consists of target speech enhancement for hearing-aid (HA) devices in noisy-reverberant environments with multiple interferers such as music and competing speakers. Our approach builds upon the powerful iterative neural/beamforming enhancement (iNeuBe) framework introduced in our recent work, and this paper extends it for target speaker extraction. We therefore name the proposed approach as iNeuBe-X, where the X stands for extraction. To address the challenges encountered in the CEC2 setting, we introduce four major novelties: (1) we extend the state-of-the-art TF-GridNet model, originally designed for monaural speaker separation, for multi-channel, causal speech enhancement, and large improvements are observed by replacing the TCNDenseNet used in iNeuBe with this new architecture; (2) we leverage a recent dual window size approach with future-frame prediction to ensure that iNueBe-X satisfies the 5 ms constraint on algorithmic latency required by CEC2; (3) we introduce a novel speaker-conditioning branch for TF-GridNet to achieve target speaker extraction; (4) we propose a fine-tuning step, where we compute an additional loss with respect to the target speaker signal compensated with the listener audiogram. Without using external data, on the official development set our best model reaches a hearing-aid speech perception index (HASPI) score of 0.942 and a scale-invariant signal-to-distortion ratio improvement (SI-SDRi) of 18.8 dB. These results are promising given the fact that the CEC2 data is extremely challenging (e.g., on the development set the mixture SI-SDR is -12.3 dB). A demo of our submitted system is available at WAVLab CEC2 demo.|Author: Shinji Watanabe Title: The Multimodal Information Based Speech Processing (MISP) 2023 Challenge: Audio-Visual Target Speaker Extraction Publication year: 2023 Coauthors: Shilong Wu, Chenxi Wang, Hang Chen, Yusheng Dai, Chenyue Zhang, Ruoyu Wang, Hongbo Lan, Jun Du, Chin-Hui Lee, Jingdong Chen, Shinji Watanabe, Sabato Marco Siniscalchi, O. Scharenborg, Zhong-Qiu Wang, Jia Pan, Jianqing Gao Abstract: Previous Multimodal Information based Speech Processing (MISP) challenges mainly focused on audio-visual speech recognition (AVSR) with commendable success. However, the most advanced back-end recognition systems often hit performance limits due to the complex acoustic environments. This has prompted a shift in focus towards the Audio-Visual Target Speaker Extraction (AVTSE) task for the MISP 2023 challenge in ICASSP 2024 Signal Processing Grand Challenges. Unlike existing audio-visual speech enhance-ment challenges primarily focused on simulation data, the MISP 2023 challenge uniquely explores how front-end speech processing, combined with visual clues, impacts back-end tasks in real-world scenarios. This pioneering effort aims to set the first benchmark for the AVTSE task, offering fresh insights into enhancing the ac-curacy of back-end speech recognition systems through AVTSE in challenging and real acoustic environments. This paper delivers a thorough overview of the task setting, dataset, and baseline system of the MISP 2023 challenge. It also includes an in-depth analysis of the challenges participants may encounter. The experimental results highlight the demanding nature of this task, and we look forward to the innovative solutions participants will bring forward.",
        "retrieved_relevant_part": true,
        "model_answer": "The paper details the authors' submission to the Clarity ICASSP 2023 grand challenge, which involves developing a strong target speech enhancement system for hearing-aid (HA) devices in noisy-reverberant environments with multiple interferers",
        "actual_answer": "In this work we detail our submission to the Clarity ICASSP 2023 grand challenge, in which participants have to develop a strong target speech enhancement system for hearing-aid (HA) devices in noisy-reverberant environments. Our system builds on our previous submission at the Second Clarity Enhancement Challenge (CEC2): iNeuBe-X, which consists in an iterative neural/conventional beamforming enhancement pipeline, guided by an enrollment utterance from the target speaker. This model, which won by a large margin the CEC2, is an extension of the state-of-the-art TF-GridNet model for multi-channel, streamable target-speaker speech enhancement. Here, this approach is extended and further improved by leveraging generative adversarial training, which we show proves especially useful when the training data is limited. Using only the official 6k training scenes data, our best model achieves 0.80 hearing-aid speech perception index (HASPI) and 0.41 hearing-aid speech quality index (HASQI) scores on the synthetic evaluation set. However, our model generalized poorly on the semi-real evaluation set. This highlights the fact that our community should focus more on real-world evaluation and less on fully synthetic datasets.",
        "exact_match": false,
        "f1_max_min_avg": [
            0.2,
            0.2,
            0.2
        ],
        "recall_max_min_avg": [
            0.12,
            0.12,
            0.12
        ]
    },
    {
        "question": "Who is the author of the LTI paper Completing Visual Objects via Bridging Generation and Segmentation published in 2023?",
        "context": "Author: Rita Singh Title: Completing Visual Objects via Bridging Generation and Segmentation Publication year: 2023 Coauthors: Xiang Li, Yinpeng Chen, Chung-Ching Lin, Rita Singh, Bhiksha Raj, Zicheng Liu Abstract: This paper presents a novel approach to object completion, with the primary goal of reconstructing a complete object from its partially visible components. Our method, named MaskComp, delineates the completion process through iterative stages of generation and segmentation. In each iteration, the object mask is provided as an additional condition to boost image generation, and, in return, the generated images can lead to a more accurate mask by fusing the segmentation of images. We demonstrate that the combination of one generation and one segmentation stage effectively functions as a mask denoiser. Through alternation between the generation and segmentation stages, the partial object mask is progressively refined, providing precise shape guidance and yielding superior object completion results. Our experiments demonstrate the superiority of MaskComp over existing approaches, e.g., ControlNet and Stable Diffusion, establishing it as an effective solution for object completion.",
        "retrieved_doc": "Author: Rita Singh Title: Completing Visual Objects via Bridging Generation and Segmentation Publication year: 2023 Coauthors: Xiang Li, Yinpeng Chen, Chung-Ching Lin, Rita Singh, Bhiksha Raj, Zicheng Liu Abstract: This paper presents a novel approach to object completion, with the primary goal of reconstructing a complete object from its partially visible components. Our method, named MaskComp, delineates the completion process through iterative stages of generation and segmentation. In each iteration, the object mask is provided as an additional condition to boost image generation, and, in return, the generated images can lead to a more accurate mask by fusing the segmentation of images. We demonstrate that the combination of one generation and one segmentation stage effectively functions as a mask denoiser. Through alternation between the generation and segmentation stages, the partial object mask is progressively refined, providing precise shape guidance and yielding superior object completion results. Our experiments demonstrate the superiority of MaskComp over existing approaches, e.g., ControlNet and Stable Diffusion, establishing it as an effective solution for object completion.|Author: Yonatan Bisk Title: MOSAIC: Learning Unified Multi-Sensory Object Property Representations for Robot Perception Publication year: 2023 Coauthors: Gyan Tatiya, Jonathan M Francis, Ho-Hsiang Wu, Yonatan Bisk, J. Sinapov Abstract: A holistic understanding of object properties across diverse sensory modalities (e.g., visual, audio, and haptic) is essential for tasks ranging from object categorization to complex manipulation. Drawing inspiration from cognitive science studies that emphasize the significance of multi-sensory integration in human perception, we introduce MOSAIC (Multi-modal Object property learning with Self-Attention and Integrated Comprehension), a novel framework designed to facilitate the learning of unified multi-sensory object property representations. While it is undeniable that visual information plays a prominent role, we acknowledge that many fundamental object properties extend beyond the visual domain to encompass attributes like texture, mass distribution, or sounds, which significantly influence how we interact with objects. In MOSAIC, we leverage this profound insight by distilling knowledge from the extensive pre-trained Contrastive Language-Image Pre-training (CLIP) model, aligning these representations not only across vision but also haptic and auditory sensory modalities. Through extensive experiments on a dataset where a humanoid robot interacts with 100 objects across 10 exploratory behaviors, we demonstrate the versatility of MOSAIC in two task families: object categorization and object-fetching tasks. Our results underscore the efficacy of MOSAIC's unified representations, showing competitive performance in category recognition through a simple linear probe setup and excelling in the fetch object task under zero-shot transfer conditions. This work pioneers the application of CLIP-based sensory grounding in robotics, promising a significant leap in multi-sensory perception capabilities for autonomous systems. We have released the code, datasets, and additional results: https://github.com/gtatiya/MOSAIC.|Author: Yonatan Bisk Title: SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs Publication year: 2023 Coauthors: Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey, Yanping Huang, David A. Ross, Irfan Essa, Yonatan Bisk, Ming Yang, K. Murphy, A. Hauptmann, Lu Jiang Abstract: In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos. SPAE converts between raw pixels and interpretable lexical tokens (or words) extracted from the LLM's vocabulary. The resulting tokens capture both the semantic meaning and the fine-grained details needed for visual reconstruction, effectively translating the visual content into a language comprehensible to the LLM, and empowering it to perform a wide array of multimodal tasks. Our approach is validated through in-context learning experiments with frozen PaLM 2 and GPT 3.5 on a diverse set of image understanding and generation tasks. Our method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%.",
        "retrieved_relevant_part": true,
        "model_answer": "Rita Singh",
        "actual_answer": "Rita Singh",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Which LTI faculty published the paper An In-depth Look at Gemini's Language Abilities in 2023?",
        "context": "Author: Graham Neubig Title: An In-depth Look at Gemini's Language Abilities Publication year: 2023 Coauthors: Syeda Nahida Akter, Zichun Yu, Aashiq Muhamed, Tianyue Ou, Alex Bauerle, \u00c1ngel Alexander Cabrera, Krish Dholakia, Chenyan Xiong, Graham Neubig Abstract: The recently released Google Gemini class of models are the first to comprehensively report results that rival the OpenAI GPT series across a wide variety of tasks. In this paper, we do an in-depth exploration of Gemini's language abilities, making two contributions. First, we provide a third-party, objective comparison of the abilities of the OpenAI GPT and Google Gemini models with reproducible code and fully transparent results. Second, we take a closer look at the results, identifying areas where one of the two model classes excels. We perform this analysis over 10 datasets testing a variety of language abilities, including reasoning, answering knowledge-based questions, solving math problems, translating between languages, generating code, and acting as instruction-following agents. From this analysis, we find that Gemini Pro achieves accuracy that is close but slightly inferior to the corresponding GPT 3.5 Turbo on all tasks that we benchmarked. We further provide explanations for some of this under-performance, including failures in mathematical reasoning with many digits, sensitivity to multiple-choice answer ordering, aggressive content filtering, and others. We also identify areas where Gemini demonstrates comparably high performance, including generation into non-English languages, and handling longer and more complex reasoning chains. Code and data for reproduction can be found at https://github.com/neulab/gemini-benchmark",
        "retrieved_doc": "Author: Graham Neubig Title: An In-depth Look at Gemini's Language Abilities Publication year: 2023 Coauthors: Syeda Nahida Akter, Zichun Yu, Aashiq Muhamed, Tianyue Ou, Alex Bauerle, \u00c1ngel Alexander Cabrera, Krish Dholakia, Chenyan Xiong, Graham Neubig Abstract: The recently released Google Gemini class of models are the first to comprehensively report results that rival the OpenAI GPT series across a wide variety of tasks. In this paper, we do an in-depth exploration of Gemini's language abilities, making two contributions. First, we provide a third-party, objective comparison of the abilities of the OpenAI GPT and Google Gemini models with reproducible code and fully transparent results. Second, we take a closer look at the results, identifying areas where one of the two model classes excels. We perform this analysis over 10 datasets testing a variety of language abilities, including reasoning, answering knowledge-based questions, solving math problems, translating between languages, generating code, and acting as instruction-following agents. From this analysis, we find that Gemini Pro achieves accuracy that is close but slightly inferior to the corresponding GPT 3.5 Turbo on all tasks that we benchmarked. We further provide explanations for some of this under-performance, including failures in mathematical reasoning with many digits, sensitivity to multiple-choice answer ordering, aggressive content filtering, and others. We also identify areas where Gemini demonstrates comparably high performance, including generation into non-English languages, and handling longer and more complex reasoning chains. Code and data for reproduction can be found at https://github.com/neulab/gemini-benchmark|Author: Chenyan Xiong Title: An In-depth Look at Gemini's Language Abilities Publication year: 2023 Coauthors: Syeda Nahida Akter, Zichun Yu, Aashiq Muhamed, Tianyue Ou, Alex Bauerle, \u00c1ngel Alexander Cabrera, Krish Dholakia, Chenyan Xiong, Graham Neubig Abstract: The recently released Google Gemini class of models are the first to comprehensively report results that rival the OpenAI GPT series across a wide variety of tasks. In this paper, we do an in-depth exploration of Gemini's language abilities, making two contributions. First, we provide a third-party, objective comparison of the abilities of the OpenAI GPT and Google Gemini models with reproducible code and fully transparent results. Second, we take a closer look at the results, identifying areas where one of the two model classes excels. We perform this analysis over 10 datasets testing a variety of language abilities, including reasoning, answering knowledge-based questions, solving math problems, translating between languages, generating code, and acting as instruction-following agents. From this analysis, we find that Gemini Pro achieves accuracy that is close but slightly inferior to the corresponding GPT 3.5 Turbo on all tasks that we benchmarked. We further provide explanations for some of this under-performance, including failures in mathematical reasoning with many digits, sensitivity to multiple-choice answer ordering, aggressive content filtering, and others. We also identify areas where Gemini demonstrates comparably high performance, including generation into non-English languages, and handling longer and more complex reasoning chains. Code and data for reproduction can be found at https://github.com/neulab/gemini-benchmark|LTI Ph.D. Graduate Student Handbook Page 10 This handbook applies to the LTI Ph.D. The Ph.D. in Language and Information Technologies (LTI Ph.D. ) is focused on understanding and extending the state of the art in computational linguistics, natural language processing, dialogue systems, information retrieval, machine translation, speech processing, video understanding, multimodal systems, automated reasoning, and other topics related to analysis and understanding of unstructured information (e.g., machine learning, and software engineering of intelligent systems). 1.2 Department Personnel The people responsible for administering the LTI Ph.D. degree are: Jamie Callan Ph.D. Program Director Professor GHC 5419 callan@cs.cmu.edu 412-268-4525 Stacey Young Ph.D. Academic Program Manager LTI Graduate Program Manager GHC 6415 staceyy@cs.cmu.edu 412-268-2623 Mona Diab LTI Director Professor GHC 5723 mdiab@andrew.cmu.edu 412-268-3669 Joan Axelson Office Manager GHC 5405 jaxelson@andrew.cmu.edu 412-268-7517 Julie Nys Employment Processes Manger GHC 5405 jnys@andrew.cmu.edu 412-268-3515 1.3 University Policies and Expectations It is the responsibility of each member of the Carnegie Mellon community to be familiar with university policies and guidelines. In addition to this departmental graduate student handbook, the following resources are available to assist you in understanding community expectations. \u2022 The Word/Student Handbook: https://www.cmu.edu/student-a ffairs/theword//index.html \u2022 Academic Integrity Policy: https:// www.cmu.edu/policies/student-and-student-life/academic-integrity.html",
        "retrieved_relevant_part": true,
        "model_answer": "Graham Neubig",
        "actual_answer": "Graham Neubig",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "What is the abstract of the paper ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages published by LTI faculty Graham Neubig in 2023?",
        "context": "Author: Graham Neubig Title: ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages Publication year: 2023 Coauthors: Nathaniel R. Robinson, Perez Ogayo, David R. Mortensen, Graham Neubig Abstract: Large language models (LLMs) implicitly learn to perform a range of language tasks, including machine translation (MT). Previous studies explore aspects of LLMs\u2019 MT capabilities. However, there exist a wide variety of languages for which recent LLM MT performance has never before been evaluated. Without published experimental evidence on the matter, it is difficult for speakers of the world\u2019s diverse languages to know how and whether they can use LLMs for their languages. We present the first experimental evidence for an expansive set of 204 languages, along with MT cost analysis, using the FLORES-200 benchmark. Trends reveal that GPT models approach or exceed traditional MT model performance for some high-resource languages (HRLs) but consistently lag for low-resource languages (LRLs), under-performing traditional MT for 84.1% of languages we covered. Our analysis reveals that a language\u2019s resource level is the most important feature in determining ChatGPT\u2019s relative ability to translate it, and suggests that ChatGPT is especially disadvantaged for LRLs and African languages.",
        "retrieved_doc": "Author: Graham Neubig Title: ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages Publication year: 2023 Coauthors: Nathaniel R. Robinson, Perez Ogayo, David R. Mortensen, Graham Neubig Abstract: Large language models (LLMs) implicitly learn to perform a range of language tasks, including machine translation (MT). Previous studies explore aspects of LLMs\u2019 MT capabilities. However, there exist a wide variety of languages for which recent LLM MT performance has never before been evaluated. Without published experimental evidence on the matter, it is difficult for speakers of the world\u2019s diverse languages to know how and whether they can use LLMs for their languages. We present the first experimental evidence for an expansive set of 204 languages, along with MT cost analysis, using the FLORES-200 benchmark. Trends reveal that GPT models approach or exceed traditional MT model performance for some high-resource languages (HRLs) but consistently lag for low-resource languages (LRLs), under-performing traditional MT for 84.1% of languages we covered. Our analysis reveals that a language\u2019s resource level is the most important feature in determining ChatGPT\u2019s relative ability to translate it, and suggests that ChatGPT is especially disadvantaged for LRLs and African languages.|Author: David R. Mortensen Title: ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages Publication year: 2023 Coauthors: Nathaniel R. Robinson, Perez Ogayo, David R. Mortensen, Graham Neubig Abstract: Large language models (LLMs) implicitly learn to perform a range of language tasks, including machine translation (MT). Previous studies explore aspects of LLMs\u2019 MT capabilities. However, there exist a wide variety of languages for which recent LLM MT performance has never before been evaluated. Without published experimental evidence on the matter, it is difficult for speakers of the world\u2019s diverse languages to know how and whether they can use LLMs for their languages. We present the first experimental evidence for an expansive set of 204 languages, along with MT cost analysis, using the FLORES-200 benchmark. Trends reveal that GPT models approach or exceed traditional MT model performance for some high-resource languages (HRLs) but consistently lag for low-resource languages (LRLs), under-performing traditional MT for 84.1% of languages we covered. Our analysis reveals that a language\u2019s resource level is the most important feature in determining ChatGPT\u2019s relative ability to translate it, and suggests that ChatGPT is especially disadvantaged for LRLs and African languages.|Author: Graham Neubig Title: The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation Publication year: 2023 Coauthors: Patrick Fernandes, Daniel Deutsch, M. Finkelstein, Parker Riley, Andr\u00e9 F. T. Martins, Graham Neubig, Ankush Garg, J. Clark, Markus Freitag, Orhan Firat Abstract: Automatic evaluation of machine translation (MT) is a critical tool driving the rapid iterative development of MT systems. While considerable progress has been made on estimating a single scalar quality score, current metrics lack the informativeness of more detailed schemes that annotate individual errors, such as Multidimensional Quality Metrics (MQM). In this paper, we help fill this gap by proposing AutoMQM, a prompting technique which leverages the reasoning and in-context learning capabilities of large language models (LLMs) and asks them to identify and categorize errors in translations. We start by evaluating recent LLMs, such as PaLM and PaLM-2, through simple score prediction prompting, and we study the impact of labeled data through in-context learning and finetuning. We then evaluate AutoMQM with PaLM-2 models, and we find that it improves performance compared to just prompting for scores (with particularly large gains for larger models) while providing interpretability through error spans that align with human annotations.",
        "retrieved_relevant_part": true,
        "model_answer": "The paper presents the first experimental evidence for an expansive set of 204 languages and provides MT cost analysis using the FLORES-200 benchmark",
        "actual_answer": "Large language models (LLMs) implicitly learn to perform a range of language tasks, including machine translation (MT). Previous studies explore aspects of LLMs\u2019 MT capabilities. However, there exist a wide variety of languages for which recent LLM MT performance has never before been evaluated. Without published experimental evidence on the matter, it is difficult for speakers of the world\u2019s diverse languages to know how and whether they can use LLMs for their languages. We present the first experimental evidence for an expansive set of 204 languages, along with MT cost analysis, using the FLORES-200 benchmark. Trends reveal that GPT models approach or exceed traditional MT model performance for some high-resource languages (HRLs) but consistently lag for low-resource languages (LRLs), under-performing traditional MT for 84.1% of languages we covered. Our analysis reveals that a language\u2019s resource level is the most important feature in determining ChatGPT\u2019s relative ability to translate it, and suggests that ChatGPT is especially disadvantaged for LRLs and African languages.",
        "exact_match": false,
        "f1_max_min_avg": [
            0.18,
            0.18,
            0.18
        ],
        "recall_max_min_avg": [
            0.11,
            0.11,
            0.11
        ]
    },
    {
        "question": "Who is the author of the LTI paper Energy and Carbon Considerations of Fine-Tuning BERT published in 2023?",
        "context": "Author: Emma Strubell Title: Energy and Carbon Considerations of Fine-Tuning BERT Publication year: 2023 Coauthors: Xiaorong Wang, Clara Na, Emma Strubell, Sorelle A. Friedler, Sasha Luccioni Abstract: Despite the popularity of the `pre-train then fine-tune' paradigm in the NLP community, existing work quantifying energy costs and associated carbon emissions has largely focused on language model pre-training. Although a single pre-training run draws substantially more energy than fine-tuning, fine-tuning is performed more frequently by many more individual actors, and thus must be accounted for when considering the energy and carbon footprint of NLP. In order to better characterize the role of fine-tuning in the landscape of energy and carbon emissions in NLP, we perform a careful empirical study of the computational costs of fine-tuning across tasks, datasets, hardware infrastructure and measurement modalities. Our experimental results allow us to place fine-tuning energy and carbon costs into perspective with respect to pre-training and inference, and outline recommendations to NLP researchers and practitioners who wish to improve their fine-tuning energy efficiency.",
        "retrieved_doc": "Author: Emma Strubell Title: Energy and Carbon Considerations of Fine-Tuning BERT Publication year: 2023 Coauthors: Xiaorong Wang, Clara Na, Emma Strubell, Sorelle A. Friedler, Sasha Luccioni Abstract: Despite the popularity of the `pre-train then fine-tune' paradigm in the NLP community, existing work quantifying energy costs and associated carbon emissions has largely focused on language model pre-training. Although a single pre-training run draws substantially more energy than fine-tuning, fine-tuning is performed more frequently by many more individual actors, and thus must be accounted for when considering the energy and carbon footprint of NLP. In order to better characterize the role of fine-tuning in the landscape of energy and carbon emissions in NLP, we perform a careful empirical study of the computational costs of fine-tuning across tasks, datasets, hardware infrastructure and measurement modalities. Our experimental results allow us to place fine-tuning energy and carbon costs into perspective with respect to pre-training and inference, and outline recommendations to NLP researchers and practitioners who wish to improve their fine-tuning energy efficiency.|Author: Emma Strubell Title: Power Hungry Processing: Watts Driving the Cost of AI Deployment? Publication year: 2023 Coauthors: A. Luccioni, Yacine Jernite, Emma Strubell Abstract: Recent years have seen a surge in the popularity of commercial AI products based on generative, multi-purpose AI systems promising a unified approach to building machine learning (ML) models into technology. However, this ambition of\"generality\"comes at a steep cost to the environment, given the amount of energy these systems require and the amount of carbon that they emit. In this work, we propose the first systematic comparison of the ongoing inference cost of various categories of ML systems, covering both task-specific (i.e. finetuned models that carry out a single task) and `general-purpose' models, (i.e. those trained for multiple tasks). We measure deployment cost as the amount of energy and carbon required to perform 1,000 inferences on representative benchmark dataset using these models. We find that multi-purpose, generative architectures are orders of magnitude more expensive than task-specific systems for a variety of tasks, even when controlling for the number of model parameters. We conclude with a discussion around the current trend of deploying multi-purpose generative ML systems, and caution that their utility should be more intentionally weighed against increased costs in terms of energy and emissions. All the data from our study can be accessed via an interactive demo to carry out further exploration and analysis.|Author: Yonatan Bisk Title: Open X-Embodiment: Robotic Learning Datasets and RT-X Models Publication year: 2023 Coauthors: A. Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, A. Irpan, Alexander Khazatsky, Anant Rai, Anika Singh, Anthony Brohan, A. Raffin, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Sch\u00f6lkopf, Brian Ichter, Cewu Lu, Charles Xu, Chelsea Finn, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Chuer Pan, Chuyuan Fu, Coline Devin, Danny Driess, Deepak Pathak, Dhruv Shah, Dieter B\u00fcchler, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Federico Ceola, Fei Xia, F. Stulp, Gaoyue Zhou, G. Sukhatme, G. Salhotra, Ge Yan, Giulio Schiavi, G. Kahn, Hao Su, Haoshu Fang, Haochen Shi, H. B. Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jan Peters, Jan Schneider, Jasmine Hsu, J. Bohg, Jeff Bingham, Jiajun Wu, Jialin Wu, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jitendra Malik, Jonathan Tompson, Jonathan Yang, Joseph J. Lim, Jo\u00e3o Silv\u00e9rio, Junhyek Han, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, K. Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Zhang, Krishan Rana, K. Srinivasan, Lawrence Yunliang Chen, Lerrel Pinto, Liam Tan, Lionel Ott, Lisa Lee, Masayoshi Tomizuka, Maximilian Du, Michael Ahn, Mingtong Zhang, Mingyu Ding, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, N. Heess, Nikhil Joshi, Niko Suenderhauf, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, P. Sanketi, Paul Wohlhart, Peng Xu, P. Sermanet, Priya Sundaresan, Q. Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan C. Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham D. Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, S. Adebola, Simon Guist, Soroush Nasiriany, S. Schaal, Stefan Welker, Stephen Tian, Sudeep Dasari, Suneel Belkhale, Takayuki Osa, Tatsuya Harada, T. Matsushima, Ted Xiao, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Zhao, Travis Armstrong, Trevor Darrell, Vidhi Jain, Vincent Vanhoucke, W. Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xuanlin Li, Yao Lu, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yueh-hua Wu, Yujin Tang, Yuke Zhu, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zhuo Xu, Zichen Jeff Cui Abstract: \u2014Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer",
        "retrieved_relevant_part": true,
        "model_answer": "Emma Strubell",
        "actual_answer": "Emma Strubell",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Who are the coauthors of the paper Cross-Modal Fine-Tuning: Align then Refine published by LTI faculty Graham Neubig in 2023?",
        "context": "Author: Graham Neubig Title: Cross-Modal Fine-Tuning: Align then Refine Publication year: 2023 Coauthors: Junhong Shen, Liam Li, L. Dery, Corey Staten, M. Khodak, Graham Neubig, Ameet Talwalkar Abstract: Fine-tuning large-scale pretrained models has led to tremendous progress in well-studied modalities such as vision and NLP. However, similar gains have not been observed in many other modalities due to a lack of relevant pretrained models. In this work, we propose ORCA, a general cross-modal fine-tuning framework that extends the applicability of a single large-scale pretrained model to diverse modalities. ORCA adapts to a target task via an align-then-refine workflow: given the target input, ORCA first learns an embedding network that aligns the embedded feature distribution with the pretraining modality. The pretrained model is then fine-tuned on the embedded data to exploit the knowledge shared across modalities. Through extensive experiments, we show that ORCA obtains state-of-the-art results on 3 benchmarks containing over 60 datasets from 12 modalities, outperforming a wide range of hand-designed, AutoML, general-purpose, and task-specific methods. We highlight the importance of data alignment via a series of ablation studies and demonstrate ORCA's utility in data-limited regimes.",
        "retrieved_doc": "Author: Graham Neubig Title: Cross-Modal Fine-Tuning: Align then Refine Publication year: 2023 Coauthors: Junhong Shen, Liam Li, L. Dery, Corey Staten, M. Khodak, Graham Neubig, Ameet Talwalkar Abstract: Fine-tuning large-scale pretrained models has led to tremendous progress in well-studied modalities such as vision and NLP. However, similar gains have not been observed in many other modalities due to a lack of relevant pretrained models. In this work, we propose ORCA, a general cross-modal fine-tuning framework that extends the applicability of a single large-scale pretrained model to diverse modalities. ORCA adapts to a target task via an align-then-refine workflow: given the target input, ORCA first learns an embedding network that aligns the embedded feature distribution with the pretraining modality. The pretrained model is then fine-tuned on the embedded data to exploit the knowledge shared across modalities. Through extensive experiments, we show that ORCA obtains state-of-the-art results on 3 benchmarks containing over 60 datasets from 12 modalities, outperforming a wide range of hand-designed, AutoML, general-purpose, and task-specific methods. We highlight the importance of data alignment via a series of ablation studies and demonstrate ORCA's utility in data-limited regimes.|Author: Graham Neubig Title: DiffusER: Diffusion via Edit-based Reconstruction Publication year: 2023 Coauthors: Machel Reid, V. Hellendoorn, Graham Neubig |Author: Eric P. Xing Title: One-for-All: Generalized LoRA for Parameter-Efficient Fine-tuning Publication year: 2023 Coauthors: Arnav Chavan, Zhuang Liu, D. Gupta, Eric P. Xing, Zhiqiang Shen Abstract: We present Generalized LoRA (GLoRA), an advanced approach for universal parameter-efficient fine-tuning tasks. Enhancing Low-Rank Adaptation (LoRA), GLoRA employs a generalized prompt module to optimize pre-trained model weights and adjust intermediate activations, providing more flexibility and capability across diverse tasks and datasets. Moreover, GLoRA facilitates efficient parameter adaptation by employing a scalable, modular, layer-wise structure search that learns individual adapter of each layer. Originating from a unified mathematical formulation, GLoRA exhibits strong transfer learning, few-shot learning and domain generalization abilities, as it adapts to new tasks through not only weights but also additional dimensions like activations. Comprehensive experiments demonstrate that GLoRA outperforms all previous methods in natural, specialized, and structured vision benchmarks, achieving superior accuracy with fewer parameters and computations. The proposed method on LLaMA-1 and LLaMA-2 also show considerable enhancements compared to the original LoRA in the language domain. Furthermore, our structural re-parameterization design ensures that GLoRA incurs no extra inference cost, rendering it a practical solution for resource-limited applications. Code and models are available at: https://github.com/Arnav0400/ViT-Slim/tree/master/GLoRA.",
        "retrieved_relevant_part": true,
        "model_answer": "Junhong Shen, Liam Li, L. Dery, Corey Staten, M. Khodak, and Ameet Talwalkar",
        "actual_answer": "Junhong Shen, Liam Li, L. Dery, Corey Staten, M. Khodak, Graham Neubig, Ameet Talwalkar",
        "exact_match": false,
        "f1_max_min_avg": [
            0.89,
            0.89,
            0.89
        ],
        "recall_max_min_avg": [
            0.86,
            0.86,
            0.86
        ]
    },
    {
        "question": "Which LTI faculty published the paper KEEC: Embed to Control on An Equivariant Geometry in 2023?",
        "context": "Author: Yiming Yang Title: KEEC: Embed to Control on An Equivariant Geometry Publication year: 2023 Coauthors: Xiaoyuan Cheng, Yiming Yang, Wei Jiang, Yukun Hu Abstract: This paper investigates how representation learning can enable optimal control in unknown and complex dynamics, such as chaotic and non-linear systems, without relying on prior domain knowledge of the dynamics. The core idea is to establish an equivariant geometry that is diffeomorphic to the manifold defined by a dynamical system and to perform optimal control within this corresponding geometry, which is a non-trivial task. To address this challenge, Koopman Embed to Equivariant Control (KEEC) is proposed for model learning and control. Inspired by Lie theory, KEEC begins by learning a non-linear dynamical system defined on a manifold and embedding trajectories into a Lie group. Subsequently, KEEC formulates an equivariant value function equation in reinforcement learning on the equivariant geometry, ensuring an invariant effect as the value function on the original manifold. By deriving analytical-form optimal actions on the equivariant value function, KEEC theoretically achieves quadratic convergence for the optimal equivariant value function by leveraging the differential information on the equivariant geometry. The effectiveness of KEEC is demonstrated in challenging dynamical systems, including chaotic ones like Lorenz-63. Notably, our results show that isometric functions, which maintain the compactness and completeness of geometry while preserving metric and differential information, consistently outperform loss functions lacking these characteristics.",
        "retrieved_doc": "Author: Yiming Yang Title: KEEC: Embed to Control on An Equivariant Geometry Publication year: 2023 Coauthors: Xiaoyuan Cheng, Yiming Yang, Wei Jiang, Yukun Hu Abstract: This paper investigates how representation learning can enable optimal control in unknown and complex dynamics, such as chaotic and non-linear systems, without relying on prior domain knowledge of the dynamics. The core idea is to establish an equivariant geometry that is diffeomorphic to the manifold defined by a dynamical system and to perform optimal control within this corresponding geometry, which is a non-trivial task. To address this challenge, Koopman Embed to Equivariant Control (KEEC) is proposed for model learning and control. Inspired by Lie theory, KEEC begins by learning a non-linear dynamical system defined on a manifold and embedding trajectories into a Lie group. Subsequently, KEEC formulates an equivariant value function equation in reinforcement learning on the equivariant geometry, ensuring an invariant effect as the value function on the original manifold. By deriving analytical-form optimal actions on the equivariant value function, KEEC theoretically achieves quadratic convergence for the optimal equivariant value function by leveraging the differential information on the equivariant geometry. The effectiveness of KEEC is demonstrated in challenging dynamical systems, including chaotic ones like Lorenz-63. Notably, our results show that isometric functions, which maintain the compactness and completeness of geometry while preserving metric and differential information, consistently outperform loss functions lacking these characteristics.|Author: Yonatan Bisk Title: Open X-Embodiment: Robotic Learning Datasets and RT-X Models Publication year: 2023 Coauthors: A. Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, A. Irpan, Alexander Khazatsky, Anant Rai, Anika Singh, Anthony Brohan, A. Raffin, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Sch\u00f6lkopf, Brian Ichter, Cewu Lu, Charles Xu, Chelsea Finn, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Chuer Pan, Chuyuan Fu, Coline Devin, Danny Driess, Deepak Pathak, Dhruv Shah, Dieter B\u00fcchler, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Federico Ceola, Fei Xia, F. Stulp, Gaoyue Zhou, G. Sukhatme, G. Salhotra, Ge Yan, Giulio Schiavi, G. Kahn, Hao Su, Haoshu Fang, Haochen Shi, H. B. Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jan Peters, Jan Schneider, Jasmine Hsu, J. Bohg, Jeff Bingham, Jiajun Wu, Jialin Wu, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jitendra Malik, Jonathan Tompson, Jonathan Yang, Joseph J. Lim, Jo\u00e3o Silv\u00e9rio, Junhyek Han, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, K. Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Zhang, Krishan Rana, K. Srinivasan, Lawrence Yunliang Chen, Lerrel Pinto, Liam Tan, Lionel Ott, Lisa Lee, Masayoshi Tomizuka, Maximilian Du, Michael Ahn, Mingtong Zhang, Mingyu Ding, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, N. Heess, Nikhil Joshi, Niko Suenderhauf, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, P. Sanketi, Paul Wohlhart, Peng Xu, P. Sermanet, Priya Sundaresan, Q. Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan C. Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham D. Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, S. Adebola, Simon Guist, Soroush Nasiriany, S. Schaal, Stefan Welker, Stephen Tian, Sudeep Dasari, Suneel Belkhale, Takayuki Osa, Tatsuya Harada, T. Matsushima, Ted Xiao, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Zhao, Travis Armstrong, Trevor Darrell, Vidhi Jain, Vincent Vanhoucke, W. Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xuanlin Li, Yao Lu, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yueh-hua Wu, Yujin Tang, Yuke Zhu, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zhuo Xu, Zichen Jeff Cui Abstract: \u2014Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer|Author: Yonatan Bisk Title: Open X-Embodiment: Robotic Learning Datasets and RT-X Models Publication year: 2023 Coauthors: A. Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, A. Irpan, Alexander Khazatsky, Anant Rai, Anika Singh, Anthony Brohan, A. Raffin, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Sch\u00f6lkopf, Brian Ichter, Cewu Lu, Charles Xu, Chelsea Finn, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Chuer Pan, Chuyuan Fu, Coline Devin, Danny Driess, Deepak Pathak, Dhruv Shah, Dieter B\u00fcchler, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Federico Ceola, Fei Xia, F. Stulp, Gaoyue Zhou, G. Sukhatme, G. Salhotra, Ge Yan, Giulio Schiavi, G. Kahn, Hao Su, Haoshu Fang, Haochen Shi, H. B. Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jan Peters, Jan Schneider, Jasmine Hsu, J. Bohg, Jeff Bingham, Jiajun Wu, Jialin Wu, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jitendra Malik, Jonathan Tompson, Jonathan Yang, Joseph J. Lim, Jo\u00e3o Silv\u00e9rio, Junhyek Han, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, K. Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Zhang, Krishan Rana, K. Srinivasan, Lawrence Yunliang Chen, Lerrel Pinto, Liam Tan, Lionel Ott, Lisa Lee, Masayoshi Tomizuka, Maximilian Du, Michael Ahn, Mingtong Zhang, Mingyu Ding, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, N. Heess, Nikhil Joshi, Niko Suenderhauf, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, P. Sanketi, Paul Wohlhart, Peng Xu, P. Sermanet, Priya Sundaresan, Q. Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan C. Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham D. Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, S. Adebola, Simon Guist, Soroush Nasiriany, S. Schaal, Stefan Welker, Stephen Tian, Sudeep Dasari, Suneel Belkhale, Takayuki Osa, Tatsuya Harada, T. Matsushima, Ted Xiao, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Zhao, Travis Armstrong, Trevor Darrell, Vidhi Jain, Vincent Vanhoucke, W. Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xuanlin Li, Yao Lu, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yueh-hua Wu, Yujin Tang, Yuke Zhu, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zhuo Xu, Zichen Jeff Cui Abstract: \u2014Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer",
        "retrieved_relevant_part": true,
        "model_answer": "Yiming Yang",
        "actual_answer": "Yiming Yang",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Who are the coauthors of the paper Reducing Barriers to Self-Supervised Learning: HuBERT Pre-training with Academic Compute published by LTI faculty Shinji Watanabe in 2023?",
        "context": "Author: Shinji Watanabe Title: Reducing Barriers to Self-Supervised Learning: HuBERT Pre-training with Academic Compute Publication year: 2023 Coauthors: William Chen, Xuankai Chang, Yifan Peng, Zhaoheng Ni, Soumi Maiti, Shinji Watanabe Abstract: Self-supervised learning (SSL) has led to great strides in speech processing. However, the resources needed to train these models has become prohibitively large as they continue to scale. Currently, only a few groups with substantial resources are capable of creating SSL models, which harms reproducibility. In this work, we optimize HuBERT SSL to fit in academic constraints. We reproduce HuBERT independently from the original implementation, with no performance loss. Our code and training optimizations make SSL feasible with only 8 GPUs, instead of the 32 used in the original work. We also explore a semi-supervised route, using an ASR model to skip the first pre-training iteration. Within one iteration of pre-training, our models improve over HuBERT on several tasks. Furthermore, our HuBERT Large variant requires only 8 GPUs, achieving similar performance to the original trained on 128. As our contribution to the community, all models, configurations, and code are made open-source in ESPnet.",
        "retrieved_doc": "Author: Shinji Watanabe Title: Reducing Barriers to Self-Supervised Learning: HuBERT Pre-training with Academic Compute Publication year: 2023 Coauthors: William Chen, Xuankai Chang, Yifan Peng, Zhaoheng Ni, Soumi Maiti, Shinji Watanabe Abstract: Self-supervised learning (SSL) has led to great strides in speech processing. However, the resources needed to train these models has become prohibitively large as they continue to scale. Currently, only a few groups with substantial resources are capable of creating SSL models, which harms reproducibility. In this work, we optimize HuBERT SSL to fit in academic constraints. We reproduce HuBERT independently from the original implementation, with no performance loss. Our code and training optimizations make SSL feasible with only 8 GPUs, instead of the 32 used in the original work. We also explore a semi-supervised route, using an ASR model to skip the first pre-training iteration. Within one iteration of pre-training, our models improve over HuBERT on several tasks. Furthermore, our HuBERT Large variant requires only 8 GPUs, achieving similar performance to the original trained on 128. As our contribution to the community, all models, configurations, and code are made open-source in ESPnet.|Author: Shinji Watanabe Title: HuBERTopic: Enhancing Semantic Representation of HuBERT through Self-supervision Utilizing Topic Model Publication year: 2023 Coauthors: Takashi Maekaku, Jiatong Shi, Xuankai Chang, Yuya Fujita, Shinji Watanabe Abstract: Recently, the usefulness of self-supervised representation learning (SSRL) methods has been confirmed in various downstream tasks. Many of these models, as exemplified by HuBERT and WavLM, use pseudo-labels generated from spectral features or the model's own representation features. From previous studies, it is known that the pseudo-labels contain semantic information. However, the masked prediction task, the learning criterion of HuBERT, focuses on local contextual information and may not make effective use of global semantic information such as speaker, theme of speech, and so on. In this paper, we propose a new approach to enrich the semantic representation of HuBERT. We apply topic model to pseudo-labels to generate a topic label for each utterance. An auxiliary topic classification task is added to HuBERT by using topic labels as teachers. This allows additional global semantic information to be incorporated in an unsupervised manner. Experimental results demonstrate that our method achieves comparable or better performance than the baseline in most tasks, including automatic speech recognition and five out of the eight SUPERB tasks. Moreover, we find that topic labels include various information about utterance, such as gender, speaker, and its theme. This highlights the effectiveness of our approach in capturing multifaceted semantic nuances.|Author: Shinji Watanabe Title: Exploration on HuBERT with Multiple Resolutions Publication year: 2023 Coauthors: Jiatong Shi, Yun Tang, H. Inaguma, Hongyu Gong, J. Pino, Shinji Watanabe Abstract: Hidden-unit BERT (HuBERT) is a widely-used self-supervised learning (SSL) model in speech processing. However, we argue that its fixed 20ms resolution for hidden representations would not be optimal for various speech-processing tasks since their attributes (e.g., speaker characteristics and semantics) are based on different time scales. To address this limitation, we propose utilizing HuBERT representations at multiple resolutions for downstream tasks. We explore two approaches, namely the parallel and hierarchical approaches, for integrating HuBERT features with different resolutions. Through experiments, we demonstrate that HuBERT with multiple resolutions outperforms the original model. This highlights the potential of utilizing multiple resolutions in SSL models like HuBERT to capture diverse information from speech signals.",
        "retrieved_relevant_part": true,
        "model_answer": "William Chen, Xuankai Chang, Yifan Peng, Zhaoheng Ni, Soumi Maiti, and Shinji Watanabe",
        "actual_answer": "William Chen, Xuankai Chang, Yifan Peng, Zhaoheng Ni, Soumi Maiti, Shinji Watanabe",
        "exact_match": false,
        "f1_max_min_avg": [
            0.96,
            0.96,
            0.96
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Who are the coauthors of the paper Reasoning about the Unseen for Efficient Outdoor Object Navigation published by LTI faculty Yonatan Bisk in 2023?",
        "context": "Author: Yonatan Bisk Title: Reasoning about the Unseen for Efficient Outdoor Object Navigation Publication year: 2023 Coauthors: Quanting Xie, Tianyi Zhang, Kedi Xu, M. Johnson-Roberson, Yonatan Bisk Abstract: Robots should exist anywhere humans do: indoors, outdoors, and even unmapped environments. In contrast, the focus of recent advancements in Object Goal Navigation(OGN) has targeted navigating in indoor environments by leveraging spatial and semantic cues that do not generalize outdoors. While these contributions provide valuable insights into indoor scenarios, the broader spectrum of real-world robotic applications often extends to outdoor settings. As we transition to the vast and complex terrains of outdoor environments, new challenges emerge. Unlike the structured layouts found indoors, outdoor environments lack clear spatial delineations and are riddled with inherent semantic ambiguities. Despite this, humans navigate with ease because we can reason about the unseen. We introduce a new task OUTDOOR, a new mechanism for Large Language Models (LLMs) to accurately hallucinate possible futures, and a new computationally aware success metric for pushing research forward in this more complex domain. Additionally, we show impressive results on both a simulated drone and physical quadruped in outdoor environments. Our agent has no premapping and our formalism outperforms naive LLM-based approaches",
        "retrieved_doc": "Author: Yonatan Bisk Title: Reasoning about the Unseen for Efficient Outdoor Object Navigation Publication year: 2023 Coauthors: Quanting Xie, Tianyi Zhang, Kedi Xu, M. Johnson-Roberson, Yonatan Bisk Abstract: Robots should exist anywhere humans do: indoors, outdoors, and even unmapped environments. In contrast, the focus of recent advancements in Object Goal Navigation(OGN) has targeted navigating in indoor environments by leveraging spatial and semantic cues that do not generalize outdoors. While these contributions provide valuable insights into indoor scenarios, the broader spectrum of real-world robotic applications often extends to outdoor settings. As we transition to the vast and complex terrains of outdoor environments, new challenges emerge. Unlike the structured layouts found indoors, outdoor environments lack clear spatial delineations and are riddled with inherent semantic ambiguities. Despite this, humans navigate with ease because we can reason about the unseen. We introduce a new task OUTDOOR, a new mechanism for Large Language Models (LLMs) to accurately hallucinate possible futures, and a new computationally aware success metric for pushing research forward in this more complex domain. Additionally, we show impressive results on both a simulated drone and physical quadruped in outdoor environments. Our agent has no premapping and our formalism outperforms naive LLM-based approaches|Yonatan Bisk Assistant Professor Email: ybisk@cs.cmu.edu Office: Gates & Hillman Centers Research Areas: Grounding, RoboNLP, Vision and Language, Embodiment, Unsupervised Learning|Author: Yonatan Bisk Title: EXCALIBUR: Encouraging and Evaluating Embodied Exploration Publication year: 2023 Coauthors: Hao Zhu, Raghav Kapoor, So Yeon Min, Winson Han, Jiatai Li, Kaiwen Geng, Graham Neubig, Yonatan Bisk, Aniruddha Kembhavi, Luca Weihs Abstract: Experience precedes understanding. Humans constantly explore and learn about their environment out of curiosity, gather information, and update their models of the world. On the other hand, machines are either trained to learn passively from static and fixed datasets, or taught to complete specific goal-conditioned tasks. To encourage the development of exploratory interactive agents, we present the EXCALIBUR benchmark. EXCALIBUR allows agents to explore their environment for long durations and then query their understanding of the physical world via inquiries like: \u201cis the small heavy red bowl made from glass?\u201d or \u201cis there a silver spoon heavier than the egg?\u201d. This design encourages agents to perform free-form home exploration without myopia induced by goal conditioning. Once the agents have answered a series of questions, they can renter the scene to refine their knowledge, update their beliefs, and improve their performance on the questions. Our experiments demonstrate the challenges posed by this dataset for the present-day state-of-the-art embodied systems and the headroom afforded to develop new innovative methods. Finally, we present a virtual reality interface that enables humans to seamlessly interact within the simulated world and use it to gather human performance measures. EXCALIBUR affords unique challenges in comparison to presentday benchmarks and represents the next frontier for embodied AI research.",
        "retrieved_relevant_part": true,
        "model_answer": "Quanting Xie, Tianyi Zhang, Kedi Xu, M. Johnson-Roberson, and Yonatan Bisk",
        "actual_answer": "Quanting Xie, Tianyi Zhang, Kedi Xu, M. Johnson-Roberson, Yonatan Bisk",
        "exact_match": false,
        "f1_max_min_avg": [
            0.95,
            0.95,
            0.95
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Who are the coauthors of the paper Leveraging body pose estimation for gesture recognition in human-robot interaction using synthetic data published by LTI faculty Alexander Hauptmann in 2023?",
        "context": "Author: Alexander Hauptmann Title: Leveraging body pose estimation for gesture recognition in human-robot interaction using synthetic data Publication year: 2023 Coauthors: Xiaoyu Zhu, Celso de Melo, Alexander Hauptmann Abstract: Effectively recognizing human gestures from variant viewpoints plays a fundamental role in the successful collaboration between humans and robots. Deep learning approaches have achieved promising performance in gesture recognition. However, they are usually data-hungry and require large-scale labeled data, which are not usually accessible in a practical setting. Synthetic data, on the other hand, can be easily obtained from simulators with fine-grained annotations and variant modalities. Existing state-of-the-art approaches have shown promising results using synthetic data, but there is still a large performance gap between the models trained on synthetic data and real data. To learn domain-invariant feature representations, we propose a novel approach which jointly takes RGB videos and 3D meshes as inputs to perform robust action recognition. We empirically validate our model on the RoCoG-v2 dataset, which consists of a variety of real and synthetic videos of gestures from the ground and air perspectives. We show that our model trained on synthetic data can outperform state-of-the-art models under the same training setting and models trained on real data.",
        "retrieved_doc": "Author: Alexander Hauptmann Title: Leveraging body pose estimation for gesture recognition in human-robot interaction using synthetic data Publication year: 2023 Coauthors: Xiaoyu Zhu, Celso de Melo, Alexander Hauptmann Abstract: Effectively recognizing human gestures from variant viewpoints plays a fundamental role in the successful collaboration between humans and robots. Deep learning approaches have achieved promising performance in gesture recognition. However, they are usually data-hungry and require large-scale labeled data, which are not usually accessible in a practical setting. Synthetic data, on the other hand, can be easily obtained from simulators with fine-grained annotations and variant modalities. Existing state-of-the-art approaches have shown promising results using synthetic data, but there is still a large performance gap between the models trained on synthetic data and real data. To learn domain-invariant feature representations, we propose a novel approach which jointly takes RGB videos and 3D meshes as inputs to perform robust action recognition. We empirically validate our model on the RoCoG-v2 dataset, which consists of a variety of real and synthetic videos of gestures from the ground and air perspectives. We show that our model trained on synthetic data can outperform state-of-the-art models under the same training setting and models trained on real data.|Author: Yonatan Bisk Title: Open X-Embodiment: Robotic Learning Datasets and RT-X Models Publication year: 2023 Coauthors: A. Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, A. Irpan, Alexander Khazatsky, Anant Rai, Anika Singh, Anthony Brohan, A. Raffin, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Sch\u00f6lkopf, Brian Ichter, Cewu Lu, Charles Xu, Chelsea Finn, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Chuer Pan, Chuyuan Fu, Coline Devin, Danny Driess, Deepak Pathak, Dhruv Shah, Dieter B\u00fcchler, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Federico Ceola, Fei Xia, F. Stulp, Gaoyue Zhou, G. Sukhatme, G. Salhotra, Ge Yan, Giulio Schiavi, G. Kahn, Hao Su, Haoshu Fang, Haochen Shi, H. B. Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jan Peters, Jan Schneider, Jasmine Hsu, J. Bohg, Jeff Bingham, Jiajun Wu, Jialin Wu, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jitendra Malik, Jonathan Tompson, Jonathan Yang, Joseph J. Lim, Jo\u00e3o Silv\u00e9rio, Junhyek Han, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, K. Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Zhang, Krishan Rana, K. Srinivasan, Lawrence Yunliang Chen, Lerrel Pinto, Liam Tan, Lionel Ott, Lisa Lee, Masayoshi Tomizuka, Maximilian Du, Michael Ahn, Mingtong Zhang, Mingyu Ding, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, N. Heess, Nikhil Joshi, Niko Suenderhauf, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, P. Sanketi, Paul Wohlhart, Peng Xu, P. Sermanet, Priya Sundaresan, Q. Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan C. Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham D. Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, S. Adebola, Simon Guist, Soroush Nasiriany, S. Schaal, Stefan Welker, Stephen Tian, Sudeep Dasari, Suneel Belkhale, Takayuki Osa, Tatsuya Harada, T. Matsushima, Ted Xiao, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Zhao, Travis Armstrong, Trevor Darrell, Vidhi Jain, Vincent Vanhoucke, W. Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xuanlin Li, Yao Lu, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yueh-hua Wu, Yujin Tang, Yuke Zhu, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zhuo Xu, Zichen Jeff Cui Abstract: \u2014Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer|Author: Yonatan Bisk Title: Open X-Embodiment: Robotic Learning Datasets and RT-X Models Publication year: 2023 Coauthors: A. Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, A. Irpan, Alexander Khazatsky, Anant Rai, Anika Singh, Anthony Brohan, A. Raffin, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Sch\u00f6lkopf, Brian Ichter, Cewu Lu, Charles Xu, Chelsea Finn, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Chuer Pan, Chuyuan Fu, Coline Devin, Danny Driess, Deepak Pathak, Dhruv Shah, Dieter B\u00fcchler, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Federico Ceola, Fei Xia, F. Stulp, Gaoyue Zhou, G. Sukhatme, G. Salhotra, Ge Yan, Giulio Schiavi, G. Kahn, Hao Su, Haoshu Fang, Haochen Shi, H. B. Amor, Henrik I Christensen, Hiroki Furuta, Homer Walke, Hongjie Fang, Igor Mordatch, Ilija Radosavovic, Isabel Leal, Jacky Liang, Jad Abou-Chakra, Jaehyung Kim, Jan Peters, Jan Schneider, Jasmine Hsu, J. Bohg, Jeff Bingham, Jiajun Wu, Jialin Wu, Jianlan Luo, Jiayuan Gu, Jie Tan, Jihoon Oh, Jitendra Malik, Jonathan Tompson, Jonathan Yang, Joseph J. Lim, Jo\u00e3o Silv\u00e9rio, Junhyek Han, Kanishka Rao, Karl Pertsch, Karol Hausman, Keegan Go, K. Gopalakrishnan, Ken Goldberg, Kendra Byrne, Kenneth Oslund, Kento Kawaharazuka, Kevin Zhang, Krishan Rana, K. Srinivasan, Lawrence Yunliang Chen, Lerrel Pinto, Liam Tan, Lionel Ott, Lisa Lee, Masayoshi Tomizuka, Maximilian Du, Michael Ahn, Mingtong Zhang, Mingyu Ding, Mohan Kumar Srirama, Mohit Sharma, Moo Jin Kim, Naoaki Kanazawa, Nicklas Hansen, N. Heess, Nikhil Joshi, Niko Suenderhauf, Norman Di Palo, Nur Muhammad Mahi Shafiullah, Oier Mees, Oliver Kroemer, P. Sanketi, Paul Wohlhart, Peng Xu, P. Sermanet, Priya Sundaresan, Q. Vuong, Rafael Rafailov, Ran Tian, Ria Doshi, Russell Mendonca, Rutav Shah, Ryan Hoque, Ryan C. Julian, Samuel Bustamante, Sean Kirmani, Sergey Levine, Sherry Moore, Shikhar Bahl, Shivin Dass, Shubham D. Sonawani, Shuran Song, Sichun Xu, Siddhant Haldar, S. Adebola, Simon Guist, Soroush Nasiriany, S. Schaal, Stefan Welker, Stephen Tian, Sudeep Dasari, Suneel Belkhale, Takayuki Osa, Tatsuya Harada, T. Matsushima, Ted Xiao, Tianhe Yu, Tianli Ding, Todor Davchev, Tony Zhao, Travis Armstrong, Trevor Darrell, Vidhi Jain, Vincent Vanhoucke, W. Zhan, Wenxuan Zhou, Wolfram Burgard, Xi Chen, Xiaolong Wang, Xinghao Zhu, Xuanlin Li, Yao Lu, Yevgen Chebotar, Yifan Zhou, Yifeng Zhu, Ying Xu, Yixuan Wang, Yonatan Bisk, Yoonyoung Cho, Youngwoon Lee, Yuchen Cui, Yueh-hua Wu, Yujin Tang, Yuke Zhu, Yunzhu Li, Yusuke Iwasawa, Yutaka Matsuo, Zhuo Xu, Zichen Jeff Cui Abstract: \u2014Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer",
        "retrieved_relevant_part": true,
        "model_answer": "The coauthors of the paper are A. Padalkar, Acorn Pooley, Ajinkya Jain, Alex Bewley, Alex Herzog, A. Irpan, Alexander Khazatsky, Anant Rai, Anika Singh, Anthony Brohan, A. Raffin, Ayzaan Wahid, Ben Burgess-Limerick, Beomjoon Kim, Bernhard Sch\u00f6lkopf, Brian Ichter, Cewu Lu, Charles Xu, Chelsea Finn, Chenfeng Xu, Cheng Chi, Chenguang Huang, Christine Chan, Chuer Pan, Chuyuan Fu, Coline Devin, Daniel B\u00fcchler, Dmitry Kalashnikov, Dorsa Sadigh, Edward Johns, Federico Ceola, Fei Xia, F. Stulp, Gaoyue Zhou, G. Sukhatme, G. Salhotra, Ge Yan, Giulio Schiavi, G. Kahn, Hao Su<|endoftext|>",
        "actual_answer": "Xiaoyu Zhu, Celso de Melo, Alexander Hauptmann",
        "exact_match": false,
        "f1_max_min_avg": [
            0.02,
            0.02,
            0.02
        ],
        "recall_max_min_avg": [
            0.14,
            0.14,
            0.14
        ]
    },
    {
        "question": "Which LTI faculty published the paper The DARPA Wikidata Overlay: Wikidata as an ontology for natural language processing in 2023?",
        "context": "Author: A. Gershman Title: The DARPA Wikidata Overlay: Wikidata as an ontology for natural language processing Publication year: 2023 Coauthors: Elizabeth Spaulding, Kathryn Conger, A. Gershman, Rosario Uceda-Sosa, S. Brown, James Pustejovsky, Peter Anick, Martha Palmer Abstract: With 102,530,067 items currently in its crowd-sourced knowledge base, Wikidata provides NLP practitioners a unique and powerful resource for inference and reasoning over real-world entities. However, because Wikidata is very entity focused, events and actions are often labeled with eventive nouns (e.g., the process of diagnosing a person\u2019s illness is labeled \u201cdiagnosis\u201d), and the typical participants in an event are not described or linked to that event concept (e.g., the medical professional or patient). Motivated by a need for an adaptable, comprehensive, domain-flexible ontology for information extraction, including identifying the roles entities are playing in an event, we present a curated subset of Wikidata in which events have been enriched with PropBank roles. To enable richer narrative understanding between events from Wikidata concepts, we have also provided a comprehensive mapping from temporal Qnodes and Pnodes to the Allen Interval Temporal Logic relations.",
        "retrieved_doc": "Author: A. Gershman Title: The DARPA Wikidata Overlay: Wikidata as an ontology for natural language processing Publication year: 2023 Coauthors: Elizabeth Spaulding, Kathryn Conger, A. Gershman, Rosario Uceda-Sosa, S. Brown, James Pustejovsky, Peter Anick, Martha Palmer Abstract: With 102,530,067 items currently in its crowd-sourced knowledge base, Wikidata provides NLP practitioners a unique and powerful resource for inference and reasoning over real-world entities. However, because Wikidata is very entity focused, events and actions are often labeled with eventive nouns (e.g., the process of diagnosing a person\u2019s illness is labeled \u201cdiagnosis\u201d), and the typical participants in an event are not described or linked to that event concept (e.g., the medical professional or patient). Motivated by a need for an adaptable, comprehensive, domain-flexible ontology for information extraction, including identifying the roles entities are playing in an event, we present a curated subset of Wikidata in which events have been enriched with PropBank roles. To enable richer narrative understanding between events from Wikidata concepts, we have also provided a comprehensive mapping from temporal Qnodes and Pnodes to the Allen Interval Temporal Logic relations.|LTI Ph.D. Graduate Student Handbook Page 10 This handbook applies to the LTI Ph.D. The Ph.D. in Language and Information Technologies (LTI Ph.D. ) is focused on understanding and extending the state of the art in computational linguistics, natural language processing, dialogue systems, information retrieval, machine translation, speech processing, video understanding, multimodal systems, automated reasoning, and other topics related to analysis and understanding of unstructured information (e.g., machine learning, and software engineering of intelligent systems). 1.2 Department Personnel The people responsible for administering the LTI Ph.D. degree are: Jamie Callan Ph.D. Program Director Professor GHC 5419 callan@cs.cmu.edu 412-268-4525 Stacey Young Ph.D. Academic Program Manager LTI Graduate Program Manager GHC 6415 staceyy@cs.cmu.edu 412-268-2623 Mona Diab LTI Director Professor GHC 5723 mdiab@andrew.cmu.edu 412-268-3669 Joan Axelson Office Manager GHC 5405 jaxelson@andrew.cmu.edu 412-268-7517 Julie Nys Employment Processes Manger GHC 5405 jnys@andrew.cmu.edu 412-268-3515 1.3 University Policies and Expectations It is the responsibility of each member of the Carnegie Mellon community to be familiar with university policies and guidelines. In addition to this departmental graduate student handbook, the following resources are available to assist you in understanding community expectations. \u2022 The Word/Student Handbook: https://www.cmu.edu/student-a ffairs/theword//index.html \u2022 Academic Integrity Policy: https:// www.cmu.edu/policies/student-and-student-life/academic-integrity.html|Author: Alexander I. Rudnicky Title: Tartan: an LLM Driven SocialBot Publication year: 2023 Coauthors: Liangze Li, Ziyuan Liu, Li-Wei Chen, Ta-Chung Chi, Alexander I. Rudnicky Abstract: We describe our work to date on using large language models (LLMs) in our Alexa Prize Socialbot. Our work has laid the groundwork for looking more closely at using LLMs in a conversational system. We also analyze common patterns in conversations our bot has had with users",
        "retrieved_relevant_part": true,
        "model_answer": "A. Gershman",
        "actual_answer": "A. Gershman",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Who is the author of the LTI paper Divergences between Language Models and Human Brains published in 2023?",
        "context": "Author: Graham Neubig Title: Divergences between Language Models and Human Brains Publication year: 2023 Coauthors: Yuchen Zhou, Emmy Liu, Graham Neubig, Leila Wehbe Abstract: Do machines and humans process language in similar ways? Recent research has hinted in the affirmative, finding that brain signals can be effectively predicted using the internal representations of language models (LMs). Although such results are thought to reflect shared computational principles between LMs and human brains, there are also clear differences in how LMs and humans represent and use language. In this work, we systematically explore the divergences between human and machine language processing by examining the differences between LM representations and human brain responses to language as measured by Magnetoencephalography (MEG) across two datasets in which subjects read and listened to narrative stories. Using a data-driven approach, we identify two domains that are not captured well by LMs: social/emotional intelligence and physical commonsense. We then validate these domains with human behavioral experiments and show that fine-tuning LMs on these domains can improve their alignment with human brain responses.",
        "retrieved_doc": "Author: Graham Neubig Title: Divergences between Language Models and Human Brains Publication year: 2023 Coauthors: Yuchen Zhou, Emmy Liu, Graham Neubig, Leila Wehbe Abstract: Do machines and humans process language in similar ways? Recent research has hinted in the affirmative, finding that brain signals can be effectively predicted using the internal representations of language models (LMs). Although such results are thought to reflect shared computational principles between LMs and human brains, there are also clear differences in how LMs and humans represent and use language. In this work, we systematically explore the divergences between human and machine language processing by examining the differences between LM representations and human brain responses to language as measured by Magnetoencephalography (MEG) across two datasets in which subjects read and listened to narrative stories. Using a data-driven approach, we identify two domains that are not captured well by LMs: social/emotional intelligence and physical commonsense. We then validate these domains with human behavioral experiments and show that fine-tuning LMs on these domains can improve their alignment with human brain responses.|Author: Mona T. Diab Title: The Troubling Emergence of Hallucination in Large Language Models - An Extensive Definition, Quantification, and Prescriptive Remediations Publication year: 2023 Coauthors: Vipula Rawte, Swagata Chakraborty, Agnibh Pathak, Anubhav Sarkar, S.M. Towhidul Islam Tonmoy, Islam Tonmoy, Aman Chadha, Amit P. Sheth, Amitava Das, Paris, A. Sridhar, Erik Visser, Improved, Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, Yunfeng Liu, Roformer, Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, Tatsunori Hashimoto, Stanford, Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timoth\u00e9e Lacroix, Baptiste Rozi\u00e8re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R\u00e9mi Louf, Morgan Funtow-icz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, Susan Zhang, Stephen Roller, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher De-wan, Mona T. Diab, Xi Xian Li, Todor Victoria Lin, Myle Ott, Kurt Shuster, Punit Daniel Simig, Singh Koura, Anjali Sridhar, Tianlu Wang, Luke Zettlemoyer. 2022, Daniel M. Ziegler, Nisan Stiennon, Jeffrey Wu, Tom Brown, Alec Radford, Dario Amodei, Paul F. Chris-tiano Abstract: The recent advancements in Large Language Models (LLMs) have garnered widespread acclaim for their remarkable emerging capabilities. However, the issue of hallucination has parallelly emerged as a by-product, posing significant concerns. While some recent endeavors have been made to identify and mitigate different types of hallucination, there has been a limited emphasis on the nuanced categorization of hallucination and associated mitigation methods. To address this gap, we offer a fine-grained discourse on profiling hallucination based on its degree, orientation, and category, along with offering strategies for alleviation. As such, we define two overarching orientations of hallucination: (i) factual mirage (FM) and (ii) silver lining (SL). To provide a more comprehensive understanding, both orientations are further sub-categorized into intrinsic and extrinsic, with three degrees of severity - (i) mild, (ii) moderate, and (iii) alarming. We also meticulously categorize hallucination into six types: (i) acronym ambiguity, (ii) numeric nuisance, (iii) generated golem, (iv) virtual voice, (v) geographic erratum, and (vi) time wrap. Furthermore, we curate HallucInation eLiciTation (HILT), a publicly available dataset comprising of 75,000 samples generated using 15 contemporary LLMs along with human annotations for the aforementioned categories. Finally, to establish a method for quantifying and to offer a comparative spectrum that allows us to evaluate and rank LLMs based on their vulnerability to producing hallucinations, we propose Hallucination Vulnerability Index (HVI). We firmly believe that HVI holds significant value as a tool for the wider NLP community, with the potential to serve as a rubric in AI-related policy-making. In conclusion, we propose two solution strategies for mitigating hallucinations.|Author: Maarten Sap Title: Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models Publication year: 2023 Coauthors: Natalie Shapira, Mosh Levy, S. Alavi, Xuhui Zhou, Yejin Choi, Yoav Goldberg, Maarten Sap, Vered Shwartz Abstract: The escalating debate on AI's capabilities warrants developing reliable metrics to assess machine\"intelligence\". Recently, many anecdotal examples were used to suggest that newer large language models (LLMs) like ChatGPT and GPT-4 exhibit Neural Theory-of-Mind (N-ToM); however, prior work reached conflicting conclusions regarding those abilities. We investigate the extent of LLMs' N-ToM through an extensive evaluation on 6 tasks and find that while LLMs exhibit certain N-ToM abilities, this behavior is far from being robust. We further examine the factors impacting performance on N-ToM tasks and discover that LLMs struggle with adversarial examples, indicating reliance on shallow heuristics rather than robust ToM abilities. We caution against drawing conclusions from anecdotal examples, limited benchmark testing, and using human-designed psychological tests to evaluate models.",
        "retrieved_relevant_part": true,
        "model_answer": "Graham Neubig",
        "actual_answer": "Graham Neubig",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "What is the abstract of the paper Active Retrieval Augmented Generation published by LTI faculty Jamie Callan in 2023?",
        "context": "Author: Jamie Callan Title: Active Retrieval Augmented Generation Publication year: 2023 Coauthors: Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig Abstract: Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.",
        "retrieved_doc": "Author: Jamie Callan Title: Active Retrieval Augmented Generation Publication year: 2023 Coauthors: Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig Abstract: Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.|Author: Yiming Yang Title: Active Retrieval Augmented Generation Publication year: 2023 Coauthors: Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig Abstract: Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.|Author: Graham Neubig Title: Active Retrieval Augmented Generation Publication year: 2023 Coauthors: Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig Abstract: Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.",
        "retrieved_relevant_part": true,
        "model_answer": "The paper provides a generalized view of active retrieval augmented generation and proposes a method called Forward-Looking Active REtrieval augmented generation (FLARE)",
        "actual_answer": "Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.",
        "exact_match": false,
        "f1_max_min_avg": [
            0.13,
            0.13,
            0.13
        ],
        "recall_max_min_avg": [
            0.07,
            0.07,
            0.07
        ]
    },
    {
        "question": "Who is the author of the LTI paper RedCoast: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU/TPUs published in 2023?",
        "context": "Author: Eric P. Xing Title: RedCoast: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU/TPUs Publication year: 2023 Coauthors: Bowen Tan, Yun Zhu, Lijuan Liu, Hongyi Wang, Yonghao Zhuang, Jindong Chen, Eric P. Xing, Zhiting Hu Abstract: The recent progress of AI can be largely attributed to large language models (LLMs). However, their escalating memory requirements introduce challenges for machine learning (ML) researchers and engineers. Addressing this requires developers to partition a large model to distribute it across multiple GPUs or TPUs. This necessitates considerable coding and intricate configuration efforts with existing model parallel tools, such as Megatron-LM, DeepSpeed, and Alpa. These tools require users' expertise in machine learning systems (MLSys), creating a bottleneck in LLM development, particularly for developers without MLSys background. In this work, we present RedCoast(Redco), a lightweight and user-friendly tool crafted to automate distributed training and inference for LLMs, as well as to simplify ML pipeline development. The design of Redco emphasizes two key aspects. Firstly, to automate model parallism, our study identifies two straightforward rules to generate tensor parallel strategies for any given LLM. Integrating these rules into Redco facilitates effortless distributed LLM training and inference, eliminating the need of additional coding or complex configurations. We demonstrate the effectiveness by applying Redco on a set of LLM architectures, such as GPT-J, LLaMA, T5, and OPT, up to the size of 66B. Secondly, we propose a mechanism that allows for the customization of diverse ML pipelines through the definition of merely three functions, avoiding redundant and formulaic code like multi-host related processing. This mechanism proves adaptable across a spectrum of ML algorithms, from foundational language modeling to complex algorithms like meta-learning and reinforcement learning. Consequently, Redco implementations exhibit much fewer code lines compared to their official counterparts.",
        "retrieved_doc": "Author: Eric P. Xing Title: RedCoast: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU/TPUs Publication year: 2023 Coauthors: Bowen Tan, Yun Zhu, Lijuan Liu, Hongyi Wang, Yonghao Zhuang, Jindong Chen, Eric P. Xing, Zhiting Hu Abstract: The recent progress of AI can be largely attributed to large language models (LLMs). However, their escalating memory requirements introduce challenges for machine learning (ML) researchers and engineers. Addressing this requires developers to partition a large model to distribute it across multiple GPUs or TPUs. This necessitates considerable coding and intricate configuration efforts with existing model parallel tools, such as Megatron-LM, DeepSpeed, and Alpa. These tools require users' expertise in machine learning systems (MLSys), creating a bottleneck in LLM development, particularly for developers without MLSys background. In this work, we present RedCoast(Redco), a lightweight and user-friendly tool crafted to automate distributed training and inference for LLMs, as well as to simplify ML pipeline development. The design of Redco emphasizes two key aspects. Firstly, to automate model parallism, our study identifies two straightforward rules to generate tensor parallel strategies for any given LLM. Integrating these rules into Redco facilitates effortless distributed LLM training and inference, eliminating the need of additional coding or complex configurations. We demonstrate the effectiveness by applying Redco on a set of LLM architectures, such as GPT-J, LLaMA, T5, and OPT, up to the size of 66B. Secondly, we propose a mechanism that allows for the customization of diverse ML pipelines through the definition of merely three functions, avoiding redundant and formulaic code like multi-host related processing. This mechanism proves adaptable across a spectrum of ML algorithms, from foundational language modeling to complex algorithms like meta-learning and reinforcement learning. Consequently, Redco implementations exhibit much fewer code lines compared to their official counterparts.|Author: Eric P. Xing Title: RedCoast: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU/TPUs Publication year: 2023 Coauthors: Bowen Tan, Yun Zhu, Lijuan Liu, Hongyi Wang, Yonghao Zhuang, Jindong Chen, Eric P. Xing, Zhiting Hu Abstract: The recent progress of AI can be largely attributed to large language models (LLMs). However, their escalating memory requirements introduce challenges for machine learning (ML) researchers and engineers. Addressing this requires developers to partition a large model to distribute it across multiple GPUs or TPUs. This necessitates considerable coding and intricate configuration efforts with existing model parallel tools, such as Megatron-LM, DeepSpeed, and Alpa. These tools require users' expertise in machine learning systems (MLSys), creating a bottleneck in LLM development, particularly for developers without MLSys background. In this work, we present RedCoast(Redco), a lightweight and user-friendly tool crafted to automate distributed training and inference for LLMs, as well as to simplify ML pipeline development. The design of Redco emphasizes two key aspects. Firstly, to automate model parallism, our study identifies two straightforward rules to generate tensor parallel strategies for any given LLM. Integrating these rules into Redco facilitates effortless distributed LLM training and inference, eliminating the need of additional coding or complex configurations. We demonstrate the effectiveness by applying Redco on a set of LLM architectures, such as GPT-J, LLaMA, T5, and OPT, up to the size of 66B. Secondly, we propose a mechanism that allows for the customization of diverse ML pipelines through the definition of merely three functions, avoiding redundant and formulaic code like multi-host related processing. This mechanism proves adaptable across a spectrum of ML algorithms, from foundational language modeling to complex algorithms like meta-learning and reinforcement learning. Consequently, Redco implementations exhibit much fewer code lines compared to their official counterparts.|Author: Eric P. Xing Title: Redco: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU/TPUs Publication year: 2023 Coauthors: Bowen Tan, Yun Zhu, Lijuan Liu, Hongyi Wang, Yonghao Zhuang, Jindong Chen, Eric P. Xing, Zhiting Hu Abstract: .",
        "retrieved_relevant_part": true,
        "model_answer": "Eric P. Xing",
        "actual_answer": "Eric P. Xing",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "Who are the coauthors of the paper Analysis of Volatile Components in Dried Fruits and Branch Exudates of Schisandra chinensis with Different Fruit Colors Using GC-IMS Technology published by LTI faculty Yiming Yang in 2023?",
        "context": "Author: Yiming Yang Title: Analysis of Volatile Components in Dried Fruits and Branch Exudates of Schisandra chinensis with Different Fruit Colors Using GC-IMS Technology Publication year: 2023 Coauthors: Yiping Yan, Wenpeng Lu, Taiping Tian, Nan Shu, Yiming Yang, Shutian Fan, Xianyan Han, Yunhua Ge, Peilei Xu Abstract: To investigate the volatile components of Schisandra chinensis (Turcz.) Bail (commonly known as northern Schisandra) of different colors and to explore their similarities and differences, to identify the main flavor substances in the volatile components of the branch exudates of northern schisandra, and finally to establish a fingerprint map of the volatile components of the dried fruits and branch exudates of northern Schisandra of different colors, we used GC-IMS technology to analyze the volatile components of the dried fruits and branch exudates of three different colors of northern Schisandra and established a fingerprint spectra. The results showed that a total of 60 different volatile chemical components were identified in the branch exudates and dried fruits of Schisandra. The components of germplasm resources with different fruit colors were significantly different. The ion mobility spectrum and OPLS-DA results showed that white and yellow fruits were more similar compared to red fruits. The volatile components in dried fruits were significantly higher than those in branch exudates. After VIP (variable importance in projection) screening, 41 key volatile substances in dried fruits and 30 key volatile substances in branch exudates were obtained. After screening by odor activity value (OAV), there were 24 volatile components greater than 1 in both dried fruits and branch exudates. The most important contributing volatile substance was 3-methyl-butanal, and the most important contributing volatile substance in white fruit was (E)-2-hexenal.",
        "retrieved_doc": "Author: Yiming Yang Title: Analysis of Volatile Components in Dried Fruits and Branch Exudates of Schisandra chinensis with Different Fruit Colors Using GC-IMS Technology Publication year: 2023 Coauthors: Yiping Yan, Wenpeng Lu, Taiping Tian, Nan Shu, Yiming Yang, Shutian Fan, Xianyan Han, Yunhua Ge, Peilei Xu Abstract: To investigate the volatile components of Schisandra chinensis (Turcz.) Bail (commonly known as northern Schisandra) of different colors and to explore their similarities and differences, to identify the main flavor substances in the volatile components of the branch exudates of northern schisandra, and finally to establish a fingerprint map of the volatile components of the dried fruits and branch exudates of northern Schisandra of different colors, we used GC-IMS technology to analyze the volatile components of the dried fruits and branch exudates of three different colors of northern Schisandra and established a fingerprint spectra. The results showed that a total of 60 different volatile chemical components were identified in the branch exudates and dried fruits of Schisandra. The components of germplasm resources with different fruit colors were significantly different. The ion mobility spectrum and OPLS-DA results showed that white and yellow fruits were more similar compared to red fruits. The volatile components in dried fruits were significantly higher than those in branch exudates. After VIP (variable importance in projection) screening, 41 key volatile substances in dried fruits and 30 key volatile substances in branch exudates were obtained. After screening by odor activity value (OAV), there were 24 volatile components greater than 1 in both dried fruits and branch exudates. The most important contributing volatile substance was 3-methyl-butanal, and the most important contributing volatile substance in white fruit was (E)-2-hexenal.|Author: Yiming Yang Title: Comprehensive evaluation of nine grape varieties based on fundamental physical and chemical indicators, color and volatile compounds Publication year: 2023 Coauthors: W. Cao, Nan Shu, Yiming Yang, Jinli Wen, Wenpeng Lu Abstract: BACKGROUND: In todays\u2019 society, the rapid development of the wine industry and the improvement of peoples\u2019 living standards make people pay more and more attention to wine grape quality. OBJECTIVE: To evaluate the wine grape varieties in Northeast Chinas\u2019 grape growing regions for better wine grape quality, we evaluated the quality of different varieties of wine grapes. METHODS: The grape varieties \u2018Hassan\u2019 \u2018Zuoshaner\u2019 \u2018Beibinghong\u2019 \u2018Zuoyouhong\u2019 \u2018Beta\u2019 \u2018Shuanghong\u2019 \u2018Zijingganlu\u2019 \u2018Cabernet Sauvignon\u2019 and \u2018Syrah\u2019 were planted in the grape growing area of Jilin, Northeast China, were used as the subjects of this study. The grape berries were analyzed and tested for morphological indicators, basic physicochemical indicators, color, and phenolic and aromatic composition. RESULTS: According to lab results, \u2018Hassan\u2019 contained the highest amount of total phenolics; \u2018Zuoyouhong\u2019 had the highest solids and total sugar content; \u2018Shuanghong\u2019 had the most elevated total acid and anthocyanin content; \u2018Zijngganlu\u2019 had the highest tannin content and acid fixation ratio; Seventy-one volatile compounds were detected in nine grape varieties. CONCLUSIONS: Each of the nine grape varieties has a distinctive flavor, and because of this, grape processing products with regional flavors can be created. The same offer valuable data for future scientific grape resource collection, conservation, and exploitation.|Author: Yiming Yang Title: Effects of S Content on Inclusion Formation in the Al and Ti\u2013Mg Complex Deoxidized Steel Publication year: 2023 Coauthors: Pengliang Jin, Yiming Yang, Lei Cao, Xinghu Yuan, Guocheng Wang Abstract: Two groups of Al and Ti\u2013Mg complex deoxidized steels with different S contents are designed, and inclusion characteristics of two groups of steel samples are observed by field\u2010emission scanning electron microscopy\u2013energy\u2010dispersive spectroscopy. The results show that there are single TiS inclusions, MgAl2O4 (Al2O3)\u2013TiS, and MgAl2O4\u2013TiN\u2013TiS complex inclusions in No. 1 steel (low\u2010sulfur content) and No. 2 steel (high\u2010sulfur content). However, there are also complex inclusions containing MnS in the No. 2 steel but not in No. 1 steel. In order to reveal the precipitation mechanism of MnS, equilibrium phase of inclusion from 1873\u2009K to liquidus temperature is further analyzed, and the mass fractions of different inclusions from liquidus to solidus temperature are quantitatively calculated using the element segregation model combined with FactSage 7.2 thermodynamic software. Furthermore, the mismatch values between different crystal planes of MnS, TiS, and TiN are calculated. The results show that MnS (110) is most likely to precipitate on TiS (001), which is consistent with the observation that there is TiS\u2013MnS interface in the complex inclusions containing MnS in No. 2 steel. This study could be helpful to the controlling sulfide precipitation in Al and Ti\u2013Mg complex deoxidized steel.",
        "retrieved_relevant_part": true,
        "model_answer": "The coauthors of the paper are Yiping Yan, Wenpeng Lu, Taiping Tian, Nan Shu, Shutian Fan, Xianyan Han, Yunhua Ge, Peilei Xu, and W. Cao",
        "actual_answer": "Yiping Yan, Wenpeng Lu, Taiping Tian, Nan Shu, Yiming Yang, Shutian Fan, Xianyan Han, Yunhua Ge, Peilei Xu",
        "exact_match": false,
        "f1_max_min_avg": [
            0.7,
            0.7,
            0.7
        ],
        "recall_max_min_avg": [
            0.83,
            0.83,
            0.83
        ]
    },
    {
        "question": "Who is the author of the LTI paper RedCoast: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU/TPUs published in 2023?",
        "context": "Author: Eric P. Xing Title: RedCoast: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU/TPUs Publication year: 2023 Coauthors: Bowen Tan, Yun Zhu, Lijuan Liu, Hongyi Wang, Yonghao Zhuang, Jindong Chen, Eric P. Xing, Zhiting Hu Abstract: The recent progress of AI can be largely attributed to large language models (LLMs). However, their escalating memory requirements introduce challenges for machine learning (ML) researchers and engineers. Addressing this requires developers to partition a large model to distribute it across multiple GPUs or TPUs. This necessitates considerable coding and intricate configuration efforts with existing model parallel tools, such as Megatron-LM, DeepSpeed, and Alpa. These tools require users' expertise in machine learning systems (MLSys), creating a bottleneck in LLM development, particularly for developers without MLSys background. In this work, we present RedCoast(Redco), a lightweight and user-friendly tool crafted to automate distributed training and inference for LLMs, as well as to simplify ML pipeline development. The design of Redco emphasizes two key aspects. Firstly, to automate model parallism, our study identifies two straightforward rules to generate tensor parallel strategies for any given LLM. Integrating these rules into Redco facilitates effortless distributed LLM training and inference, eliminating the need of additional coding or complex configurations. We demonstrate the effectiveness by applying Redco on a set of LLM architectures, such as GPT-J, LLaMA, T5, and OPT, up to the size of 66B. Secondly, we propose a mechanism that allows for the customization of diverse ML pipelines through the definition of merely three functions, avoiding redundant and formulaic code like multi-host related processing. This mechanism proves adaptable across a spectrum of ML algorithms, from foundational language modeling to complex algorithms like meta-learning and reinforcement learning. Consequently, Redco implementations exhibit much fewer code lines compared to their official counterparts.",
        "retrieved_doc": "Author: Eric P. Xing Title: RedCoast: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU/TPUs Publication year: 2023 Coauthors: Bowen Tan, Yun Zhu, Lijuan Liu, Hongyi Wang, Yonghao Zhuang, Jindong Chen, Eric P. Xing, Zhiting Hu Abstract: The recent progress of AI can be largely attributed to large language models (LLMs). However, their escalating memory requirements introduce challenges for machine learning (ML) researchers and engineers. Addressing this requires developers to partition a large model to distribute it across multiple GPUs or TPUs. This necessitates considerable coding and intricate configuration efforts with existing model parallel tools, such as Megatron-LM, DeepSpeed, and Alpa. These tools require users' expertise in machine learning systems (MLSys), creating a bottleneck in LLM development, particularly for developers without MLSys background. In this work, we present RedCoast(Redco), a lightweight and user-friendly tool crafted to automate distributed training and inference for LLMs, as well as to simplify ML pipeline development. The design of Redco emphasizes two key aspects. Firstly, to automate model parallism, our study identifies two straightforward rules to generate tensor parallel strategies for any given LLM. Integrating these rules into Redco facilitates effortless distributed LLM training and inference, eliminating the need of additional coding or complex configurations. We demonstrate the effectiveness by applying Redco on a set of LLM architectures, such as GPT-J, LLaMA, T5, and OPT, up to the size of 66B. Secondly, we propose a mechanism that allows for the customization of diverse ML pipelines through the definition of merely three functions, avoiding redundant and formulaic code like multi-host related processing. This mechanism proves adaptable across a spectrum of ML algorithms, from foundational language modeling to complex algorithms like meta-learning and reinforcement learning. Consequently, Redco implementations exhibit much fewer code lines compared to their official counterparts.|Author: Eric P. Xing Title: RedCoast: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU/TPUs Publication year: 2023 Coauthors: Bowen Tan, Yun Zhu, Lijuan Liu, Hongyi Wang, Yonghao Zhuang, Jindong Chen, Eric P. Xing, Zhiting Hu Abstract: The recent progress of AI can be largely attributed to large language models (LLMs). However, their escalating memory requirements introduce challenges for machine learning (ML) researchers and engineers. Addressing this requires developers to partition a large model to distribute it across multiple GPUs or TPUs. This necessitates considerable coding and intricate configuration efforts with existing model parallel tools, such as Megatron-LM, DeepSpeed, and Alpa. These tools require users' expertise in machine learning systems (MLSys), creating a bottleneck in LLM development, particularly for developers without MLSys background. In this work, we present RedCoast(Redco), a lightweight and user-friendly tool crafted to automate distributed training and inference for LLMs, as well as to simplify ML pipeline development. The design of Redco emphasizes two key aspects. Firstly, to automate model parallism, our study identifies two straightforward rules to generate tensor parallel strategies for any given LLM. Integrating these rules into Redco facilitates effortless distributed LLM training and inference, eliminating the need of additional coding or complex configurations. We demonstrate the effectiveness by applying Redco on a set of LLM architectures, such as GPT-J, LLaMA, T5, and OPT, up to the size of 66B. Secondly, we propose a mechanism that allows for the customization of diverse ML pipelines through the definition of merely three functions, avoiding redundant and formulaic code like multi-host related processing. This mechanism proves adaptable across a spectrum of ML algorithms, from foundational language modeling to complex algorithms like meta-learning and reinforcement learning. Consequently, Redco implementations exhibit much fewer code lines compared to their official counterparts.|Author: Eric P. Xing Title: Redco: A Lightweight Tool to Automate Distributed Training of LLMs on Any GPU/TPUs Publication year: 2023 Coauthors: Bowen Tan, Yun Zhu, Lijuan Liu, Hongyi Wang, Yonghao Zhuang, Jindong Chen, Eric P. Xing, Zhiting Hu Abstract: .",
        "retrieved_relevant_part": true,
        "model_answer": "Eric P. Xing",
        "actual_answer": "Eric P. Xing",
        "exact_match": true,
        "f1_max_min_avg": [
            1.0,
            1.0,
            1.0
        ],
        "recall_max_min_avg": [
            1.0,
            1.0,
            1.0
        ]
    },
    {
        "question": "What is the abstract of the paper Prompt2Model: Generating Deployable Models from Natural Language Instructions published by LTI faculty Graham Neubig in 2023?",
        "context": "Author: Graham Neubig Title: Prompt2Model: Generating Deployable Models from Natural Language Instructions Publication year: 2023 Coauthors: Vijay Viswanathan, Chenyang Zhao, Amanda Bertsch, Tongshuang Sherry Wu, Graham Neubig Abstract: Large language models (LLMs) enable system builders today to create competent NLP systems through prompting, where they only need to describe the task in natural language and provide a few examples. However, in other ways, LLMs are a step backward from traditional special-purpose NLP models; they require extensive computational resources for deployment and can be gated behind APIs. In this paper, we propose Prompt2Model, a general-purpose method that takes a natural language task description like the prompts provided to LLMs, and uses it to train a special-purpose model that is conducive to deployment. This is done through a multi-step process of retrieval of existing datasets and pretrained models, dataset generation using LLMs, and supervised fine-tuning on these retrieved and generated datasets. Over three tasks, we demonstrate that given the same few-shot prompt as input, Prompt2Model trains models that outperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20% while being up to 700 times smaller. We also show that this data can be used to obtain reliable performance estimates of model performance, enabling model developers to assess model reliability before deployment. Prompt2Model is available open-source at https://github.com/neulab/prompt2model.",
        "retrieved_doc": "Author: Graham Neubig Title: Prompt2Model: Generating Deployable Models from Natural Language Instructions Publication year: 2023 Coauthors: Vijay Viswanathan, Chenyang Zhao, Amanda Bertsch, Tongshuang Sherry Wu, Graham Neubig Abstract: Large language models (LLMs) enable system builders today to create competent NLP systems through prompting, where they only need to describe the task in natural language and provide a few examples. However, in other ways, LLMs are a step backward from traditional special-purpose NLP models; they require extensive computational resources for deployment and can be gated behind APIs. In this paper, we propose Prompt2Model, a general-purpose method that takes a natural language task description like the prompts provided to LLMs, and uses it to train a special-purpose model that is conducive to deployment. This is done through a multi-step process of retrieval of existing datasets and pretrained models, dataset generation using LLMs, and supervised fine-tuning on these retrieved and generated datasets. Over three tasks, we demonstrate that given the same few-shot prompt as input, Prompt2Model trains models that outperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20% while being up to 700 times smaller. We also show that this data can be used to obtain reliable performance estimates of model performance, enabling model developers to assess model reliability before deployment. Prompt2Model is available open-source at https://github.com/neulab/prompt2model.|Author: Graham Neubig Title: Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation Publication year: 2023 Coauthors: Patrick Fernandes, Aman Madaan, Emmy Liu, Ant\u00f3nio Farinhas, Pedro Henrique Martins, Amanda Bertsch, Jos\u00e9 G. C. de Souza, Shuyan Zhou, Tongshuang Sherry Wu, Graham Neubig, Andr\u00e9 F. T. Martins Abstract: Many recent advances in natural language generation have been fueled by training large language models on internet-scale data. However, this paradigm can lead to models that generate toxic, inaccurate, and unhelpful content, and automatic evaluation metrics often fail to identify these behaviors. As models become more capable, human feedback is an invaluable signal for evaluating and improving models. This survey aims to provide an overview of the recent research that has leveraged human feedback to improve natural language generation. First, we introduce an encompassing formalization of feedback, and identify and organize existing research into a taxonomy following this formalization. Next, we discuss how feedback can be described by its format and objective, and cover the two approaches proposed to use feedback (either for training or decoding): directly using the feedback or training feedback models. We also discuss existing datasets for human-feedback data collection, and concerns surrounding feedback collection. Finally, we provide an overview of the nascent field of AI feedback, which exploits large language models to make judgments based on a set of principles and minimize the need for human intervention.|Author: Graham Neubig Title: Program-Aided Reasoners (better) Know What They Know Publication year: 2023 Coauthors: Anubha Kabra, Sanketh Rangreji, Yash Mathur, Aman Madaan, Emmy Liu, Graham Neubig Abstract: Prior work shows that program-aided reasoning, in which large language models (LLMs) are combined with programs written in programming languages such as Python, can significantly improve accuracy on various reasoning tasks. However, while accuracy is essential, it is also important for such reasoners to\"know what they know\", which can be quantified through the calibration of the model. In this paper, we compare the calibration of Program Aided Language Models (PAL) and text-based Chain-of-thought (COT) prompting techniques over 5 datasets and 2 model types: LLaMA models and OpenAI models. Our results indicate that PAL leads to improved calibration in 75% of the instances. Our analysis uncovers that prompting styles that produce lesser diversity in generations also have more calibrated results, and thus we also experiment with inducing lower generation diversity using temperature scaling and find that for certain temperatures, PAL is not only more accurate but is also more calibrated than COT. Overall, we demonstrate that, in the majority of cases, program-aided reasoners better know what they know than text-based counterparts.",
        "retrieved_relevant_part": true,
        "model_answer": "The paper proposes a method called Prompt2Model that takes a natural language task description and uses it to train a special-purpose model for deployment",
        "actual_answer": "Large language models (LLMs) enable system builders today to create competent NLP systems through prompting, where they only need to describe the task in natural language and provide a few examples. However, in other ways, LLMs are a step backward from traditional special-purpose NLP models",
        "exact_match": false,
        "f1_max_min_avg": [
            0.21,
            0.2,
            0.20500000000000002
        ],
        "recall_max_min_avg": [
            0.16,
            0.12,
            0.14
        ]
    },
    {
        "f1": 0.4361936936936937,
        "recall": 0.47040540540540543,
        "retrieval_acc": 0.9054054054054054
    }
]